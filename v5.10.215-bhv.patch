diff --git arch/Kconfig arch/Kconfig
index 240277d56..7bb6f9462 100644
--- arch/Kconfig
+++ arch/Kconfig
@@ -69,6 +69,7 @@ config KPROBES
 	bool "Kprobes"
 	depends on MODULES
 	depends on HAVE_KPROBES
+	depends on !BHV_LOCKDOWN
 	select KALLSYMS
 	help
 	  Kprobes allows you to trap at almost any kernel address and
diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index d6465823b..f1ea15a79 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,4 +3,5 @@ obj-y			+= kernel/ mm/
 obj-$(CONFIG_NET)	+= net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 000000000..c9474f695
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/arm64/bhv/init/init.c arch/arm64/bhv/init/init.c
new file mode 100644
index 000000000..8756dfac5
--- /dev/null
+++ arch/arm64/bhv/init/init.c
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <bhv/integrity.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
+
+void __init bhv_init_arch(void)
+{
+}
diff --git arch/arm64/bhv/init/start.c arch/arm64/bhv/init/start.c
new file mode 100644
index 000000000..487cd26c8
--- /dev/null
+++ arch/arm64/bhv/init/start.c
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/integrity.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 000000000..89e9bde15
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,115 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+extern char vdso_start[], vdso_end[];
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif
+
+/************************************************************
+ * start
+ ************************************************************/
+int __init_km bhv_start_integrity_arch(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 3
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON(KLN_SYM(vdso_start) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_start) >= KLN_SYM(__end_rodata));
+	BUG_ON(KLN_SYM(vdso_end) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_end) >= KLN_SYM(__end_rodata));
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 */
+	bhv_mem_region_create_ctor(
+		&n[0]->region, NULL,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, __start_rodata)),
+		KLN_SYM(vdso_start) - KLN_SYM(__start_rodata),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[1]->region, &n[0]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_end)),
+		KLN_SYM(__end_rodata) - KLN_SYM(vdso_end),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[2]->region, &n[1]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_start)),
+		KLN_SYM(vdso_end) - KLN_SYM(vdso_start),
+		HypABI__Integrity__MemType__VDSO,
+		HypABI__Integrity__MemFlags__NONE, "KERNEL VDSO SECTION");
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+}
+/************************************************************/
+
+/************************************************************
+ * start
+ ************************************************************/
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	*pgd_offset = 0;
+	*pgd_value = 0;
+}
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	init_ptpg_arg->init_pgd = 0;
+	init_ptpg_arg->pt_levels = 0;
+	init_ptpg_arg->num_ranges = 0;
+}
+/************************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	return true;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 000000000..9a6662489
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,455 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+#if defined(BHV_KVERS_6_1)
+#include <asm/cacheflush.h>
+#endif
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_patch_hypercall((void *)&updptr[i], (uint8_t *)&insn,
+				    sizeof(insn), false);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+#if defined(BHV_KVERS_6_1)
+#define ALT_CAP(a) ((a)->cpufeature & ~ARM64_CB_BIT)
+#define ALT_HAS_CB(a) ((a)->cpufeature & ARM64_CB_BIT)
+
+extern DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(
+			 ctr_el0, CTR_EL0_DminLine_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+		int cap = ALT_CAP(alt);
+
+		if (!test_bit(cap, feature_mask))
+			continue;
+
+		if (!cpus_have_cap(cap))
+			continue;
+
+		if (ALT_HAS_CB(alt))
+			BUG_ON(alt->alt_len != 0);
+		else
+			BUG_ON(alt->alt_len != alt->orig_len);
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (ALT_HAS_CB(alt)) {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		} else {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	/*
+	 * The core module code takes care of cache maintenance in
+	 * flush_module_icache().
+	 */
+	if (!mod->arch.is_module) {
+		dsb(ish);
+		icache_inval_all_pou();
+		isb();
+
+		/* Ignore ARM64_CB bit from feature mask */
+		bitmap_or(applied_alternatives, applied_alternatives,
+			  feature_mask, ARM64_NCAPS);
+		bitmap_and(applied_alternatives, applied_alternatives,
+			   cpu_hwcaps, ARM64_NCAPS);
+	}
+
+	return 0;
+}
+#else /* BHV_KVERS_6_1 */
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(ctr_el0,
+							   CTR_DMINLINE_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+
+		if (!test_bit(alt->cpufeature, feature_mask))
+			continue;
+
+		/* Use ARM64_CB_PATCH as an unconditional patch */
+		if (alt->cpufeature < ARM64_CB_PATCH &&
+		    !cpus_have_cap(alt->cpufeature))
+			continue;
+
+		if (alt->cpufeature == ARM64_CB_PATCH) {
+			if (alt->alt_len != 0) {
+				return -EACCES;
+			}
+		} else {
+			if (alt->alt_len != alt->orig_len) {
+				return -EACCES;
+			}
+		}
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (alt->cpufeature < ARM64_CB_PATCH) {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		} else {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	return 0;
+}
+#endif /* BHV_KVERS_6_1 */
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 000000000..610215168
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+u32 __always_inline bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/reg_protect.c arch/arm64/bhv/reg_protect.c
new file mode 100644
index 000000000..c9ae1094c
--- /dev/null
+++ arch/arm64/bhv/reg_protect.c
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+}
+/***************************************************/
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index 3cb3c4ab3..a0a1983d8 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -14,6 +14,8 @@
 #include <linux/stddef.h>
 #include <linux/stringify.h>
 
+#include <bhv/interface/abi_base_autogen.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
diff --git arch/arm64/include/asm/bhv/domain.h arch/arm64/include/asm/bhv/domain.h
new file mode 100644
index 000000000..16c7b588e
--- /dev/null
+++ arch/arm64/include/asm/bhv/domain.h
@@ -0,0 +1,76 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) pgd
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_RDONLY);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pte_read(pmd_pte(pmd));
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pte_read(pud_pte(pud));
+}
+
+static inline bool pte_exec(pte_t pte)
+{
+	return !!(pte_val(pte) & (PTE_PXN | PTE_UXN));
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return pte_exec(pmd_pte(pmd));
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return pte_exec(pud_pte(pud));
+}
+
+static inline bool pmd_large(pmd_t pmd)
+{
+	return pmd_thp_or_huge(pmd);
+}
+
+static inline bool pud_large(pud_t pud)
+{
+	return pud_huge(pud);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return bhv_domain_is_user_pte(pmd_pte(pmd));
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return bhv_domain_is_user_pte(pud_pte(pud));
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..dfd047fd7
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 000000000..dfe86c075
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+#ifndef VASKM // inside kernel tree
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+#endif // VASKM
+
+#else /* !CONFIG_BHV_VAS */
+#ifndef VASKM // inside kernel tree
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+#endif // VASKM
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 47dafd6ab..d7ff5ca2e 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -80,6 +80,10 @@ alternative_cb_end
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
 void kvm_compute_layout(void);
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 5f8e4c2df..3ccc5e604 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -17,6 +17,9 @@
 #include <asm/sections.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
 #define __ALT_PTR(a,f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -94,8 +97,8 @@ static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
 	return insn;
 }
 
-static noinstr void patch_alternative(struct alt_instr *alt,
-			      __le32 *origptr, __le32 *updptr, int nr_inst)
+static noinstr void patch_alternative(struct alt_instr *alt, __le32 *origptr,
+				      __le32 *updptr, int nr_inst)
 {
 	__le32 *replptr;
 	int i;
@@ -132,14 +135,22 @@ static void clean_dcache_range_nopatch(u64 start, u64 end)
 	} while (cur += d_size, cur < end);
 }
 
-static void __apply_alternatives(void *alt_region,  bool is_module,
+static void __apply_alternatives(void *alt_region, bool is_module,
 				 unsigned long *feature_mask)
 {
-	struct alt_instr *alt;
 	struct alt_region *region = alt_region;
+	struct alt_instr *alt;
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 
@@ -254,10 +265,15 @@ void apply_alternatives_module(void *start, size_t length)
 		.begin	= start,
 		.end	= start + length,
 	};
+
 	DECLARE_BITMAP(all_capabilities, ARM64_NPATCHABLE);
 
 	bitmap_fill(all_capabilities, ARM64_NPATCHABLE);
 
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
 #endif
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index 9a8a0ae1e..c95fe7342 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -9,10 +9,14 @@
 #include <linux/jump_label.h>
 #include <asm/insn.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -23,6 +27,13 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 }
 
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index 9c0e9d9ee..89eaa38ed 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -31,6 +31,13 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -593,8 +600,20 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_final_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
@@ -602,8 +621,8 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
  * to call into firmware to adjust the mitigation state.
  */
 void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst)
 {
 	u32 insn;
 
@@ -620,7 +639,17 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1114,7 +1143,17 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, loop_count, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index eb4b24652..6d5815027 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -51,6 +51,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/init/init.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -323,6 +325,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT) && ((u64)_text % MIN_KIMG_ALIGN) != 0)
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index 71f4b5f24..8e94b7ed8 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -121,7 +121,7 @@ SECTIONS
 		_text = .;
 		HEAD_TEXT
 	}
-	.text : {			/* Real text segment		*/
+	.text : ALIGN(SEGMENT_ALIGN) {	/* Real text segment		*/
 		_stext = .;		/* Text and read-only data	*/
 			IRQENTRY_TEXT
 			SOFTIRQENTRY_TEXT
@@ -135,6 +135,7 @@ SECTIONS
 			IDMAP_TEXT
 			HIBERNATE_TEXT
 			TRAMP_TEXT
+			BHV_TEXT
 			*(.fixup)
 			*(.gnu.warning)
 		. = ALIGN(16);
@@ -176,13 +177,27 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
 		__alt_instructions = .;
 		*(.altinstructions)
@@ -250,6 +265,17 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index e0404bcab..88494d756 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -12,6 +12,11 @@
 #include <asm/insn.h>
 #include <asm/kvm_mmu.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -97,8 +102,21 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn, bhv_patch_arg_t *bhv_arg)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+	bhv_patch_hypercall((void *)dest, &le32_insn, sizeof(le32_insn), false,
+			    bhv_arg);
+}
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst,
+			       bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	int i;
 
@@ -116,7 +134,17 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (has_vhe() || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop(),
+						  bhv_arg);
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -127,15 +155,29 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn, bhv_arg);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
 void *__kvm_bp_vect_base;
 int __kvm_harden_el2_vector_slot;
 
+#ifdef CONFIG_BHV_VAS
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst,
+			     bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
 void kvm_patch_vector_branch(struct alt_instr *alt,
 			     __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u64 addr;
 	u32 insn;
@@ -170,7 +212,15 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 						-16,
 						AARCH64_INSN_VARIANT_64BIT,
 						AARCH64_INSN_LDST_STORE_PAIR_PRE_INDEX);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movz x0, #(addr & 0xffff) */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -178,15 +228,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -194,10 +258,26 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
diff --git arch/arm64/mm/fault.c arch/arm64/mm/fault.c
index d8baedd16..9c7a03ffc 100644
--- arch/arm64/mm/fault.c
+++ arch/arm64/mm/fault.c
@@ -39,6 +39,10 @@
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr,
 		      struct pt_regs *regs);
@@ -313,12 +317,28 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 		return;
 
 	if (is_el1_permission_fault(addr, esr, regs)) {
+#ifdef CONFIG_BHV_VAS
+		uint8_t type;
+		if (esr & ESR_ELx_WNR) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+			msg = "write to read-only memory";
+		} else if (is_el1_instruction_abort(esr)) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+			msg = "execute from non-executable memory";
+		} else {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+			msg = "read from unreadable memory";
+		}
+		if (bhv_guestlog_log_kaccess_events())
+			bhv_guestlog_log_kaccess((uint64_t)addr, type);
+#else /* CONFIG_BHV_VAS */
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
 		else if (is_el1_instruction_abort(esr))
 			msg = "execute from non-executable memory";
 		else
 			msg = "read from unreadable memory";
+#endif /* CONFIG_BHV_VAS */
 	} else if (addr < PAGE_SIZE) {
 		msg = "NULL pointer dereference";
 	} else {
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 80cc79760..8282752bb 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -43,6 +43,8 @@
 #include <asm/tlb.h>
 #include <asm/alternative.h>
 
+#include <bhv/init/start.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -592,6 +594,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+	bhv_start();
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
 			   POISON_FREE_INITMEM, "unused kernel");
diff --git arch/x86/Kbuild arch/x86/Kbuild
index 30dec0197..3235bbc86 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -13,6 +13,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 000000000..3e2cc4d4d
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,16 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/x86/bhv/init/init.c arch/x86/bhv/init/init.c
new file mode 100644
index 000000000..cf092248d
--- /dev/null
+++ arch/x86/bhv/init/init.c
@@ -0,0 +1,147 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/types.h> // This is here so that hypervisor.h knows bool
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+
+#include <vdso/datapage.h>
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+#include <asm/vvar.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_64.data),
+				   vdso_image_64.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 64");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_x32.data),
+				   vdso_image_x32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE X32");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_32.data),
+				   vdso_image_32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 32");
+	(*region_counter)++;
+
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static __always_inline void
+bhv_init_add_vvar(bhv_mem_region_t *init_phys_mem_regions,
+		  unsigned int *region_counter)
+{
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   __pa_symbol(&__vvar_page), PAGE_SIZE,
+				   HypABI__Integrity__MemType__VVAR, HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL VVAR PAGE");
+	(*region_counter)++;
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vvar(init_phys_mem_regions, region_counter);
+}
+
+static uint32_t __init bhv_detect(void)
+{
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0; /* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return hypervisor_cpuid_base("BHV.VMM.VAS.", 0);
+
+	return 0;
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform
+};
+
+#else // out of tree
+
+#include <common.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	// We handle VDSOs in integrity.c, so no need to do anything here.
+}
+
+#endif // VASKM
+
+void __init bhv_init_arch(void)
+{
+#ifndef VASKM // inside kernel tree
+	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+#endif // VASKM
+}
diff --git arch/x86/bhv/init/start.c arch/x86/bhv/init/start.c
new file mode 100644
index 000000000..25cce874a
--- /dev/null
+++ arch/x86/bhv/init/start.c
@@ -0,0 +1,12 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <bhv/integrity.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 000000000..c6a119510
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,587 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/pgtable.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_EFI
+#include <linux/efi.h>
+#endif /* CONFIG_EFI */
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <linux/pagewalk.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif //VASKM
+
+struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} typedef table_data_t;
+
+static table_data_t table_data __ro_after_init = { 0 };
+
+struct {
+	uint64_t start_addr;
+	uint64_t size;
+	const char *label;
+	uint32_t mem_type;
+} typedef ro_region_t;
+
+/**********************************************************
+ * start
+ **********************************************************/
+static int __init_km bhv_start_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = bhv_virt_to_phys((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(
+		head, addr, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE, "IDT");
+}
+
+static int __init_km bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_start_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region.create);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#ifndef VASKM // inside kernel tree
+
+static inline int __init_km bhv_start_rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int __init_km bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = bhv_start_rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#endif // VASKM
+
+static void __init_km bhv_start_integrity_add_vdso_common(
+	uint64_t start_addr, uint64_t size, const char *label,
+	ro_region_t *range, unsigned int *cur_entry, size_t range_sz)
+{
+	unsigned int i;
+	uint64_t end = start_addr + size;
+	uint64_t cur_end = 0;
+
+	BUG_ON(size == 0);
+	BUG_ON((*cur_entry) >= range_sz);
+	BUG_ON(start_addr < range[0].start_addr);
+
+	// Check for overlaps
+	for (i = 0; i < (*cur_entry); i++) {
+		cur_end = range[i].start_addr + range[i].size;
+
+		// No overlap. Nothing to do.
+		if (end <= range[i].start_addr || start_addr >= cur_end)
+			continue;
+
+		// No range that we add should be exactly the same as an
+		// existing one.
+		if (start_addr == range[i].start_addr && end == cur_end) {
+			BUG(); // Range already exists
+		}
+
+		// Overlap. Split range.
+		// Case 1 (overlap left): New range starts before current range
+		//                        with/before the current range.
+		//                        Update the new range to end when cur starts.
+		//                        Thus creating range A and B.
+		//  -----------------------
+		// |      A      ##########B##########
+		// |     new     #         |   cur   #
+		// |             ##########|##########
+		// ------------------------
+		if (start_addr < range[i].start_addr && end <= cur_end) {
+			size = range[i].start_addr - start_addr;
+			continue;
+		}
+
+		// Case 2 (overlap right): New range starts within current range
+		//                         and ends with/after the current range.
+		//                         Update cur to end when new starts.
+		//                         Thus creating range A and B.
+		//                         -----------------------
+		//              #####A####|##########  B          |
+		//              #   cur   |         #     new     |
+		//              ##########|##########             |
+		//                        ------------------------
+		if (range[i].start_addr < start_addr && cur_end <= end) {
+			range[i].size = start_addr - range[i].start_addr;
+			continue;
+		}
+
+		// Case 3: New range fully encompasses current range.
+		//         Create three ranges A, B, C.
+		//  --------------------------------------------
+		// |     A     ##########B##########     C      |
+		// |    new    #        cur        #    new     |
+		// |           #####################            |
+		// ---------------------------------------------
+		if (start_addr <= range[i].start_addr && cur_end <= end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				// No. Add new Range A.
+				range[(*cur_entry)].start_addr = start_addr;
+				range[(*cur_entry)].size =
+					range[i].start_addr - start_addr;
+				range[(*cur_entry)].label = label;
+				range[(*cur_entry)].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Update new to be C. It will be added after the loop.
+				// B is already in our range list.
+				start_addr = cur_end;
+				size = end - cur_end;
+			} else {
+				// Remaining ranges are the same. Just update B with new label/type.
+				// Since a range can only have one label, it will take the
+				// label of the new range. This generally seems to make sense
+				// since we add the entire read-only section and then split it
+				// with smaller sections.
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+			continue;
+		}
+
+		// Case 4: Current range fully encompasses new range.
+		//         Create three ranges A, B, C.
+		// ##############################################
+		// #     A      ---------B---------      C      #
+		// #    cur    |        new        |    cur     #
+		// #            -------------------             #
+		// ##############################################
+		if (range[i].start_addr <= start_addr && end <= cur_end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				range[(*cur_entry)].start_addr =
+					range[i].start_addr;
+				range[(*cur_entry)].size =
+					start_addr - range[i].start_addr;
+				range[(*cur_entry)].label = range[i].label;
+				range[(*cur_entry)].mem_type =
+					range[i].mem_type;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Create C. B will be added after the loop.
+				range[i].start_addr = end;
+				range[i].size = cur_end - end;
+			} else {
+				// Remaining ranges are the same.
+				// Shrink the current range to B. We also update its label/type.
+				// This generally seems to make sense since we add the
+				// entire read-only section and then split it with smaller
+				// sections.
+				range[i].start_addr = start_addr;
+				range[i].size = size;
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+
+			continue;
+		}
+
+		BUG(); // "Unexpected case"
+	}
+
+	if (size != 0) {
+		range[(*cur_entry)].start_addr = start_addr;
+		range[(*cur_entry)].size = size;
+		range[(*cur_entry)].label = label;
+		range[(*cur_entry)].mem_type = HypABI__Integrity__MemType__VDSO;
+		(*cur_entry)++;
+	}
+}
+
+#define VDSO_KLN_SYM(sym) KLN_SYMBOL_P(const struct vdso_image *, sym)
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_64_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_64)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_64)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 64",
+					    range, cur_entry, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_x32_to_range(ro_region_t *range,
+						unsigned int *cur_entry,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_x32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_x32)->size;
+	bhv_start_integrity_add_vdso_common(start, size,
+					    "KERNEL VDSO IMAGE X32", range,
+					    cur_entry, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_32_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_32)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 32",
+					    range, cur_entry, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void __init_km bhv_start_integrity_get_ro_ranges(ro_region_t *range,
+							unsigned int *cur_entry,
+							size_t range_sz)
+{
+	BUG_ON(range_sz < 1);
+
+	range[(*cur_entry)].start_addr =
+		bhv_virt_to_phys((void *)(KLN_SYM(__start_rodata) & PAGE_MASK));
+	range[(*cur_entry)].size = PAGE_ALIGN(KLN_SYM(__end_rodata)) -
+				   (KLN_SYM(__start_rodata) & PAGE_MASK);
+	range[(*cur_entry)].label = "KERNEL READ-ONLY DATA SECTION";
+	range[(*cur_entry)].mem_type =
+		HypABI__Integrity__MemType__DATA_READ_ONLY;
+	(*cur_entry)++;
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, cur_entry,
+						       range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, cur_entry,
+							range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, cur_entry,
+						       range_sz);
+}
+
+static int __init_km bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+	unsigned int i;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	unsigned int nr_ro_ranges = 0;
+	ro_region_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_MAX_RO_RANGES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, &nr_ro_ranges,
+					  BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   nr_ro_ranges, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < nr_ro_ranges; i++) {
+		bhv_mem_region_create_ctor(
+			&n[i]->region, (prev ? &prev->region : NULL),
+			ro_ranges[i].start_addr, ro_ranges[i].size,
+			ro_ranges[i].mem_type,
+			HypABI__Integrity__MemFlags__NONE, ro_ranges[i].label);
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, nr_ro_ranges, (void **)&n);
+
+	return rc;
+}
+
+#ifdef CONFIG_EFI
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		bhv_mem_region_create_ctor(&n->region, NULL, md->phys_addr,
+					   (md->num_pages) << PAGE_SHIFT,
+					   HypABI__Integrity__MemType__CODE,
+					   HypABI__Integrity__MemFlags__NONE,
+					   "EFI REGION");
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1,
+							 &(n->region.create));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+#else /* CONFIG_EFI */
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	return 0;
+}
+#endif /* CONFIG_EFI */
+
+int __init_km bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+#ifndef VASKM // inside kernel tree
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+#endif // VASKM
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+	return bhv_start_integrity_add_ro();
+}
+/**********************************************************/
+
+#define BHV_DIR_PTR_ENTRY_MASK 0x800FFFFFFFFFF89D
+
+/**********************************************************
+ * start
+ **********************************************************/
+struct bhv_pw_data {
+	bool set;
+	uint64_t pgd_offset;
+	uint64_t pgd_value;
+};
+
+static int __init_km bhv_start_pgd_entry(pgd_t *pgd, unsigned long addr,
+					 unsigned long next,
+					 struct mm_walk *walk)
+{
+	struct bhv_pw_data *data = (struct bhv_pw_data *)walk->private;
+	pr_info("%s: pgd=%llx (%llx) addr=%lx next=%lx\n", __FUNCTION__,
+		(uint64_t)pgd, bhv_virt_to_phys(pgd), addr, next);
+	BUG_ON(data->set);
+	data->pgd_offset = ((uint64_t)pgd & 0xFFF);
+	data->pgd_value = *((uint64_t *)pgd);
+	data->set = true;
+	return 1;
+}
+
+static const struct mm_walk_ops bhv_start_walk_ops = {
+	.pgd_entry = bhv_start_pgd_entry,
+	.p4d_entry = NULL,
+	.pud_entry = NULL,
+	.pmd_entry = NULL,
+	.pte_entry = NULL,
+	.pte_hole = NULL,
+	.hugetlb_entry = NULL,
+	.test_walk = NULL,
+	.pre_vma = NULL,
+	.post_vma = NULL
+};
+
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	struct bhv_pw_data text_pw_data = { 0 };
+	struct bhv_pw_data ro_pw_data = { 0 };
+
+	mmap_write_lock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(_stext), KLN_SYM(_etext),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &text_pw_data);
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(__start_rodata), KLN_SYM(__end_rodata),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &ro_pw_data);
+	mmap_write_unlock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+
+	BUG_ON(text_pw_data.pgd_offset != ro_pw_data.pgd_offset ||
+	       text_pw_data.pgd_value != ro_pw_data.pgd_value);
+
+	*pgd_offset = text_pw_data.pgd_offset;
+	*pgd_value = (text_pw_data.pgd_value & BHV_DIR_PTR_ENTRY_MASK);
+}
+/**********************************************************/
+
+/**********************************************************
+ * late_start
+ **********************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	BUG_ON(CONFIG_PGTABLE_LEVELS < 4 || CONFIG_PGTABLE_LEVELS > 5);
+
+	init_ptpg_arg->init_pgd = (uint64_t)bhv_virt_to_phys(
+		KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd);
+	init_ptpg_arg->pt_levels = pgtable_l5_enabled() ? 5 : 4;
+	init_ptpg_arg->num_ranges = 2;
+
+	init_ptpg_arg->ranges[0] = (uint64_t)KLN_SYM(_stext);
+	init_ptpg_arg->ranges[1] = (uint64_t)KLN_SYM(_etext);
+	init_ptpg_arg->ranges[2] = (uint64_t)KLN_SYM(__start_rodata);
+	init_ptpg_arg->ranges[3] = (uint64_t)KLN_SYM(__end_rodata);
+}
+/**********************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	uint64_t value = *((uint64_t *)(((uint8_t *)mm->pgd) + pgd_offset));
+	return ((value & BHV_DIR_PTR_ENTRY_MASK) == pgd_value);
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 000000000..ce2125de0
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,841 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/kversion.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+#define NOPS x86_nops
+#elif defined BHV_KVERS_5_10
+#define NOPS ideal_nops
+#endif // BHV_KVERS
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end,
+				      const s32 *locks_begin,
+				      const s32 *locks_end, u8 *text_begin,
+				      u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len, bool patch)
+{
+	size_t total_length = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) > sizeof(buf))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(buf + total_length, NOPS[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, NOPS[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_patch_hypercall(insns, buf, total_length, false);
+	}
+}
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off, bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(u8 *target,
+								   bool lock)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target,
+				    "Invalid altinst smp unlock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch.
+		}
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target, "Invalid altinst smp lock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch
+		}
+
+		opcode = unlock_opcode;
+	}
+
+	r = bhv_patch_hypercall((void *)target, &opcode, 1, false);
+
+	if (r) {
+		panic("BHV patch hypercall failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+		    ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+		bytes[i++] = BYTES_NOP1;
+#elif defined BHV_KVERS_5_10
+		bytes[i++] = GENERIC_NOP1;
+#endif // BHV_KVERS
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = HypABI__Vault__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (invalid op)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+
+	// Retpolines may be disabled or there is another error
+	// this is not an attack.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr,
+			    "Invalid altinst retpoline (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_optimize_nops(bytes, len, true);
+	bhv_patch_hypercall((void *)addr, bytes, len, false);
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+#ifdef CONFIG_RETHUNK
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		return;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+	    WARN_ONCE(dest != &__x86_return_thunk,
+		      "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+		      addr))
+		return;
+
+	ret = HypABI__Vault__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	// Feature may be disabled.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_patch_hypercall(addr, bytes, len, false);
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE)*/
+
+#ifdef CONFIG_PARAVIRT
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used;
+
+	ret = HypABI__Vault__Open__hypercall();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+
+#if defined(BHV_KVERS_5_10)
+	used = pv_ops.init.patch(p->type, insn_buff, (unsigned long)p->instr,
+				 p->len);
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+#endif // BHV_KVERS
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, false);
+
+	bhv_patch_hypercall((void *)(p->instr), insn_buff, p->len, false);
+
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_PARAVIRT */
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		if (bhv_patch_violation_hypercall(
+			    instr, "Invalid altinst patch (too big)")) {
+			// Block attempt.
+			return -EACCES;
+		}
+
+		// Allow patch.
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		// This can happen legitmately. We just return.
+		return -EINVAL;
+	}
+
+	if (!boot_cpu_has(a->cpuid & ~ALTINSTR_FLAG_INV) ==
+	    !(a->cpuid & ALTINSTR_FLAG_INV)) {
+		bhv_optimize_nops(instr, a->instrlen, true);
+		return 0;
+	}
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+#if defined(BHV_KVERS_5_10)
+	if (a->instrlen > a->replacementlen) {
+		bhv_add_nops(insn_buff + a->replacementlen,
+			     a->instrlen - a->replacementlen, false);
+		insn_buff_sz += a->instrlen - a->replacementlen;
+	}
+
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+#endif // BHV_KVERS
+
+	if (insn_buff_sz >= HypABI__Patch__MAX_PATCH_SZ)
+		panic("Instruction buffer size too small!");
+
+	rv = bhv_patch_hypercall((void *)instr, insn_buff, insn_buff_sz, false);
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	bhv_optimize_nops(instr, a->instrlen, true);
+#endif // BHV_KVERS_5_15
+
+	return rv;
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a);
+		else
+			bhv_alternatives_patch_vault(a);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+	    cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __bhv_text bhv_apply_ibt_endbr_vault(s32 *s)
+{
+	int ret;
+	u32 endbr, poison;
+	void *addr;
+
+	ret = HypABI__Vault__Open__hypercall();
+	if (ret)
+		return;
+
+	poison = gen_endbr_poison();
+	addr = (void *)s + *s;
+
+	if (get_kernel_nofault(endbr, addr))
+		goto out;
+
+	if (!is_endbr(endbr))
+		goto out;
+
+	bhv_patch_hypercall(addr, (uint8_t *)&poison, 4, false);
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __init_or_module bhv_apply_ibt_endbr(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_ibt_endbr_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 000000000..18d202148
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif // VASKM
+
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len == 5) {
+		DEF_CHECK_NOP(STATIC_KEY_INIT_NOP);
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *,
+				     ideal_nops)[NOP_ATOMIC5]);
+	}
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len == 2) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[2]);
+	} else if (len == 5) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[5]);
+	}
+#endif // LINUX_VERSION_CODE
+
+	return false;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len != 5)
+		return false;
+
+	code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+
+
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+#endif // LINUX_VERSION_CODE
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 000000000..6e24b59f9
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,162 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <linux/version.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static void __always_inline bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static void __always_inline bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+extern void __static_call_return(void);
+
+static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
+{
+	u8 ret = 0;
+
+	if (insn[0] == 0x0f) {
+		u8 tmp = insn[1];
+		if ((tmp & 0xf0) == 0x80)
+			ret = tmp;
+	}
+
+	return ret;
+}
+#endif // LINUX_VERSION_CODE >= 6.1
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func)
+{
+	int ret;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	const void *emulate = NULL;
+#endif // LINUX_VERSION_CODE >= 5.12
+	int size = CALL_INSN_SIZE;
+	const void *code;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	u8 buf[6];
+#endif // LINUX_VERSION_CODE >= 6.1
+
+	ret = HypABI__Vault__Open__hypercall();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+#endif // LINUX_VERSION_CODE >= 5.12
+
+		break;
+
+	case NOP:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 13, 0)
+		code = KLN_SYMBOL(const uint8_t *const *, x86_nops)[5];
+#else // LINUX_VERSION_CODE <= 5.13
+		code = KLN_SYMBOL(const uint8_t *const *,
+				  ideal_nops)[NOP_ATOMIC5];
+#endif // LINUX_VERSION_CODE
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+#else // LINUX_VERSION_CODE <= 5.14
+		code = text_gen_insn(RET_INSN_OPCODE, insn, func);
+		size = RET_INSN_SIZE;
+#endif // LINUX_VERSION_CODE
+		break;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	case JCC:
+		if (!func) {
+			func = KLN_SYMBOL(void *, __static_call_return);
+			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+				func = __x86_return_thunk;
+		}
+
+		buf[0] = 0x0f;
+		__text_gen_insn(buf + 1, __is_Jcc(insn), insn + 1, func, 5);
+		code = buf;
+		size = 6;
+
+		break;
+#endif // LINUX_VERSION_CODE >= 6.1
+	}
+
+	if (memcmp(insn, code, size) == 0) {
+		if (bhv_patch_violation_hypercall(insn,
+						  "Invalid static call")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow patch
+	}
+
+	if (size > HypABI__Patch__MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_patch_hypercall((void *)insn, code, size, false);
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func)
+{
+	unsigned long flags;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
diff --git arch/x86/bhv/reg_protect.c arch/x86/bhv/reg_protect.c
new file mode 100644
index 000000000..9ae72fb02
--- /dev/null
+++ arch/x86/bhv/reg_protect.c
@@ -0,0 +1,44 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/reg_protect.h>
+
+#include <asm/processor-flags.h>
+
+#define BHV_CR0_FREEZE X86_CR0_WP
+#define BHV_CR4_FREEZE 0x0ULL
+#define BHV_EFER_FREEZE 0x0ULL
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+	int r;
+
+	if (BHV_CR0_FREEZE) {
+		r = bhv_reg_protect_freeze(BHV_REG_PROTECT_REG_CR0,
+					   BHV_CR0_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR0!\n");
+	}
+
+	if (BHV_CR4_FREEZE) {
+		r = bhv_reg_protect_freeze(BHV_REG_PROTECT_REG_CR4,
+					   BHV_CR4_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR4!\n");
+	}
+
+	if (BHV_EFER_FREEZE) {
+		r = bhv_reg_protect_freeze(BHV_REG_PROTECT_REG_EFER,
+					   BHV_EFER_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze EFER!\n");
+	}
+}
+/***************************************************/
diff --git arch/x86/entry/common.c arch/x86/entry/common.c
index 93a3122cd..e5b32e57d 100644
--- arch/x86/entry/common.c
+++ arch/x86/entry/common.c
@@ -35,11 +35,15 @@
 #include <asm/syscall.h>
 #include <asm/irq_stack.h>
 
+#include <bhv/integrity.h>
+
 #ifdef CONFIG_X86_64
 __visible noinstr void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 {
 	nr = syscall_enter_from_user_mode(regs, nr);
 
+	bhv_pt_protect_check_pgd(current->active_mm);
+
 	instrumentation_begin();
 	if (likely(nr < NR_syscalls)) {
 		nr = array_index_nospec(nr, NR_syscalls);
diff --git arch/x86/entry/entry_64.S arch/x86/entry/entry_64.S
index 1631a9a15..8bdad7560 100644
--- arch/x86/entry/entry_64.S
+++ arch/x86/entry/entry_64.S
@@ -240,6 +240,11 @@ SYM_FUNC_START(__switch_to_asm)
 	pushq	%r14
 	pushq	%r15
 
+#ifdef CONFIG_MEM_NS
+	movq	PER_CPU_VAR(bhv_domain_current_domain), %r12
+	pushq	%r12
+#endif
+
 	/* switch stack */
 	movq	%rsp, TASK_threadsp(%rdi)
 	movq	TASK_threadsp(%rsi), %rsp
@@ -258,6 +263,17 @@ SYM_FUNC_START(__switch_to_asm)
 	 */
 	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
 
+#ifdef CONFIG_MEM_NS
+	// Save arguments in r12 and r13
+	movq	%rdi, %r12
+	movq	%rsi, %r13
+	popq	%rdi
+	callq	bhv_domain_switch
+	// Restore original arguments
+	movq	%r12, %rdi
+	movq	%r13, %rsi
+#endif
+
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
diff --git arch/x86/include/asm/bhv/domain.h arch/x86/include/asm/bhv/domain.h
new file mode 100644
index 000000000..5640739ea
--- /dev/null
+++ arch/x86/include/asm/bhv/domain.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) kernel_to_user_pgdp(pgd)
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return pte_present(pte);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pmd_present(pmd);
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pud_present(pud);
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return !(pgprot_val(pmd_pgprot(pmd)) & _PAGE_NX);
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return !(pgprot_val(pud_pgprot(pud)) & _PAGE_NX);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pgprot_val(pte_pgprot(pte)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return !!(pgprot_val(pmd_pgprot(pmd)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return !!(pgprot_val(pud_pgprot(pud)) & _PAGE_USER);
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..2c2890de4
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 000000000..8e8756875
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+#endif // VASKM
+#else /* CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 000000000..d8bef2cff
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL is used in 6.1
+#if defined(CONFIG_RETPOLINE) &&                                               \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __bhv_text bhv_apply_retpolines_vault(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __bhv_text bhv_apply_returns_vault(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#ifdef CONFIG_PARAVIRT
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+#endif // VASKM
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	JCC = 4,
+#endif // LINUX_VERSION_CODE >= 6.1
+};
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func);
+
+#if defined BHV_KVERS_5_15 || defined(BHV_KVERS_6_1)
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func, bool modinit)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+
+#elif defined BHV_KVERS_5_10
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+#endif // BHV_KVERS
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
\ No newline at end of file
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec..591e48b05 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/include/asm/pgtable.h arch/x86/include/asm/pgtable.h
index 9bacde3ff..932469f11 100644
--- arch/x86/include/asm/pgtable.h
+++ arch/x86/include/asm/pgtable.h
@@ -27,6 +27,22 @@
 #include <asm/fpu/api.h>
 #include <asm-generic/pgtable_uffd.h>
 
+#ifdef CONFIG_MEM_NS
+extern void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte);
+extern void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+				  pmd_t *pmdp, pmd_t pmd);
+extern void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+				  pud_t *pudp, pud_t pud);
+
+extern void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr,
+				 pte_t *ptep, pte_t pte);
+extern void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr,
+				 pmd_t *pmdp, pmd_t pmd);
+extern void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr,
+				 pud_t *pudp, pud_t pud);
+#endif
+
 extern pgd_t early_top_pgt[PTRS_PER_PGD];
 bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
@@ -1035,18 +1051,27 @@ static inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_set_pte_at(mm, addr, ptep, pte);
+#endif
 	set_pte(ptep, pte);
 }
 
 static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
 			      pmd_t *pmdp, pmd_t pmd)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_set_pmd_at(mm, addr, pmdp, pmd);
+#endif
 	set_pmd(pmdp, pmd);
 }
 
 static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
 			      pud_t *pudp, pud_t pud)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_set_pud_at(mm, addr, pudp, pud);
+#endif
 	native_set_pud(pudp, pud);
 }
 
@@ -1076,7 +1101,11 @@ extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
-	pte_t pte = native_ptep_get_and_clear(ptep);
+	pte_t pte;
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pte(mm, addr, ptep, *ptep);
+#endif
+	pte = native_ptep_get_and_clear(ptep);
 	return pte;
 }
 
@@ -1091,6 +1120,9 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
 		 * Full address destruction in progress; paravirt does not
 		 * care about updates and native needs no locking
 		 */
+#ifdef CONFIG_MEM_NS
+		bhv_domain_clear_pte(mm, addr, ptep, pte);
+#endif
 		pte = native_local_ptep_get_and_clear(ptep);
 	} else {
 		pte = ptep_get_and_clear(mm, addr, ptep);
@@ -1103,6 +1135,9 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pte_t *ptep)
 {
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_set_pte_at(mm, addr, ptep, *ptep);
+#endif
 }
 
 #define flush_tlb_fix_spurious_fault(vma, address) do { } while (0)
@@ -1138,6 +1173,9 @@ static inline int pmd_write(pmd_t pmd)
 static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pmd_t *pmdp)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pmd(mm, addr, pmdp, *pmdp);
+#endif
 	return native_pmdp_get_and_clear(pmdp);
 }
 
@@ -1145,6 +1183,9 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long
 static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 					unsigned long addr, pud_t *pudp)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pud(mm, addr, pudp, *pudp);
+#endif
 	return native_pudp_get_and_clear(pudp);
 }
 
@@ -1152,6 +1193,9 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pmd_t *pmdp)
 {
+#ifdef CONFIG_MEM_NS
+	bhv_domain_set_pmd_at(mm, addr, pmdp, *pmdp);
+#endif
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
 }
 
diff --git arch/x86/include/asm/switch_to.h arch/x86/include/asm/switch_to.h
index 9f69cc497..f2b8695dc 100644
--- arch/x86/include/asm/switch_to.h
+++ arch/x86/include/asm/switch_to.h
@@ -19,6 +19,9 @@ asmlinkage void ret_from_fork(void);
  * order of the fields must match the code in __switch_to_asm().
  */
 struct inactive_task_frame {
+#ifdef CONFIG_MEM_NS
+	unsigned long domain;
+#endif
 #ifdef CONFIG_X86_64
 	unsigned long r15;
 	unsigned long r14;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 9ceef8515..4922ff834 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -30,6 +30,10 @@
 #include <asm/fixmap.h>
 #include <asm/asm-prototypes.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/domain.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
@@ -423,6 +427,13 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 	u8 *instr, *replacement;
 	u8 insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	DPRINTK("alt table %px, -> %px", start, end);
 
 	/*
@@ -633,6 +644,14 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+    if (bhv_integrity_is_enabled()) {
+        for (s = start; s < end; s++)
+            bhv_apply_retpolines(s);
+        return;
+    }
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -711,6 +730,14 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+    if (bhv_integrity_is_enabled()) {
+        for (s = start; s < end; s++)
+            bhv_apply_returns(s);
+        return;
+    }
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *dest = NULL, *addr = (void *)s + *s;
 		struct insn insn;
@@ -787,7 +814,6 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 			text_poke(ptr, ((unsigned char []){0x3E}), 1);
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -813,6 +839,11 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+
 	mutex_lock(&text_mutex);
 	if (!uniproc_patched)
 		goto unlock;
@@ -838,7 +869,26 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end,
+							 locks, locks_end, text,
+							 text_end);
+			// Apply
+			smp_lock = false;
+			p.locks_begin = locks;
+			p.locks_end = locks_end;
+			bhv_alternatives_apply_custom_filter(
+				&p, &smp_lock, bhv_alternatives_find_by_lock);
+		}
+	} else {
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
+	}
+#else /* !CONFIG_BHV_VAS */
 	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#endif /* CONFIG_BHV_VAS */
 unlock:
 	mutex_unlock(&text_mutex);
 }
@@ -872,9 +922,26 @@ void alternatives_enable_smp(void)
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else {
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+			}
+#else /* !CONFIG_BHV_VAS */
 			alternatives_smp_lock(mod->locks, mod->locks_end,
 					      mod->text, mod->text_end);
+#endif /* CONFIG_BHV_VAS */
+		}
 		uniproc_patched = false;
 	}
 	mutex_unlock(&text_mutex);
@@ -915,6 +982,14 @@ void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
 	struct paravirt_patch_site *p;
 	char insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (p = start; p < end; p++)
+			bhv_apply_paravirt(p);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (p = start; p < end; p++) {
 		unsigned int used;
 
@@ -1141,6 +1216,9 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 	switch_mm_irqs_off(NULL, mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -1163,6 +1241,9 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(prev_state.mm == NULL ? NULL : prev_state.mm->owner);
+#endif
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3..20bfa373b 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index ee1a283f8..93615b106 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -10,6 +10,7 @@
 #include <asm/proto.h>
 #include <asm/desc.h>
 #include <asm/hw_irq.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -310,6 +311,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index 5ba8477c2..0fd92158d 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -15,6 +15,9 @@
 #include <asm/kprobes.h>
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
 
 static void bug_at(const void *ip, int line)
 {
@@ -58,9 +61,30 @@ __jump_label_set_jump_code(struct jump_entry *entry, enum jump_label_type type,
 	return code;
 }
 
+#ifdef CONFIG_BHV_VAS
+static void __orig_jump_label_transform(struct jump_entry *entry,
+					enum jump_label_type type, int init);
+
 static inline void __jump_label_transform(struct jump_entry *entry,
 					  enum jump_label_type type,
 					  int init)
+{
+	if (bhv_integrity_is_enabled()) {
+		const void *opcode =
+			__jump_label_set_jump_code(entry, type, init);
+		bhv_patch_jump_label(entry, opcode, JUMP_LABEL_NOP_SIZE);
+	} else {
+		__orig_jump_label_transform(entry, type, init);
+	}
+}
+
+static inline void __orig_jump_label_transform(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       int init)
+#else /* CONFIG_BHV_VAS */
+static inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type, int init)
+#endif /* CONFIG_BHV_VAS */
 {
 	const void *opcode = __jump_label_set_jump_code(entry, type, init);
 
@@ -112,20 +136,38 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		return true;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#endif
+
 	mutex_lock(&text_mutex);
 	opcode = __jump_label_set_jump_code(entry, type, 0);
-	text_poke_queue((void *)jump_entry_code(entry),
-			opcode, JUMP_LABEL_NOP_SIZE, NULL);
+	text_poke_queue((void *)jump_entry_code(entry), opcode,
+			JUMP_LABEL_NOP_SIZE, NULL);
 	mutex_unlock(&text_mutex);
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+void arch_jump_label_transform_apply(void)
+{
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+		text_poke_finish();
+		mutex_unlock(&text_mutex);
+	}
+}
+#else /* CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
 
 static enum {
 	JL_STATE_START,
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index 455e19584..8f99c2e22 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -255,6 +258,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*retpolines = NULL, *returns = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#ifdef CONFIG_BHV_VAS
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".text", secstrings + s->sh_name))
 			text = s;
@@ -282,9 +291,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#ifdef CONFIG_BHV_VAS
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks && text) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = (void *)text->sh_addr;
+		arch.text_end = (void *)text->sh_addr + text->sh_size;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif /* CONFIG_BHV_VAS */
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (locks && text) {
diff --git arch/x86/kernel/process.c arch/x86/kernel/process.c
index 1cba09a9f..37cb1c15e 100644
--- arch/x86/kernel/process.c
+++ arch/x86/kernel/process.c
@@ -44,6 +44,8 @@
 #include <asm/proto.h>
 #include <asm/frame.h>
 
+#include <bhv/domain.h>
+
 #include "process.h"
 
 /*
@@ -161,6 +163,10 @@ int copy_thread(unsigned long clone_flags, unsigned long sp, unsigned long arg,
 	frame->flags = X86_EFLAGS_FIXED;
 #endif
 
+#ifdef CONFIG_MEM_NS
+	frame->domain = bhv_get_domain(p);
+#endif
+
 	/* Kernel thread ? */
 	if (unlikely(p->flags & PF_KTHREAD)) {
 		memset(childregs, 0, sizeof(struct pt_regs));
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index 273e9b77b..b6927f462 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,12 +4,17 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
+#ifndef CONFIG_BHV_VAS
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
 	JMP = 2,  /* tramp / site tail-call */
 	RET = 3,  /* tramp / site cond-tail-call */
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
@@ -26,6 +31,13 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	int size = CALL_INSN_SIZE;
 	const void *code;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	switch (type) {
 	case CALL:
 		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
diff --git arch/x86/kernel/traps.c arch/x86/kernel/traps.c
index 98838b784..71a27e64d 100644
--- arch/x86/kernel/traps.c
+++ arch/x86/kernel/traps.c
@@ -29,6 +29,7 @@
 #include <linux/errno.h>
 #include <linux/kexec.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/timer.h>
 #include <linux/init.h>
@@ -84,6 +85,16 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
+__always_inline static bool is_vault(unsigned long addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
+
 __always_inline int is_valid_bugaddr(unsigned long addr)
 {
 	if (addr < TASK_SIZE_MAX)
@@ -219,6 +230,11 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 {
 	bool handled = false;
 
+	if (is_vault(regs->ip)){
+		show_regs(regs);
+		panic("[%s] BUG in vault at %pS\n", __FUNCTION__, (void*)regs->ip);
+	}
+
 	if (!is_valid_bugaddr(regs->ip))
 		return handled;
 
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index 740f87d8a..b70172d52 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -150,6 +150,7 @@ SECTIONS
 		ALIGN_ENTRY_TEXT_END
 		SOFTIRQENTRY_TEXT
 		STATIC_CALL_TEXT
+		BHV_TEXT
 		*(.fixup)
 		*(.gnu.warning)
 
@@ -348,15 +349,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -390,6 +405,17 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/mm/fault.c arch/x86/mm/fault.c
index cdb337cf9..e720c1698 100644
--- arch/x86/mm/fault.c
+++ arch/x86/mm/fault.c
@@ -31,6 +31,10 @@
 #include <asm/pgtable_areas.h>		/* VMALLOC_START, ...		*/
 #include <asm/kvm_para.h>		/* kvm_handle_async_pf		*/
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
@@ -513,6 +517,31 @@ static void show_ldttss(const struct desc_ptr *gdt, const char *name, u16 index)
 static void
 show_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kaccess_events()) {
+		// is kernel?
+		if (!(error_code & X86_PF_USER)) {
+			// is perm violation?
+			if ((error_code & X86_PF_PROT) &&
+			    !(error_code & X86_PF_RSVD) &&
+			    !(error_code & X86_PF_PK)) {
+				uint8_t type;
+				if (error_code &
+				    X86_PF_INSTR) { // is instr fetch?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+				} else if (error_code &
+					   X86_PF_WRITE) { // is write?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+				} else {
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+				}
+				bhv_guestlog_log_kaccess((uint64_t)address,
+							 type);
+			}
+		}
+	}
+#endif
+
 	if (!oops_may_print())
 		return;
 
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index dd15fdee4..618a855e6 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -39,6 +39,8 @@
 
 #include "mm_internal.h"
 
+#include <bhv/init/start.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -959,6 +961,8 @@ void __ref free_initmem(void)
 
 	mem_encrypt_free_decrypted_mem();
 
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git arch/x86/mm/pat/memtype.c arch/x86/mm/pat/memtype.c
index adc76b413..462feb81b 100644
--- arch/x86/mm/pat/memtype.c
+++ arch/x86/mm/pat/memtype.c
@@ -312,7 +312,7 @@ void init_cache_modes(void)
 		 * NOTE: When WC or WP is used, it is redirected to UC- per
 		 * the default setup in __cachemode2pte_tbl[].
 		 */
-		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, UC_MINUS) | PAT(3, UC) |
+		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, WC) | PAT(3, UC) |
 		      PAT(4, WB) | PAT(5, WT) | PAT(6, UC_MINUS) | PAT(7, UC);
 	}
 
diff --git arch/x86/mm/pgtable.c arch/x86/mm/pgtable.c
index 204b25ee2..1271e8d71 100644
--- arch/x86/mm/pgtable.c
+++ arch/x86/mm/pgtable.c
@@ -7,6 +7,10 @@
 #include <asm/fixmap.h>
 #include <asm/mtrr.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 phys_addr_t physical_mask __ro_after_init = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;
 EXPORT_SYMBOL(physical_mask);
@@ -489,8 +493,12 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 {
 	int changed = !pte_same(*ptep, entry);
 
-	if (changed && dirty)
+	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pte_at(vma->vm_mm, address, ptep, entry);
+#endif
 		set_pte(ptep, entry);
+	}
 
 	return changed;
 }
@@ -505,6 +513,9 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pmd_at(vma->vm_mm, address, pmdp, entry);
+#endif
 		set_pmd(pmdp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pmd
@@ -525,6 +536,9 @@ int pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,
 	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pud_at(vma->vm_mm, address, pudp, entry);
+#endif
 		set_pud(pudp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pud
diff --git arch/x86/mm/tlb.c arch/x86/mm/tlb.c
index 569ac1d57..c892cbd91 100644
--- arch/x86/mm/tlb.c
+++ arch/x86/mm/tlb.c
@@ -15,6 +15,8 @@
 #include <asm/cache.h>
 #include <asm/apic.h>
 
+#include <bhv/domain.h>
+
 #include "mm_internal.h"
 
 #ifdef CONFIG_PARAVIRT
@@ -314,6 +316,9 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	local_irq_save(flags);
 	switch_mm_irqs_off(prev, next, tsk);
 	local_irq_restore(flags);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(next == NULL ? NULL : next->owner);
+#endif
 }
 
 static inline unsigned long mm_mangle_tif_spec_ib(struct task_struct *next)
@@ -513,7 +518,7 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 		smp_mb();
 		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
 		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
-				next_tlb_gen)
+		    next_tlb_gen)
 			return;
 
 		/*
diff --git certs/blacklist.c certs/blacklist.c
index c973de883..49b23c932 100644
--- certs/blacklist.c
+++ certs/blacklist.c
@@ -18,6 +18,8 @@
 #include "blacklist.h"
 #include "common.h"
 
+#include <bhv/keyring.h>
+
 static struct key *blacklist_keyring;
 
 #ifdef CONFIG_SYSTEM_REVOCATION_LIST
@@ -182,7 +184,9 @@ int add_key_to_revocation_list(const char *data, size_t size)
  */
 int is_key_on_revocation_list(struct pkcs7_message *pkcs7)
 {
-	int ret;
+	int ret = bhv_keyring_verify(blacklist_keyring, &blacklist_keyring);
+	if (ret)
+		return -EPERM;
 
 	ret = pkcs7_validate_trust(pkcs7, blacklist_keyring);
 
@@ -219,6 +223,10 @@ static int __init blacklist_init(void)
 	for (bl = blacklist_hashes; *bl; bl++)
 		if (mark_hash_blacklisted(*bl) < 0)
 			pr_err("- blacklisting failed\n");
+
+	if (bhv_keyring_register_system_trusted(&blacklist_keyring))
+		panic("Can't register system blacklist keyring\n");
+
 	return 0;
 }
 
diff --git certs/system_keyring.c certs/system_keyring.c
index a44a8915c..ec7ba5bf2 100644
--- certs/system_keyring.c
+++ certs/system_keyring.c
@@ -17,6 +17,8 @@
 #include <crypto/pkcs7.h>
 #include "common.h"
 
+#include <bhv/keyring.h>
+
 static struct key *builtin_trusted_keys;
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 static struct key *secondary_trusted_keys;
@@ -39,6 +41,11 @@ int restrict_link_by_builtin_trusted(struct key *dest_keyring,
 				     const union key_payload *payload,
 				     struct key *restriction_key)
 {
+	int rc = bhv_keyring_verify_locked(builtin_trusted_keys,
+					   &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  builtin_trusted_keys);
 }
@@ -58,6 +65,8 @@ int restrict_link_by_builtin_and_secondary_trusted(
 	const union key_payload *payload,
 	struct key *restrict_key)
 {
+	int rc = 0;
+
 	/* If we have a secondary trusted keyring, then that contains a link
 	 * through to the builtin keyring and the search will follow that link.
 	 */
@@ -67,6 +76,12 @@ int restrict_link_by_builtin_and_secondary_trusted(
 		/* Allow the builtin keyring to be added to the secondary */
 		return 0;
 
+	rc = bhv_keyring_verify_locked(secondary_trusted_keys,
+				       &secondary_trusted_keys);
+	if (rc)
+		return rc;
+
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  secondary_trusted_keys);
 }
@@ -107,6 +122,9 @@ static __init int system_trusted_keyring_init(void)
 	if (IS_ERR(builtin_trusted_keys))
 		panic("Can't allocate builtin trusted keyring\n");
 
+	if (bhv_keyring_register_system_trusted(&builtin_trusted_keys))
+		panic("Can't register builtin trusted keyring\n");
+
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 	secondary_trusted_keys =
 		keyring_alloc(".secondary_trusted_keys",
@@ -122,6 +140,9 @@ static __init int system_trusted_keyring_init(void)
 
 	if (key_link(secondary_trusted_keys, builtin_trusted_keys) < 0)
 		panic("Can't link trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&secondary_trusted_keys))
+		panic("Can't register secondary trusted keyring\n");
 #endif
 
 	return 0;
@@ -137,6 +158,11 @@ device_initcall(system_trusted_keyring_init);
  */
 static __init int load_system_certificate_list(void)
 {
+	int rc = bhv_keyring_verify(builtin_trusted_keys,
+				    &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	pr_notice("Loading compiled-in X.509 certificates\n");
 
 	return load_certificate_list(system_certificate_list, system_certificate_list_size,
@@ -180,15 +206,35 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 		goto error;
 
 	if (!trusted_keys) {
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 	} else if (trusted_keys == VERIFY_USE_SECONDARY_KEYRING) {
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
+		ret = bhv_keyring_verify(secondary_trusted_keys,
+					 &secondary_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = secondary_trusted_keys;
 #else
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 #endif
 	} else if (trusted_keys == VERIFY_USE_PLATFORM_KEYRING) {
 #ifdef CONFIG_INTEGRITY_PLATFORM_KEYRING
+		ret = bhv_keyring_verify(platform_trusted_keys,
+					 &platform_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = platform_trusted_keys;
 #else
 		trusted_keys = NULL;
@@ -205,6 +251,7 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 			goto error;
 		}
 	}
+
 	ret = pkcs7_validate_trust(pkcs7, trusted_keys);
 	if (ret < 0) {
 		if (ret == -ENOKEY)
@@ -273,5 +320,7 @@ EXPORT_SYMBOL_GPL(verify_pkcs7_signature);
 void __init set_platform_trusted_keys(struct key *keyring)
 {
 	platform_trusted_keys = keyring;
+	if (bhv_keyring_register_system_trusted(&platform_trusted_keys))
+		panic("Can't register platform trusted keyring\n");
 }
 #endif
diff --git drivers/block/drbd/drbd_main.c drivers/block/drbd/drbd_main.c
index 420bdaf8c..ed18e5847 100644
--- drivers/block/drbd/drbd_main.c
+++ drivers/block/drbd/drbd_main.c
@@ -97,9 +97,13 @@ module_param_named(proc_details, drbd_proc_details, int, 0644);
 unsigned int drbd_minor_count = DRBD_MINOR_COUNT_DEF;
 /* Module parameter for setting the user mode helper program
  * to run. Default is /sbin/drbdadm */
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+const char drbd_usermode_helper[] __section(".rodata") = "/sbin/drbdadm";
+#else
 char drbd_usermode_helper[80] = "/sbin/drbdadm";
-module_param_named(minor_count, drbd_minor_count, uint, 0444);
 module_param_string(usermode_helper, drbd_usermode_helper, sizeof(drbd_usermode_helper), 0644);
+#endif
+module_param_named(minor_count, drbd_minor_count, uint, 0444);
 
 /* in 2.6.x, our device mapping and config info contains our virtual gendisks
  * as member "struct gendisk *vdisk;"
diff --git drivers/block/zram/zram_drv.c drivers/block/zram/zram_drv.c
index 0636df6b6..5ed083a6e 100644
--- drivers/block/zram/zram_drv.c
+++ drivers/block/zram/zram_drv.c
@@ -926,7 +926,7 @@ static ssize_t read_block_state(struct file *file, char __user *buf,
 	return written;
 }
 
-static const struct file_operations proc_zram_block_state_op = {
+const struct file_operations proc_zram_block_state_op __section(".rodata") = {
 	.open = simple_open,
 	.read = read_block_state,
 	.llseek = default_llseek,
diff --git drivers/char/mem.c drivers/char/mem.c
index 7d483c332..dc8c479d8 100644
--- drivers/char/mem.c
+++ drivers/char/mem.c
@@ -904,7 +904,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define open_mem	open_port
 #define open_kmem	open_mem
 
-static const struct file_operations __maybe_unused mem_fops = {
+const struct file_operations __maybe_unused mem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
@@ -916,7 +916,7 @@ static const struct file_operations __maybe_unused mem_fops = {
 #endif
 };
 
-static const struct file_operations __maybe_unused kmem_fops = {
+static const struct file_operations __maybe_unused kmem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_kmem,
 	.write		= write_kmem,
@@ -928,7 +928,7 @@ static const struct file_operations __maybe_unused kmem_fops = {
 #endif
 };
 
-static const struct file_operations null_fops = {
+const struct file_operations null_fops __section(".rodata") = {
 	.llseek		= null_lseek,
 	.read		= read_null,
 	.write		= write_null,
@@ -937,14 +937,14 @@ static const struct file_operations null_fops = {
 	.splice_write	= splice_write_null,
 };
 
-static const struct file_operations __maybe_unused port_fops = {
+const struct file_operations __maybe_unused port_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_port,
 	.write		= write_port,
 	.open		= open_port,
 };
 
-static const struct file_operations zero_fops = {
+const struct file_operations zero_fops __section(".rodata") = {
 	.llseek		= zero_lseek,
 	.write		= write_zero,
 	.read_iter	= read_iter_zero,
@@ -957,7 +957,7 @@ static const struct file_operations zero_fops = {
 #endif
 };
 
-static const struct file_operations full_fops = {
+const struct file_operations full_fops __section(".rodata") = {
 	.llseek		= full_lseek,
 	.read_iter	= read_iter_zero,
 	.write		= write_full,
diff --git drivers/char/random.c drivers/char/random.c
index b54481e66..224f40c48 100644
--- drivers/char/random.c
+++ drivers/char/random.c
@@ -1383,7 +1383,7 @@ static int random_fasync(int fd, struct file *filp, int on)
 	return fasync_helper(fd, filp, on, &fasync);
 }
 
-const struct file_operations random_fops = {
+const struct file_operations random_fops __section(".rodata") = {
 	.read_iter = random_read_iter,
 	.write_iter = random_write_iter,
 	.poll = random_poll,
@@ -1395,7 +1395,7 @@ const struct file_operations random_fops = {
 	.splice_write = iter_file_splice_write,
 };
 
-const struct file_operations urandom_fops = {
+const struct file_operations urandom_fops __section(".rodata") = {
 	.read_iter = urandom_read_iter,
 	.write_iter = random_write_iter,
 	.unlocked_ioctl = random_ioctl,
diff --git drivers/tty/tty_io.c drivers/tty/tty_io.c
index 984e3098e..b368d7585 100644
--- drivers/tty/tty_io.c
+++ drivers/tty/tty_io.c
@@ -471,7 +471,7 @@ static void tty_show_fdinfo(struct seq_file *m, struct file *file)
 		tty->ops->show_fdinfo(tty, m);
 }
 
-static const struct file_operations tty_fops = {
+const struct file_operations tty_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= tty_write,
@@ -486,7 +486,7 @@ static const struct file_operations tty_fops = {
 	.show_fdinfo	= tty_show_fdinfo,
 };
 
-static const struct file_operations console_fops = {
+const struct file_operations console_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= redirected_tty_write,
@@ -500,7 +500,7 @@ static const struct file_operations console_fops = {
 	.fasync		= tty_fasync,
 };
 
-static const struct file_operations hung_up_tty_fops = {
+const struct file_operations hung_up_tty_fops = {
 	.llseek		= no_llseek,
 	.read_iter	= hung_up_tty_read,
 	.write_iter	= hung_up_tty_write,
diff --git drivers/video/fbdev/uvesafb.c drivers/video/fbdev/uvesafb.c
index d999a7cdb..a8321a97c 100644
--- drivers/video/fbdev/uvesafb.c
+++ drivers/video/fbdev/uvesafb.c
@@ -34,7 +34,13 @@ static struct cb_id uvesafb_cn_id = {
 	.idx = CN_IDX_V86D,
 	.val = CN_VAL_V86D_UVESAFB
 };
+
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static const char v86d_path[] __section(".rodata") = "/sbin/v86d";
+#else
 static char v86d_path[PATH_MAX] = "/sbin/v86d";
+#endif
 static char v86d_started;	/* has v86d been started by uvesafb? */
 
 static const struct fb_fix_screeninfo uvesafb_fix = {
@@ -118,7 +124,7 @@ static int uvesafb_helper_start(void)
 	};
 
 	char *argv[] = {
-		v86d_path,
+		(char *)v86d_path,
 		NULL,
 	};
 
@@ -1869,6 +1875,9 @@ static ssize_t v86d_show(struct device_driver *dev, char *buf)
 	return snprintf(buf, PAGE_SIZE, "%s\n", v86d_path);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static DRIVER_ATTR_RO(v86d);
+#else
 static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 		size_t count)
 {
@@ -1876,6 +1885,7 @@ static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 	return count;
 }
 static DRIVER_ATTR_RW(v86d);
+#endif
 
 static int uvesafb_init(void)
 {
@@ -1996,8 +2006,11 @@ MODULE_PARM_DESC(mode_option,
 module_param(vbemode, ushort, 0);
 MODULE_PARM_DESC(vbemode,
 	"VBE mode number to set, overrides the 'mode' option");
+
+#ifndef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
 module_param_string(v86d, v86d_path, PATH_MAX, 0660);
 MODULE_PARM_DESC(v86d, "Path to the v86d userspace helper.");
+#endif
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Michal Januszewski <spock@gentoo.org>");
diff --git fs/aio.c fs/aio.c
index 93b6bbf01..59345a9c4 100644
--- fs/aio.c
+++ fs/aio.c
@@ -164,6 +164,8 @@ struct kioctx {
 	struct page		*internal_pages[AIO_RING_PAGES];
 	struct file		*aio_ring_file;
 
+	struct mm_struct        *owner;
+
 	unsigned		id;
 };
 
@@ -751,6 +753,8 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
 	if (!ctx)
 		return ERR_PTR(-ENOMEM);
 
+	ctx->owner = mm;
+
 	ctx->max_reqs = max_reqs;
 
 	spin_lock_init(&ctx->ctx_lock);
@@ -1108,6 +1112,9 @@ static void aio_complete(struct aio_kiocb *iocb)
 	struct io_event	*ev_page, *event;
 	unsigned tail, pos, head;
 	unsigned long	flags;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	/*
 	 * Add a completion event to the ring buffer. Must be done holding
@@ -1125,8 +1132,17 @@ static void aio_complete(struct aio_kiocb *iocb)
 	ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 	event = ev_page + pos % AIO_EVENTS_PER_PAGE;
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(ctx->owner->owner);
+#endif
+
 	*event = iocb->ki_res;
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	kunmap_atomic(ev_page);
 	flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 
diff --git fs/attr.c fs/attr.c
index fefcdc778..67a0d402e 100644
--- fs/attr.c
+++ fs/attr.c
@@ -18,6 +18,8 @@
 #include <linux/evm.h>
 #include <linux/ima.h>
 
+#include <bhv/inode.h>
+
 #include "internal.h"
 
 /**
@@ -415,6 +417,8 @@ int notify_change(struct dentry * dentry, struct iattr * attr, struct inode **de
 		fsnotify_change(dentry, ia_valid);
 		ima_inode_post_setattr(dentry);
 		evm_inode_post_setattr(dentry, ia_valid);
+		/* XXX: Make an LSM hook out of me! */
+		bhv_inode_post_setattr(dentry, ia_valid, mode);
 	}
 
 	return error;
diff --git fs/coredump.c fs/coredump.c
index 9d91e831e..24df8292b 100644
--- fs/coredump.c
+++ fs/coredump.c
@@ -58,7 +58,11 @@ static void free_vma_snapshot(struct coredump_params *cprm);
 
 int core_uses_pid;
 unsigned int core_pipe_limit;
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+char core_pattern[CORENAME_MAX_SIZE] __section(".rodata") = "core";
+#else
 char core_pattern[CORENAME_MAX_SIZE] = "core";
+#endif
 static int core_name_size = CORENAME_MAX_SIZE;
 
 struct core_name {
diff --git fs/debugfs/file.c fs/debugfs/file.c
index 9c0aadedf..bb92daf30 100644
--- fs/debugfs/file.c
+++ fs/debugfs/file.c
@@ -38,12 +38,13 @@ static ssize_t default_write_file(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations debugfs_noop_file_operations = {
-	.read =		default_read_file,
-	.write =	default_write_file,
-	.open =		simple_open,
-	.llseek =	noop_llseek,
-};
+const struct file_operations
+	debugfs_noop_file_operations __section(".rodata") = {
+		.read = default_read_file,
+		.write = default_write_file,
+		.open = simple_open,
+		.llseek = noop_llseek,
+	};
 
 #define F_DENTRY(filp) ((filp)->f_path.dentry)
 
@@ -209,28 +210,29 @@ static int open_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_open_proxy_file_operations = {
-	.open = open_proxy_open,
-};
+const struct file_operations
+	debugfs_open_proxy_file_operations __section(".rodata") = {
+		.open = open_proxy_open,
+	};
 
 #define PROTO(args...) args
 #define ARGS(args...) args
 
-#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)		\
-static ret_type full_proxy_ ## name(proto)				\
-{									\
-	struct dentry *dentry = F_DENTRY(filp);			\
-	const struct file_operations *real_fops;			\
-	ret_type r;							\
-									\
-	r = debugfs_file_get(dentry);					\
-	if (unlikely(r))						\
-		return r;						\
-	real_fops = debugfs_real_fops(filp);				\
-	r = real_fops->name(args);					\
-	debugfs_file_put(dentry);					\
-	return r;							\
-}
+#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)                     \
+	ret_type full_proxy_##name(proto)                                      \
+	{                                                                      \
+		struct dentry *dentry = F_DENTRY(filp);                        \
+		const struct file_operations *real_fops;                       \
+		ret_type r;                                                    \
+                                                                               \
+		r = debugfs_file_get(dentry);                                  \
+		if (unlikely(r))                                               \
+			return r;                                              \
+		real_fops = debugfs_real_fops(filp);                           \
+		r = real_fops->name(args);                                     \
+		debugfs_file_put(dentry);                                      \
+		return r;                                                      \
+	}
 
 FULL_PROXY_FUNC(llseek, loff_t, filp,
 		PROTO(struct file *filp, loff_t offset, int whence),
@@ -250,8 +252,7 @@ FULL_PROXY_FUNC(unlocked_ioctl, long, filp,
 		PROTO(struct file *filp, unsigned int cmd, unsigned long arg),
 		ARGS(filp, cmd, arg));
 
-static __poll_t full_proxy_poll(struct file *filp,
-				struct poll_table_struct *wait)
+__poll_t full_proxy_poll(struct file *filp, struct poll_table_struct *wait)
 {
 	struct dentry *dentry = F_DENTRY(filp);
 	__poll_t r = 0;
@@ -266,7 +267,7 @@ static __poll_t full_proxy_poll(struct file *filp,
 	return r;
 }
 
-static int full_proxy_release(struct inode *inode, struct file *filp)
+int full_proxy_release(struct inode *inode, struct file *filp)
 {
 	const struct dentry *dentry = F_DENTRY(filp);
 	const struct file_operations *real_fops = debugfs_real_fops(filp);
@@ -367,9 +368,10 @@ static int full_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_full_proxy_file_operations = {
-	.open = full_proxy_open,
-};
+const struct file_operations
+	debugfs_full_proxy_file_operations __section(".rodata") = {
+		.open = full_proxy_open,
+	};
 
 ssize_t debugfs_attr_read(struct file *file, char __user *buf,
 			size_t len, loff_t *ppos)
diff --git fs/exec.c fs/exec.c
index ebe901195..686967c76 100644
--- fs/exec.c
+++ fs/exec.c
@@ -47,6 +47,7 @@
 #include <linux/personality.h>
 #include <linux/binfmts.h>
 #include <linux/utsname.h>
+#include <linux/mem_namespace.h>
 #include <linux/pid_namespace.h>
 #include <linux/module.h>
 #include <linux/namei.h>
@@ -74,6 +75,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -278,6 +281,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 		goto err;
 
 	mm->stack_vm = mm->total_vm = 1;
+
 	mmap_write_unlock(mm);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
@@ -614,6 +618,9 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 {
 	int len = strnlen(arg, MAX_ARG_STRLEN) + 1 /* terminating NUL */;
 	unsigned long pos = bprm->p;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (len == 0)
 		return -EFAULT;
@@ -640,9 +647,18 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 		if (!page)
 			return -E2BIG;
 		kaddr = kmap_atomic(page);
+
+#ifdef CONFIG_MEM_NS
+		domain = bhv_get_active_domain();
+		bhv_domain_enter(bprm->mm->owner);
+#endif
 		flush_arg_page(bprm, pos & PAGE_MASK, page);
 		memcpy(kaddr + offset_in_page(pos), arg, bytes_to_copy);
 		flush_kernel_dcache_page(page);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
+
 		kunmap_atomic(kaddr);
 		put_arg_page(page);
 	}
@@ -853,8 +869,10 @@ int setup_arg_pages(struct linux_binprm *bprm,
 #endif
 	current->mm->start_stack = bprm->p;
 	ret = expand_stack(vma, stack_base);
-	if (ret)
+	if (ret) {
 		ret = -EFAULT;
+		goto out_unlock;
+	}
 
 out_unlock:
 	mmap_write_unlock(mm);
diff --git fs/ext4/dir.c fs/ext4/dir.c
index 70a0f5e56..0523cadcb 100644
--- fs/ext4/dir.c
+++ fs/ext4/dir.c
@@ -655,7 +655,7 @@ int ext4_check_all_de(struct inode *dir, struct buffer_head *bh, void *buf,
 	return 0;
 }
 
-const struct file_operations ext4_dir_operations = {
+const struct file_operations ext4_dir_operations __section(".rodata") = {
 	.llseek		= ext4_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= ext4_readdir,
diff --git fs/ext4/file.c fs/ext4/file.c
index f42cc1fe0..dd2fffd12 100644
--- fs/ext4/file.c
+++ fs/ext4/file.c
@@ -909,7 +909,7 @@ loff_t ext4_llseek(struct file *file, loff_t offset, int whence)
 	return vfs_setpos(file, offset, maxbytes);
 }
 
-const struct file_operations ext4_file_operations = {
+const struct file_operations ext4_file_operations __section(".rodata") = {
 	.llseek		= ext4_llseek,
 	.read_iter	= ext4_file_read_iter,
 	.write_iter	= ext4_file_write_iter,
diff --git fs/inode.c fs/inode.c
index 5c7139aa2..0db17eb9a 100644
--- fs/inode.c
+++ fs/inode.c
@@ -24,6 +24,8 @@
 #include <trace/events/writeback.h>
 #include "internal.h"
 
+#include <bhv/inode.h>
+
 /*
  * Inode locking rules:
  *
@@ -1720,6 +1722,7 @@ void iput(struct inode *inode)
 			mark_inode_dirty_sync(inode);
 			goto retry;
 		}
+		bhv_inode_iput_final(inode);
 		iput_final(inode);
 	}
 }
diff --git fs/libfs.c fs/libfs.c
index aa0fbd720..9176d53ee 100644
--- fs/libfs.c
+++ fs/libfs.c
@@ -227,7 +227,7 @@ ssize_t generic_read_dir(struct file *filp, char __user *buf, size_t siz, loff_t
 }
 EXPORT_SYMBOL(generic_read_dir);
 
-const struct file_operations simple_dir_operations = {
+const struct file_operations simple_dir_operations __section(".rodata") = {
 	.open		= dcache_dir_open,
 	.release	= dcache_dir_close,
 	.llseek		= dcache_dir_lseek,
@@ -1354,7 +1354,7 @@ static int empty_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-static const struct file_operations empty_dir_operations = {
+const struct file_operations empty_dir_operations __section(".rodata") = {
 	.llseek		= empty_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= empty_dir_readdir,
diff --git fs/nfsd/nfs4recover.c fs/nfsd/nfs4recover.c
index 83c4e6883..83847db3c 100644
--- fs/nfsd/nfs4recover.c
+++ fs/nfsd/nfs4recover.c
@@ -1663,11 +1663,15 @@ static const struct nfsd4_client_tracking_ops nfsd4_cld_tracking_ops_v2 = {
 	.msglen		= sizeof(struct cld_msg_v2),
 };
 
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+static const char cltrack_prog[] __section(".rodata") = "/sbin/nfsdcltrack";
+#else
 /* upcall via usermodehelper */
 static char cltrack_prog[PATH_MAX] = "/sbin/nfsdcltrack";
 module_param_string(cltrack_prog, cltrack_prog, sizeof(cltrack_prog),
 			S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(cltrack_prog, "Path to the nfsdcltrack upcall program");
+#endif
 
 static bool cltrack_legacy_disable;
 module_param(cltrack_legacy_disable, bool, S_IRUGO|S_IWUSR);
diff --git fs/proc/array.c fs/proc/array.c
index 18a4588c3..864741aee 100644
--- fs/proc/array.c
+++ fs/proc/array.c
@@ -768,7 +768,7 @@ static int children_seq_open(struct inode *inode, struct file *file)
 	return seq_open(file, &children_seq_ops);
 }
 
-const struct file_operations proc_tid_children_operations = {
+const struct file_operations proc_tid_children_operations __section(".rodata") = {
 	.open    = children_seq_open,
 	.read    = seq_read,
 	.llseek  = seq_lseek,
diff --git fs/proc/base.c fs/proc/base.c
index 712948e97..0a24382db 100644
--- fs/proc/base.c
+++ fs/proc/base.c
@@ -372,7 +372,7 @@ static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_pid_cmdline_ops = {
+const struct file_operations proc_pid_cmdline_ops __section(".rodata") = {
 	.read	= proc_pid_cmdline_read,
 	.llseek	= generic_file_llseek,
 };
@@ -536,7 +536,7 @@ static ssize_t lstats_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_lstats_operations = {
+const struct file_operations proc_lstats_operations __section(".rodata") = {
 	.open		= lstats_open,
 	.read		= seq_read,
 	.write		= lstats_write,
@@ -783,7 +783,7 @@ static int proc_single_open(struct inode *inode, struct file *filp)
 	return single_open(filp, proc_single_show, inode);
 }
 
-static const struct file_operations proc_single_file_operations = {
+const struct file_operations proc_single_file_operations __section(".rodata") = {
 	.open		= proc_single_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -923,7 +923,7 @@ static int mem_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_mem_operations = {
+const struct file_operations proc_mem_operations __section(".rodata") = {
 	.llseek		= mem_lseek,
 	.read		= mem_read,
 	.write		= mem_write,
@@ -999,7 +999,7 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_environ_operations = {
+const struct file_operations proc_environ_operations __section(".rodata") = {
 	.open		= environ_open,
 	.read		= environ_read,
 	.llseek		= generic_file_llseek,
@@ -1026,7 +1026,7 @@ static ssize_t auxv_read(struct file *file, char __user *buf,
 				       nwords * sizeof(mm->saved_auxv[0]));
 }
 
-static const struct file_operations proc_auxv_operations = {
+const struct file_operations proc_auxv_operations __section(".rodata") = {
 	.open		= auxv_open,
 	.read		= auxv_read,
 	.llseek		= generic_file_llseek,
@@ -1186,7 +1186,7 @@ static ssize_t oom_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_adj_operations = {
+const struct file_operations proc_oom_adj_operations __section(".rodata") = {
 	.read		= oom_adj_read,
 	.write		= oom_adj_write,
 	.llseek		= generic_file_llseek,
@@ -1237,7 +1237,7 @@ static ssize_t oom_score_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_score_adj_operations = {
+const struct file_operations proc_oom_score_adj_operations __section(".rodata") = {
 	.read		= oom_score_adj_read,
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
@@ -1305,7 +1305,7 @@ static ssize_t proc_loginuid_write(struct file * file, const char __user * buf,
 	return count;
 }
 
-static const struct file_operations proc_loginuid_operations = {
+const struct file_operations proc_loginuid_operations __section(".rodata") = {
 	.read		= proc_loginuid_read,
 	.write		= proc_loginuid_write,
 	.llseek		= generic_file_llseek,
@@ -1327,7 +1327,7 @@ static ssize_t proc_sessionid_read(struct file * file, char __user * buf,
 	return simple_read_from_buffer(buf, count, ppos, tmpbuf, length);
 }
 
-static const struct file_operations proc_sessionid_operations = {
+const struct file_operations proc_sessionid_operations __section(".rodata") = {
 	.read		= proc_sessionid_read,
 	.llseek		= generic_file_llseek,
 };
@@ -1382,7 +1382,7 @@ static ssize_t proc_fault_inject_write(struct file * file,
 	return count;
 }
 
-static const struct file_operations proc_fault_inject_operations = {
+const struct file_operations proc_fault_inject_operations __section(".rodata") = {
 	.read		= proc_fault_inject_read,
 	.write		= proc_fault_inject_write,
 	.llseek		= generic_file_llseek,
@@ -1423,7 +1423,7 @@ static ssize_t proc_fail_nth_read(struct file *file, char __user *buf,
 	return simple_read_from_buffer(buf, count, ppos, numbuf, len);
 }
 
-static const struct file_operations proc_fail_nth_operations = {
+const struct file_operations proc_fail_nth_operations __section(".rodata") = {
 	.read		= proc_fail_nth_read,
 	.write		= proc_fail_nth_write,
 };
@@ -1472,7 +1472,7 @@ static int sched_open(struct inode *inode, struct file *filp)
 	return single_open(filp, sched_show, inode);
 }
 
-static const struct file_operations proc_pid_sched_operations = {
+const struct file_operations proc_pid_sched_operations __section(".rodata") = {
 	.open		= sched_open,
 	.read		= seq_read,
 	.write		= sched_write,
@@ -1547,7 +1547,7 @@ static int sched_autogroup_open(struct inode *inode, struct file *filp)
 	return ret;
 }
 
-static const struct file_operations proc_pid_sched_autogroup_operations = {
+const struct file_operations proc_pid_sched_autogroup_operations __section(".rodata") = {
 	.open		= sched_autogroup_open,
 	.read		= seq_read,
 	.write		= sched_autogroup_write,
@@ -1650,7 +1650,7 @@ static int timens_offsets_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timens_offsets_show, inode);
 }
 
-static const struct file_operations proc_timens_offsets_operations = {
+const struct file_operations proc_timens_offsets_operations __section(".rodata") = {
 	.open		= timens_offsets_open,
 	.read		= seq_read,
 	.write		= timens_offsets_write,
@@ -1707,7 +1707,7 @@ static int comm_open(struct inode *inode, struct file *filp)
 	return single_open(filp, comm_show, inode);
 }
 
-static const struct file_operations proc_pid_set_comm_operations = {
+const struct file_operations proc_pid_set_comm_operations __section(".rodata") = {
 	.open		= comm_open,
 	.read		= seq_read,
 	.write		= comm_write,
@@ -2425,7 +2425,7 @@ proc_map_files_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-static const struct file_operations proc_map_files_operations = {
+const struct file_operations proc_map_files_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_map_files_readdir,
 	.llseek		= generic_file_llseek,
@@ -2524,7 +2524,7 @@ static int proc_timers_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_timers_operations = {
+const struct file_operations proc_timers_operations __section(".rodata") = {
 	.open		= proc_timers_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -2616,7 +2616,7 @@ static int timerslack_ns_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timerslack_ns_show, inode);
 }
 
-static const struct file_operations proc_pid_set_timerslack_ns_operations = {
+const struct file_operations proc_pid_set_timerslack_ns_operations __section(".rodata") = {
 	.open		= timerslack_ns_open,
 	.read		= seq_read,
 	.write		= timerslack_ns_write,
@@ -2789,7 +2789,7 @@ static ssize_t proc_pid_attr_write(struct file * file, const char __user * buf,
 	return rv;
 }
 
-static const struct file_operations proc_pid_attr_operations = {
+const struct file_operations proc_pid_attr_operations __section(".rodata") = {
 	.open		= proc_pid_attr_open,
 	.read		= proc_pid_attr_read,
 	.write		= proc_pid_attr_write,
@@ -2865,7 +2865,7 @@ static int proc_attr_dir_readdir(struct file *file, struct dir_context *ctx)
 				   attr_dir_stuff, ARRAY_SIZE(attr_dir_stuff));
 }
 
-static const struct file_operations proc_attr_dir_operations = {
+const struct file_operations proc_attr_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_attr_dir_readdir,
 	.llseek		= generic_file_llseek,
@@ -2957,7 +2957,7 @@ static ssize_t proc_coredump_filter_write(struct file *file,
 	return count;
 }
 
-static const struct file_operations proc_coredump_filter_operations = {
+const struct file_operations proc_coredump_filter_operations __section(".rodata") = {
 	.read		= proc_coredump_filter_read,
 	.write		= proc_coredump_filter_write,
 	.llseek		= generic_file_llseek,
@@ -3080,7 +3080,7 @@ static int proc_projid_map_open(struct inode *inode, struct file *file)
 	return proc_id_map_open(inode, file, &proc_projid_seq_operations);
 }
 
-static const struct file_operations proc_uid_map_operations = {
+const struct file_operations proc_uid_map_operations __section(".rodata") = {
 	.open		= proc_uid_map_open,
 	.write		= proc_uid_map_write,
 	.read		= seq_read,
@@ -3088,7 +3088,7 @@ static const struct file_operations proc_uid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_gid_map_operations = {
+const struct file_operations proc_gid_map_operations __section(".rodata") = {
 	.open		= proc_gid_map_open,
 	.write		= proc_gid_map_write,
 	.read		= seq_read,
@@ -3096,7 +3096,7 @@ static const struct file_operations proc_gid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_projid_map_operations = {
+const struct file_operations proc_projid_map_operations __section(".rodata") = {
 	.open		= proc_projid_map_open,
 	.write		= proc_projid_map_write,
 	.read		= seq_read,
@@ -3147,7 +3147,7 @@ static int proc_setgroups_release(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static const struct file_operations proc_setgroups_operations = {
+const struct file_operations proc_setgroups_operations __section(".rodata") = {
 	.open		= proc_setgroups_open,
 	.write		= proc_setgroups_write,
 	.read		= seq_read,
@@ -3194,7 +3194,7 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 /*
  * Thread groups
  */
-static const struct file_operations proc_task_operations;
+const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
 static const struct pid_entry tgid_base_stuff[] = {
@@ -3312,7 +3312,7 @@ static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
 				   tgid_base_stuff, ARRAY_SIZE(tgid_base_stuff));
 }
 
-static const struct file_operations proc_tgid_base_operations = {
+const struct file_operations proc_tgid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3650,7 +3650,7 @@ static struct dentry *proc_tid_base_lookup(struct inode *dir, struct dentry *den
 				  tid_base_stuff + ARRAY_SIZE(tid_base_stuff));
 }
 
-static const struct file_operations proc_tid_base_operations = {
+const struct file_operations proc_tid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3858,7 +3858,7 @@ static const struct inode_operations proc_task_inode_operations = {
 	.permission	= proc_pid_permission,
 };
 
-static const struct file_operations proc_task_operations = {
+const struct file_operations proc_task_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_task_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/fd.c fs/proc/fd.c
index 81882a132..6d2880867 100644
--- fs/proc/fd.c
+++ fs/proc/fd.c
@@ -74,7 +74,7 @@ static int seq_fdinfo_open(struct inode *inode, struct file *file)
 	return single_open(file, seq_show, inode);
 }
 
-static const struct file_operations proc_fdinfo_file_operations = {
+const struct file_operations proc_fdinfo_file_operations __section(".rodata") = {
 	.open		= seq_fdinfo_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -278,7 +278,7 @@ static int proc_readfd(struct file *file, struct dir_context *ctx)
 	return proc_readfd_common(file, ctx, proc_fd_instantiate);
 }
 
-const struct file_operations proc_fd_operations = {
+const struct file_operations proc_fd_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfd,
 	.llseek		= generic_file_llseek,
@@ -356,7 +356,7 @@ const struct inode_operations proc_fdinfo_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
-const struct file_operations proc_fdinfo_operations = {
+const struct file_operations proc_fdinfo_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfdinfo,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/generic.c fs/proc/generic.c
index 589876169..792c8445b 100644
--- fs/proc/generic.c
+++ fs/proc/generic.c
@@ -343,7 +343,7 @@ int proc_readdir(struct file *file, struct dir_context *ctx)
  * use the in-memory "struct proc_dir_entry" tree to parse
  * the /proc directory.
  */
-static const struct file_operations proc_dir_operations = {
+const struct file_operations proc_dir_operations __section(".rodata") = {
 	.llseek			= generic_file_llseek,
 	.read			= generic_read_dir,
 	.iterate_shared		= proc_readdir,
diff --git fs/proc/inode.c fs/proc/inode.c
index bde6b6f69..bda0e40bf 100644
--- fs/proc/inode.c
+++ fs/proc/inode.c
@@ -581,7 +581,7 @@ static int proc_reg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_reg_file_ops = {
+const struct file_operations proc_reg_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -593,7 +593,7 @@ static const struct file_operations proc_reg_file_ops = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops = {
+const struct file_operations proc_iter_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.write		= proc_reg_write,
@@ -607,7 +607,7 @@ static const struct file_operations proc_iter_file_ops = {
 };
 
 #ifdef CONFIG_COMPAT
-static const struct file_operations proc_reg_file_ops_compat = {
+const struct file_operations proc_reg_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -620,7 +620,7 @@ static const struct file_operations proc_reg_file_ops_compat = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops_compat = {
+const struct file_operations proc_iter_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/proc/namespaces.c fs/proc/namespaces.c
index 8e159fc78..df908ffc8 100644
--- fs/proc/namespaces.c
+++ fs/proc/namespaces.c
@@ -11,7 +11,6 @@
 #include <linux/user_namespace.h>
 #include "internal.h"
 
-
 static const struct proc_ns_operations *ns_entries[] = {
 #ifdef CONFIG_NET_NS
 	&netns_operations,
@@ -23,8 +22,7 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&ipcns_operations,
 #endif
 #ifdef CONFIG_PID_NS
-	&pidns_operations,
-	&pidns_for_children_operations,
+	&pidns_operations,    &pidns_for_children_operations,
 #endif
 #ifdef CONFIG_USER_NS
 	&userns_operations,
@@ -34,8 +32,10 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&cgroupns_operations,
 #endif
 #ifdef CONFIG_TIME_NS
-	&timens_operations,
-	&timens_for_children_operations,
+	&timens_operations,   &timens_for_children_operations,
+#endif
+#ifdef CONFIG_MEM_NS
+	&memns_operations,
 #endif
 };
 
@@ -142,7 +142,7 @@ static int proc_ns_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-const struct file_operations proc_ns_dir_operations = {
+const struct file_operations proc_ns_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_ns_dir_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/proc_net.c fs/proc/proc_net.c
index 707477e27..dc23a11a4 100644
--- fs/proc/proc_net.c
+++ fs/proc/proc_net.c
@@ -326,7 +326,7 @@ static int proc_tgid_net_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-const struct file_operations proc_net_operations = {
+const struct file_operations proc_net_operations __section(".rodata") = {
 	.llseek		= generic_file_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_net_readdir,
diff --git fs/proc/proc_sysctl.c fs/proc/proc_sysctl.c
index aff9593fe..a02dc69c0 100644
--- fs/proc/proc_sysctl.c
+++ fs/proc/proc_sysctl.c
@@ -20,9 +20,9 @@
 #include "internal.h"
 
 static const struct dentry_operations proc_sys_dentry_operations;
-static const struct file_operations proc_sys_file_operations;
+const struct file_operations proc_sys_file_operations;
 static const struct inode_operations proc_sys_inode_operations;
-static const struct file_operations proc_sys_dir_file_operations;
+const struct file_operations proc_sys_dir_file_operations;
 static const struct inode_operations proc_sys_dir_operations;
 
 /* shared constants to be used in various sysctls */
@@ -849,7 +849,7 @@ static int proc_sys_getattr(const struct path *path, struct kstat *stat,
 	return 0;
 }
 
-static const struct file_operations proc_sys_file_operations = {
+const struct file_operations proc_sys_file_operations __section(".rodata") = {
 	.open		= proc_sys_open,
 	.poll		= proc_sys_poll,
 	.read_iter	= proc_sys_read,
@@ -859,7 +859,7 @@ static const struct file_operations proc_sys_file_operations = {
 	.llseek		= default_llseek,
 };
 
-static const struct file_operations proc_sys_dir_file_operations = {
+const struct file_operations proc_sys_dir_file_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_sys_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/root.c fs/proc/root.c
index 5e444d4f9..6f913e798 100644
--- fs/proc/root.c
+++ fs/proc/root.c
@@ -341,7 +341,7 @@ static int proc_root_readdir(struct file *file, struct dir_context *ctx)
  * <pid> directories. Thus we don't use the generic
  * directory handling functions for that..
  */
-static const struct file_operations proc_root_operations = {
+const struct file_operations proc_root_operations __section(".rodata") = {
 	.read		 = generic_read_dir,
 	.iterate_shared	 = proc_root_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/task_mmu.c fs/proc/task_mmu.c
index 39b103807..0d16b47ef 100644
--- fs/proc/task_mmu.c
+++ fs/proc/task_mmu.c
@@ -349,7 +349,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return do_maps_open(inode, file, &proc_pid_maps_op);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1022,14 +1022,14 @@ static int smaps_rollup_release(struct inode *inode, struct file *file)
 	return single_release(inode, file);
 }
 
-const struct file_operations proc_pid_smaps_operations = {
+const struct file_operations proc_pid_smaps_operations __section(".rodata") = {
 	.open		= pid_smaps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
 	.release	= proc_map_release,
 };
 
-const struct file_operations proc_pid_smaps_rollup_operations = {
+const struct file_operations proc_pid_smaps_rollup_operations __section(".rodata") = {
 	.open		= smaps_rollup_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1293,7 +1293,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations proc_clear_refs_operations = {
+const struct file_operations proc_clear_refs_operations __section(".rodata") = {
 	.write		= clear_refs_write,
 	.llseek		= noop_llseek,
 };
@@ -1702,7 +1702,7 @@ static int pagemap_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations proc_pagemap_operations = {
+const struct file_operations proc_pagemap_operations __section(".rodata") = {
 	.llseek		= mem_lseek, /* borrow this */
 	.read		= pagemap_read,
 	.open		= pagemap_open,
@@ -1967,7 +1967,7 @@ static int pid_numa_maps_open(struct inode *inode, struct file *file)
 				sizeof(struct numa_maps_private));
 }
 
-const struct file_operations proc_pid_numa_maps_operations = {
+const struct file_operations proc_pid_numa_maps_operations __section(".rodata") = {
 	.open		= pid_numa_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc/task_nommu.c fs/proc/task_nommu.c
index 97f387d30..427f6eae3 100644
--- fs/proc/task_nommu.c
+++ fs/proc/task_nommu.c
@@ -296,7 +296,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return maps_open(inode, file, &proc_pid_maps_ops);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc_namespace.c fs/proc_namespace.c
index eafb75755..cebc9294d 100644
--- fs/proc_namespace.c
+++ fs/proc_namespace.c
@@ -318,7 +318,7 @@ static int mountstats_open(struct inode *inode, struct file *file)
 	return mounts_open_common(inode, file, show_vfsstat);
 }
 
-const struct file_operations proc_mounts_operations = {
+const struct file_operations proc_mounts_operations __section(".rodata") = {
 	.open		= mounts_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -327,7 +327,7 @@ const struct file_operations proc_mounts_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountinfo_operations = {
+const struct file_operations proc_mountinfo_operations __section(".rodata") = {
 	.open		= mountinfo_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -336,7 +336,7 @@ const struct file_operations proc_mountinfo_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountstats_operations = {
+const struct file_operations proc_mountstats_operations __section(".rodata") = {
 	.open		= mountstats_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/read_write.c fs/read_write.c
index 0066acb6b..f19ecd331 100644
--- fs/read_write.c
+++ fs/read_write.c
@@ -25,6 +25,8 @@
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
 
+#include "bhv/file_protection.h"
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read_iter	= generic_file_read_iter,
@@ -605,6 +607,9 @@ ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_
 		ret = new_sync_write(file, buf, count, pos);
 	else
 		ret = -EINVAL;
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_modify(file);
 		add_wchar(current, ret);
@@ -892,6 +897,9 @@ ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
 		return ret;
 
 	ret = call_write_iter(file, iocb, iter);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0)
 		fsnotify_modify(file);
 
@@ -902,9 +910,14 @@ EXPORT_SYMBOL(vfs_iocb_iter_write);
 ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
 		rwf_t flags)
 {
+	ssize_t ret = 0;
 	if (!file->f_op->write_iter)
 		return -EINVAL;
-	return do_iter_write(file, iter, ppos, flags);
+	ret = do_iter_write(file, iter, ppos, flags);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
+	return ret;
 }
 EXPORT_SYMBOL(vfs_iter_write);
 
@@ -940,6 +953,9 @@ static ssize_t vfs_writev(struct file *file, const struct iovec __user *vec,
 		file_end_write(file);
 		kfree(iov);
 	}
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	return ret;
 }
 
@@ -1533,6 +1549,9 @@ ssize_t vfs_copy_file_range(struct file *file_in, loff_t pos_in,
 				      flags);
 
 done:
+
+	bhv_check_file_dirty_cred(file_out, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_access(file_in);
 		add_rchar(current, ret);
diff --git fs/xfs/xfs_file.c fs/xfs/xfs_file.c
index 9b6c5ba5f..a2dfad1de 100644
--- fs/xfs/xfs_file.c
+++ fs/xfs/xfs_file.c
@@ -1379,7 +1379,7 @@ xfs_file_mmap(
 	return 0;
 }
 
-const struct file_operations xfs_file_operations = {
+const struct file_operations xfs_file_operations __section(".rodata") = {
 	.llseek		= xfs_file_llseek,
 	.read_iter	= xfs_file_read_iter,
 	.write_iter	= xfs_file_write_iter,
@@ -1401,7 +1401,7 @@ const struct file_operations xfs_file_operations = {
 	.remap_file_range = xfs_file_remap_range,
 };
 
-const struct file_operations xfs_dir_file_operations = {
+const struct file_operations xfs_dir_file_operations __section(".rodata") = {
 	.open		= xfs_dir_open,
 	.read		= generic_read_dir,
 	.iterate_shared	= xfs_file_readdir,
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index 72f1e2a8c..5201954a9 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifndef dereference_function_descriptor
 #define dereference_function_descriptor(p) ((void *)(p))
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 44103f948..9a2ba3858 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -657,6 +657,17 @@
 		*(.static_call.text)					\
 		__static_call_text_end = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_start = .;				\
+		*(.bhv.text)						\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_end = .;
+#else
+#define BHV_TEXT
+#endif
+
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  KEEP(*(.head.text))
 
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 000000000..188c6153c
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#if defined CONFIG_BHV_VAS && !defined VASKM
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_acl(void);
+/************************************************************/
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_PROC_ACL, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_DRIVER_ACL, bhv_configuration_bitmap);
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else // defined CONFIG_BHV_VAS && !defined VASKM
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_block_driver(const char *target)
+{
+	return false;
+}
+
+static inline bool bhv_block_process(const char *target)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS && !defined VASKM
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 000000000..c1f21a721
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#define __bhv_text __section(".bhv.text") noinline
+
+#ifndef VASKM // inside kernel tree
+#define __bhv_data __section(".bhv.data") noinline
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+#define __init_km
+#else // out of tree
+#define __init_km __init
+#endif // VASKM
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+extern bool bhv_initialized __ro_after_init;
+extern unsigned long *bhv_configuration_bitmap __ro_after_init;
+
+static inline bool is_bhv_initialized(void)
+{
+	BUG_ON(bhv_initialized && bhv_configuration_bitmap == NULL);
+	return bhv_initialized;
+}
+
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr);
+
+extern bool __bhv_init_done;
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys(void *address)
+{
+	BUG_ON(!address);
+
+	if (__bhv_init_done && is_vmalloc_or_module_addr(address)) {
+		struct page *p = bhv_vmalloc_to_page(address);
+		uint64_t offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		BUG_ON(!p);
+		BUG_ON(PageCompound(p));
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	} else {
+		return virt_to_phys(address);
+	}
+}
+#else /* CONFIG_BHV_VAS */
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+
+static inline phys_addr_t bhv_virt_to_phys(volatile void *address)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 000000000..35d267d47
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BHV_VAS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/bhv_trace.h include/bhv/bhv_trace.h
new file mode 100644
index 000000000..bee38e495
--- /dev/null
+++ include/bhv/bhv_trace.h
@@ -0,0 +1,50 @@
+#ifdef CONFIG_BHV_TRACEPOINTS
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM bhv
+
+#if !defined(__BHV_TRACE_H__) || defined(TRACE_HEADER_MULTI_READ)
+#define __BHV_TRACE_H__
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(bhv_trace_class,
+	TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+	TP_ARGS(bhv_target, bhv_backend, op),
+
+	TP_STRUCT__entry(
+		__field(uint32_t, bhv_target)
+		__field(uint32_t, bhv_backend)
+		__field(uint32_t, op)
+	),
+
+	TP_fast_assign(
+		__entry->bhv_target = bhv_target;
+		__entry->bhv_backend = bhv_backend;
+		__entry->op = op;
+	),
+
+	TP_printk("BHV hypercall: target=%u backend=%u op=%u",
+		  __entry->bhv_target, __entry->bhv_backend, __entry->op)
+);
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_start,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_end,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+#endif /* __BHV_TRACE_H__ */
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+
+#define TRACE_INCLUDE_PATH ../../include/bhv
+#define TRACE_INCLUDE_FILE bhv_trace
+
+#include <trace/define_trace.h>
+
+#endif /* CONFIG_BHV_TRACEPOINTS */
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 000000000..7ff39fd6d
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_CREDS, bhv_configuration_bitmap);
+}
+
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+#ifdef VASKM // out of tree
+int bhv_cred_assign_init(struct task_struct *t);
+#endif // VASKM
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+/******************************************************************
+ * init
+ ******************************************************************/
+int __init bhv_init_cred(void);
+/******************************************************************/
+
+/******************************************************************
+ * mm_init
+ ******************************************************************/
+void __init bhv_mm_init_cred(void);
+/******************************************************************/
+
+#else /* CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+#ifdef VASKM // out of tree
+static inline int bhv_cred_assign_init(struct task_struct *t) {
+	return 0;
+}
+#endif // VASKM
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/domain.h include/bhv/domain.h
new file mode 100644
index 000000000..18c7e5447
--- /dev/null
+++ include/bhv/domain.h
@@ -0,0 +1,225 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_H__
+#define __BHV_DOMAIN_H__
+
+#include <linux/mem_namespace.h>
+#include <linux/sched.h>
+#include <linux/sched/task.h>
+#include <linux/mm_types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#include <asm/bhv/domain.h>
+#endif
+
+#define BHV_INIT_DOMAIN         0UL
+#define BHV_INVALID_DOMAIN      (-1UL)
+
+#ifdef CONFIG_MEM_NS
+
+#if defined(CONFIG_X86_64)
+// #define BHV_VAS_DOMAIN_DEBUG 1
+#endif
+
+DECLARE_PER_CPU(uint64_t, bhv_domain_current_domain);
+extern bool bhv_domain_initialized;
+
+/*
+ * As long as the memory namespaces are not part of the official Linux mainline
+ * sources, they re-purpose the PID namespace instantiation request for their
+ * own creation. Once user space tools become aware of Linux memory namespaces,
+ * we will have to remove this function; otherwise, flags requesting both memory
+ * and PID namespaces would create two memory namespaces.
+ */
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	if (flags & (CLONE_NEWMEM | CLONE_NEWPID))
+		return true;
+
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_STRONG_ISOLATION,
+			  bhv_configuration_bitmap);
+}
+
+// Initialized and enabled
+static inline bool bhv_domain_is_active(void)
+{
+	if (!bhv_domain_initialized)
+		return false;
+
+	return bhv_domain_is_enabled();
+}
+
+int bhv_domain_mm_init(void);
+
+int bhv_domain_create(uint64_t *domid);
+void bhv_domain_destroy(uint64_t domid);
+int bhv_domain_switch(uint64_t domid);
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns);
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma,
+		      unsigned int gup_flags);
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma,
+				     bool write, bool foreign);
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte);
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte);
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd);
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud);
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte);
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd);
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud);
+
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec);
+
+static inline uint64_t bhv_get_active_domain(void)
+{
+	return this_cpu_read(bhv_domain_current_domain);
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return memns_of_task(task)->domain;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+	uint64_t domain_next = bhv_get_domain(next);
+	bhv_domain_switch(domain_next);
+}
+
+static inline pgd_t *bhv_domain_get_user_pgd(const pgd_t *pgd)
+{
+	pgd_t *pgd_normalized = (pgd_t *)pgd;
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	/*
+	 * We need to ensure that the kernel was not configured to disable KPTI,
+	 * despite CONFIG_PAGE_TABLE_ISOLATION being set. Only then, we can
+	 * normalize the PGD pointer. The normalized PGD pointer ensures that
+	 * BRASS becomes able to always associate both user and kernel memory
+	 * accesses with the virtual address space, and the domain it belongs
+	 * to.
+	 */
+	if (static_cpu_has(X86_FEATURE_PTI))
+		pgd_normalized = bhv_domain_arch_get_user_pgd(pgd_normalized);
+#endif
+	return pgd_normalized;
+}
+
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm);
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+				  struct mm_struct *mm);
+#else
+static inline void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+						struct mm_struct *mm)
+{
+}
+#endif
+
+#else /* CONFIG_MEM_NS */
+
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_domain_create(uint64_t *domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_destroy(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_switch(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_transfer_mm(struct mm_struct *const mm,
+					 struct nsproxy *const old_ns,
+					 struct nsproxy *const new_ns)
+{
+	return 0;
+}
+
+static inline int bhv_domain_report(const struct task_struct *t,
+				    const struct mm_struct *mm_target,
+				    const struct vm_area_struct *vma,
+				    unsigned int gup_flags)
+{
+	return 0;
+}
+
+static inline bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma,
+				     		   bool write, bool foreign)
+{
+	return true;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return 0;
+}
+
+static inline void bhv_domain_set_pte_at_kernel(struct mm_struct *mm,
+						unsigned long addr, pte_t *ptep,
+						pte_t pte)
+{
+}
+
+static inline void bhv_domain_clear_pte(struct mm_struct *mm,
+					unsigned long addr, pte_t *ptep,
+					pte_t pte)
+{
+}
+
+static inline int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn,
+					uint64_t nr_pages, bool read,
+					bool write, bool exec)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* __BHV_DOMAIN_H__ */
diff --git include/bhv/event.h include/bhv/event.h
new file mode 100644
index 000000000..690997f19
--- /dev/null
+++ include/bhv/event.h
@@ -0,0 +1,128 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_EVENT_H__
+#define __BHV_EVENT_H__
+
+#include <linux/fs.h>
+#include <linux/dcache.h>
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+static inline int get_file_path(struct path *f_path, char *buf, size_t max_size)
+{
+	char *tmp_buf = NULL;
+	char *path = NULL;
+
+	BUG_ON(f_path == NULL);
+	BUG_ON(buf == NULL);
+
+	/* Avoid allocating this buffer on the stack. */
+	tmp_buf = kmalloc(GuestConnABI__GuestLog__Context__MAX_PATH_SZ,
+			  GFP_KERNEL);
+	if (tmp_buf == NULL)
+		return -ENOMEM;
+
+	path = d_path(f_path, tmp_buf,
+		      GuestConnABI__GuestLog__Context__MAX_PATH_SZ);
+	if (IS_ERR(path)) {
+		kfree(tmp_buf);
+		return PTR_ERR(path);
+	}
+
+	strncpy(buf, path, max_size);
+	buf[max_size - 1] = '\0';
+
+	kfree(tmp_buf);
+
+	return 0;
+}
+
+static inline int get_path_from_task(struct task_struct *task, char *buf,
+				     bool get_full_path)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+
+	if (!get_full_path || task->active_mm == NULL ||
+	    task->active_mm->exe_file == NULL) {
+		strncpy(buf, task->comm,
+			GuestConnABI__GuestLog__Context__MAX_PATH_SZ);
+		buf[GuestConnABI__GuestLog__Context__MAX_PATH_SZ - 1] = '\0';
+		return 0;
+	}
+
+	return get_file_path(&task->active_mm->exe_file->f_path, buf,
+			     GuestConnABI__GuestLog__Context__MAX_PATH_SZ);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+static inline void _copy_cap(GuestConnABI__GuestLog__Context__T *context)
+{
+	u64 val = current_cred_xxx(cap_effective).val;
+	memcpy(&context->cap_effective, &val, sizeof(context->cap_effective));
+	val = current_cred_xxx(cap_permitted).val;
+	memcpy(&context->cap_permitted, &val, sizeof(context->cap_permitted));
+}
+#else
+static inline void _copy_cap(GuestConnABI__GuestLog__Context__T *context)
+{
+	memcpy(&context->cap_effective, &current_cred_xxx(cap_effective).cap[0],
+	       sizeof(context->cap_effective));
+	memcpy(&context->cap_permitted, &current_cred_xxx(cap_permitted).cap[0],
+	       sizeof(context->cap_permitted));
+}
+#endif
+
+static inline int
+populate_event_context(GuestConnABI__GuestLog__Context__T *context,
+		       bool get_full_path)
+{
+	int rv;
+
+	BUG_ON(context == NULL);
+
+	context->vcpu_id = raw_smp_processor_id();
+	context->uid = current_uid().val;
+	context->euid = current_euid().val;
+	context->gid = current_gid().val;
+	context->egid = current_egid().val;
+	_copy_cap(context);
+	context->pid = current->tgid;
+
+	rv = get_path_from_task(current, context->comm, get_full_path);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	if (current->real_parent != NULL) {
+		context->parent_pid = current->real_parent->tgid;
+
+		rv = get_path_from_task(current->real_parent,
+					context->parent_comm, get_full_path);
+		if (rv != 0) {
+			context->valid = false;
+			return rv;
+		}
+	} else {
+		context->parent_pid = 0;
+		context->parent_comm[0] = '\0';
+	}
+
+	context->valid = true;
+	return 0;
+}
+
+#endif /* __BHV_EVENT_H__ */
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 000000000..e5f21de1d
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/file_protection.h>
+
+extern bhv_file_protection_config_t bhv_file_protection_config __ro_after_init;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void);
+/***********************************************************************/
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILE_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!test_bit(
+		    BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION,
+		    (unsigned long *)&bhv_file_protection_config.feature_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_fileops_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!test_bit(
+		    BHV_VAS_FILE_PROTECTION_FEATURE_FILEOPS_PROTECTION,
+		    (unsigned long *)&bhv_file_protection_config.feature_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_dirtycred_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!test_bit(
+		    BHV_VAS_FILE_PROTECTION_FEATURE_DIRTYCRED_MITIGATION,
+		    (unsigned long *)&bhv_file_protection_config.feature_bitmap))
+		return false;
+
+	return true;
+}
+
+
+bool bhv_block_read_only_file_write(const char *target, bool dirtycred);
+void bhv_check_file_dirty_cred(struct file* file, int mask);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+static void bhv_check_file_dirty_cred(struct file*, int) {
+	return;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_internal.h include/bhv/fileops_internal.h
new file mode 100644
index 000000000..4bebccef2
--- /dev/null
+++ include/bhv/fileops_internal.h
@@ -0,0 +1,45 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS__
+#define __BHV_FILEOPS__
+
+// used by security/bhv.c
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h> // simple_dir_operations
+#include <linux/ramfs.h> // ramfs_file_operations
+#include <linux/printk.h> // kmsg_fops
+#include <linux/mnt_namespace.h> // proc_mount{s,stats,info}_operations
+
+#include <bhv/interface/file_protection.h>
+
+#define FOPS(sym) extern const struct file_operations sym;
+#include <bhv/fileops_internal_symlist.h>
+
+typedef const struct file_operations *fops_t[2];
+
+// basic regular file + directory file ops
+#ifndef VASKM // inside kernel tree
+extern const fops_t fileops_map[];
+#else // out of tree
+extern fops_t *fileops_map;
+#endif // VASKM
+
+// additional /proc/ file operations
+extern struct file_operations const *proc_fops[] __ro_after_init;
+
+/******************************************************************
+ * init
+ ******************************************************************/
+void __init bhv_init_fileops(void);
+/******************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **);
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr);
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS__ */
diff --git include/bhv/fileops_internal_fopsmap.h include/bhv/fileops_internal_fopsmap.h
new file mode 100644
index 000000000..7a1d1ddb0
--- /dev/null
+++ include/bhv/fileops_internal_fopsmap.h
@@ -0,0 +1,46 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#if defined CONFIG_EXT4_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // ext4 files and dirs
+FOPS_MAP(ext4, BHV_VAS_FILEOPS_PROTECTION_EXT4, ext4_file_operations, ext4_dir_operations)
+#endif
+        // tmpfs files and dirs
+FOPS_MAP(tmpfs, BHV_VAS_FILEOPS_PROTECTION_TMPFS, shmem_file_operations, simple_dir_operations)
+        // sockets
+FOPS_MAP_DIRNULL(sockfs, BHV_VAS_FILEOPS_PROTECTION_SOCKET, socket_file_ops)
+        // pipes
+FOPS_MAP_DIRNULL(pipefs, BHV_VAS_FILEOPS_PROTECTION_PIPE, pipefifo_fops)
+        // special chardevs
+#if defined CONFIG_DEVMEM || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+FOPS_MAP_DIRNULL(mem, BHV_VAS_FILEOPS_PROTECTION_MEM, mem_fops)
+#endif
+FOPS_MAP_DIRNULL(null, BHV_VAS_FILEOPS_PROTECTION_NULL, null_fops)
+FOPS_MAP_DIRNULL(port, BHV_VAS_FILEOPS_PROTECTION_PORT, port_fops)
+FOPS_MAP_DIRNULL(zero, BHV_VAS_FILEOPS_PROTECTION_ZERO, zero_fops)
+FOPS_MAP_DIRNULL(full, BHV_VAS_FILEOPS_PROTECTION_FULL, full_fops)
+FOPS_MAP_DIRNULL(random, BHV_VAS_FILEOPS_PROTECTION_RANDOM, random_fops)
+FOPS_MAP_DIRNULL(urandom, BHV_VAS_FILEOPS_PROTECTION_URANDOM, urandom_fops)
+FOPS_MAP_DIRNULL(kmsg, BHV_VAS_FILEOPS_PROTECTION_KMSG, kmsg_fops)
+FOPS_MAP_DIRNULL(tty, BHV_VAS_FILEOPS_PROTECTION_TTY, tty_fops)
+FOPS_MAP_DIRNULL(console, BHV_VAS_FILEOPS_PROTECTION_CONSOLE, console_fops)
+        // proc basic
+FOPS_MAP(proc, BHV_VAS_FILEOPS_PROTECTION_PROC, proc_reg_file_ops, proc_root_operations)
+#if defined CONFIG_XFS_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // xfs files and dirs
+FOPS_MAP(xfs, BHV_VAS_FILEOPS_PROTECTION_XFS, xfs_file_operations, xfs_dir_file_operations)
+#endif
+        // sys fs
+FOPS_MAP(sysfs, BHV_VAS_FILEOPS_PROTECTION_SYSFS, kernfs_file_fops, kernfs_dir_fops)
+
+#undef FOPS_MAP
+#undef FOPS_MAP_DIRNULL
+#ifdef FILEOPS_INTERNAL_FOPSMAP_ALL
+#undef FILEOPS_INTERNAL_FOPSMAP_ALL
+#endif
diff --git include/bhv/fileops_internal_symlist.h include/bhv/fileops_internal_symlist.h
new file mode 100644
index 000000000..1c250cb8f
--- /dev/null
+++ include/bhv/fileops_internal_symlist.h
@@ -0,0 +1,170 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#ifndef FOPS_PROC
+#define FOPS_PROC FOPS
+#endif
+
+// fs/char_dev.c
+FOPS(def_chr_fops)
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+FOPS(ext4_dir_operations)
+FOPS(ext4_file_operations)
+#endif
+// mm/shmem.c
+#ifdef CONFIG_SHMEM
+FOPS(shmem_file_operations)
+#else
+FOPS(ramfs_file_operations)
+#define shmem_file_operations ramfs_file_operations
+#endif
+// fs/libfs.c
+FOPS(simple_dir_operations)
+// drivers/char/mem.c
+#ifdef CONFIG_DEVMEM
+FOPS(mem_fops)
+#endif
+FOPS(null_fops)
+FOPS(port_fops)
+FOPS(zero_fops)
+FOPS(full_fops)
+// drivers/char/random.c
+FOPS(random_fops)
+FOPS(urandom_fops)
+// kernel/printk/printk.c
+FOPS(kmsg_fops)
+// drivers/tty/tty_io.c
+FOPS(tty_fops)
+FOPS(console_fops)
+FOPS(hung_up_tty_fops)
+
+#ifdef CONFIG_XFS_FS
+// fs/xfs/xfs_iops.h
+FOPS(xfs_dir_file_operations)
+FOPS(xfs_file_operations)
+#endif
+
+// net/sockets.c
+FOPS(socket_file_ops)
+// fs/pipe.c
+FOPS(pipefifo_fops)
+
+// fs/kernfs/file.c
+FOPS(kernfs_file_fops)
+// fs/kernfs/dir.c
+FOPS(kernfs_dir_fops)
+
+// DEBUGFS
+// sys/kernel/debug
+FOPS(debugfs_noop_file_operations)
+FOPS(debugfs_open_proxy_file_operations)
+FOPS(debugfs_full_proxy_file_operations)
+
+// PROC:
+// fs/proc/inode.c
+FOPS_PROC(proc_reg_file_ops)
+FOPS_PROC(proc_iter_file_ops)
+#ifdef CONFIG_COMPAT
+FOPS_PROC(proc_reg_file_ops_compat)
+FOPS_PROC(proc_iter_file_ops_compat)
+#endif
+// fs/proc/root.c
+FOPS_PROC(proc_root_operations)
+// fs/proc/proc_sysctl.c
+FOPS_PROC(proc_sys_file_operations)
+FOPS_PROC(proc_sys_dir_file_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fd_operations)
+// fs/proc/base.c
+FOPS_PROC(proc_oom_score_adj_operations)
+FOPS_PROC(proc_pid_cmdline_ops)
+#ifdef CONFIG_LATENCYTOP
+FOPS_PROC(proc_lstats_operations)
+#endif
+FOPS_PROC(proc_mem_operations)
+FOPS_PROC(proc_environ_operations)
+FOPS_PROC(proc_auxv_operations)
+FOPS_PROC(proc_oom_adj_operations)
+FOPS_PROC(proc_loginuid_operations)
+#ifdef CONFIG_AUDIT
+FOPS_PROC(proc_sessionid_operations)
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+FOPS_PROC(proc_fault_inject_operations)
+FOPS_PROC(proc_fail_nth_operations)
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+FOPS_PROC(proc_pid_sched_operations)
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+FOPS_PROC(proc_pid_sched_autogroup_operations)
+#endif
+#ifdef CONFIG_TIME_NS
+FOPS_PROC(proc_timens_offsets_operations)
+#endif
+FOPS_PROC(proc_pid_set_comm_operations)
+FOPS_PROC(proc_map_files_operations)
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+FOPS_PROC(proc_timers_operations)
+#endif
+FOPS_PROC(proc_pid_set_timerslack_ns_operations)
+#ifdef CONFIG_SECURITY
+FOPS_PROC(proc_pid_attr_operations)
+FOPS_PROC(proc_attr_dir_operations)
+#endif
+#ifdef CONFIG_ELF_CORE
+FOPS_PROC(proc_coredump_filter_operations)
+#endif
+#ifdef CONFIG_USER_NS
+FOPS_PROC(proc_uid_map_operations)
+FOPS_PROC(proc_gid_map_operations)
+FOPS_PROC(proc_projid_map_operations)
+FOPS_PROC(proc_setgroups_operations)
+#endif
+FOPS_PROC(proc_tgid_base_operations)
+FOPS_PROC(proc_tid_base_operations)
+FOPS_PROC(proc_task_operations)
+FOPS_PROC(proc_single_file_operations)
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+FOPS_PROC(proc_zram_block_state_op)
+#endif
+#ifdef CONFIG_PAGE_OWNER
+// mm/page_owner.c
+FOPS_PROC(proc_page_owner_operations)
+#endif
+// fs/proc/internal.h
+FOPS_PROC(proc_ns_dir_operations)
+FOPS_PROC(proc_net_operations)
+FOPS_PROC(proc_pid_maps_operations)
+#ifdef CONFIG_NUMA
+FOPS_PROC(proc_pid_numa_maps_operations)
+#endif
+FOPS_PROC(proc_pid_smaps_operations)
+FOPS_PROC(proc_pid_smaps_rollup_operations)
+FOPS_PROC(proc_clear_refs_operations)
+FOPS_PROC(proc_pagemap_operations)
+#ifdef CONFIG_PROC_CHILDREN
+FOPS_PROC(proc_tid_children_operations)
+#endif
+// fs/proc/generic.c
+FOPS_PROC(proc_dir_operations)
+// fs/proc/fd.h
+FOPS_PROC(proc_fdinfo_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fdinfo_file_operations)
+// fs/proc_namespace.c
+FOPS_PROC(proc_mounts_operations)
+FOPS_PROC(proc_mountinfo_operations)
+FOPS_PROC(proc_mountstats_operations)
+
+FOPS_PROC(empty_dir_operations)
+
+#undef FOPS
+#undef FOPS_PROC
\ No newline at end of file
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 000000000..1bdd71a05
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <linux/init.h>
+#include <bhv/interface/common.h>
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_fileops_protection(void);
+/***********************************************************************/
+
+bool bhv_strict_fileops_enforced(void);
+bool bhv_block_fileops(const char *, u8, bool, const void *);
+u8 bhv_fileops_type(u32 fs_magic);
+bool bhv_fileops_is_ro(u64 f_op);
+
+#endif // CONFIG_BHV_VAS
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 000000000..bcd7d6d69
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_H__
+#define __BHV_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+#define BHV_GUESTCONN_MAX_PAYLOAD_SZ \
+	GuestConnABI__MAX_MSG_SZ - GuestConnABI__Header__SZ
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port);
+/*********************************************************/
+
+/*********************************************************
+ * start
+ *********************************************************/
+void bhv_start_guestconn(void);
+/*********************************************************/
+
+/*********************************************************
+ * mm_init
+ *********************************************************/
+void __init bhv_mm_init_guestconn(void);
+/*********************************************************/
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void);
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg);
+
+int bhv_guestconn_send(uint16_t type, GuestConnABI__Header__T *data,
+		       size_t size);
+
+#endif /* __BHV_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 000000000..e3bcc67b3
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,87 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#include <linux/cgroup-defs.h>
+#include <linux/types.h>
+#include <linux/binfmts.h>
+#include <linux/nsproxy.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init;
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestlog(void);
+/*********************************************************/
+
+static inline bool bhv_guestlog_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(
+		bhv_configuration_bitmap);
+}
+
+#define BHV_GUESTLOG_EVENT_ENABLED(EVT)                         \
+	(bhv_guestlog_enabled() && bhv_guestlog_config.valid && \
+	 HypABI__Guestlog__Init__GuestlogFlags__has_##EVT(      \
+		 (HypABI__Guestlog__Init__GuestlogFlags__T      \
+			  *)&bhv_guestlog_config.log_bitmap))
+
+static inline bool bhv_guestlog_log_process_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(PROCESS_EVENTS);
+}
+static inline bool bhv_guestlog_log_driver_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(DRIVER_EVENTS);
+}
+static inline bool bhv_guestlog_log_kaccess_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_ACCESS);
+}
+static inline bool bhv_guestlog_log_unknown_fileops(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(UNKNOWN_FILEOPS);
+}
+static inline bool bhv_guestlog_log_kernel_exec_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_EXEC_EVENTS);
+}
+static inline bool bhv_guestlog_log_container_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(CONTAINER_EVENTS);
+}
+
+#undef BHV_GUESTLOG_EVENT_ENABLED
+
+int bhv_guestlog_log_str(char *fmt, ...);
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm);
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm);
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_driver_load(const char *name);
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t event_id);
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint64_t event_id, const void *fops_ptr);
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp);
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp);
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp);
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset);
+
+#endif /* __BHV_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 000000000..8f4ebeb4f
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_GUEST_POLICY,
+			      bhv_configuration_bitmap);
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init/init.h include/bhv/init/init.h
new file mode 100644
index 000000000..6ccb63e9b
--- /dev/null
+++ include/bhv/init/init.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_INIT_H__
+#define __BHV_INIT_INIT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/integrity.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+#define BHV_INIT_MAX_REGIONS 7
+
+void __init bhv_init_platform(void);
+void __init bhv_init_arch(void);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_init_platform(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_INIT_H__ */
diff --git include/bhv/init/late_start.h include/bhv/init/late_start.h
new file mode 100644
index 000000000..af4dc71e2
--- /dev/null
+++ include/bhv/init/late_start.h
@@ -0,0 +1,17 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_LATE_START_H__
+#define __BHV_INIT_LATE_START_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_late_start(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_late_start(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_LATE_START_H__ */
diff --git include/bhv/init/mm_init.h include/bhv/init/mm_init.h
new file mode 100644
index 000000000..48578cf8b
--- /dev/null
+++ include/bhv/init/mm_init.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_MM_INIT_H__
+#define __BHV_INIT_MM_INIT_H__
+#ifdef CONFIG_BHV_VAS
+void __init bhv_mm_init(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_mm_init(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_MM_INIT_H__ */
diff --git include/bhv/init/start.h include/bhv/init/start.h
new file mode 100644
index 000000000..23752340d
--- /dev/null
+++ include/bhv/init/start.h
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_START_H__
+#define __BHV_INIT_START_H__
+
+#ifdef CONFIG_BHV_VAS
+bool bhv_start(void);
+int bhv_start_arch(void);
+#else /* CONFIG_BHV_VAS */
+static inline bool bhv_start(void)
+{
+	return true;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_START_H__ */
diff --git include/bhv/inode.h include/bhv/inode.h
new file mode 100644
index 000000000..1d830e511
--- /dev/null
+++ include/bhv/inode.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INODE_H__
+#define __BHV_INODE_H__
+
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_inode_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_INODE, bhv_configuration_bitmap);
+}
+
+int __init bhv_inode_init(void);
+
+/* LSM Hooks */
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   struct file *file);
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old,
+			      int flags);
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old,
+			      int flags);
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode);
+
+void bhv_inode_iput_final(struct inode *inode);
+
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid, umode_t mode);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_inode_iput_final(i)			do { } while(0)
+#define bhv_inode_post_setattr(d,v,m)		do { } while(0)
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INODE_H__ */
+
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 000000000..0f507719a
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,281 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+typedef union {
+	HypABI__Integrity__Create__Mem_Region__T create;
+	HypABI__Integrity__Update__Mem_Region__T update;
+	HypABI__Integrity__Remove__Mem_Region__T remove;
+} bhv_mem_region_t;
+
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * start
+ ************************************************************/
+int bhv_start_integrity_arch(void);
+void __init_km bhv_start_ptpg(void);
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value);
+/************************************************************/
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void);
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+int bhv_late_start_init_ptpg(void);
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg);
+/************************************************************/
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(
+		bhv_configuration_bitmap);
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	if (!bhv_integrity_is_enabled())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(
+		bhv_configuration_bitmap);
+}
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
+extern bool bhv_integrity_freeze_create_currently_frozen;
+extern bool bhv_integrity_freeze_update_currently_frozen;
+extern bool bhv_integrity_freeze_remove_currently_frozen;
+extern bool bhv_integrity_freeze_patch_currently_frozen;
+int bhv_integrity_freeze_events(uint64_t flags);
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks);
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head);
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner);
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm);
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline void bhv_mem_region_create_ctor(bhv_mem_region_t *curr_item,
+					      bhv_mem_region_t *prev_item,
+					      uint64_t addr, uint64_t size,
+					      uint32_t type, uint64_t flags,
+					      const char *label)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_mem_region_t){
+		.create =
+			(HypABI__Integrity__Create__Mem_Region__T){
+				.start_addr = addr,
+				.size = size,
+				.type = type,
+				.flags = flags,
+				.next = BHV_INVALID_PHYS_ADDR,
+			}
+	};
+	strncpy(curr_item->create.label, label,
+		HypABI__Integrity__MAX_LABEL_SIZE);
+	curr_item->create.label[HypABI__Integrity__MAX_LABEL_SIZE - 1] = '\0';
+
+	if (prev_item)
+		prev_item->create.next = bhv_virt_to_phys(curr_item);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags, const char *label)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	bhv_mem_region_create_ctor(&n->region, NULL, addr, size, type, flags,
+				   label);
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.create.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.update.start_addr = addr;
+	n->region.update.type = type;
+	n->region.update.flags = flags;
+	n->region.update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.update.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.remove.start_addr = addr;
+	n->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.remove.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_integrity_freeze_events(uint64_t)
+{
+	return 0;
+}
+
+static inline int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return 0;
+}
+
+static inline void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+}
+
+#define bhv_allow_kmod_loads true
+#define bhv_allow_patch true
+#define bhv_integrity_freeze_create_currently_frozen false
+#define bhv_integrity_freeze_update_currently_frozen false
+#define bhv_integrity_freeze_remove_currently_frozen false
+#define bhv_integrity_freeze_patch_currently_frozen false
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/interface/abi_base_autogen.h include/bhv/interface/abi_base_autogen.h
new file mode 100644
index 000000000..314a53b12
--- /dev/null
+++ include/bhv/interface/abi_base_autogen.h
@@ -0,0 +1,794 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-06-25T12:28:17).
+ */
+
+#pragma once
+
+#include <linux/types.h>
+
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#define BHV__HypABI__VAS__TARGET_ID 1
+
+_Static_assert(sizeof(char) == sizeof(uint8_t), "Unexpected char size");
+
+typedef uint64_t HypABI__MemoryRegionOwner;
+
+#define HypABI__Init__BACKEND_ID 1
+
+#define HypABI__Init__Init__OP_ID 0
+
+enum HypABI__Init__Init__BHVSectionRun__BHVSectionRunType {
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__VAULT = 42,
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA = 43,
+};
+
+
+// start of HypABI__Init__Init__BHVSectionRun
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVSectionRun {
+        /** Guest-physical start address of this run. Must be page-aligned. */
+        uint64_t gpa_start;
+        /** Size in bytes. Must be a non-zero multiple of the page size. */
+        uint64_t size;
+        /** Type of the run */
+        uint8_t type;
+        /** The guest-physical address of the next item in the linked list, or `INVALID_GPA` if this item is the last one. */
+        uint64_t next;
+};
+typedef struct HypABI__Init__Init__BHVSectionRun HypABI__Init__Init__BHVSectionRun__T;
+#define HypABI__Init__Init__BHVSectionRun__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVSectionRun) == HypABI__Init__Init__BHVSectionRun__SZ, "Unexpected size for HypABI__Init__Init__BHVSectionRun");
+
+
+// start of HypABI__Init__Init__BHVData__HypABI__Init__Init__BHVData__BHVConfigBitmap
+
+typedef unsigned long HypABI__Init__Init__BHVData__BHVConfigBitmap__T;
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__NONE 0UL
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT 0
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY (1UL<<0)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT 1
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL (1UL<<1)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT 2
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL (1UL<<2)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT 3
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING (1UL<<3)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT 4
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY (1UL<<4)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT 5
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION (1UL<<5)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT 6
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY (1UL<<6)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT 7
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION (1UL<<7)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT 8
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION (1UL<<8)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT 9
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT (1UL<<9)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT 10
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY (1UL<<10)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT 11
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY (1UL<<11)
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr);
+
+
+// start of HypABI__Init__Init__BHVData
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVData {
+        /** This is a bitmap that is used to communicate which mechanisms the host has enabled such that the guest can leverage them. */
+        uint64_t config_bitmap;
+        /** This field contains the cid of the vsocket endpoint the guest should can connect to. If this value is `0`, this indicates the host is not listening for a vsocket connection. */
+        uint32_t vsocket_cid;
+        /** This field contains the port of the vsocket endpoint the guest should can connect to. If the `vsocket_cid` value is `0`, this value is to be ignored. */
+        uint32_t vsocket_port;
+};
+typedef struct HypABI__Init__Init__BHVData HypABI__Init__Init__BHVData__T;
+#define HypABI__Init__Init__BHVData__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVData) == HypABI__Init__Init__BHVData__SZ, "Unexpected size for HypABI__Init__Init__BHVData");
+
+
+// start of HypABI__Init__Init__arg
+
+struct __attribute__((packed)) HypABI__Init__Init__arg {
+        /** The size of the buffer passed in `modprobe_path`. This field is ignored if `modprobe_path == INVALID_GPA`. */
+        uint64_t modprobe_path_sz;
+        /** The physical address of the guest's modprobe path. In certain cases, this can be set from the host. This field can be set to `INVALID_GPA` and it will be ignored by the host. */
+        uint64_t modprobe_path;
+        /** This establishes the owner id of the guest kernel. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0`. */
+        uint64_t owner;
+        /** The physical address of the head of a list of BHV-specific memory regions to be protected. */
+        uint64_t bhv_region_head;
+        /** The physical address of the head of a list of memory regions to be protected. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Init__Init__arg HypABI__Init__Init__arg__T;
+#define HypABI__Init__Init__arg__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__arg) == HypABI__Init__Init__arg__SZ, "Unexpected size for HypABI__Init__Init__arg");
+
+#define HypABI__Init__Start__OP_ID 1
+
+
+// start of HypABI__Init__Start__arg
+
+struct __attribute__((packed)) HypABI__Init__Start__arg {
+        /** This field is used to indicate whether the `data_sz` and `data` fields are valid. When this field is set to `false`, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        uint8_t padding[1];
+        /** The guest is expected to provide the size of the buffer (in 4k pages) that the `bhv_init_start_config_t` struct resides in. The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire guest policy into the `data` field of the `bhv_init_start_config_t` struct. */
+        uint16_t num_pages;
+        /** If the `valid` field is true, this field contains the size of buffer in bytes stored in the `data` field. */
+        uint32_t data_sz;
+        /** If the `valid` field is true and the `data_sz` field is greater than 0, this field contains the guest policy buffer. This can be used to load an arbitrary guest policy into the guest from the host. For example, this is used by the BHV Linux kernel to load SELinux policies into the guest. */
+        uint8_t data[];
+};
+typedef struct HypABI__Init__Start__arg HypABI__Init__Start__arg__T;
+#define HypABI__Init__Start__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Init__Start__arg) == HypABI__Init__Start__arg__SZ, "Unexpected size for HypABI__Init__Start__arg");
+
+#define HypABI__Integrity__BACKEND_ID 2
+
+enum HypABI__Integrity__MemType {
+        HypABI__Integrity__MemType__UNKNOWN = 0,
+        HypABI__Integrity__MemType__CODE = 1,
+        HypABI__Integrity__MemType__CODE_WRITABLE = 2,
+        HypABI__Integrity__MemType__CODE_PATCHABLE = 3,
+        HypABI__Integrity__MemType__DATA = 4,
+        HypABI__Integrity__MemType__DATA_READ_ONLY = 5,
+        HypABI__Integrity__MemType__VDSO = 6,
+        HypABI__Integrity__MemType__VVAR = 7,
+};
+
+
+// start of HypABI__Integrity__HypABI__Integrity__MemFlags
+
+typedef unsigned long HypABI__Integrity__MemFlags__T;
+#define HypABI__Integrity__MemFlags__NONE 0UL
+#define HypABI__Integrity__MemFlags__TRANSIENT__BIT 0
+#define HypABI__Integrity__MemFlags__TRANSIENT (1UL<<0)
+#define HypABI__Integrity__MemFlags__MUTABLE__BIT 1
+#define HypABI__Integrity__MemFlags__MUTABLE (1UL<<1)
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr);
+
+#define HypABI__Integrity__MAX_LABEL_SIZE 32ULL
+
+#define HypABI__Integrity__Create__OP_ID 0
+
+
+// start of HypABI__Integrity__Create__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Create__Mem_Region {
+        /** The start address of the memory range to create. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The size of the memory region to create. This must be greater than 0 and must be a multiple of 4K. */
+        uint64_t size;
+        /** The type of the memory region to create. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The flags associated with the memory region to create. */
+        uint64_t flags;
+        /** A custom label for this section. The purpose of this label is to easier identify a memory range if an event is observed. */
+        char label[HypABI__Integrity__MAX_LABEL_SIZE];
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Create__Mem_Region HypABI__Integrity__Create__Mem_Region__T;
+#define HypABI__Integrity__Create__Mem_Region__SZ 72ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__Mem_Region) == HypABI__Integrity__Create__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Create__Mem_Region");
+
+
+// start of HypABI__Integrity__Create__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Create__arg {
+        /** This establishes the owner id of this memory region. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0` and kernel modules/drivers use unique non-zero ids. This allows the user to remove regions by owner later. */
+        uint64_t owner;
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Create__arg HypABI__Integrity__Create__arg__T;
+#define HypABI__Integrity__Create__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__arg) == HypABI__Integrity__Create__arg__SZ, "Unexpected size for HypABI__Integrity__Create__arg");
+
+#define HypABI__Integrity__Update__OP_ID 1
+
+
+// start of HypABI__Integrity__Update__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Update__Mem_Region {
+        /** The start address of the memory range to update. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The type of the memory region to update to. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The new flags to associate with the region. */
+        uint64_t flags;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Update__Mem_Region HypABI__Integrity__Update__Mem_Region__T;
+#define HypABI__Integrity__Update__Mem_Region__SZ 32ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__Mem_Region) == HypABI__Integrity__Update__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Update__Mem_Region");
+
+
+// start of HypABI__Integrity__Update__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Update__arg {
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Update__arg HypABI__Integrity__Update__arg__T;
+#define HypABI__Integrity__Update__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__arg) == HypABI__Integrity__Update__arg__SZ, "Unexpected size for HypABI__Integrity__Update__arg");
+
+#define HypABI__Integrity__Remove__OP_ID 2
+
+
+// start of HypABI__Integrity__Remove__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__Mem_Region {
+        /** The start address of the memory range to remove. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Remove__Mem_Region HypABI__Integrity__Remove__Mem_Region__T;
+#define HypABI__Integrity__Remove__Mem_Region__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__Mem_Region) == HypABI__Integrity__Remove__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Remove__Mem_Region");
+
+
+// start of HypABI__Integrity__Remove__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__arg {
+        /** This parameter determines whether the call is used remove multiple sections by an owner id or whether to explicitly remove defined sections. */
+        uint8_t rm_by_owner;
+        uint8_t padding[7];
+        union {
+                /** This field is only considered if `rm_by_owner` is true. When considered, this field contains the owner id of one or more regions to be removed. */
+                uint64_t owner;
+                /** This field is only considered if `rm_by_owner` is false. When considered, this field is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+                uint64_t region_head;
+        };
+};
+typedef struct HypABI__Integrity__Remove__arg HypABI__Integrity__Remove__arg__T;
+#define HypABI__Integrity__Remove__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__arg) == HypABI__Integrity__Remove__arg__SZ, "Unexpected size for HypABI__Integrity__Remove__arg");
+
+#define HypABI__Integrity__Freeze__OP_ID 3
+
+
+// start of HypABI__Integrity__Freeze__HypABI__Integrity__Freeze__FreezeFlags
+
+typedef unsigned long HypABI__Integrity__Freeze__FreezeFlags__T;
+#define HypABI__Integrity__Freeze__FreezeFlags__NONE 0UL
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT 0
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE (1UL<<0)
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT 1
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE (1UL<<1)
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT 2
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE (1UL<<2)
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT 3
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH (1UL<<3)
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr);
+
+
+// start of HypABI__Integrity__Freeze__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Freeze__arg {
+        /** This field contains a bitfield that specifies which operations should be frozen. */
+        uint64_t flags;
+};
+typedef struct HypABI__Integrity__Freeze__arg HypABI__Integrity__Freeze__arg__T;
+#define HypABI__Integrity__Freeze__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Freeze__arg) == HypABI__Integrity__Freeze__arg__SZ, "Unexpected size for HypABI__Integrity__Freeze__arg");
+
+#define HypABI__Integrity__PtpgInit__OP_ID 4
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES 8ULL
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO 16ULL
+
+
+// start of HypABI__Integrity__PtpgInit__arg
+
+struct __attribute__((packed)) HypABI__Integrity__PtpgInit__arg {
+        /** This must contain the physical address of the kernel's page table. In Linux this is the `pgd` stored in `init_mm`. */
+        uint64_t init_pgd;
+        /** This must contain the number of paging levels the kernel is using.  Currently only 4 and 5 level paging is supported. */
+        uint8_t pt_levels;
+        uint8_t padding[3];
+        /** This must contain the number of ranges in the succeeding `ranges` array. This must be a number >= 0 and <= `MAX_RANGES`. */
+        uint32_t num_ranges;
+        /** This is an array of pairs of `uint64_t` values in which the first value represents the start GVA of a range and the second value of the pair represents the end GVA of a range. */
+        uint64_t ranges[HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO];
+};
+typedef struct HypABI__Integrity__PtpgInit__arg HypABI__Integrity__PtpgInit__arg__T;
+#define HypABI__Integrity__PtpgInit__arg__SZ 144ULL
+_Static_assert(sizeof(struct HypABI__Integrity__PtpgInit__arg) == HypABI__Integrity__PtpgInit__arg__SZ, "Unexpected size for HypABI__Integrity__PtpgInit__arg");
+
+#define HypABI__Integrity__PtpgReport__OP_ID 5
+
+#define HypABI__Patch__BACKEND_ID 3
+
+#define HypABI__Patch__MAX_PATCH_SZ 32ULL
+
+#define HypABI__Patch__Patch__OP_ID 0
+
+
+// start of HypABI__Patch__Patch__arg
+
+struct __attribute__((packed)) HypABI__Patch__Patch__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__Patch__arg HypABI__Patch__Patch__arg__T;
+#define HypABI__Patch__Patch__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__Patch__arg) == HypABI__Patch__Patch__arg__SZ, "Unexpected size for HypABI__Patch__Patch__arg");
+
+#define HypABI__Patch__PatchNoClose__OP_ID 1
+
+
+// start of HypABI__Patch__PatchNoClose__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchNoClose__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__PatchNoClose__arg HypABI__Patch__PatchNoClose__arg__T;
+#define HypABI__Patch__PatchNoClose__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchNoClose__arg) == HypABI__Patch__PatchNoClose__arg__SZ, "Unexpected size for HypABI__Patch__PatchNoClose__arg");
+
+#define HypABI__Patch__PatchViolation__OP_ID 2
+
+#define HypABI__Patch__PatchViolation__MAX_MSG_SZ 256ULL
+
+
+// start of HypABI__Patch__PatchViolation__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchViolation__arg {
+        /** The virtual address that the guest attempted to patch. */
+        uint64_t dest_virt_addr;
+        /** The physical address that the guest attempted to patch. This is the translated `dest_virt_addr`. */
+        uint64_t dest_phys_addr;
+        /** A custom message that provides additional information about the violation. This allows each subsystem (e.g. jump labels) to provide detailed information about the violation. */
+        char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+        /** This field is set by the host and will be processed by the guest after the hypercall. It tells the guest whether to block the violation or whether to allow the patch. */
+        uint8_t block;
+};
+typedef struct HypABI__Patch__PatchViolation__arg HypABI__Patch__PatchViolation__arg__T;
+#define HypABI__Patch__PatchViolation__arg__SZ 273ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchViolation__arg) == HypABI__Patch__PatchViolation__arg__SZ, "Unexpected size for HypABI__Patch__PatchViolation__arg");
+
+#define HypABI__Vault__BACKEND_ID 4
+
+#define HypABI__Vault__Open__OP_ID 0
+
+#define HypABI__Vault__Close__OP_ID 1
+
+#define HypABI__Acl__BACKEND_ID 5
+
+#define HypABI__Guestlog__BACKEND_ID 6
+
+#define HypABI__Guestlog__Init__OP_ID 0
+
+
+// start of HypABI__Guestlog__Init__HypABI__Guestlog__Init__GuestlogFlags
+
+typedef unsigned long HypABI__Guestlog__Init__GuestlogFlags__T;
+#define HypABI__Guestlog__Init__GuestlogFlags__NONE 0UL
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT 0
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS (1UL<<0)
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT 1
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS (1UL<<1)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT 2
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS (1UL<<2)
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT 3
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS (1UL<<3)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT 4
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS (1UL<<4)
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT 5
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS (1UL<<5)
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr);
+
+
+// start of HypABI__Guestlog__Init__arg
+
+struct __attribute__((packed)) HypABI__Guestlog__Init__arg {
+        /** This contains a bit map that is valid if the `valid` field is true and is written by the **host**. This bit map is used to indicate to the guest which logging information is to be sent to the host. */
+        uint64_t log_bitmap;
+        /** This field is written by the **host**. If this is false, no logging events are expected by the host. If this field is true, the `log_bitmap` field is valid and should be respected by the guest. */
+        uint8_t valid;
+};
+typedef struct HypABI__Guestlog__Init__arg HypABI__Guestlog__Init__arg__T;
+#define HypABI__Guestlog__Init__arg__SZ 9ULL
+_Static_assert(sizeof(struct HypABI__Guestlog__Init__arg) == HypABI__Guestlog__Init__arg__SZ, "Unexpected size for HypABI__Guestlog__Init__arg");
+
+#define HypABI__Creds__BACKEND_ID 7
+
+#define HypABI__FileProtection__BACKEND_ID 8
+
+#define HypABI__RegisterProtection__BACKEND_ID 9
+
+#define HypABI__Domain__BACKEND_ID 10
+
+#define HypABI__Inode__BACKEND_ID 11
+
+#define HypABI__Keyring__BACKEND_ID 12
+
+#define HypABI__Confserver__BACKEND_ID 13
+
+
+
+// GuestConnABI
+
+#define GuestConnABI__MAX_MSG_SZ 4096ULL
+
+
+// start of GuestConnABI__Header
+
+struct __attribute__((packed)) GuestConnABI__Header {
+        uint16_t backend;
+        uint16_t sz;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__Header GuestConnABI__Header__T;
+#define GuestConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__Header) == GuestConnABI__Header__SZ, "Unexpected size for GuestConnABI__Header");
+
+#define GuestConnABI__GuestLog__BACKEND_ID 0
+
+#define GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ 32ULL
+
+#define GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ 1024ULL
+
+#define GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ 128ULL
+
+#define GuestConnABI__GuestLog__PROC_COMM_SZ 16ULL
+
+
+// start of GuestConnABI__GuestLog__Header
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Header {
+        uint16_t evt;
+        uint16_t sz;
+};
+typedef struct GuestConnABI__GuestLog__Header GuestConnABI__GuestLog__Header__T;
+#define GuestConnABI__GuestLog__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Header) == GuestConnABI__GuestLog__Header__SZ, "Unexpected size for GuestConnABI__GuestLog__Header");
+
+#define GuestConnABI__GuestLog__Context__MAX_PATH_SZ 256ULL
+
+
+// start of GuestConnABI__GuestLog__Context
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Context {
+        /** This field indicates to the host whether the event context is valid. */
+        uint8_t valid;
+        uint8_t padding[3];
+        /** The ID of the vCPU that the event occurred on. */
+        uint32_t vcpu_id;
+        /** The user ID that was in use when the event occurred. This helps us to tie user information to an event. For example, it may allow us to answer the question which user account caused a certain security violation. Note that not this information is not always reliable. Just because an attack happened in a certain context does not necessarily mean that it was caused by the given user. */
+        uint32_t uid;
+        /** The effective user ID that was in use when the event occurred. */
+        uint32_t euid;
+        /** The group ID that was in use when the event occurred. */
+        uint32_t gid;
+        /** The and effective group ID that was in use when the event occurred. */
+        uint32_t egid;
+        /** The effective capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_effective;
+        /** The permitted capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_permitted;
+        /** The PID of the process that was in context when the event occurred. */
+        uint32_t pid;
+        /** The parent PID of the process that was in context when the event occurred. */
+        uint32_t parent_pid;
+        /** The PATH of the process that was in context when the event occurred. */
+        char comm[GuestConnABI__GuestLog__Context__MAX_PATH_SZ];
+        /** The PATH of the parent binary of the process that was in context when the event occurred. */
+        char parent_comm[GuestConnABI__GuestLog__Context__MAX_PATH_SZ];
+};
+typedef struct GuestConnABI__GuestLog__Context GuestConnABI__GuestLog__Context__T;
+#define GuestConnABI__GuestLog__Context__SZ 560ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Context) == GuestConnABI__GuestLog__Context__SZ, "Unexpected size for GuestConnABI__GuestLog__Context");
+
+
+// start of GuestConnABI__GuestLog__Message
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Message {
+        struct GuestConnABI__GuestLog__Header header;
+        struct GuestConnABI__GuestLog__Context context;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__GuestLog__Message GuestConnABI__GuestLog__Message__T;
+#define GuestConnABI__GuestLog__Message__SZ 564ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Message) == GuestConnABI__GuestLog__Message__SZ, "Unexpected size for GuestConnABI__GuestLog__Message");
+
+#define GuestConnABI__GuestLog__StringMsg__EVT_ID 0
+
+
+// start of GuestConnABI__GuestLog__StringMsg
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__StringMsg {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated string that will be printed to the host logs. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__StringMsg GuestConnABI__GuestLog__StringMsg__T;
+#define GuestConnABI__GuestLog__StringMsg__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__StringMsg) == GuestConnABI__GuestLog__StringMsg__SZ, "Unexpected size for GuestConnABI__GuestLog__StringMsg");
+
+#define GuestConnABI__GuestLog__ProcessFork__EVT_ID 1
+
+
+// start of GuestConnABI__GuestLog__ProcessFork
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessFork {
+        /** This is the cgroup namespace identifier for the parent process. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier for the parent process. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier for the parent process. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier for the parent process. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier for the parent process. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier for the parent process. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier for the parent process. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier for the parent process. */
+        uint32_t uts_ns_inum;
+        /** This is the cgroup identifier for the parent process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the parent process. */
+        char cgroup_name[GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ];
+        /** The process ID of the newly forked process. */
+        uint32_t child_pid;
+        /** The process ID of the parent that forked the process. */
+        uint32_t parent_pid;
+        /** The offset from the start of the `buf` field where the command string for the child process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t child_comm_offset;
+        /** The offset from the start of the `buf` field where the command string for the parent process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t parent_comm_offset;
+        /** A buffer to store `child_comm_offset` and `parent_comm_offset`. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessFork GuestConnABI__GuestLog__ProcessFork__T;
+#define GuestConnABI__GuestLog__ProcessFork__SZ 192ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessFork) == GuestConnABI__GuestLog__ProcessFork__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessFork");
+
+#define GuestConnABI__GuestLog__ProcessExec__EVT_ID 2
+
+
+// start of GuestConnABI__GuestLog__ProcessExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExec {
+        /** This is the cgroup namespace identifier for the current process. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier for the current process. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier for the current process. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier for the current process. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier for the current process. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier for the current process. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier for the current process. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier for the current process. */
+        uint32_t uts_ns_inum;
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ];
+        /** The process ID of the process that executed the binary. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** A NUL-terminated ASCII string that contains the path of the binary to be executed. */
+        char name[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExec GuestConnABI__GuestLog__ProcessExec__T;
+#define GuestConnABI__GuestLog__ProcessExec__SZ 2264ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExec) == GuestConnABI__GuestLog__ProcessExec__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExec");
+
+#define GuestConnABI__GuestLog__ProcessExit__EVT_ID 3
+
+
+// start of GuestConnABI__GuestLog__ProcessExit
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExit {
+        /** The process ID of the process that terminated. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** A NUL-terminated ASCII string that contains the path of the binary that was terminated. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExit GuestConnABI__GuestLog__ProcessExit__T;
+#define GuestConnABI__GuestLog__ProcessExit__SZ 8ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExit) == GuestConnABI__GuestLog__ProcessExit__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExit");
+
+#define GuestConnABI__GuestLog__DriverLoad__EVT_ID 4
+
+
+// start of GuestConnABI__GuestLog__DriverLoad
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriverLoad {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated ASCII string that contains the path of the binary that was loaded as a driver. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__DriverLoad GuestConnABI__GuestLog__DriverLoad__T;
+#define GuestConnABI__GuestLog__DriverLoad__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriverLoad) == GuestConnABI__GuestLog__DriverLoad__SZ, "Unexpected size for GuestConnABI__GuestLog__DriverLoad");
+
+#define GuestConnABI__GuestLog__KernelAccess__EVT_ID 5
+
+enum GuestConnABI__GuestLog__KernelAccess__AccessType {
+        GuestConnABI__GuestLog__KernelAccess__AccessType__READ = 0,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE = 1,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE = 2,
+};
+
+
+// start of GuestConnABI__GuestLog__KernelAccess
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelAccess {
+        uint64_t address;
+        uint8_t type;
+};
+typedef struct GuestConnABI__GuestLog__KernelAccess GuestConnABI__GuestLog__KernelAccess__T;
+#define GuestConnABI__GuestLog__KernelAccess__SZ 9ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelAccess) == GuestConnABI__GuestLog__KernelAccess__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelAccess");
+
+#define GuestConnABI__GuestLog__FopsUnknown__EVT_ID 6
+
+
+// start of GuestConnABI__GuestLog__FopsUnknown
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__FopsUnknown {
+        /** The file-system magic number as defined in [Linux](https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/magic.h). */
+        uint64_t magic;
+        /** 64-bit integer which denotes the type of the file struct using the unknown file operation instance: `0`: unknown, `1`: file, `2`: directory.<br/>In case the least significant byte of `type` is `3` (special file type, i.e. chardev), then `type` decodes in the following way:<br/>2nd least significant byte: `MINORBITS`<br/>3rd to 6th lead significant byte: [device](https://elixir.bootlin.com/linux/v6.1.38/source/include/linux/fs.h#L626) (`dev_t`) which can be used to decode the [major and minor device numbers](https://elixir.bootlin.com/linux/latest/source/include/linux/kdev_t.h). */
+        uint64_t type;
+        /** This is the address of the unknown operations instance used. */
+        uint64_t address;
+        /** The name of the accessed file or directory which used the unknown file operations instance. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__FopsUnknown GuestConnABI__GuestLog__FopsUnknown__T;
+#define GuestConnABI__GuestLog__FopsUnknown__SZ 24ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__FopsUnknown) == GuestConnABI__GuestLog__FopsUnknown__SZ, "Unexpected size for GuestConnABI__GuestLog__FopsUnknown");
+
+#define GuestConnABI__GuestLog__KernelExec__EVT_ID 7
+
+
+// start of GuestConnABI__GuestLog__KernelExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelExec {
+        /** The path to the program that is being executed. */
+        char path[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        /** The arguments passed to the program that is being executed. */
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        /** The environment passed to the program that is being executed. */
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__KernelExec GuestConnABI__GuestLog__KernelExec__T;
+#define GuestConnABI__GuestLog__KernelExec__SZ 2080ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelExec) == GuestConnABI__GuestLog__KernelExec__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelExec");
+
+#define GuestConnABI__GuestLog__CgroupCreate__EVT_ID 8
+
+
+// start of GuestConnABI__GuestLog__CgroupCreate
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupCreate {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupCreate GuestConnABI__GuestLog__CgroupCreate__T;
+#define GuestConnABI__GuestLog__CgroupCreate__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupCreate) == GuestConnABI__GuestLog__CgroupCreate__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupCreate");
+
+#define GuestConnABI__GuestLog__CgroupDestroy__EVT_ID 9
+
+
+// start of GuestConnABI__GuestLog__CgroupDestroy
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupDestroy {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupDestroy GuestConnABI__GuestLog__CgroupDestroy__T;
+#define GuestConnABI__GuestLog__CgroupDestroy__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupDestroy) == GuestConnABI__GuestLog__CgroupDestroy__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupDestroy");
+
+#define GuestConnABI__GuestLog__NamespaceChange__EVT_ID 10
+
+
+// start of GuestConnABI__GuestLog__NamespaceChange
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__NamespaceChange {
+        /** This is the cgroup namespace identifier for the target process. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier for the target process. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier for the target process. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier for the target process. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier for the target process. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier for the target process. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier for the target process. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier for the target process. */
+        uint32_t uts_ns_inum;
+        /** This is the cgroup identifier for the target process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the target process. */
+        char cgroup_name[GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ];
+        /** The process ID of the process that updated. */
+        uint32_t target_task_pid;
+        /** The process name of the process that updated. */
+        char target_task_name[GuestConnABI__GuestLog__PROC_COMM_SZ];
+        /** This is the incoming cgroup namespace identifier for the target process. */
+        uint32_t incoming_cgroup_ns_inum;
+        /** This is the incoming ipc namespace identifier for the target process. */
+        uint32_t incoming_ipc_ns_inum;
+        /** This is the incoming mount namespace identifier for the target process. */
+        uint32_t incoming_mnt_ns_inum;
+        /** This is the incoming network namespace identifier for the target process. */
+        uint32_t incoming_net_ns_inum;
+        /** This is the incoming pid namespace identifier for the target process. */
+        uint32_t incoming_pid_ns_inum;
+        /** This is the incoming pid namespace identifier granted to child processes. */
+        uint32_t incoming_pid_for_children_ns_inum;
+        /** This is the incoming time namespace identifier for the target process. */
+        uint32_t incoming_time_ns_inum;
+        /** This is the incoming time namespace identifier granted to child processes. */
+        uint32_t incoming_time_for_children_ns_inum;
+        /** This is the incoming user namespace identifier for the target process. */
+        uint32_t incoming_user_ns_inum;
+        /** This is the incoming uts namespace identifier for the target process. */
+        uint32_t incoming_uts_ns_inum;
+};
+typedef struct GuestConnABI__GuestLog__NamespaceChange GuestConnABI__GuestLog__NamespaceChange__T;
+#define GuestConnABI__GuestLog__NamespaceChange__SZ 236ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__NamespaceChange) == GuestConnABI__GuestLog__NamespaceChange__SZ, "Unexpected size for GuestConnABI__GuestLog__NamespaceChange");
+
diff --git include/bhv/interface/abi_hl_autogen.h include/bhv/interface/abi_hl_autogen.h
new file mode 100644
index 000000000..a048e0227
--- /dev/null
+++ include/bhv/interface/abi_hl_autogen.h
@@ -0,0 +1,241 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-06-25T12:28:17).
+ */
+
+#pragma once
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+void HypABI__init_slabs(void);
+
+extern struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab;
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = (HypABI__Integrity__Create__Mem_Region__T *)kmem_cache_alloc(HypABI__Integrity__Create__Mem_Region__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__Mem_Region__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__ALLOC() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__Mem_Region__slab, name)
+
+
+extern struct kmem_cache *HypABI__Integrity__Create__arg__slab;
+#define HypABI__Integrity__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = (HypABI__Integrity__Create__arg__T *)kmem_cache_alloc(HypABI__Integrity__Create__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__ALLOC() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall(const HypABI__Integrity__Create__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Create__arg__T *bhv_arg = HypABI__Integrity__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Create__arg__T));
+        rc = HypABI__Integrity__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Create__HYPERCALL(...) HypABI__Integrity__Create__hypercall((HypABI__Integrity__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Update__arg__slab;
+#define HypABI__Integrity__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = (HypABI__Integrity__Update__arg__T *)kmem_cache_alloc(HypABI__Integrity__Update__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__ALLOC() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Update__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall(const HypABI__Integrity__Update__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Update__arg__T *bhv_arg = HypABI__Integrity__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Update__arg__T));
+        rc = HypABI__Integrity__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Update__HYPERCALL(...) HypABI__Integrity__Update__hypercall((HypABI__Integrity__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Remove__arg__slab;
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = (HypABI__Integrity__Remove__arg__T *)kmem_cache_alloc(HypABI__Integrity__Remove__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Remove__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__ALLOC() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Remove__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall(const HypABI__Integrity__Remove__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Remove__arg__T *bhv_arg = HypABI__Integrity__Remove__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Remove__arg__T));
+        rc = HypABI__Integrity__Remove__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Remove__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Remove__HYPERCALL(...) HypABI__Integrity__Remove__hypercall((HypABI__Integrity__Remove__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Freeze__arg__slab;
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = (HypABI__Integrity__Freeze__arg__T *)kmem_cache_alloc(HypABI__Integrity__Freeze__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__ALLOC() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Freeze__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall(const HypABI__Integrity__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Freeze__arg__T *bhv_arg = HypABI__Integrity__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Freeze__arg__T));
+        rc = HypABI__Integrity__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Freeze__HYPERCALL(...) HypABI__Integrity__Freeze__hypercall((HypABI__Integrity__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab;
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = (HypABI__Integrity__PtpgInit__arg__T *)kmem_cache_alloc(HypABI__Integrity__PtpgInit__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__PtpgInit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__ALLOC() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__PtpgInit__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall(const HypABI__Integrity__PtpgInit__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__PtpgInit__arg__T *bhv_arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__PtpgInit__arg__T));
+        rc = HypABI__Integrity__PtpgInit__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__PtpgInit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__PtpgInit__HYPERCALL(...) HypABI__Integrity__PtpgInit__hypercall((HypABI__Integrity__PtpgInit__arg__T){__VA_ARGS__})
+
+#define HypABI__Integrity__PtpgReport__hypercall() \
+        HypABI__Integrity__PtpgReport__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Patch__Patch__arg__slab;
+#define HypABI__Patch__Patch__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = (HypABI__Patch__Patch__arg__T *)kmem_cache_alloc(HypABI__Patch__Patch__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__Patch__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__ALLOC() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__Patch__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall(const HypABI__Patch__Patch__arg__T s)
+{
+        int rc;
+        HypABI__Patch__Patch__arg__T *bhv_arg = HypABI__Patch__Patch__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__Patch__arg__T));
+        rc = HypABI__Patch__Patch__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__Patch__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__Patch__HYPERCALL(...) HypABI__Patch__Patch__hypercall((HypABI__Patch__Patch__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab;
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = (HypABI__Patch__PatchNoClose__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchNoClose__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchNoClose__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__ALLOC() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchNoClose__arg__slab, name)
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall(const HypABI__Patch__PatchNoClose__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchNoClose__arg__T *bhv_arg = HypABI__Patch__PatchNoClose__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchNoClose__arg__T));
+        rc = HypABI__Patch__PatchNoClose__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__PatchNoClose__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchNoClose__HYPERCALL(...) HypABI__Patch__PatchNoClose__hypercall((HypABI__Patch__PatchNoClose__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab;
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = (HypABI__Patch__PatchViolation__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchViolation__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__ALLOC() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchViolation__arg__slab, name)
+
+
+#define HypABI__Vault__Open__hypercall() \
+        HypABI__Vault__Open__hypercall_noalloc()
+
+#define HypABI__Vault__Close__hypercall() \
+        HypABI__Vault__Close__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Guestlog__Init__arg__slab;
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = (HypABI__Guestlog__Init__arg__T *)kmem_cache_alloc(HypABI__Guestlog__Init__arg__slab, gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Guestlog__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__ALLOC() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__Guestlog__Init__arg__slab, name)
+
+
diff --git include/bhv/interface/abi_ml_autogen.h include/bhv/interface/abi_ml_autogen.h
new file mode 100644
index 000000000..7cb5470a6
--- /dev/null
+++ include/bhv/interface/abi_ml_autogen.h
@@ -0,0 +1,181 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-06-25T12:28:17).
+ */
+
+#pragma once
+
+#include <asm/bitops.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT, addr);
+}
+bool static inline HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Init__Init__hypercall_noalloc(HypABI__Init__Init__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Init__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Init__Start__hypercall_noalloc(HypABI__Init__Start__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Start__OP_ID, arg);
+}
+
+bool static inline HypABI__Integrity__MemFlags__has_TRANSIENT(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__TRANSIENT__BIT, addr);
+}
+bool static inline HypABI__Integrity__MemFlags__has_MUTABLE(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__MUTABLE__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall_noalloc(HypABI__Integrity__Create__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Create__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall_noalloc(HypABI__Integrity__Update__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Update__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall_noalloc(HypABI__Integrity__Remove__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Remove__OP_ID, arg);
+}
+
+bool static inline HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT, addr);
+}
+bool static inline HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT, addr);
+}
+bool static inline HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT, addr);
+}
+bool static inline HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall_noalloc(HypABI__Integrity__Freeze__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Freeze__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall_noalloc(HypABI__Integrity__PtpgInit__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgInit__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgReport__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgReport__OP_ID, NULL);
+}
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall_noalloc(HypABI__Patch__Patch__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__Patch__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall_noalloc(HypABI__Patch__PatchNoClose__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchNoClose__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall_noalloc(HypABI__Patch__PatchViolation__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchViolation__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Vault__Open__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Vault__BACKEND_ID, HypABI__Vault__Open__OP_ID, NULL);
+}
+
+static __always_inline __must_check int HypABI__Vault__Close__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Vault__BACKEND_ID, HypABI__Vault__Close__OP_ID, NULL);
+}
+
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT, addr);
+}
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT, addr);
+}
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT, addr);
+}
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT, addr);
+}
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT, addr);
+}
+bool static inline HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall_noalloc(HypABI__Guestlog__Init__arg__T *arg)
+{
+    return bhv_hypercall_vas(HypABI__Guestlog__BACKEND_ID, HypABI__Guestlog__Init__OP_ID, arg);
+}
+
diff --git include/bhv/interface/abi_version_autogen.h include/bhv/interface/abi_version_autogen.h
new file mode 100644
index 000000000..2d6de9a75
--- /dev/null
+++ include/bhv/interface/abi_version_autogen.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-06-25T12:28:17).
+ */
+
+#pragma once
+
+#define HypABI__ABI_VERSION __BHV_VAS_ABI_VERSION(24, 26, 0, 270)
+
diff --git include/bhv/interface/acl.h include/bhv/interface/acl.h
new file mode 100644
index 000000000..f88a80fb1
--- /dev/null
+++ include/bhv/interface/acl.h
@@ -0,0 +1,36 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_ACL_H__
+#define __BHV_INTERFACE_ACL_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+/* BHV VAS ACL BACKEND OPS */
+#define BHV_VAS_ACL_OP_INIT_PROC_ACL 0U
+#define BHV_VAS_ACL_OP_INIT_DRIVER_ACL 1U
+#define BHV_VAS_ACL_OP_VIOLATION_PROC_ACL 2U
+#define BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL 3U
+
+typedef struct {
+	uint8_t valid;
+	uint8_t is_allow;
+	uint16_t num_pages;
+	uint16_t list_len;
+	uint16_t padding;
+	uint64_t list[];
+} __attribute__((__packed__)) bhv_acl_config_t;
+
+typedef struct {
+	GuestConnABI__GuestLog__Context__T context;
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_acl_violation_t;
+#endif /* __BHV_INTERFACE_ACL_H__ */
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 000000000..6e416ac6d
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,62 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define __BHV_VAS_ABI_VERSION(rel_year, rel_week, rel_extra, internal_info)    \
+	({                                                                     \
+		static_assert((unsigned long)(rel_year) <= 99);                \
+		static_assert((unsigned long)(rel_year) >= 23);                \
+		static_assert((unsigned long)(rel_week) < 53);                 \
+		static_assert((unsigned long)(rel_extra) < 0xff);              \
+		static_assert((unsigned long)(internal_info) > 0x00000);       \
+		static_assert((unsigned long)(internal_info) < 0xfffff);       \
+		(unsigned long)0xbedUL << (13 * 4) |                           \
+			(unsigned long)(rel_year) << (11 * 4) |                \
+			(unsigned long)(rel_week) << (9 * 4) |                 \
+			(unsigned long)(rel_extra) << (7 * 4) |                \
+			(unsigned long)(0x00UL) << (5 * 4) |                   \
+			(unsigned long)(internal_info) << (0 * 4);             \
+	})
+
+#include <bhv/version.h>
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_REGISTER_PROTECTION 9
+#define BHV_VAS_BACKEND_DOMAIN 10
+#define BHV_VAS_BACKEND_INODE 11
+#define BHV_VAS_BACKEND_KEYRING 12
+#define BHV_VAS_BACKEND_CONFSERVER 13
+
+/* BHV CONFIGURATION BITS */
+#define BHV_CONFIG_PROC_ACL 1
+#define BHV_CONFIG_DRIVER_ACL 2
+#define BHV_CONFIG_LOGGING 3
+#define BHV_CONFIG_CREDS 4
+#define BHV_CONFIG_FILE_PROTECTION 5
+#define BHV_CONFIG_GUEST_POLICY 6
+#define BHV_CONFIG_REGISTER_PROTECTION 7
+#define BHV_CONFIG_STRONG_ISOLATION 8
+#define BHV_CONFIG_INODE 10
+#define BHV_CONFIG_KEYRING 11
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/confserver.h include/bhv/interface/confserver.h
new file mode 100644
index 000000000..317423447
--- /dev/null
+++ include/bhv/interface/confserver.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_CONFSERVER_H__
+#define __BHV_INTERFACE_CONFSERVER_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/hypercall.h>
+
+#define BHV_VAS_CONFSERVER_OP_FREEZEMEMORY 0U
+#define BHV_VAS_CONFSERVER_OP_STRICTFILEOPS 1U
+
+#define BHV_CONFSERVER_HYP(op, arg)                                            \
+	bhv_hypercall_vas(BHV_VAS_BACKEND_CONFSERVER, op, arg)
+
+#define BHV_CONFSERVER_FREEZEMEMORY_HYP(arg)                                   \
+	BHV_CONFSERVER_HYP(BHV_VAS_CONFSERVER_OP_FREEZEMEMORY, arg)
+#define BHV_CONFSERVER_STRICTFILEOPS_HYP(arg)                                  \
+	BHV_CONFSERVER_HYP(BHV_VAS_CONFSERVER_OP_STRICTFILEOPS, arg)
+
+typedef struct {
+	uint8_t freeze_memory_after_boot;
+} __attribute__((__packed__)) bhv_confserver_req_freeze_memory_after_boot_t;
+
+typedef struct {
+	uint8_t strict_fileops;
+} __attribute__((__packed__)) bhv_confserver_req_strict_fileops_t;
+
+#endif /* __BHV_INTERFACE_CONFSERVER_H__ */
diff --git include/bhv/interface/creds.h include/bhv/interface/creds.h
new file mode 100644
index 000000000..f2426cdc9
--- /dev/null
+++ include/bhv/interface/creds.h
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_CREDS_H__
+#define __BHV_INTERFACE_CREDS_H__
+
+#include <linux/types.h>
+
+#include <bhv/event.h>
+
+/* BHV VAS CREDS BACKEND OPS */
+#define BHV_VAS_CREDS_OP_CONFIGURE 0U
+#define BHV_VAS_CREDS_OP_REGISTER_INIT_TASK 1U
+#define BHV_VAS_CREDS_OP_ASSIGN 2U
+#define BHV_VAS_CREDS_OP_ASSIGN_PRIV 3U
+#define BHV_VAS_CREDS_OP_COMMIT 4U
+#define BHV_VAS_CREDS_OP_RELEASE 5U
+#define BHV_VAS_CREDS_OP_VERIFICATION 6U
+#define BHV_VAS_CREDS_OP_LOG 7U
+
+enum event_type {
+	EVENT_NONE = 0,
+	CORRUPTION,
+	INVALID_ASSIGNMENT,
+	DOUBLE_ASSIGNMENT,
+	INVALID_COMMIT,
+	DOUBLE_COMMIT,
+	MAX_EVENTS
+};
+
+typedef struct {
+	uint64_t addr;
+	uint64_t cred;
+	uint64_t hmac;
+} __attribute__((__packed__)) bhv_task_cred_t;
+
+typedef struct {
+	bhv_task_cred_t init_task;
+} __attribute__((__packed__)) bhv_creds_init_task_arg_t;
+
+typedef struct {
+	bhv_task_cred_t new_task;
+	bhv_task_cred_t parent;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_arg_t;
+
+typedef struct {
+	uint64_t cred;
+	uint64_t daemon;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_priv_arg_t;
+
+typedef struct {
+	bhv_task_cred_t cur;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_commit_arg_t;
+
+typedef struct {
+	uint64_t cred;
+} __attribute__((__packed__)) bhv_creds_release_arg_t;
+
+typedef struct {
+	bhv_task_cred_t task;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_verification_arg_t;
+
+typedef struct {
+	GuestConnABI__GuestLog__Context__T context;
+	uint8_t event_type;
+	uint8_t block;
+	uint16_t pad1;
+	uint32_t task_pid;
+	uint64_t task_addr;
+	uint64_t task_cred;
+	char task_name[TASK_COMM_LEN];
+} __attribute__((__packed__)) bhv_creds_log_arg_t;
+
+typedef struct {
+	union {
+		bhv_creds_init_task_arg_t creds_register;
+		bhv_creds_assign_arg_t creds_assign;
+		bhv_creds_assign_priv_arg_t creds_assign_priv;
+		bhv_creds_commit_arg_t creds_commit;
+		bhv_creds_release_arg_t creds_release;
+		bhv_creds_verification_arg_t creds_verify;
+		bhv_creds_log_arg_t creds_log;
+	};
+} __attribute__((__packed__)) bhv_creds_arg_t;
+
+#endif /* __BHV_INTERFACE_CREDS_H__ */
diff --git include/bhv/interface/domain.h include/bhv/interface/domain.h
new file mode 100644
index 000000000..23688c770
--- /dev/null
+++ include/bhv/interface/domain.h
@@ -0,0 +1,114 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_DOMAIN_H__
+#define __BHV_INTERFACE_DOMAIN_H__
+
+#include <linux/types.h>
+
+#include <bhv/domain.h>
+#include <bhv/event.h>
+
+#define BHV_VAS_DOMAIN_OP_CONFIGURE 0U
+#define BHV_VAS_DOMAIN_OP_CREATE 1U
+#define BHV_VAS_DOMAIN_OP_DESTROY 2U
+#define BHV_VAS_DOMAIN_OP_SWITCH 3U
+#define BHV_VAS_DOMAIN_OP_TRANSFER 4U
+#define BHV_VAS_DOMAIN_OP_MAP 5U
+#define BHV_VAS_DOMAIN_OP_UPDATE 6U
+#define BHV_VAS_DOMAIN_OP_UNMAP 7U
+#define BHV_VAS_DOMAIN_OP_BATCH 8U
+#define BHV_VAS_DOMAIN_OP_PGD_DESTROY 9U
+#define BHV_VAS_DOMAIN_OP_REPORT 10U
+#define BHV_VAS_DOMAIN_OP_REPORT_FORCED_MEM_ACCESS 11U
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#define BHV_VAS_DOMAIN_OP_DEBUG 42U
+#endif
+
+typedef struct {
+	uint64_t pti; // guest -> host
+	uint64_t batched_region; // guest -> host
+	uint8_t isolate; // host -> guest
+	uint8_t allow_forced_mem_access; // host -> guest
+} __attribute__((__packed__)) bhv_domain_config_arg_t;
+
+typedef struct {
+	uint64_t id;
+	uint64_t pgd;
+} __attribute__((__packed__)) bhv_domain_t;
+
+typedef struct {
+	uint64_t pfn;
+	uint64_t count;
+} __attribute__((__packed__)) bhv_domain_pfn_arg_t;
+
+typedef struct {
+	bool read;
+	bool write;
+	bool exec;
+	bool kernel;
+	bhv_domain_pfn_arg_t pfn;
+} __attribute__((__packed__)) bhv_domain_map_arg_t;
+
+typedef struct {
+	bhv_domain_pfn_arg_t pfn;
+} __attribute__((__packed__)) bhv_domain_unmap_arg_t;
+
+typedef struct {
+	GuestConnABI__GuestLog__Context__T context;
+	bhv_domain_t domain_src;
+	bhv_domain_t domain_target;
+	struct {
+		uint64_t start;
+		uint64_t end;
+	} range;
+	bool write;
+	volatile bool block;
+} __attribute__((__packed__)) bhv_domain_report_arg_t;
+
+typedef struct {
+	bhv_domain_t domain;
+	union {
+		bhv_domain_config_arg_t config;
+		bhv_domain_report_arg_t report;
+		uint64_t id;
+	};
+} __attribute__((__packed__)) bhv_domain_arg_t;
+
+#define BHV_VAS_DOMAIN_BATCH_STATE_INVALID 0
+#define BHV_VAS_DOMAIN_BATCH_STATE_VALID 1
+#define BHV_VAS_DOMAIN_BATCH_STATE_PROCESSED 2
+
+typedef struct {
+	atomic_t state;
+	uint32_t op;
+	bhv_domain_t domain;
+	union {
+		bhv_domain_map_arg_t map;
+		bhv_domain_unmap_arg_t unmap;
+	};
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#define BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE 256
+	char stack_trace[BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE];
+#endif
+} __attribute__((__packed__)) bhv_domain_batched_entry_arg_t;
+
+#define BHV_DOMAIN_MAX_ENTRIES 4096
+typedef struct {
+	atomic64_t info; // Contains the head and the tail pointer.
+	bhv_domain_batched_entry_arg_t entries[BHV_DOMAIN_MAX_ENTRIES];
+} __attribute__((__packed__)) bhv_domain_batched_arg_t;
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#define BHV_VAS_DOMAIN_DEBUG_DESTROY_PGD 1U
+typedef struct {
+	bhv_domain_t domain;
+	uint32_t msg_type;
+} __attribute__((__packed__)) bhv_domain_debug_arg_t;
+#endif
+
+#endif /* __BHV_INTERFACE_DOMAIN_H__ */
diff --git include/bhv/interface/file_protection.h include/bhv/interface/file_protection.h
new file mode 100644
index 000000000..63f1b60c0
--- /dev/null
+++ include/bhv/interface/file_protection.h
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_FILE_PROTECTION_H__
+#define __BHV_INTERFACE_FILE_PROTECTION_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+/* BHV VAS FILE PROTECTION BACKEND OPS */
+#define BHV_VAS_FILE_PROTECTION_OP_INIT 0U
+#define BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION 1U
+#define BHV_VAS_FILE_PROTECTION_OP_VIOLATION_FILEOPS_PROTECTION 2U
+#define BHV_VAS_FILE_PROTECTION_OP_VIOLATION_DIRTYCRED 3U
+
+/* BHV VAS FILE PROTECTION FEATURES */
+#define BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION 0U
+#define BHV_VAS_FILE_PROTECTION_FEATURE_FILEOPS_PROTECTION 1U
+#define BHV_VAS_FILE_PROTECTION_FEATURE_DIRTYCRED_MITIGATION 2U
+
+#define BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ 256
+
+/* supported file system and device types */
+#define BHV_VAS_FILEOPS_PROTECTION_EXT4 0U
+#define BHV_VAS_FILEOPS_PROTECTION_TMPFS 1U
+#define BHV_VAS_FILEOPS_PROTECTION_MEM 2U
+#define BHV_VAS_FILEOPS_PROTECTION_NULL 3U
+#define BHV_VAS_FILEOPS_PROTECTION_PORT 4U
+#define BHV_VAS_FILEOPS_PROTECTION_ZERO 5U
+#define BHV_VAS_FILEOPS_PROTECTION_FULL 6U
+#define BHV_VAS_FILEOPS_PROTECTION_RANDOM 7U
+#define BHV_VAS_FILEOPS_PROTECTION_URANDOM 8U
+#define BHV_VAS_FILEOPS_PROTECTION_KMSG 9U
+#define BHV_VAS_FILEOPS_PROTECTION_TTY 10U
+#define BHV_VAS_FILEOPS_PROTECTION_CONSOLE 11U
+#define BHV_VAS_FILEOPS_PROTECTION_PROC 12U
+#define BHV_VAS_FILEOPS_PROTECTION_XFS 13U
+#define BHV_VAS_FILEOPS_PROTECTION_SOCKET 14U
+#define BHV_VAS_FILEOPS_PROTECTION_PIPE 15U
+#define BHV_VAS_FILEOPS_PROTECTION_SYSFS 16U
+#define BHV_VAS_FILEOPS_PROTECTION_DEBUGFS 17U
+#define BHV_VAS_FILEOPS_PROTECTION_MAP_LENGTH 18
+#define BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED 255U
+
+#define BHV_VAS_FILEOPS_PATH_MAX_SZ 1024
+
+typedef struct {
+	uint64_t feature_bitmap;
+} __attribute__((__packed__)) bhv_file_protection_config_t;
+
+typedef struct {
+	GuestConnABI__GuestLog__Context__T context;
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_file_protection_violation_t;
+
+typedef struct {
+	GuestConnABI__GuestLog__Context__T context;
+	uint8_t fops_type;
+	uint8_t is_dir;
+	uint8_t block;
+	char padding[5];
+	uint64_t fops_ptr;
+	char path_name[BHV_VAS_FILEOPS_PATH_MAX_SZ];
+} __attribute__((__packed__)) bhv_fileops_protection_violation_t;
+#endif /* __BHV_INTERFACE_FILE_PROTECTION_H__ */
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 000000000..990df4704
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,55 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static __always_inline int bhv_hypercall_vas(uint32_t backend, uint32_t op,
+					     void *arg)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	if (arg != NULL)
+		phys_addr = bhv_virt_to_phys(arg);
+
+#ifdef CONFIG_BHV_TRACEPOINTS
+	trace_bhv_hypercall_start(TARGET_BHV_VAS, backend, op);
+#endif
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			   phys_addr);
+#ifdef CONFIG_BHV_TRACEPOINTS
+	trace_bhv_hypercall_end(TARGET_BHV_VAS, backend, op);
+#endif
+
+	if (rv) {
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION, arg,
+		      phys_addr);
+#else
+		pr_warn("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+			rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			arg, phys_addr);
+		dump_stack();
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/inode.h include/bhv/interface/inode.h
new file mode 100644
index 000000000..2392a1222
--- /dev/null
+++ include/bhv/interface/inode.h
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_INODE_H__
+#define __BHV_INTERFACE_INODE_H__
+
+#include <linux/types.h>
+
+#include <bhv/event.h>
+
+/* BHV VAS INODE BACKEND OPS */
+#define BHV_VAS_INODE_OP_REGISTER 0U
+#define BHV_VAS_INODE_OP_UPDATE 1U
+#define BHV_VAS_INODE_OP_RELEASE 2U
+#define BHV_VAS_INODE_OP_VERIFY 3U
+#define BHV_VAS_INODE_OP_LOG 4U
+
+enum event_type {
+	EVENT_NONE = 0,
+	NO_INODE,
+	CORRUPTION,
+	MAX_EVENTS
+};
+
+typedef struct {
+	uint64_t addr;
+	uint64_t hmac;
+} __attribute__((__packed__)) bhv_inode_t;
+
+typedef struct {
+	bhv_inode_t inode;
+} __attribute__((__packed__)) bhv_inode_register_arg_t;
+
+typedef struct {
+	bhv_inode_t inode;
+} __attribute__((__packed__)) bhv_inode_update_arg_t;
+
+typedef struct {
+	bhv_inode_t inode;
+} __attribute__((__packed__)) bhv_inode_release_arg_t;
+
+typedef struct {
+	bhv_inode_t inode;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_inode_verify_arg_t;
+
+typedef struct {
+        GuestConnABI__GuestLog__Context__T context;
+        uint64_t inode_addr;
+        uint32_t inode_uid;
+        uint32_t inode_gid;
+        uint16_t inode_mode;
+        uint16_t pad1[2];
+        uint8_t event_type;
+	uint8_t block;
+        char file_path[GuestConnABI__GuestLog__Context__MAX_PATH_SZ];
+} __attribute__((__packed__)) bhv_inode_log_arg_t;
+
+typedef struct {
+	union {
+		bhv_inode_register_arg_t inode_register;
+		bhv_inode_update_arg_t inode_update;
+		bhv_inode_release_arg_t inode_release;
+		bhv_inode_verify_arg_t inode_verify;
+		bhv_inode_log_arg_t inode_log;
+	};
+} __attribute__((__packed__)) bhv_inode_arg_t;
+
+#endif /* __BHV_INTERFACE_INODE_H__ */
diff --git include/bhv/interface/keyring.h include/bhv/interface/keyring.h
new file mode 100644
index 000000000..23f69a995
--- /dev/null
+++ include/bhv/interface/keyring.h
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_KEYRING_H__
+#define __BHV_INTERFACE_KEYRING_H__
+
+#include <linux/types.h>
+
+#include <bhv/event.h>
+
+/* BHV VAS KEYRING BACKEND OPS */
+#define BHV_VAS_KEYRING_OP_REGISTER 0U
+#define BHV_VAS_KEYRING_OP_RELEASE 1U
+#define BHV_VAS_KEYRING_OP_VERIFY 2U
+#define BHV_VAS_KEYRING_OP_LOG 3U
+
+enum event_type { EVENT_NONE = 0, NO_KEYRING, CORRUPTION, MAX_EVENTS };
+
+typedef struct {
+	uint64_t addr;
+	uint64_t hmac;
+} __attribute__((__packed__)) bhv_keyring_t;
+
+typedef struct {
+	bhv_keyring_t keyring;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_keyring_register_arg_t;
+
+typedef struct {
+	bhv_keyring_t keyring;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_keyring_verify_arg_t;
+
+typedef struct {
+        GuestConnABI__GuestLog__Context__T context;
+        uint64_t addr;
+        uint32_t uid;
+        uint32_t gid;
+	uint32_t perm;
+	uint32_t serial;
+        uint8_t event_type;
+	uint8_t block;
+	char desc[GuestConnABI__GuestLog__Context__MAX_PATH_SZ];
+} __attribute__((__packed__)) bhv_keyring_log_arg_t;
+
+typedef struct {
+	union {
+		bhv_keyring_register_arg_t keyring_register;
+		bhv_keyring_verify_arg_t keyring_verify;
+		bhv_keyring_log_arg_t keyring_log;
+	};
+} __attribute__((__packed__)) bhv_keyring_arg_t;
+
+#endif /* __BHV_INTERFACE_KEYRING_H__ */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 000000000..ef57c2700
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,159 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_PATCH_H
+#define _ASM_INTERFACE_BHV_PATCH_H
+
+#include <linux/kernel.h>
+
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define PATCH_BODY(TYP)                                             \
+	int rc;                                                     \
+	static HypABI__Patch__##TYP##__arg__T early_arg;            \
+	HypABI__Patch__##TYP##__arg__T *bhv_arg = &early_arg;       \
+                                                                    \
+	if (HypABI__Patch__##TYP##__arg__slab) {                    \
+		bhv_arg = HypABI__Patch__##TYP##__arg__ALLOC();     \
+	}                                                           \
+                                                                    \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                 \
+                                                                    \
+	bhv_arg->dest_phys_addr = bhv_virt_to_phys(dest_virt_addr); \
+	memcpy(bhv_arg->src_value, src, size);                      \
+	bhv_arg->size = size;                                       \
+                                                                    \
+	rc = HypABI__Patch__##TYP##__hypercall_noalloc(bhv_arg);    \
+                                                                    \
+	if (bhv_arg != &early_arg)                                  \
+		HypABI__Patch__##TYP##__arg__FREE(bhv_arg);         \
+                                                                    \
+	return rc;
+
+static __always_inline int
+__bhv_patchnoclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				    uint64_t size)
+{
+	PATCH_BODY(PatchNoClose);
+}
+
+static __always_inline int
+__bhv_patchclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				  uint64_t size)
+{
+	PATCH_BODY(Patch);
+}
+
+#undef PATCH_BODY
+
+static __always_inline uint64_t __bhv_patch_round(void **const dest_virt_addr,
+						  const uint8_t *const src,
+						  uint64_t *const size,
+						  const bool close_vault,
+						  unsigned long *const rc)
+{
+	unsigned long r;
+	uint64_t bytes_until_page_boundary =
+		PAGE_SIZE - ((uint64_t)*dest_virt_addr % PAGE_SIZE);
+	uint64_t this_patch_size = min3(*size, bytes_until_page_boundary,
+					(uint64_t)HypABI__Patch__MAX_PATCH_SZ);
+
+	if (close_vault && *size == this_patch_size) {
+		r = __bhv_patchclose_hypercall_single(*dest_virt_addr, src,
+						      this_patch_size);
+	} else {
+		r = __bhv_patchnoclose_hypercall_single(*dest_virt_addr, src,
+							this_patch_size);
+	}
+	if (r)
+		*rc = r;
+
+	*dest_virt_addr += this_patch_size;
+	*size -= this_patch_size;
+	return this_patch_size;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ */
+static __always_inline int bhv_patch_hypercall(void *dest_virt_addr,
+					       const uint8_t *src,
+					       uint64_t size, bool close_vault)
+{
+	unsigned long rc = 0;
+
+	BUG_ON(!src);
+
+	while (size)
+		src += __bhv_patch_round(&dest_virt_addr, src, &size,
+					 close_vault, &rc);
+
+	return rc;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ * Sets memory using a one-byte pattern rather than just copying (like memset)
+ */
+static __always_inline int bhv_patch_hypercall_memset(void *dest_virt_addr,
+						      uint64_t size,
+						      uint8_t pattern,
+						      bool close_vault)
+{
+	unsigned long rc = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+	memset(buf, pattern, HypABI__Patch__MAX_PATCH_SZ);
+
+	while (size)
+		__bhv_patch_round(&dest_virt_addr, buf, &size, close_vault,
+				  &rc);
+
+	return rc;
+}
+
+/**
+* Handle patch violation hypercalls
+*
+* This function sends a patch violation hypercall and determines whether the
+* patch should be blocked.
+*
+* \returns True if the patch should be blocked, false otherwise.
+*/
+static __always_inline bool bhv_patch_violation_hypercall(void *dest_virt_addr,
+							  const char *message)
+{
+	unsigned long r;
+	bool rc;
+
+	HypABI__Patch__PatchViolation__arg__T *bhv_arg =
+		HypABI__Patch__PatchViolation__arg__ALLOC();
+
+	// Setup arguments. We block by default
+	bhv_arg->dest_virt_addr = (uint64_t)dest_virt_addr;
+	bhv_arg->dest_phys_addr = bhv_virt_to_phys(dest_virt_addr);
+	bhv_arg->block = true;
+	if (message != NULL)
+		strncpy(bhv_arg->message, message,
+			HypABI__Patch__PatchViolation__MAX_MSG_SZ);
+	bhv_arg->message[HypABI__Patch__PatchViolation__MAX_MSG_SZ - 1] = '\0';
+
+	r = HypABI__Patch__PatchViolation__hypercall_noalloc(bhv_arg);
+	rc = r || bhv_arg->block;
+
+	HypABI__Patch__PatchViolation__arg__FREE(bhv_arg);
+
+	// Block in case of error or if block is set
+	return rc;
+}
+
+#endif /* _ASM_INTERFACE_BHV_PATCH_H */
diff --git include/bhv/interface/reg_protect.h include/bhv/interface/reg_protect.h
new file mode 100644
index 000000000..9ba795844
--- /dev/null
+++ include/bhv/interface/reg_protect.h
@@ -0,0 +1,66 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_REGPROTECT_H__
+#define __BHV_INTERFACE_REGPROTECT_H__
+
+#include <linux/types.h>
+
+#define BHV_VAS_REGPROTECT_OP_FREEEZE_PHYS 0U
+
+#ifdef CONFIG_X86_64
+#define BHV_NUM_FREEZABLE_REGISTERS 4
+#define BHV_FREEZABLE_REGISTERS Q(CR0) Q(CR3) Q(CR4) Q(EFER)
+
+#define BHV_NUM_TRAPPABLE_REGISTERS 4
+#define BHV_REG_PROTECT_REG_INVALID 0UL
+#define BHV_REG_PROTECT_REG_CR0 (1UL << 0)
+#define BHV_REG_PROTECT_REG_CR3 (1UL << 1)
+#define BHV_REG_PROTECT_REG_CR4 (1UL << 2)
+#define BHV_REG_PROTECT_REG_EFER (1UL << 3)
+#define BHV_REG_PROTECT_REG_MAX_VALUE (1UL << BHV_NUM_TRAPPABLE_REGISTERS) - 1
+
+#elif CONFIG_ARM64
+#define BHV_NUM_FREEZABLE_REGISTERS 5
+#define BHV_FREEZABLE_REGISTERS Q(TTBR0) Q(TTBR1) Q(TCR) Q(SCTLR) Q(VBAR)
+
+#define BHV_REG_PROTECT_REG_INVALID 0UL
+#define BHV_REG_PROTECT_REG_TTBR0 (1UL << 0)
+#define BHV_REG_PROTECT_REG_TTBR1 (1UL << 1)
+#define BHV_REG_PROTECT_REG_TCR (1UL << 2)
+#define BHV_REG_PROTECT_REG_SCTLR (1UL << 3)
+#define BHV_REG_PROTECT_REG_VBAR (1UL << 4)
+// #define BHV_REG_PROTECT_REG_MAX_VALUE (1UL << NUM_TRAPPABLE_REGISTERS) - 1
+
+#else
+#error Unsupported architecture
+#endif
+
+// inline const char *bhv_reg_protect_reg_to_str(uint64_t reg)
+// {
+// 	switch (reg) {
+// #define Q(reg)
+// 	case BHV_REG_PROTECT_REG_##reg:
+// 		return #reg;
+// 		BHV_FREEZABLE_REGISTERS
+// #undef Q
+// 	default:
+// 		return "UNKNOWN";
+// 	}
+// }
+
+typedef struct {
+	uint64_t register_selector;
+	uint64_t freeze_bitfield;
+} __attribute__((__packed__)) bhv_reg_protect_freeze_t;
+
+typedef struct {
+	union {
+		bhv_reg_protect_freeze_t bhv_reg_protect_freeze;
+	};
+} __attribute__((__packed__)) bhv_reg_protect_t;
+
+#endif /* __BHV_INTERFACE_REGPROTECT_H__ */
diff --git include/bhv/kernel-kln.h include/bhv/kernel-kln.h
new file mode 100644
index 000000000..89729b2c6
--- /dev/null
+++ include/bhv/kernel-kln.h
@@ -0,0 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#define KLN_SYM(sym) ((unsigned long) sym)
+#define KLN_SYMBOL(ty, sym) ((ty)sym)
+#define KLN_SYMBOL_P(ty, sym) ((ty)&sym)
diff --git include/bhv/keyring.h include/bhv/keyring.h
new file mode 100644
index 000000000..e570940ab
--- /dev/null
+++ include/bhv/keyring.h
@@ -0,0 +1,40 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_KEYRING_H__
+#define __BHV_KEYRING_H__
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_KEYS
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_keyring_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_KEYRING, bhv_configuration_bitmap);
+}
+
+int __init bhv_init_keyring(void);
+
+int bhv_keyring_register_system_trusted(struct key **k);
+int bhv_keyring_verify(struct key *keyring, void *anchor);
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_keyring_register_system_trusted(k) 0
+#define bhv_keyring_verify(k, a) 0
+#define bhv_keyring_verify_locked(k, a) 0
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* CONFIG_KEYS */
+
+#endif /* __BHV_KEYRING_H__ */
+
diff --git include/bhv/kversion.h include/bhv/kversion.h
new file mode 100644
index 000000000..4aae1b3f3
--- /dev/null
+++ include/bhv/kversion.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/version.h>
+
+#ifndef VASKM // inside kernel tree
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 186) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+#define BHV_KVERS_5_10
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 72) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+#define BHV_KVERS_5_15
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 31) && LINUX_VERSION_CODE < KERNEL_VERSION(6, 2, 0)
+#define BHV_KVERS_6_1
+#else
+#error Unsupported linux version
+#endif
+
+#endif // VASKM
+
+#undef LINUX_VERSION_CODE
+#undef KERNEL_VERSION
+#undef LINUX_VERSION_MAJOR
+#undef LINUX_VERSION_PATCHLEVEL
+#undef LINUX_VERSION_SUBLEVEL
diff --git include/bhv/memory_freeze.h include/bhv/memory_freeze.h
new file mode 100644
index 000000000..cafa242b1
--- /dev/null
+++ include/bhv/memory_freeze.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#ifdef CONFIG_BHV_VAS
+
+void bhv_memory_freeze_init(void);
+
+#else // !CONFIG_BHV_VAS
+
+static inline void bhv_memory_freeze_init(void)
+{
+}
+
+#endif // CONFIG_BHV_VAS
\ No newline at end of file
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 000000000..b5d14513b
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description);
+#endif //VASKM
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+#if 0
+static inline void bhv_protect_generic_memory(uint64_t owner, const void *base,
+					      uint64_t size, uint32_t type,
+					      uint64_t flags, char *description)
+{
+}
+#endif
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 000000000..209fded75
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,130 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+extern struct mutex bhv_alternatives_mutex;
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void);
+/**************************************************/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+int bhv_bpf_write(void *dst, void *src, size_t sz);
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz);
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages);
+void bhv_rm_bpf_code_range(uint64_t pfn);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_JUMP_LABEL */
+
+static void __always_inline bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static void __always_inline bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __init_or_module bhv_apply_retpolines(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#if defined CONFIG_PARAVIRT && defined CONFIG_X86
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* defined CONFIG_PARAVIRT && defined CONFIG_X86 */
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __init_or_module bhv_apply_ibt_endbr(s32 *s);
+#endif
+#endif // VASKM
+
+#else // CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif // CONFIG_BHV_VAS
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/reg_protect.h include/bhv/reg_protect.h
new file mode 100644
index 000000000..f9ec002c3
--- /dev/null
+++ include/bhv/reg_protect.h
@@ -0,0 +1,39 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void);
+void bhv_start_reg_protect_arch(void);
+/***************************************************/
+
+int bhv_reg_protect_freeze(uint64_t reg_selector, uint64_t freeze_bitfield);
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs.h include/bhv/sysfs.h
new file mode 100644
index 000000000..100139430
--- /dev/null
+++ include/bhv/sysfs.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_sysfs(void);
+/**********************************************************/
diff --git include/bhv/sysfs_fops.h include/bhv/sysfs_fops.h
new file mode 100644
index 000000000..6703abb71
--- /dev/null
+++ include/bhv/sysfs_fops.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_fileops_protection(struct kobject *fops,
+					struct kobject *status);
+/***************************************************/
+
+#ifdef VASKM // out of tree
+int bhv_freeze_fops_map(void);
+extern bool bhv_allow_update_fileops_map;
+#endif // VASKM
+#endif /* CONFIG_BHV_VAS */
+
+
+
diff --git include/bhv/sysfs_integrity_freeze.h include/bhv/sysfs_integrity_freeze.h
new file mode 100644
index 000000000..e60e0f070
--- /dev/null
+++ include/bhv/sysfs_integrity_freeze.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_integrity_freeze(struct kobject *kobj);
+/***************************************************/
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
\ No newline at end of file
diff --git include/bhv/sysfs_reg_protect.h include/bhv/sysfs_reg_protect.h
new file mode 100644
index 000000000..adc18c413
--- /dev/null
+++ include/bhv/sysfs_reg_protect.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_reg_protect(struct kobject *kobj);
+/***************************************************/
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs_version.h include/bhv/sysfs_version.h
new file mode 100644
index 000000000..5a9f9c87d
--- /dev/null
+++ include/bhv/sysfs_version.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_version(struct kobject *kobj);
+/***************************************************/
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/version.h include/bhv/version.h
new file mode 100644
index 000000000..ca4903268
--- /dev/null
+++ include/bhv/version.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VERSION_H__
+#define __BHV_VERSION_H__
+
+#define BHV_VERSION __BHV_VERSION(24, 26, 0)
+
+#include <bhv/interface/abi_version_autogen.h>
+
+#endif //__BHV_VERSION_H__
diff --git include/keys/system_keyring.h include/keys/system_keyring.h
index 875e002a4..5e57cbb29 100644
--- include/keys/system_keyring.h
+++ include/keys/system_keyring.h
@@ -69,6 +69,9 @@ extern struct key *ima_blacklist_keyring;
 
 static inline struct key *get_ima_blacklist_keyring(void)
 {
+	if (bhv_keyring_verify(ima_blacklist_keyring, &ima_blacklist_keyring))
+		return NULL;
+
 	return ima_blacklist_keyring;
 }
 #else
diff --git include/linux/filter.h include/linux/filter.h
index 840b2a05c..dad6a3417 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -29,6 +29,9 @@
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
+#include <bhv/module.h>
+#include <bhv/integrity.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -832,6 +835,10 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 	if (!fp->jited) {
 		set_vm_flush_reset_perms(fp);
 		set_memory_ro((unsigned long)fp, fp->pages);
+		if (!bhv_integrity_freeze_create_currently_frozen &&
+		    !bhv_integrity_freeze_update_currently_frozen &&
+		    !bhv_integrity_freeze_patch_currently_frozen)
+			bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
 	}
 #endif
 }
@@ -841,6 +848,7 @@ static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->pages);
 	set_memory_x((unsigned long)hdr, hdr->pages);
+	bhv_bpf_protect_x(hdr, hdr->pages << PAGE_SHIFT);
 }
 
 static inline struct bpf_binary_header *
@@ -878,6 +886,8 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited && !bhv_integrity_freeze_remove_currently_frozen)
+		bhv_bpf_unprotect(fp);
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/highmem.h include/linux/highmem.h
index b25df1f8d..7bed5678f 100644
--- include/linux/highmem.h
+++ include/linux/highmem.h
@@ -9,6 +9,8 @@
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
 
+#include <bhv/domain.h>
+
 #include <asm/cacheflush.h>
 
 #ifndef ARCH_HAS_FLUSH_ANON_PAGE
@@ -321,11 +323,19 @@ static inline void copy_user_highpage(struct page *to, struct page *from,
 {
 	char *vfrom, *vto;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
 	vfrom = kmap_atomic(from);
 	vto = kmap_atomic(to);
 	copy_user_page(vto, vfrom, vaddr, to);
 	kunmap_atomic(vto);
 	kunmap_atomic(vfrom);
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 #endif
diff --git include/linux/kmod.h include/linux/kmod.h
index 68f69362d..a54d81bee 100644
--- include/linux/kmod.h
+++ include/linux/kmod.h
@@ -17,7 +17,11 @@
 #define KMOD_PATH_LEN 256
 
 #ifdef CONFIG_MODULES
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+extern const char modprobe_path[] __section(".rodata"); /* for sysctl */
+#else
 extern char modprobe_path[]; /* for sysctl */
+#endif
 /* modprobe exit status on success, -ve on error.  Return value
  * usually useless though. */
 extern __printf(2, 3)
diff --git include/linux/lsm_hook_defs.h include/linux/lsm_hook_defs.h
index 07abcd384..0a229d6ec 100644
--- include/linux/lsm_hook_defs.h
+++ include/linux/lsm_hook_defs.h
@@ -395,3 +395,9 @@ LSM_HOOK(void, LSM_RET_VOID, perf_event_free, struct perf_event *event)
 LSM_HOOK(int, 0, perf_event_read, struct perf_event *event)
 LSM_HOOK(int, 0, perf_event_write, struct perf_event *event)
 #endif /* CONFIG_PERF_EVENTS */
+
+LSM_HOOK(void, LSM_RET_VOID, module_loaded, struct module *mod)
+LSM_HOOK(int, 0, unshare, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, setns, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, cgroup_mkdir, struct cgroup *cgrp)
+LSM_HOOK(void, LSM_RET_VOID, cgroup_rmdir, struct cgroup *cgrp)
diff --git include/linux/lsm_hooks.h include/linux/lsm_hooks.h
index bbf9c8c7b..930d6d93d 100644
--- include/linux/lsm_hooks.h
+++ include/linux/lsm_hooks.h
@@ -1532,6 +1532,24 @@
  * 	Read perf_event security info if allowed.
  * @perf_event_write:
  * 	Write perf_event security info if allowed.
+ *
+ * Security hooks for BedRock Security Module
+ *
+ * @module_loaded:
+ * 	A module has successfully loaded.
+ *
+ * @unshare:
+ * 	A process unshares a part of its context.
+ *
+ * @setns:
+ * 	A setns system call is made.
+ *
+ * @cgroup_mkdir:
+ * 	A cgroup is created.
+ *
+ * @cgroup_rmdir:
+ * 	A cgroup is destroyed.
+ *
  */
 union security_list_options {
 	#define LSM_HOOK(RET, DEFAULT, NAME, ...) RET (*NAME)(__VA_ARGS__);
diff --git include/linux/mem_namespace.h include/linux/mem_namespace.h
new file mode 100644
index 000000000..58a84fd77
--- /dev/null
+++ include/linux/mem_namespace.h
@@ -0,0 +1,89 @@
+#ifndef _LINUX_MEM_NS_H
+#define _LINUX_MEM_NS_H
+
+#include <linux/kref.h>
+#include <linux/nsproxy.h>
+#include <linux/ns_common.h>
+
+#ifdef CONFIG_MEM_NS
+#define bhv_pr_info(msg, ...)   pr_info("[-BHV-] %s: " msg "\n", __FUNCTION__, ##__VA_ARGS__)
+#else
+#define bhv_pr_info(msg, ...)
+#endif
+
+struct mem_namespace {
+	struct kref kref;
+	struct user_namespace *user_ns;
+	struct ucounts *ucounts;
+	struct ns_common ns;
+	struct mem_namespace *parent;
+	unsigned int level;
+	uint64_t domain;
+} __randomize_layout;
+
+extern struct mem_namespace init_mem_ns;
+
+#ifdef CONFIG_MEM_NS
+static inline struct mem_namespace *get_mem_ns(struct mem_namespace *ns)
+{
+	if (ns != &init_mem_ns)
+		kref_get(&ns->kref);
+	return ns;
+}
+
+extern void free_mem_ns(struct kref *kref);
+
+static inline void put_mem_ns(struct mem_namespace *ns)
+{
+	struct mem_namespace *parent = NULL;
+
+	while (ns != &init_mem_ns) {
+		parent = ns->parent;
+		if (!kref_put(&ns->kref, free_mem_ns))
+			break;
+		ns = parent;
+	}
+}
+
+extern struct mem_namespace *copy_mem_ns(unsigned long flags,
+					 struct user_namespace *user_ns,
+					 struct mem_namespace *old_ns);
+
+extern struct mem_namespace *memns_of_task(const struct task_struct *task);
+
+extern bool current_in_same_mem_ns(const struct task_struct *task);
+
+extern bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns);
+
+static inline unsigned int task_memns_level(struct task_struct *task) {
+	return memns_of_task(task)->level;
+}
+
+#else /* CONFIG_MEM_NS */
+
+static inline void get_mem_ns(struct mem_namespace *ns) {}
+static inline void put_mem_ns(struct mem_namespace *ns) {}
+
+static inline struct mem_namespace *copy_mem_ns(unsigned long flags,
+						struct user_namespace *user_ns,
+						struct mem_namespace *old_ns)
+{
+	if (flags & CLONE_NEWMEM)
+		return ERR_PTR(-EINVAL);
+
+	return old_ns;
+}
+
+static inline bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return true;
+}
+
+static inline unsigned int task_memns_level(struct task_struct *task)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* _LINUX_MEM_NS_H */
diff --git include/linux/nsproxy.h include/linux/nsproxy.h
index cdb171efc..499668ef1 100644
--- include/linux/nsproxy.h
+++ include/linux/nsproxy.h
@@ -11,6 +11,7 @@ struct ipc_namespace;
 struct pid_namespace;
 struct cgroup_namespace;
 struct fs_struct;
+struct mem_namespace;
 
 /*
  * A structure to contain pointers to all per-process
@@ -38,6 +39,7 @@ struct nsproxy {
 	struct time_namespace *time_ns;
 	struct time_namespace *time_ns_for_children;
 	struct cgroup_namespace *cgroup_ns;
+	struct mem_namespace *mem_ns;
 };
 extern struct nsproxy init_nsproxy;
 
diff --git include/linux/proc_ns.h include/linux/proc_ns.h
index 75807ecef..ff1046249 100644
--- include/linux/proc_ns.h
+++ include/linux/proc_ns.h
@@ -34,6 +34,7 @@ extern const struct proc_ns_operations mntns_operations;
 extern const struct proc_ns_operations cgroupns_operations;
 extern const struct proc_ns_operations timens_operations;
 extern const struct proc_ns_operations timens_for_children_operations;
+extern const struct proc_ns_operations memns_operations;
 
 /*
  * We always define these enumerators
@@ -46,6 +47,7 @@ enum {
 	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
 	PROC_CGROUP_INIT_INO	= 0xEFFFFFFBU,
 	PROC_TIME_INIT_INO	= 0xEFFFFFFAU,
+	PROC_MEM_INIT_INO	= 0xEFFFFFF9U,
 };
 
 #ifdef CONFIG_PROC_FS
diff --git include/linux/reboot.h include/linux/reboot.h
index 3734cd8f3..cd9f40005 100644
--- include/linux/reboot.h
+++ include/linux/reboot.h
@@ -75,7 +75,11 @@ extern int C_A_D; /* for sysctl */
 void ctrl_alt_del(void);
 
 #define POWEROFF_CMD_PATH_LEN	256
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+extern const char poweroff_cmd[] __section(".rodata");
+#else
 extern char poweroff_cmd[POWEROFF_CMD_PATH_LEN];
+#endif
 
 extern void orderly_poweroff(bool force);
 extern void orderly_reboot(void);
diff --git include/linux/security.h include/linux/security.h
index 5b61aa19f..d1939374e 100644
--- include/linux/security.h
+++ include/linux/security.h
@@ -32,6 +32,7 @@
 #include <linux/string.h>
 #include <linux/mm.h>
 #include <linux/sockptr.h>
+#include <linux/nsproxy.h>
 
 struct linux_binprm;
 struct cred;
@@ -462,6 +463,11 @@ int security_inode_notifysecctx(struct inode *inode, void *ctx, u32 ctxlen);
 int security_inode_setsecctx(struct dentry *dentry, void *ctx, u32 ctxlen);
 int security_inode_getsecctx(struct inode *inode, void **ctx, u32 *ctxlen);
 int security_locked_down(enum lockdown_reason what);
+void security_module_loaded(struct module *mod);
+int security_unshare(struct task_struct *tsk, struct nsset *nsset);
+int security_setns(struct task_struct *tsk, struct nsset *nsset);
+int security_cgroup_mkdir(struct cgroup *cgrp);
+void security_cgroup_rmdir(struct cgroup *cgrp);
 #else /* CONFIG_SECURITY */
 
 static inline int call_blocking_lsm_notifier(enum lsm_event event, void *data)
@@ -1322,6 +1328,24 @@ static inline int security_locked_down(enum lockdown_reason what)
 {
 	return 0;
 }
+static inline void security_module_loaded(struct module *)
+{
+}
+static inline int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return 0;
+}
+static inline void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+}
 #endif	/* CONFIG_SECURITY */
 
 #if defined(CONFIG_SECURITY) && defined(CONFIG_WATCH_QUEUE)
diff --git include/linux/user_namespace.h include/linux/user_namespace.h
index 7616c7bf4..9ca51802f 100644
--- include/linux/user_namespace.h
+++ include/linux/user_namespace.h
@@ -50,6 +50,7 @@ enum ucount_type {
 	UCOUNT_INOTIFY_INSTANCES,
 	UCOUNT_INOTIFY_WATCHES,
 #endif
+	UCOUNT_MEM_NAMESPACES,
 	UCOUNT_COUNTS,
 };
 
diff --git include/uapi/linux/perf_event.h include/uapi/linux/perf_event.h
index 6ca63ab6b..3d0eacdcb 100644
--- include/uapi/linux/perf_event.h
+++ include/uapi/linux/perf_event.h
@@ -727,6 +727,7 @@ enum {
 	USER_NS_INDEX		= 4,
 	MNT_NS_INDEX		= 5,
 	CGROUP_NS_INDEX		= 6,
+	MEM_NS_INDEX		= 7,
 
 	NR_NAMESPACES,		/* number of available namespaces */
 };
diff --git include/uapi/linux/sched.h include/uapi/linux/sched.h
index 3bac0a8ce..94224cf51 100644
--- include/uapi/linux/sched.h
+++ include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
  * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
  * syscalls only:
  */
+#define CLONE_NEWMEM	0x00000040	/* New memory namespace */
 #define CLONE_NEWTIME	0x00000080	/* New time namespace */
 
 #ifndef __ASSEMBLY__
diff --git init/main.c init/main.c
index 298989b0d..f743fa235 100644
--- init/main.c
+++ init/main.c
@@ -108,6 +108,10 @@
 
 #include <kunit/test.h>
 
+#include <bhv/init/mm_init.h>
+#include <bhv/init/late_start.h>
+#include <bhv/memory_freeze.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -836,6 +840,8 @@ static void __init mm_init(void)
 	/* Should be run after espfix64 is set up. */
 	pti_init();
 	mm_cache_init();
+
+	bhv_mm_init();
 }
 
 void __init __weak arch_call_rest_init(void)
@@ -901,6 +907,7 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	trap_init();
 	mm_init();
 	poking_init();
+
 	ftrace_init();
 
 	/* trace_printk can be enabled here */
@@ -1321,6 +1328,7 @@ static void __init do_pre_smp_initcalls(void)
 static int run_init_process(const char *init_filename)
 {
 	const char *const *p;
+	int ret;
 
 	argv_init[0] = init_filename;
 	pr_info("Run %s as init process\n", init_filename);
@@ -1330,7 +1338,10 @@ static int run_init_process(const char *init_filename)
 	pr_debug("  with environment:\n");
 	for (p = envp_init; *p; p++)
 		pr_debug("    %s\n", *p);
-	return kernel_execve(init_filename, argv_init, envp_init);
+	ret = kernel_execve(init_filename, argv_init, envp_init);
+	if (!ret)
+		bhv_memory_freeze_init();
+	return ret;
 }
 
 static int try_to_run_init_process(const char *init_filename)
@@ -1406,6 +1417,8 @@ static int __ref kernel_init(void *unused)
 	free_initmem();
 	mark_readonly();
 
+	bhv_late_start();
+
 	/*
 	 * Kernel mappings are now finalized - update the userspace page-table
 	 * to finalize PTI.
diff --git kernel/Makefile kernel/Makefile
index 82e9c8436..40d7731d5 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -76,6 +76,7 @@ obj-$(CONFIG_CGROUPS) += cgroup/
 obj-$(CONFIG_UTS_NS) += utsname.o
 obj-$(CONFIG_USER_NS) += user_namespace.o
 obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_MEM_NS) += mem_namespace.o
 obj-$(CONFIG_IKCONFIG) += configs.o
 obj-$(CONFIG_IKHEADERS) += kheaders.o
 obj-$(CONFIG_SMP) += stop_machine.o
diff --git kernel/bpf/bpf_struct_ops.c kernel/bpf/bpf_struct_ops.c
index ac283f9b2..4afe69e3c 100644
--- kernel/bpf/bpf_struct_ops.c
+++ kernel/bpf/bpf_struct_ops.c
@@ -452,6 +452,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 
 	set_memory_ro((long)st_map->image, 1);
 	set_memory_x((long)st_map->image, 1);
+	bhv_bpf_protect_x(st_map->image, PAGE_SIZE);
 	err = st_ops->reg(kdata);
 	if (likely(!err)) {
 		/* Pair with smp_load_acquire() during lookup_elem().
@@ -470,6 +471,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 	 */
 	set_memory_nx((long)st_map->image, 1);
 	set_memory_rw((long)st_map->image, 1);
+	bhv_bpf_unprotect(st_map->image);
 	bpf_map_put(map);
 
 reset_unlock:
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 33ea6ab12..f4d66f493 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -908,6 +908,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 pages = hdr->pages;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(pages);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index 87becf77c..c163f2c3c 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -38,6 +38,11 @@ void *bpf_jit_alloc_exec_page(void)
 	 * everytime new program is attached or detached.
 	 */
 	set_memory_x((long)image, 1);
+	/*
+	 * XXX: Make sure that we foresee all cases that would allow new
+	 * programs to attach/detach and handle the permissions appropriately.
+	 */
+	bhv_bpf_protect_x(image, PAGE_SIZE);
 	return image;
 }
 
@@ -171,6 +176,7 @@ static void __bpf_tramp_image_put_deferred(struct work_struct *work)
 
 	im = container_of(work, struct bpf_tramp_image, work);
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 	bpf_jit_uncharge_modmem(1);
 	percpu_ref_exit(&im->pcref);
@@ -289,6 +295,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key, u32 idx)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 out_uncharge:
 	bpf_jit_uncharge_modmem(1);
diff --git kernel/cgroup/cgroup.c kernel/cgroup/cgroup.c
index 11400eba6..7735238a2 100644
--- kernel/cgroup/cgroup.c
+++ kernel/cgroup/cgroup.c
@@ -5498,6 +5498,10 @@ int cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)
 	if (ret)
 		goto out_destroy;
 
+	ret = security_cgroup_mkdir(cgrp);
+	if (ret)
+		goto out_destroy;
+
 	TRACE_CGROUP_PATH(mkdir, cgrp);
 
 	/* let's create and online css's */
@@ -5696,8 +5700,10 @@ int cgroup_rmdir(struct kernfs_node *kn)
 		return 0;
 
 	ret = cgroup_destroy_locked(cgrp);
-	if (!ret)
+	if (!ret) {
+		security_cgroup_rmdir(cgrp);
 		TRACE_CGROUP_PATH(rmdir, cgrp);
+	}
 
 	cgroup_kn_unlock(kn);
 	return ret;
diff --git kernel/cred.c kernel/cred.c
index 54042ebed..1349ce8d3 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -17,6 +17,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -112,6 +114,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 #endif
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -451,6 +454,8 @@ int commit_creds(struct cred *new)
 #endif
 	BUG_ON(atomic_long_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -690,6 +695,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	if (daemon)
 		old = get_task_cred(daemon);
 	else
diff --git kernel/entry/common.c kernel/entry/common.c
index 09f58853f..0c0f096c7 100644
--- kernel/entry/common.c
+++ kernel/entry/common.c
@@ -195,6 +195,10 @@ static void exit_to_user_mode_prepare(struct pt_regs *regs)
 
 	lockdep_assert_irqs_disabled();
 
+
+	// Make sure we are on the current domain before exiting to userspace
+	bhv_domain_enter(current);
+
 	if (unlikely(ti_work & EXIT_TO_USER_MODE_WORK))
 		ti_work = exit_to_user_mode_loop(regs, ti_work);
 
diff --git kernel/events/core.c kernel/events/core.c
index e0b47bed8..b2ae36924 100644
--- kernel/events/core.c
+++ kernel/events/core.c
@@ -8018,6 +8018,10 @@ void perf_event_namespaces(struct task_struct *task)
 	perf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],
 			       task, &cgroupns_operations);
 #endif
+#ifdef CONFIG_MEM_NS
+	perf_fill_ns_link_info(&ns_link_info[MEM_NS_INDEX],
+			       task, &memns_operations);
+#endif
 
 	perf_iterate_sb(perf_event_namespaces_output,
 			&namespaces_event,
diff --git kernel/exit.c kernel/exit.c
index bacdaf980..22afda36a 100644
--- kernel/exit.c
+++ kernel/exit.c
@@ -70,6 +70,8 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 /*
  * The default value should be high enough to not crash a system that randomly
  * crashes its kernel from time to time, but low enough to at least not permit
diff --git kernel/fork.c kernel/fork.c
index 633b0af1d..49762fea5 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -96,6 +96,9 @@
 #include <linux/kasan.h>
 #include <linux/scs.h>
 #include <linux/io_uring.h>
+#include <linux/mem_namespace.h>
+
+#include <bhv/creds.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -603,6 +606,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	}
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
+
 out:
 	mmap_write_unlock(mm);
 	flush_tlb_mm(oldmm);
@@ -1094,6 +1098,11 @@ static inline void __mmput(struct mm_struct *mm)
 {
 	VM_BUG_ON(atomic_read(&mm->mm_users));
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_destroy_pgd(current, mm);
+	bhv_domain_debug_destroy_pgd(current, mm);
+#endif
+
 	uprobe_clear_state(mm);
 	exit_aio(mm);
 	ksm_exit(mm);
@@ -1913,6 +1922,8 @@ static __latent_entropy struct task_struct *copy_process(
 	/*
 	 * If the new process will be in a different pid or user namespace
 	 * do not allow it to share a thread group with the forking task.
+	 *
+	 * XXX: Consider adding additional constraints for memory namespaces.
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
@@ -2108,9 +2119,12 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2123,15 +2137,15 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_signal(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_sighand;
-	retval = copy_mm(clone_flags, p);
+	retval = copy_namespaces(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_signal;
-	retval = copy_namespaces(clone_flags, p);
+	retval = copy_mm(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_mm;
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_io(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_mm;
 	retval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
@@ -2361,13 +2375,13 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);
-bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm) {
 		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
 	}
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2379,6 +2393,7 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
@@ -2878,7 +2893,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
 				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+				CLONE_NEWTIME|CLONE_NEWMEM))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -2981,6 +2996,11 @@ int ksys_unshare(unsigned long unshare_flags)
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
 
+	/*
+	 * XXX: Consider CLONE_NEWMEM! Do we need to unshare the thread group
+	 * via CLONE_THREAD?
+	 */
+
 	err = check_unshare_flags(unshare_flags);
 	if (err)
 		goto bad_unshare_out;
@@ -3006,6 +3026,15 @@ int ksys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_cleanup_cred;
 
 	if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
+		struct nsset nsset = { .flags = unshare_flags,
+				       .nsproxy = new_nsproxy,
+				       .fs = new_fs,
+				       .cred = new_cred };
+
+		err = security_unshare(current, &nsset);
+		if (err)
+			goto bad_unshare_cleanup_cred;
+
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
diff --git kernel/jump_label.c kernel/jump_label.c
index 4ae693ce7..2c36eed6e 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,6 +19,8 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
@@ -627,6 +629,7 @@ static int jump_label_add_module(struct module *mod)
 	struct jump_entry *iter;
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, *jlm2;
+	int rc;
 
 	/* if the module doesn't have jump label entries, just return */
 	if (iter_start == iter_stop)
@@ -634,6 +637,9 @@ static int jump_label_add_module(struct module *mod)
 
 	jump_label_sort_entries(iter_start, iter_stop);
 
+	if ((rc = bhv_jump_label_add_module(mod)))
+		return rc;
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		struct static_key *iterk;
 
@@ -689,6 +695,8 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+	bhv_jump_label_del_module(mod);
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
diff --git kernel/kmod.c kernel/kmod.c
index 3cd075ce2..0fc927702 100644
--- kernel/kmod.c
+++ kernel/kmod.c
@@ -58,7 +58,11 @@ static DECLARE_WAIT_QUEUE_HEAD(kmod_wq);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+const char modprobe_path[] __section(".rodata") = "/sbin/modprobe";
+#else
 char modprobe_path[KMOD_PATH_LEN] = "/sbin/modprobe";
+#endif
 
 static void free_modprobe_argv(struct subprocess_info *info)
 {
@@ -84,7 +88,7 @@ static int call_modprobe(char *module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)modprobe_path;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
diff --git kernel/kthread.c kernel/kthread.c
index 508fe5278..b90deabd5 100644
--- kernel/kthread.c
+++ kernel/kthread.c
@@ -30,6 +30,7 @@
 #include <linux/sched/isolation.h>
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
@@ -1327,6 +1328,9 @@ void kthread_use_mm(struct mm_struct *mm)
 	tsk->mm = mm;
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
@@ -1359,6 +1363,7 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	/* active_mm is still 'mm' */
 	enter_lazy_tlb(mm, tsk);
 	local_irq_enable();
+	bhv_domain_enter(NULL);
 	task_unlock(tsk);
 }
 EXPORT_SYMBOL_GPL(kthread_unuse_mm);
diff --git kernel/mem_namespace.c kernel/mem_namespace.c
new file mode 100644
index 000000000..2d993160a
--- /dev/null
+++ kernel/mem_namespace.c
@@ -0,0 +1,255 @@
+#include <linux/user_namespace.h>
+#include <linux/mem_namespace.h>
+#include <linux/proc_ns.h>
+#include <linux/cred.h>
+#include <linux/sched/task.h>
+#include <linux/slab.h>
+
+#include <bhv/domain.h>
+
+uint64_t get_free_domain(void)
+{
+	uint64_t domain = BHV_INVALID_DOMAIN;
+	bhv_domain_create(&domain);
+	return domain;
+}
+
+void put_domain(uint64_t domain)
+{
+	/*
+	 * XXX: Do we need to destroy nested, higher-level domains that belong
+	 * to the acestor tree of this domain if they are still around?
+	 */
+
+	if (domain == BHV_INVALID_DOMAIN)
+		return;
+
+	/*
+	 * We assume that the caller takes the necessary steps to switch to
+	 * another, valid domain before putting/releasing the given domain.
+	 */
+
+	BUG_ON(bhv_get_domain(current) == domain);
+
+	bhv_domain_destroy(domain);
+}
+
+static struct kmem_cache *mem_ns_cache;
+
+struct mem_namespace init_mem_ns = {
+	.kref = KREF_INIT(2),
+	.user_ns = &init_user_ns,
+	.domain = BHV_INIT_DOMAIN,
+	.ns.inum = PROC_MEM_INIT_INO,
+	.level = 0,
+	.parent = NULL,
+#ifdef CONFIG_MEM_NS
+	.ns.ops = &memns_operations,
+#endif
+};
+EXPORT_SYMBOL_GPL(init_mem_ns);
+
+struct mem_namespace *memns_of_task(const struct task_struct *task)
+{
+	/*
+	 * Kernel threads, and threads that do not act on behalf of a user space
+	 * task, do not have a valid nsproxy. These threads shall switch to the
+	 * default memory namespace that we use for the init_task.
+	 * Alternatively, we can define a dedicated memory namespace, which all
+	 * kernel threads will enter if they do not execute on behalf of a user
+	 * space task.
+	 */
+	if (task == NULL || task->nsproxy == NULL)
+		return init_task.nsproxy->mem_ns;
+
+	return task->nsproxy->mem_ns;
+}
+
+bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return memns_of_task(current) == memns_of_task(task);
+}
+
+static struct ucounts *inc_mem_namespaces(struct user_namespace *ns)
+{
+	return inc_ucount(ns, current_euid(), UCOUNT_MEM_NAMESPACES);
+}
+
+static void dec_mem_namespaces(struct ucounts *ucounts)
+{
+	dec_ucount(ucounts, UCOUNT_MEM_NAMESPACES);
+}
+
+static struct mem_namespace *create_mem_namespace(struct user_namespace *user_ns,
+						  struct mem_namespace *parent_ns)
+{
+	struct mem_namespace *ns = NULL;
+	unsigned int level = parent_ns->level + 1;
+	struct ucounts *ucounts;
+	uint64_t domain = 0;
+	int err = -EINVAL;
+
+	if (!in_userns(parent_ns->user_ns, user_ns))
+		goto out;
+
+	/* XXX: Consider limiting the number of nested memory namespaces. */
+
+	domain = get_free_domain();
+	if (domain == BHV_INVALID_DOMAIN && bhv_domain_is_active())
+		goto out;
+
+	ucounts = inc_mem_namespaces(user_ns);
+	if (!ucounts)
+		goto out_domain;
+
+	err = -ENOMEM;
+	ns = kmem_cache_zalloc(mem_ns_cache, GFP_KERNEL);
+	if (ns == NULL)
+		goto out_dec;
+
+	err = ns_alloc_inum(&ns->ns);
+	if (err)
+		goto out_free;
+
+	kref_init(&ns->kref);
+	ns->level = level;
+	ns->parent = get_mem_ns(parent_ns);
+	ns->ns.ops = &memns_operations;
+	ns->domain = domain;
+	ns->user_ns = get_user_ns(user_ns);
+	ns->ucounts = ucounts;
+
+	return ns;
+
+out_free:
+	kmem_cache_free(mem_ns_cache, ns);
+out_dec:
+	dec_mem_namespaces(ucounts);
+out_domain:
+	put_domain(domain);
+out:
+	return ERR_PTR(err);
+}
+
+struct mem_namespace *copy_mem_ns(unsigned long flags,
+				  struct user_namespace *user_ns,
+				  struct mem_namespace *old_ns)
+{
+	BUG_ON(!old_ns);
+
+	if (!(flags & CLONE_NEWMEM)) {
+		return get_mem_ns(old_ns);
+	}
+
+	/*
+	 * XXX: Consider performing additional checks (see pid_namespaces.c); we
+	 * shall proceed only if the old_ns corresponds to the namespace, in
+	 * which the current task resides.
+	 */
+
+	return create_mem_namespace(user_ns, old_ns);
+}
+
+static void destroy_mem_namespace(struct mem_namespace *ns)
+{
+	put_domain(ns->domain);
+	ns_free_inum(&ns->ns);
+
+	/*
+	 * XXX: Make the namespace leverage RCU (see pid_namespace.c)!
+	 */
+
+	dec_mem_namespaces(ns->ucounts);
+	put_user_ns(ns->user_ns);
+
+	kmem_cache_free(mem_ns_cache, ns);
+}
+
+void free_mem_ns(struct kref *kref)
+{
+	struct mem_namespace *ns = container_of(kref, struct mem_namespace, kref);
+	destroy_mem_namespace(ns);
+}
+
+static inline struct mem_namespace *to_mem_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct mem_namespace, ns);
+}
+
+static struct ns_common *memns_get(struct task_struct *task)
+{
+	struct mem_namespace *ns = NULL;
+	struct nsproxy *nsproxy;
+
+	task_lock(task);
+	nsproxy = task->nsproxy;
+	if (nsproxy) {
+		ns = nsproxy->mem_ns;
+		get_mem_ns(ns);
+	}
+	task_unlock(task);
+
+	return ns ? &ns->ns : NULL;
+}
+
+static void memns_put(struct ns_common *ns)
+{
+	put_mem_ns(to_mem_ns(ns));
+}
+
+bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns)
+{
+	struct mem_namespace *task_ns = memns_of_task(task);
+	struct mem_namespace *ancestor = ns;
+
+	if (ancestor->level < task_ns->level)
+		return false;
+
+	while (ancestor->level > task_ns->level) {
+		ancestor = ancestor->parent;
+	}
+
+	return (ancestor == task_ns);
+}
+
+static int memns_install(struct nsset *nsset, struct ns_common *ns)
+{
+	struct nsproxy *nsproxy = nsset->nsproxy;
+	struct mem_namespace *new = to_mem_ns(ns);
+
+	if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
+	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!task_in_ancestor_memns(current, new))
+		return -EINVAL;
+
+	put_mem_ns(nsproxy->mem_ns);
+	nsproxy->mem_ns = get_mem_ns(new);
+
+	/* XXX: Do we need an explicit mem_ns_for_children? */
+
+	return 0;
+}
+
+static struct user_namespace *memns_owner(struct ns_common *ns)
+{
+	return to_mem_ns(ns)->user_ns;
+}
+
+const struct proc_ns_operations memns_operations = {
+	.name		= "mem",
+	.type		= CLONE_NEWMEM,
+	.get		= memns_get,
+	.put		= memns_put,
+	.install	= memns_install,
+	.owner		= memns_owner,
+};
+
+static int __init mem_ns_init(void)
+{
+	mem_ns_cache = KMEM_CACHE(mem_namespace, SLAB_PANIC);
+	return 0;
+}
+
+__initcall(mem_ns_init);
diff --git kernel/module.c kernel/module.c
index 72a5dcdcc..dd1d47188 100644
--- kernel/module.c
+++ kernel/module.c
@@ -60,6 +60,8 @@
 #include <uapi/linux/module.h>
 #include "module-internal.h"
 
+#include <bhv/module.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -89,7 +91,7 @@
  * (delete and add uses RCU list operations). */
 DEFINE_MUTEX(module_mutex);
 EXPORT_SYMBOL_GPL(module_mutex);
-static LIST_HEAD(modules);
+LIST_HEAD(modules);
 
 /* Work queue for freeing init sections in success case */
 static void do_free_init(struct work_struct *w);
@@ -273,6 +275,14 @@ static void module_assert_mutex_or_preempt(void)
 }
 
 #ifdef CONFIG_MODULE_SIG
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+#define sig_enforce true
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
@@ -280,6 +290,7 @@ void set_module_sig_enforced(void)
 {
 	sig_enforce = true;
 }
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 #else
 #define sig_enforce false
 #endif
@@ -2252,6 +2263,8 @@ static void free_module(struct module *mod)
 	synchronize_rcu();
 	mutex_unlock(&module_mutex);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
@@ -3633,6 +3646,7 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
 	module_memfree(mod->core_layout.base);
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -3777,6 +3791,7 @@ static noinline int do_init_module(struct module *mod)
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	mod->init_layout.base = NULL;
 	mod->init_layout.size = 0;
 	mod->init_layout.ro_size = 0;
@@ -4091,10 +4106,14 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
 
+	security_module_loaded(mod);
+
 	/* Module is ready to execute: parsing args may do that. */
 	after_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,
 				  -32768, 32767, mod,
diff --git kernel/nsproxy.c kernel/nsproxy.c
index 12dd41b39..7589e03fc 100644
--- kernel/nsproxy.c
+++ kernel/nsproxy.c
@@ -16,6 +16,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/utsname.h>
 #include <linux/pid_namespace.h>
+#include <linux/mem_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
 #include <linux/time_namespace.h>
@@ -27,6 +28,8 @@
 #include <linux/cgroup.h>
 #include <linux/perf_event.h>
 
+#include <bhv/domain.h>
+
 static struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = {
@@ -47,6 +50,9 @@ struct nsproxy init_nsproxy = {
 	.time_ns		= &init_time_ns,
 	.time_ns_for_children	= &init_time_ns,
 #endif
+#ifdef CONFIG_MEM_NS
+	.mem_ns			= &init_mem_ns,
+#endif
 };
 
 static inline struct nsproxy *create_nsproxy(void)
@@ -75,6 +81,10 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	if (!new_nsp)
 		return ERR_PTR(-ENOMEM);
 
+	if (bhv_check_memns_enable_flags(flags)) {
+		flags |= CLONE_NEWMEM;
+	}
+
 	new_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);
 	if (IS_ERR(new_nsp->mnt_ns)) {
 		err = PTR_ERR(new_nsp->mnt_ns);
@@ -121,8 +131,19 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	}
 	new_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);
 
+	new_nsp->mem_ns = copy_mem_ns(flags, user_ns, tsk->nsproxy->mem_ns);
+	if (IS_ERR(new_nsp->mem_ns)) {
+		err = PTR_ERR(new_nsp->mem_ns);
+		goto out_mem;
+	}
+
 	return new_nsp;
 
+out_mem:
+	if (new_nsp->time_ns)
+		put_time_ns(new_nsp->time_ns);
+	if (new_nsp->time_ns_for_children)
+		put_time_ns(new_nsp->time_ns_for_children);
 out_time:
 	put_net(new_nsp->net_ns);
 out_net:
@@ -157,7 +178,7 @@ int copy_namespaces(unsigned long flags, struct task_struct *tsk)
 
 	if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			      CLONE_NEWPID | CLONE_NEWNET |
-			      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {
+			      CLONE_NEWCGROUP | CLONE_NEWTIME | CLONE_NEWMEM)))) {
 		if (likely(old_ns->time_ns_for_children == old_ns->time_ns)) {
 			get_nsproxy(old_ns);
 			return 0;
@@ -204,6 +225,8 @@ void free_nsproxy(struct nsproxy *ns)
 		put_time_ns(ns->time_ns);
 	if (ns->time_ns_for_children)
 		put_time_ns(ns->time_ns_for_children);
+	if (ns->mem_ns)
+		put_mem_ns(ns->mem_ns);
 	put_cgroup_ns(ns->cgroup_ns);
 	put_net(ns->net_ns);
 	kmem_cache_free(nsproxy_cachep, ns);
@@ -221,7 +244,7 @@ int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 
 	if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |
-			       CLONE_NEWTIME)))
+			       CLONE_NEWTIME | CLONE_NEWMEM)))
 		return 0;
 
 	user_ns = new_cred ? new_cred->user_ns : current_user_ns();
@@ -250,6 +273,31 @@ void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
 	p->nsproxy = new;
 	task_unlock(p);
 
+	/*
+	 * Move the task's address space to the given domain only if we do not
+	 * destroy the nsproxy that the task is about to switch to is valid.
+	 * Switching to an invalid nsproxy (nsproxy == NULL) means that the task
+	 * is about to be destroyed.
+	 */
+	if (new != NULL) {
+		bhv_domain_transfer_mm(p->mm, ns, new);
+
+		// If we change the domain of the current process, we need to switch.
+		if (current == p) {
+			bhv_domain_enter(p);
+		}
+	} else {
+		/*
+		* Note that  bhv_domain_enter will automatically determine which domain
+		* to switch to (i.e., to the task's domain maintained by its nsproxy or
+		* to the default domain of init_task).
+		*
+		* XXX: If the task switches to an invalid nsproxy, we should consider
+		* switching to the parent's domain.
+		*/
+		bhv_domain_enter(p);
+	}
+
 	if (ns && atomic_dec_and_test(&ns->count))
 		free_nsproxy(ns);
 }
@@ -263,7 +311,7 @@ static int check_setns_flags(unsigned long flags)
 {
 	if (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 				 CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |
-				 CLONE_NEWPID | CLONE_NEWCGROUP)))
+				 CLONE_NEWPID | CLONE_NEWCGROUP | CLONE_NEWMEM)))
 		return -EINVAL;
 
 #ifndef CONFIG_USER_NS
@@ -294,6 +342,10 @@ static int check_setns_flags(unsigned long flags)
 	if (flags & CLONE_NEWTIME)
 		return -EINVAL;
 #endif
+#ifndef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM)
+		return -EINVAL;
+#endif
 
 	return 0;
 }
@@ -476,6 +528,14 @@ static int validate_nsset(struct nsset *nsset, struct pid *pid)
 	}
 #endif
 
+#ifdef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM) {
+		ret = validate_ns(nsset, &nsp->mem_ns->ns);
+		if (ret)
+			goto out;
+	}
+#endif
+
 out:
 	if (pid_ns)
 		put_pid_ns(pid_ns);
@@ -546,6 +606,10 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 			err = -EINVAL;
 		flags = ns->ops->type;
 	} else if (!IS_ERR(pidfd_pid(file))) {
+		if (bhv_check_memns_enable_flags(flags)) {
+			flags |= CLONE_NEWMEM;
+		}
+
 		err = check_setns_flags(flags);
 	} else {
 		err = -EINVAL;
@@ -558,12 +622,26 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 		goto out;
 
 	if (proc_ns_file(file))
+#ifdef CONFIG_MEM_NS
+		/*
+		 * XXX: Note that we cannot piggy-back memory namespaces onto
+		 * pid namespaces during nsset/nsenter if no pidfd is given.
+		 * This is because we cannot identify the associated memory
+		 * namespace without any additional links between the
+		 * pid_namespace and the mem_namespace data structures. Consider
+		 * adding links or using pid namespaces alone for creating
+		 * memory isolation domains.
+		 */
+#endif
 		err = validate_ns(&nsset, ns);
 	else
 		err = validate_nsset(&nsset, file->private_data);
 	if (!err) {
-		commit_nsset(&nsset);
-		perf_event_namespaces(current);
+		err = security_setns(current, &nsset);
+		if (!err) {
+			commit_nsset(&nsset);
+			perf_event_namespaces(current);
+		}
 	}
 	put_nsset(&nsset);
 out:
diff --git kernel/printk/printk.c kernel/printk/printk.c
index a8af93cbc..e3eef9f45 100644
--- kernel/printk/printk.c
+++ kernel/printk/printk.c
@@ -932,7 +932,7 @@ static int devkmsg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations kmsg_fops = {
+const struct file_operations kmsg_fops __section(".rodata") = {
 	.open = devkmsg_open,
 	.read = devkmsg_read,
 	.write_iter = devkmsg_write,
diff --git kernel/reboot.c kernel/reboot.c
index e297b35fc..993b1b736 100644
--- kernel/reboot.c
+++ kernel/reboot.c
@@ -417,8 +417,14 @@ void ctrl_alt_del(void)
 		kill_cad_pid(SIGINT, 1);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+const char poweroff_cmd[] __section(".rodata") = "/sbin/poweroff";
+static const char reboot_cmd[] __section(".rodata") = "/sbin/reboot";
+#else
 char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
 static const char reboot_cmd[] = "/sbin/reboot";
+#endif
+
 
 static int run_cmd(const char *cmd)
 {
diff --git kernel/sched/core.c kernel/sched/core.c
index 40f40f359..7a8de22e1 100644
--- kernel/sched/core.c
+++ kernel/sched/core.c
@@ -17,6 +17,10 @@
 #include <linux/kcov.h>
 #include <linux/scs.h>
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+#include <bhv/integrity.h>
+
 #include <asm/switch_to.h>
 #include <asm/tlb.h>
 
@@ -3773,6 +3777,8 @@ context_switch(struct rq *rq, struct task_struct *prev,
 			mmgrab(prev->active_mm);
 		else
 			prev->active_mm = NULL;
+
+		bhv_pt_protect_check_pgd(next->active_mm);
 	} else {                                        // to user
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
diff --git kernel/sysctl.c kernel/sysctl.c
index 99a191901..7a4f7f071 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -1984,7 +1984,11 @@ static struct ctl_table kern_table[] = {
 		.procname	= "core_pattern",
 		.data		= core_pattern,
 		.maxlen		= CORENAME_MAX_SIZE,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+#else
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring_coredump,
 	},
 	{
@@ -2150,9 +2154,15 @@ static struct ctl_table kern_table[] = {
 #ifdef CONFIG_MODULES
 	{
 		.procname	= "modprobe",
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.data		= (char *)&modprobe_path,
+		.maxlen		= KMOD_PATH_LEN,
+		.mode		= 0444,
+#else
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring,
 	},
 	{
@@ -2613,9 +2623,15 @@ static struct ctl_table kern_table[] = {
 #endif
 	{
 		.procname	= "poweroff_cmd",
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.data		= (char *)&poweroff_cmd,
+		.maxlen		= POWEROFF_CMD_PATH_LEN,
+		.mode		= 0444,
+#else
 		.data		= &poweroff_cmd,
 		.maxlen		= POWEROFF_CMD_PATH_LEN,
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring,
 	},
 #ifdef CONFIG_KEYS
diff --git kernel/trace/Kconfig kernel/trace/Kconfig
index 29db703f6..8b2ce9c58 100644
--- kernel/trace/Kconfig
+++ kernel/trace/Kconfig
@@ -126,6 +126,7 @@ if TRACING_SUPPORT
 menuconfig FTRACE
 	bool "Tracers"
 	default y if DEBUG_KERNEL
+	depends on !BHV_LOCKDOWN
 	help
 	  Enable the kernel tracing infrastructure.
 
@@ -360,6 +361,7 @@ config MMIOTRACE
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
+	depends on !BHV_LOCKDOWN
 	select TRACING
 	help
 	  This tracer hooks to various trace points in the kernel,
diff --git kernel/ucount.c kernel/ucount.c
index 11b1596e2..92a8ae8cd 100644
--- kernel/ucount.c
+++ kernel/ucount.c
@@ -74,6 +74,7 @@ static struct ctl_table user_table[] = {
 	UCOUNT_ENTRY("max_inotify_instances"),
 	UCOUNT_ENTRY("max_inotify_watches"),
 #endif
+	UCOUNT_ENTRY("max_mem_namespaces"),
 	{ }
 };
 #endif /* CONFIG_SYSCTL */
diff --git kernel/umh.c kernel/umh.c
index 3f646613a..50a705e54 100644
--- kernel/umh.c
+++ kernel/umh.c
@@ -30,6 +30,10 @@
 
 #include <trace/events/module.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CAP_BSET	(void *)1
 #define CAP_PI		(void *)2
 
@@ -107,6 +111,12 @@ static int call_usermodehelper_exec_async(void *data)
 
 	commit_creds(new);
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kernel_exec_events())
+		bhv_guestlog_log_kernel_exec(sub_info->path, sub_info->argv,
+					     sub_info->envp);
+#endif
+
 	retval = kernel_execve(sub_info->path,
 			       (const char *const *)sub_info->argv,
 			       (const char *const *)sub_info->envp);
diff --git lib/Kconfig.debug lib/Kconfig.debug
index 24ca61cf8..294e518b8 100644
--- lib/Kconfig.debug
+++ lib/Kconfig.debug
@@ -527,6 +527,7 @@ endmenu
 
 config DEBUG_KERNEL
 	bool "Kernel debugging"
+	depends on !BHV_LOCKDOWN
 	help
 	  Say Y here if you are developing drivers or trying to debug and
 	  identify kernel problems.
diff --git lib/Kconfig.kgdb lib/Kconfig.kgdb
index 05dae05b6..828c9bc65 100644
--- lib/Kconfig.kgdb
+++ lib/Kconfig.kgdb
@@ -28,6 +28,7 @@ config KGDB_HONOUR_BLOCKLIST
 	bool "KGDB: use kprobe blocklist to prohibit unsafe breakpoints"
 	depends on HAVE_KPROBES
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	select KPROBES
 	default y
 	help
diff --git mm/filemap.c mm/filemap.c
index 3b0d8c6dd..c74a299b8 100644
--- mm/filemap.c
+++ mm/filemap.c
@@ -2893,6 +2893,7 @@ void filemap_map_pages(struct vm_fault *vmf,
 		last_pgoff = xas.xa_index;
 		if (alloc_set_pte(vmf, page))
 			goto unlock;
+
 		unlock_page(head);
 		goto next;
 unlock:
@@ -2904,7 +2905,9 @@ void filemap_map_pages(struct vm_fault *vmf,
 		if (pmd_trans_huge(*vmf->pmd))
 			break;
 	}
+
 	rcu_read_unlock();
+
 	WRITE_ONCE(file->f_ra.mmap_miss, mmap_miss);
 }
 EXPORT_SYMBOL(filemap_map_pages);
diff --git mm/gup.c mm/gup.c
index 11307a8b2..edf6755e9 100644
--- mm/gup.c
+++ mm/gup.c
@@ -21,6 +21,8 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
+#include <linux/mem_namespace.h>
+
 #include "internal.h"
 
 struct follow_page_context {
@@ -923,6 +925,11 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 	int write = (gup_flags & FOLL_WRITE);
 	int foreign = (gup_flags & FOLL_REMOTE);
 
+#ifdef CONFIG_MEM_NS
+	struct mm_struct *mm = vma->vm_mm;
+	struct task_struct *t = mm->owner;
+#endif
+
 	if (vm_flags & (VM_IO | VM_PFNMAP))
 		return -EFAULT;
 
@@ -930,6 +937,17 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 		return -EFAULT;
 
 	if (write) {
+#ifdef CONFIG_MEM_NS
+		/*
+		 * Grant write access to remote address spaces only if both
+		 * processes are executing inside of the same mem_namespace.
+		 */
+		if (!current_in_same_mem_ns(t)) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
 		if (!(vm_flags & VM_WRITE)) {
 			if (!(gup_flags & FOLL_FORCE))
 				return -EFAULT;
@@ -944,17 +962,42 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 			 */
 			if (!is_cow_mapping(vm_flags))
 				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
 		}
-	} else if (!(vm_flags & VM_READ)) {
-		if (!(gup_flags & FOLL_FORCE))
-			return -EFAULT;
+	} else {
+#ifdef CONFIG_MEM_NS
 		/*
-		 * Is there actually any vma we can reach here which does not
-		 * have VM_MAYREAD set?
+		 * Grant read access to remote address spaces only if both
+		 * processes are part of the same ancestor tree branch the
+		 * target mem_namespace.
 		 */
-		if (!(vm_flags & VM_MAYREAD))
-			return -EFAULT;
+		if (!task_in_ancestor_memns(current, memns_of_task(t))) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
+		if (!(vm_flags & VM_READ)) {
+			if (!(gup_flags & FOLL_FORCE))
+				return -EFAULT;
+			/*
+			 * Is there actually any vma we can reach here which does not
+			 * have VM_MAYREAD set?
+			 */
+			if (!(vm_flags & VM_MAYREAD))
+				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
+		}
 	}
+
 	/*
 	 * gups are always data accesses, not instruction
 	 * fetches, so execute=false here
diff --git mm/khugepaged.c mm/khugepaged.c
index 28e18777e..02eb8fa95 100644
--- mm/khugepaged.c
+++ mm/khugepaged.c
@@ -605,6 +605,11 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 	int none_or_zero = 0, shared = 0, result = 0, referenced = 0;
 	bool writable = false;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte+HPAGE_PMD_NR;
 	     _pte++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -728,9 +733,16 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		result = SCAN_SUCCEED;
 		trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 						    referenced, writable, result);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
 		return 1;
 	}
 out:
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	release_pte_pages(pte, _pte, compound_pagelist);
 	trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 					    referenced, writable, result);
@@ -745,6 +757,12 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 {
 	struct page *src_page, *tmp;
 	pte_t *_pte;
+
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 				_pte++, page++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -761,6 +779,8 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 				 * paravirt calls inside pte_clear here are
 				 * superfluous.
 				 */
+				bhv_domain_clear_pte(vma->vm_mm, address, _pte,
+						     *_pte);
 				pte_clear(vma->vm_mm, address, _pte);
 				spin_unlock(ptl);
 			}
@@ -779,6 +799,7 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 			 * paravirt calls inside pte_clear here are
 			 * superfluous.
 			 */
+			bhv_domain_clear_pte(vma->vm_mm, address, _pte, *_pte);
 			pte_clear(vma->vm_mm, address, _pte);
 			page_remove_rmap(src_page, false);
 			spin_unlock(ptl);
@@ -790,6 +811,10 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 		list_del(&src_page->lru);
 		release_pte_page(src_page);
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 static void khugepaged_alloc_sleep(void)
diff --git mm/maccess.c mm/maccess.c
index f6ea117a6..c51a983a6 100644
--- mm/maccess.c
+++ mm/maccess.c
@@ -5,6 +5,17 @@
 #include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/uaccess.h>
+#include <asm-generic/sections.h>
+
+__always_inline static bool is_vault(const void *addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
 
 bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 		size_t size)
@@ -24,6 +35,10 @@ bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 
 long copy_from_kernel_nofault(void *dst, const void *src, size_t size)
 {
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(src, size))
 		return -ERANGE;
 
@@ -68,6 +83,10 @@ long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr, long count)
 
 	if (unlikely(count <= 0))
 		return 0;
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(unsafe_addr, count))
 		return -ERANGE;
 
diff --git mm/memory.c mm/memory.c
index 218300368..50cac204d 100644
--- mm/memory.c
+++ mm/memory.c
@@ -76,6 +76,8 @@
 
 #include <trace/events/kmem.h>
 
+#include <bhv/domain.h>
+
 #include <asm/io.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
@@ -1676,7 +1678,7 @@ static int insert_page_into_pte_locked(struct mm_struct *mm, pte_t *pte,
 	get_page(page);
 	inc_mm_counter_fast(mm, mm_counter_file(page));
 	page_add_file_rmap(page, false);
-	set_pte_at(mm, addr, pte, mk_pte(page, prot));
+	bhv_domain_set_pte_at_kernel(mm, addr, pte, mk_pte(page, prot));
 	return 0;
 }
 
@@ -1996,7 +1998,10 @@ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	}
 
-	set_pte_at(mm, addr, pte, entry);
+	if (pfn_t_devmap(pfn))
+		set_pte_at(mm, addr, pte, entry);
+	else
+		bhv_domain_set_pte_at_kernel(mm, addr, pte, entry);
 	update_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */
 
 out_unlock:
@@ -3963,6 +3968,7 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 		ret = alloc_set_pte(vmf, page);
 	if (vmf->pte)
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
+
 	return ret;
 }
 
@@ -4987,10 +4993,18 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags & FOLL_WRITE;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (mmap_read_lock_killable(mm))
 		return 0;
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(mm->owner);
+#endif
+
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
@@ -5039,6 +5053,11 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
 		buf += bytes;
 		addr += bytes;
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	mmap_read_unlock(mm);
 
 	return buf - old_buf;
@@ -5200,6 +5219,10 @@ static void clear_gigantic_page(struct page *page,
 	struct page *p = page;
 
 	might_sleep();
+
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
+
 	for (i = 0; i < pages_per_huge_page;
 	     i++, p = mem_map_next(p, page, i)) {
 		cond_resched();
@@ -5225,6 +5248,8 @@ void clear_huge_page(struct page *page,
 		return;
 	}
 
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
 	process_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);
 }
 
diff --git mm/mmap.c mm/mmap.c
index 33ebda838..2575cefc7 100644
--- mm/mmap.c
+++ mm/mmap.c
@@ -2557,6 +2557,7 @@ int expand_downwards(struct vm_area_struct *vma,
 			}
 		}
 	}
+
 	anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
 	validate_mm(mm);
@@ -2650,6 +2651,10 @@ static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 		if (vma->vm_flags & VM_ACCOUNT)
 			nr_accounted += nrpages;
 		vm_stat_account(mm, vma->vm_flags, -nrpages);
+		/*
+		 * XXX: Create a hypercall that registers all VMAs that are to
+		 * be released.
+		 */
 		vma = remove_vma(vma);
 	} while (vma);
 	vm_unacct_memory(nr_accounted);
diff --git mm/page_owner.c mm/page_owner.c
index b735a8eaf..ec20d8392 100644
--- mm/page_owner.c
+++ mm/page_owner.c
@@ -635,7 +635,7 @@ static void init_early_allocated_pages(void)
 		init_zones_in_node(pgdat);
 }
 
-static const struct file_operations proc_page_owner_operations = {
+const struct file_operations proc_page_owner_operations __section(".rodata") = {
 	.read		= read_page_owner,
 };
 
diff --git mm/page_poison.c mm/page_poison.c
index ae0482cde..a5d0f8be8 100644
--- mm/page_poison.c
+++ mm/page_poison.c
@@ -132,8 +132,12 @@ void kernel_poison_pages(struct page *page, int numpages, int enable)
 
 	if (enable)
 		unpoison_pages(page, numpages);
-	else
+	else {
+		bhv_domain_map_kernel(
+			current->mm != NULL ? current->mm : current->active_mm,
+			page_to_pfn(page), numpages, true, true, false);
 		poison_pages(page, numpages);
+	}
 }
 
 #ifndef CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC
diff --git mm/pgtable-generic.c mm/pgtable-generic.c
index 4e640baf9..36edd6ed1 100644
--- mm/pgtable-generic.c
+++ mm/pgtable-generic.c
@@ -12,6 +12,10 @@
 #include <linux/pgtable.h>
 #include <asm/tlb.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 /*
  * If a p?d_bad entry is found while walking page tables, report
  * the error, before resetting entry to p?d_none.  Usually (but
@@ -194,7 +198,11 @@ pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
 pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 		     pmd_t *pmdp)
 {
-	pmd_t old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
+	pmd_t old;
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pmd(vma->vm_mm, address, pmdp, pmd_mkinvalid(*pmdp));
+#endif
+	old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return old;
 }
diff --git mm/shmem.c mm/shmem.c
index e173d83b4..b509f56f6 100644
--- mm/shmem.c
+++ mm/shmem.c
@@ -247,7 +247,7 @@ static inline void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 
 static const struct super_operations shmem_ops;
 static const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -3899,7 +3899,7 @@ static const struct address_space_operations shmem_aops = {
 	.error_remove_page = generic_error_remove_page,
 };
 
-static const struct file_operations shmem_file_operations = {
+const struct file_operations shmem_file_operations __section(".rodata") = {
 	.mmap		= shmem_mmap,
 	.get_unmapped_area = shmem_get_unmapped_area,
 #ifdef CONFIG_TMPFS
diff --git mm/vmscan.c mm/vmscan.c
index e2b8cee1d..8343f461b 100644
--- mm/vmscan.c
+++ mm/vmscan.c
@@ -1452,8 +1452,9 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 		 */
 		if (unlikely(PageTransHuge(page)))
 			destroy_compound_page(page);
-		else
+		else {
 			list_add(&page->lru, &free_pages);
+		}
 		continue;
 
 activate_locked_split:
diff --git net/Kconfig net/Kconfig
index a22c3fb88..1d58c9d59 100644
--- net/Kconfig
+++ net/Kconfig
@@ -292,6 +292,7 @@ config BPF_JIT
 	bool "enable BPF Just In Time compiler"
 	depends on HAVE_CBPF_JIT || HAVE_EBPF_JIT
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	help
 	  Berkeley Packet Filter filtering capabilities are normally handled
 	  by an interpreter. This option allows kernel to generate a native
diff --git net/socket.c net/socket.c
index 2a48aa89c..d3e6e7c50 100644
--- net/socket.c
+++ net/socket.c
@@ -146,7 +146,7 @@ static void sock_show_fdinfo(struct seq_file *m, struct file *f)
  *	in the operation structures but are done directly via the socketcall() multiplexor.
  */
 
-static const struct file_operations socket_file_ops = {
+const struct file_operations socket_file_ops = {
 	.owner =	THIS_MODULE,
 	.llseek =	no_llseek,
 	.read_iter =	sock_read_iter,
diff --git security/Kconfig security/Kconfig
index 9893c316d..61ce04796 100644
--- security/Kconfig
+++ security/Kconfig
@@ -222,6 +222,15 @@ config STATIC_USERMODEHELPER_PATH
 	  If you wish for all usermode helper programs to be disabled,
 	  specify an empty string here (i.e. "").
 
+config MEM_NS
+	bool "Enable memory namespaces"
+	depends on MEMCG
+	depends on BHV_VAS
+	default y
+	help
+	  Enable memory namespaces.
+
+source "security/bhv/Kconfig"
 source "security/selinux/Kconfig"
 source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
@@ -269,11 +278,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf" if DEFAULT_SECURITY_SMACK
-	default "lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
-	default "lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
+	default "lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "lockdown,yama,loadpin,safesetid,integrity,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list will be ignored. This can be
diff --git security/Makefile security/Makefile
index 3baf435de..aa60209b2 100644
--- security/Makefile
+++ security/Makefile
@@ -10,6 +10,7 @@ subdir-$(CONFIG_SECURITY_TOMOYO)        += tomoyo
 subdir-$(CONFIG_SECURITY_APPARMOR)	+= apparmor
 subdir-$(CONFIG_SECURITY_YAMA)		+= yama
 subdir-$(CONFIG_SECURITY_LOADPIN)	+= loadpin
+subdir-$(CONFIG_BHV_VAS)		+= bhv
 subdir-$(CONFIG_SECURITY_SAFESETID)    += safesetid
 subdir-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown
 subdir-$(CONFIG_BPF_LSM)		+= bpf
@@ -28,6 +29,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_BHV_VAS)			+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 000000000..1475845d6
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,74 @@
+config BHV_VAS
+	bool "BHV guest support and VAS LSM"
+	default y
+	depends on (X86_64) || (ARM64 && OF)
+	select VIRTIO_VSOCKETS
+	select VSOCKETS
+	select VIRTIO_VSOCKETS_COMMON
+	select EXT4_FS
+	select XFS_FS
+	select BLOCK
+	help
+	  Say Y if you want to enable the BHV LSM and run Linux in a Virtual
+	  Machine on BHV and benefit from Virtualization-assisted Security.
+
+config BHV_PANIC_ON_FAIL
+	bool "BHV guest panics on Hypercall failure"
+	default y
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	bool "Build BHV guest support with DEBUG information"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_TRACEPOINTS
+	bool "Enable BHV Tracepoints"
+	default n
+	depends on BHV_VAS_DEBUG && TRACEPOINTS
+	help
+	  Say Y if you want to enable BHV tracepoints. Note: do not enable this option in production systems.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	bool "Make all paths passed to call_usermodehelper in the kernel constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in the kernel are constant. This implies
+	  that these paths cannot be updated via sysctl. Paths include
+	  the modprobe path and the poweroff path. This setting is recommended,
+	  since these strings are often updated in exploits.
+
+config BHV_CONST_CALL_USERMODEHELPER_MODULES
+	bool "Make all paths passed to call_usermodehelper in modules constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in drivers constant. This implies that
+	  these paths cannot be updated via sysctl. This setting is recommended,
+	  to harden modules against exploits.
+
+config BHV_LOCKDOWN
+	bool "Enable the most secure BHV settings (Lockdown)"
+	default n
+	depends on BHV_VAS
+	select BHV_PANIC_ON_FAIL
+	select BHV_CONST_MODPROBE_PATH
+	help
+	  Say Y if you want to enable the most secure BHV settings
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 000000000..c4655ddd7
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,43 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+#          Tommaso Frassetto <tommaso@bedrocksystems.com>
+
+ccflags-y+=-DKERNEL_COMMIT_HASH=\"6a03c89e4027f45a4ad35b75f91028c51d083509+c753b458d70dab0e505933915e701ee4d2247f57\"
+
+obj-$(CONFIG_BHV_VAS)		:= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= abi_autogen.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/mm_init.o
+obj-$(CONFIG_BHV_VAS)		+= init/late_start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_bpf.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn.o
+obj-$(CONFIG_BHV_VAS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_fops.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_integrity_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_version.o
+obj-$(CONFIG_BHV_VAS)		+= vmalloc_to_page.o
+obj-$(CONFIG_BHV_VAS)		+= domain.o
+obj-$(CONFIG_BHV_VAS)		+= lsm.o
+obj-$(CONFIG_BHV_VAS)		+= memory_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= inode.o
+ifeq ($(CONFIG_KEYS),y)
+obj-$(CONFIG_BHV_VAS)		+= keyring.o
+endif
diff --git security/bhv/abi_autogen.c security/bhv/abi_autogen.c
new file mode 100644
index 000000000..834519792
--- /dev/null
+++ security/bhv/abi_autogen.c
@@ -0,0 +1,141 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-06-25T12:28:17).
+ */
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        pr_info("HypABI__Init__Init__BHVData__BHVConfigBitmap: %s%s%s%s%s%s%s%s%s%s%s%s%s",
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(addr) ? "KERNEL_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(addr) ? "PROC_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(addr) ? "DRIVER_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(addr) ? "LOGGING " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(addr) ? "CREDS_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(addr) ? "FILE_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(addr) ? "GUEST_POLICY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(addr) ? "REGISTER_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(addr) ? "STRONG_ISOLATION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(addr) ? "KERNEL_INTEGRITY_PT_PROT " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(addr) ? "INODE_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(addr) ? "KEYRING_INTEGRITY " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__MemFlags: %s%s%s",
+                HypABI__Integrity__MemFlags__has_TRANSIENT(addr) ? "TRANSIENT " : "", 
+                HypABI__Integrity__MemFlags__has_MUTABLE(addr) ? "MUTABLE " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__Freeze__FreezeFlags: %s%s%s%s%s",
+                HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(addr) ? "CREATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(addr) ? "UPDATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(addr) ? "REMOVE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(addr) ? "PATCH " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        pr_info("HypABI__Guestlog__Init__GuestlogFlags: %s%s%s%s%s%s%s",
+                HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(addr) ? "PROCESS_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(addr) ? "DRIVER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(addr) ? "KERNEL_ACCESS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(addr) ? "UNKNOWN_FILEOPS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(addr) ? "KERNEL_EXEC_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(addr) ? "CONTAINER_EVENTS " : "", 
+                ""
+        );
+}
+
+struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Remove__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__Patch__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Guestlog__Init__arg__slab = NULL;
+
+void HypABI__init_slabs(void)
+{
+        HypABI__Integrity__Create__Mem_Region__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__Mem_Region__slab",
+                sizeof(HypABI__Integrity__Create__Mem_Region__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__Mem_Region__slab)
+                panic("Could not create slab HypABI__Integrity__Create__Mem_Region__slab!\n");
+        HypABI__Integrity__Create__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__arg__slab",
+                sizeof(HypABI__Integrity__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Create__arg__slab!\n");
+        HypABI__Integrity__Update__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Update__arg__slab",
+                sizeof(HypABI__Integrity__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Update__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Update__arg__slab!\n");
+        HypABI__Integrity__Remove__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Remove__arg__slab",
+                sizeof(HypABI__Integrity__Remove__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Remove__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Remove__arg__slab!\n");
+        HypABI__Integrity__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Freeze__arg__slab",
+                sizeof(HypABI__Integrity__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Freeze__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Freeze__arg__slab!\n");
+        HypABI__Integrity__PtpgInit__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__PtpgInit__arg__slab",
+                sizeof(HypABI__Integrity__PtpgInit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__PtpgInit__arg__slab)
+                panic("Could not create slab HypABI__Integrity__PtpgInit__arg__slab!\n");
+        HypABI__Patch__Patch__arg__slab = kmem_cache_create(
+                "HypABI__Patch__Patch__arg__slab",
+                sizeof(HypABI__Patch__Patch__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__Patch__arg__slab)
+                panic("Could not create slab HypABI__Patch__Patch__arg__slab!\n");
+        HypABI__Patch__PatchNoClose__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchNoClose__arg__slab",
+                sizeof(HypABI__Patch__PatchNoClose__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchNoClose__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchNoClose__arg__slab!\n");
+        HypABI__Patch__PatchViolation__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchViolation__arg__slab",
+                sizeof(HypABI__Patch__PatchViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchViolation__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchViolation__arg__slab!\n");
+        HypABI__Guestlog__Init__arg__slab = kmem_cache_create(
+                "HypABI__Guestlog__Init__arg__slab",
+                sizeof(HypABI__Guestlog__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Guestlog__Init__arg__slab)
+                panic("Could not create slab HypABI__Guestlog__Init__arg__slab!\n");
+}
diff --git security/bhv/acl.c security/bhv/acl.c
new file mode 100644
index 000000000..c94e8b680
--- /dev/null
+++ security/bhv/acl.c
@@ -0,0 +1,316 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/acl.h>
+
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define PATH_DELIMITER '/'
+
+bhv_acl_config_t *process_acl_config __ro_after_init = NULL;
+bhv_acl_config_t *driver_acl_config __ro_after_init = NULL;
+
+struct kmem_cache *bhv_acl_violation_cache;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+static void __init bhv_mm_init_acl_init(uint32_t op,
+					bhv_acl_config_t **global_acl_config)
+{
+	unsigned long r;
+	bhv_acl_config_t *acl_config;
+
+	bhv_acl_violation_cache = kmem_cache_create(
+		"bhv_acl_violation_cache", sizeof(bhv_acl_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	acl_config = (bhv_acl_config_t *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (acl_config == NULL) {
+		bhv_fail("Unable to allocate process acl config");
+		return;
+	}
+
+	acl_config->num_pages = 1;
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+	if (r) {
+		pr_err("proc acl init fail");
+		return;
+	}
+
+	if (!acl_config->valid) {
+		free_pages((unsigned long)acl_config, 0);
+
+		acl_config = (bhv_acl_config_t *)__get_free_pages(
+			GFP_KERNEL, order_base_2(acl_config->num_pages));
+
+		if (acl_config == NULL) {
+			bhv_fail("Unable to allocate process acl config");
+			return;
+		}
+
+		r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+		if (r) {
+			pr_err("proc acl init fail");
+			return;
+		}
+
+		if (!acl_config->valid) {
+			bhv_fail("host returned invalid configuration");
+			return;
+		}
+	}
+
+	// Protect memory
+	if (bhv_integrity_is_enabled()) {
+		HypABI__Integrity__Create__Mem_Region__T *config_region =
+			HypABI__Integrity__Create__Mem_Region__ALLOC();
+
+		config_region->start_addr = virt_to_phys(acl_config);
+		config_region->size = acl_config->num_pages * PAGE_SIZE;
+		config_region->type =
+			HypABI__Integrity__MemType__DATA_READ_ONLY;
+		config_region->flags = HypABI__Integrity__MemFlags__NONE;
+		config_region->next = BHV_INVALID_PHYS_ADDR;
+		strscpy(config_region->label, "ACL CONFIG",
+			HypABI__Integrity__MAX_LABEL_SIZE);
+
+		r = bhv_create_kern_phys_mem_region_hyp(0, config_region);
+
+		HypABI__Integrity__Create__Mem_Region__FREE(config_region);
+
+		if (r) {
+			pr_err("Unable to protect acl config");
+			return;
+		}
+	}
+
+	*global_acl_config = acl_config;
+}
+
+void __init bhv_mm_init_acl(void)
+{
+	if (bhv_acl_is_proc_acl_enabled())
+		bhv_mm_init_acl_init(BHV_VAS_ACL_OP_INIT_PROC_ACL,
+				     &process_acl_config);
+	if (bhv_acl_is_driver_acl_enabled())
+		bhv_mm_init_acl_init(BHV_VAS_ACL_OP_INIT_DRIVER_ACL,
+				     &driver_acl_config);
+}
+/************************************************************/
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+static bool _matches(const char *target, bool strip_ext,
+		     bhv_acl_config_t *acl_config)
+{
+	size_t target_len = 0;
+	size_t target_ext_len = 0;
+	uint16_t i;
+
+	BUG_ON(target[0] != PATH_DELIMITER);
+
+	// Get target len
+	target_len = strnlen(target, PATH_MAX);
+	if (strip_ext) {
+		target_ext_len = _get_ext_len(target);
+		target_len -= target_ext_len;
+	}
+
+	if (target_len == 0 || target_len >= PATH_MAX)
+		return false;
+
+	for (i = 0; i < acl_config->list_len; i++) {
+		const char *cur = ((char *)acl_config) + acl_config->list[i];
+		size_t cur_len = 0;
+
+		if (cur[0] != PATH_DELIMITER) {
+			if (_match_names(cur, target, target_ext_len,
+					 strip_ext))
+				return true;
+			else
+				continue;
+		}
+
+		cur_len = strnlen(cur, PATH_MAX);
+		if (strip_ext) {
+			cur_len -= _get_ext_len(cur);
+		}
+
+		if (cur_len == 0 || cur_len >= PATH_MAX)
+			continue;
+
+		if (cur[cur_len - 1] == '*') {
+			cur_len--;
+
+			if (target_len < cur_len)
+				continue;
+		} else if (target_len != cur_len)
+			continue;
+
+		if (strncmp(cur, target, cur_len) == 0)
+			return true;
+	}
+
+	return false;
+}
+
+static bool _block_entity(const char *target, bool strip_ext,
+			  bhv_acl_config_t *acl_config, uint32_t op)
+{
+	bool rv;
+	unsigned long r;
+	// Tell the compiler this points to volatile data as the hypercall
+	// will update it.
+	bhv_acl_violation_t *volatile acl_violation = NULL;
+	size_t target_len = strlen(target);
+	bool m;
+
+	if (acl_config == NULL || !acl_config->valid) {
+		bhv_fail("unable to resolve entity due to init error");
+		return false;
+	}
+
+	m = _matches(target, strip_ext, acl_config);
+
+	// Is this entity part of the allow list?
+	if (m && acl_config->is_allow)
+		return false;
+	// Is this entity _NOT_ in the deny list?
+	if (!m && !acl_config->is_allow)
+		return false;
+
+	BUG_ON(target_len >= PAGE_SIZE);
+
+	// Prepare hypercall
+	acl_violation = kmem_cache_alloc(bhv_acl_violation_cache, GFP_KERNEL);
+	if (acl_violation == NULL) {
+		bhv_fail("Unable to allocate acl violation");
+		return true;
+	}
+
+	// Get Context
+	r = populate_event_context(&acl_violation->context, true);
+	if (r) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	// Setup arg
+	acl_violation->name_len = target_len;
+	acl_violation->name = virt_to_phys((volatile void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_violation);
+	if (r) {
+		pr_err("entity hypercall failed");
+		kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+		return true;
+	}
+
+	// Read block and free
+	rv = (bool)acl_violation->block;
+	kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+	return rv;
+}
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity(target, true, driver_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity(target, false, process_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_PROC_ACL);
+}
\ No newline at end of file
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 000000000..a37a5640f
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_BHV_TRACEPOINTS
+#define CREATE_TRACE_POINTS
+#endif
+#include <bhv/bhv_trace.h>
+
+bool bhv_initialized __ro_after_init = false;
+unsigned long *bhv_configuration_bitmap __ro_after_init = NULL;
diff --git security/bhv/creds.c security/bhv/creds.c
new file mode 100644
index 000000000..d08bce7db
--- /dev/null
+++ security/bhv/creds.c
@@ -0,0 +1,501 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/inode.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/creds.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/keyring.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#define BHV_CREDS_HYP(op, arg) bhv_hypercall_vas(BHV_VAS_BACKEND_CREDS, op, arg)
+
+#define BHV_CREDS_CONFIGURE_HYP(arg)                                           \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_CONFIGURE, arg)
+#define BHV_CREDS_REGISTER_INIT_TASK_HYP(arg)                                  \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_REGISTER_INIT_TASK, arg)
+#define BHV_CREDS_ASSIGN_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN, arg)
+#define BHV_CREDS_ASSIGN_PRIV_HYP(arg)                                         \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN_PRIV, arg)
+#define BHV_CREDS_COMMIT_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_COMMIT, arg)
+#define BHV_CREDS_RELEASE_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_RELEASE, arg)
+#define BHV_CREDS_VERIFY_HYP(arg)                                              \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_VERIFICATION, arg)
+#define BHV_CREDS_LOG_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_LOG, arg)
+
+#define BHV_CRED_USAGE_SET(arg1, arg2)                                         \
+	_Generic((arg1),                                                       \
+		atomic_t *: atomic_set,                                        \
+		atomic_long_t *: atomic_long_set)(arg1, arg2)
+
+struct kmem_cache *bhv_creds_arg_cache = NULL;
+
+siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	BHV_CRED_USAGE_SET(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static int __bhv_cred_assign(struct task_struct *t,
+			     struct task_struct *_current, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	struct task_struct *parent = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(_current);
+	if (rc)
+		return -EPERM;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = _current->real_parent;
+	else
+		parent = _current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_assign.new_task.addr = (uint64_t)t;
+	arg->creds_assign.new_task.cred = (uint64_t)t->cred;
+	arg->creds_assign.new_task.hmac = hmac;
+	arg->creds_assign.parent.addr = (uint64_t)parent;
+	arg->creds_assign.parent.cred = (uint64_t)parent->cred;
+
+	rc = BHV_CREDS_ASSIGN_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		/* Note that we currently log only the parent's information. */
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)parent;
+		arg->creds_log.task_cred = (uint64_t)parent->cred;
+		arg->creds_log.task_pid = parent->pid;
+		strscpy(arg->creds_log.task_name, parent->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return __bhv_cred_assign(t, current, clone_flags);
+}
+
+#ifdef VASKM // out of tree
+int __init bhv_cred_assign_init(struct task_struct *t)
+{
+	return __bhv_cred_assign(t, t->real_parent, 0);
+}
+#endif // VASKM
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	arg->creds_assign_priv.cred = (uint64_t)c;
+	arg->creds_assign_priv.daemon = (uint64_t)daemon;
+
+	rc = BHV_CREDS_ASSIGN_PRIV_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign_priv.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	arg->creds_commit.cur.cred = (uint64_t)c;
+	arg->creds_commit.cur.addr = (uint64_t)current;
+	arg->creds_commit.cur.hmac = hmac;
+
+	rc = BHV_CREDS_COMMIT_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = arg->creds_commit.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_verify.task.cred = (uint64_t)t->cred;
+	arg->creds_verify.task.addr = (uint64_t)t;
+	arg->creds_verify.task.hmac = hmac;
+
+	rc = BHV_CREDS_VERIFY_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_verify.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)t;
+		arg->creds_log.task_cred = (uint64_t)t->cred;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, t->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_ATOMIC);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->creds_release.cred = (uint64_t)c;
+
+	rc = BHV_CREDS_RELEASE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->creds_register.init_task.addr = (uint64_t)t;
+	arg->creds_register.init_task.cred = (uint64_t)c;
+	arg->creds_register.init_task.hmac = siphash_cred_context(c, t);
+
+	rc = BHV_CREDS_REGISTER_INIT_TASK_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_cred(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_creds_arg_cache = kmem_cache_create(
+		"bhv_creds_arg_cache", sizeof(bhv_creds_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	bhv_cred_register_init_task(KLN_SYMBOL_P(struct cred *const, init_cred),
+				    &init_task);
+}
+/***********************************************************************/
+
+/***********************************************************************
+ * init
+ ***********************************************************************/
+int __init bhv_init_cred(void)
+{
+	int rc = 0;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+#ifndef VASKM // inside kernel tree
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = BHV_CREDS_CONFIGURE_HYP(&bhv_siphash_key);
+	if (rc)
+		return -EINVAL;
+
+#else // out of tree
+	siphash_key_t *bhv_siphash_key_ptr =
+		(siphash_key_t *)kmalloc(sizeof(siphash_key_t), GFP_KERNEL);
+
+	if (!bhv_siphash_key_ptr) {
+		return -EINVAL;
+	}
+
+	rc = BHV_CREDS_CONFIGURE_HYP(bhv_siphash_key_ptr);
+	if (rc) {
+		kfree(bhv_siphash_key_ptr);
+		return -EINVAL;
+	}
+
+	memcpy(&bhv_siphash_key, bhv_siphash_key_ptr, sizeof(siphash_key_t));
+	memset(bhv_siphash_key_ptr, 0, sizeof(siphash_key_t));
+	kfree(bhv_siphash_key_ptr);
+#endif // VASKM
+
+	rc = bhv_inode_init();
+	if (rc)
+		return rc;
+#ifndef VASKM
+	rc = bhv_init_keyring();
+	if (rc)
+		return rc;
+#endif
+
+	return rc;
+}
+/***********************************************************************/
diff --git security/bhv/domain.c security/bhv/domain.c
new file mode 100644
index 000000000..64f5dc7ef
--- /dev/null
+++ security/bhv/domain.c
@@ -0,0 +1,884 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifdef CONFIG_MEM_NS
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/nsproxy.h>
+#include <linux/mem_namespace.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+#include <linux/hugetlb.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/domain.h>
+#include <bhv/interface/hypercall.h>
+
+#include <asm/bhv/domain.h>
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#include <asm/stacktrace.h>
+#include <asm/unwind.h>
+#endif
+
+#define BHV_DOMAIN_HYP(op, arg)                                                \
+	bhv_hypercall_vas(BHV_VAS_BACKEND_DOMAIN, op, arg)
+
+#define BHV_DOMAIN_CONFIGURE_HYP(arg)                                          \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_CONFIGURE, arg)
+#define BHV_DOMAIN_CREATE_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_CREATE, arg)
+#define BHV_DOMAIN_DESTROY_HYP(arg)                                            \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_DESTROY, arg)
+#define BHV_DOMAIN_SWITCH_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_SWITCH, arg)
+#define BHV_DOMAIN_TRANSFER_HYP(arg)                                           \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_TRANSFER, arg)
+#define BHV_DOMAIN_MAP_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_MAP, arg)
+#define BHV_DOMAIN_UNMAP_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_UNMAP, arg)
+#define BHV_DOMAIN_BATCH_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_BATCH, arg)
+#define BHV_DOMAIN_REPORT_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_REPORT, arg)
+#define BHV_DOMAIN_REPORT_FORCED_MEM_ACCESS_HYP(arg)                           \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_REPORT_FORCED_MEM_ACCESS, arg)
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#define BHV_DOMAIN_DEBUG_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_DEBUG, arg)
+#endif
+
+DEFINE_PER_CPU(uint64_t, bhv_domain_current_domain);
+EXPORT_PER_CPU_SYMBOL_GPL(bhv_domain_current_domain);
+
+static DEFINE_XARRAY_ALLOC(xa_domids);
+
+static bhv_domain_batched_arg_t _batch_area;
+
+bool bhv_domain_initialized __ro_after_init = false;
+
+static bool isolate __ro_after_init = false;
+static bool disallow_forced_mem_access __ro_after_init = true;
+
+static int bhv_domain_create_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_CREATE_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot create new domain\n", __FUNCTION__);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+int bhv_domain_create(uint64_t *domid)
+{
+	int rc = 0;
+	unsigned int id;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == NULL)
+		return -EINVAL;
+
+	/*
+	 * Start allocating domain IDs from ID 1. Domain with ID 0 is reserved.
+	 *
+	 * NOTE: This implementation allocates domain IDs inside of the guest.
+	 * The allocations are done sequentially, yet (unless otherwise required
+	 * by BHV), do not necessarily have to be. These can be passed to BHV
+	 * for management purposes or, if needed, allocated directly by BHV
+	 * instead.
+	 *
+	 * XXX: Consider binding struct mem_namespace (or another datastructure)
+	 * to the allocated ID.
+	 */
+	rc = xa_alloc(&xa_domids, &id, NULL, xa_limit_32b, GFP_KERNEL);
+	if (rc) {
+		pr_err("%s: Cannot allocate new domain ID\n", __FUNCTION__);
+		*domid = BHV_INVALID_DOMAIN;
+		return rc;
+	}
+
+	*domid = (uint64_t)id;
+
+	if (isolate) {
+		rc = bhv_domain_create_isolated_view(*domid);
+		if (rc) {
+			xa_release(&xa_domids, id);
+			*domid = BHV_INVALID_DOMAIN;
+		}
+	}
+
+	return 0;
+}
+
+static int bhv_domain_destroy_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_DESTROY_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot destroy domain[%llu]\n", __FUNCTION__, domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+void bhv_domain_destroy(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return;
+
+	if (isolate)
+		rc = bhv_domain_destroy_isolated_view(domid);
+
+	/*
+	 * Release the allocated domain IDs only if BHV did not return an error,
+	 * or if guest isolation was not enabled in the first place.
+	 */
+	if (!rc)
+		xa_release(&xa_domids, domid);
+}
+
+static int bhv_domain_switch_isolated_view(uint64_t domid)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_SWITCH_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot switch to domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+int bhv_domain_switch(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return -EINVAL;
+
+	if (domid == this_cpu_read(bhv_domain_current_domain))
+		return 0;
+
+	if (isolate) {
+		rc = bhv_domain_switch_isolated_view(domid);
+		if (rc)
+			return rc;
+	}
+
+	this_cpu_write(bhv_domain_current_domain, domid);
+
+	return rc;
+}
+
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	if (mm == NULL)
+		return -EINVAL;
+
+	if (old_ns == NULL || new_ns == NULL)
+		return -EINVAL;
+
+	arg.domain.id = old_ns->mem_ns->domain;
+	arg.domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.id = new_ns->mem_ns->domain;
+
+	rc = BHV_DOMAIN_TRANSFER_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot transfer PGD 0x%llx to domain[%llu]\n",
+		       __FUNCTION__, arg.domain.pgd, new_ns->mem_ns->domain);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+static inline void bhv_domain_batch_from_info(uint64_t info, uint32_t *head,
+					      uint32_t *tail)
+{
+	(*tail) = (info & 0xffffffff);
+	(*head) = (info >> 32);
+}
+
+static inline uint64_t bhv_domain_batch_to_info(uint32_t head, uint32_t tail)
+{
+	return (((uint64_t)head << 32) | tail);
+}
+
+static inline uint32_t bhv_domain_get_nr_entries(uint32_t head, uint32_t tail)
+{
+	return head <= tail ? tail - head :
+			      BHV_DOMAIN_MAX_ENTRIES - head + tail;
+}
+
+static int bhv_domain_batch_send_locked(void)
+{
+	// LOCK MUST BE HELD!
+	int rc = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	rc = BHV_DOMAIN_BATCH_HYP(batch_area);
+	if (rc) {
+		uint32_t head;
+		uint32_t tail;
+		uint64_t info = atomic_long_read(&_batch_area.info);
+		bhv_domain_batch_from_info(info, &head, &tail);
+		pr_err("%s: BHV could not process %u batched hypercalls!\n",
+		       __FUNCTION__, bhv_domain_get_nr_entries(head, tail));
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+// DISCLAIMER: Disabled for now, since it may change the order of operations.
+#define BHV_BATCH_STEAL_PROCESSED 0
+
+static inline uint32_t bhv_domain_batch_get_slot(void)
+{
+	static atomic_t sending = { false };
+	uint32_t in_send = false;
+
+	uint64_t info = atomic64_read(&_batch_area.info);
+	uint64_t new_info;
+	uint32_t head;
+	uint32_t tail;
+	uint32_t nr_entries;
+
+#if BHV_BATCH_STEAL_PROCESSED
+	uint32_t i;
+#endif
+
+	while (true) {
+		// Update head and tail
+		bhv_domain_batch_from_info(info, &head, &tail);
+		// Calculate the current number of entries
+		nr_entries = bhv_domain_get_nr_entries(head, tail);
+
+		// Check if we need to send the batch area down because it is full
+		// Note that we check whether we reached BHV_DOMAIN_MAX_ENTRIES - 1
+		// as the tail always points to the next free entry
+		if (nr_entries >= BHV_DOMAIN_MAX_ENTRIES - 1) {
+			// Make sure in send is always false when we try to win
+			// the race.
+			in_send = false;
+
+			if (atomic_try_cmpxchg(&sending, &in_send, true)) {
+				// Hooray! We are a winner! Send it down.
+				bhv_domain_batch_send_locked();
+				// Reset sending afterwards.
+				atomic_set(&sending, false);
+			} else {
+#if BHV_BATCH_STEAL_PROCESSED
+				// Lets try to steal an existing entry.
+				// This works as processed entries always come after an INVALID entry
+				// Thus we can try to reuse them in case little space is left in our
+				// batch area.
+				for (i = head; i < nr_entries; i++) {
+					uint32_t cur =
+						i % BHV_DOMAIN_MAX_ENTRIES;
+					uint32_t expected =
+						BHV_VAS_DOMAIN_BATCH_STATE_PROCESSED;
+
+					if (atomic_try_cmpxchg(
+						    &_batch_area.entries[cur]
+							     .state,
+						    &expected,
+						    BHV_VAS_DOMAIN_BATCH_STATE_INVALID)) {
+						// We manged to steal an entry. Lets use it.
+						return cur;
+					}
+				}
+#endif
+			}
+
+			// Reread the the info and try again
+			info = atomic64_read(&_batch_area.info);
+			continue;
+		}
+
+		// There are still free entries! Lets see if we can get one
+		new_info = bhv_domain_batch_to_info(
+			head, (tail + 1) % BHV_DOMAIN_MAX_ENTRIES);
+		if (atomic64_try_cmpxchg(&_batch_area.info, &info, new_info)) {
+			// We won the race!
+			if (atomic_read(&_batch_area.entries[tail].state) !=
+			    BHV_VAS_DOMAIN_BATCH_STATE_INVALID) {
+				pr_err("Found valid/processed slot! (state: %u, head: %u, tail: %u, info: 0x%llx)",
+				       atomic_read(
+					       &_batch_area.entries[tail].state),
+				       head, tail,
+				       atomic64_read(&_batch_area.info));
+				panic("Batching failed!");
+			}
+			return tail;
+		}
+	}
+}
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+static void bhv_domain_add_stack_trace(bhv_domain_batched_entry_arg_t *target)
+{
+	struct unwind_state state;
+	struct stack_info stack_info = { 0 };
+	unsigned long visit_mask = 0;
+	unsigned long *stack = get_stack_pointer(current, NULL);
+	char *cur = &target->stack_trace[0];
+	size_t stack_buf_len = BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+
+	unwind_start(&state, current, NULL, stack);
+
+	stack = PTR_ALIGN(stack, sizeof(long));
+	if (get_stack_info(stack, current, &stack_info, &visit_mask)) {
+		pr_err("Could not get stack info!");
+		target->stack_trace[0] = '\0';
+		return;
+	}
+
+	for (; stack < stack_info.end && stack_buf_len > 0 &&
+	       stack_buf_len <= BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+	     stack++) {
+		unsigned long addr = READ_ONCE_NOCHECK(*stack);
+		unsigned long *ret_addr_p =
+			unwind_get_return_address_ptr(&state);
+		int reliable = 0;
+		int written;
+
+		if (!__kernel_text_address(addr))
+			continue;
+
+		if (stack == ret_addr_p)
+			reliable = 1;
+
+		// Skip unreliable for now.
+		if (!reliable)
+			continue;
+
+		written = snprintf(cur, stack_buf_len, "%c%pB\n",
+				   reliable ? ' ' : '?', (void *)addr);
+		if (written >= stack_buf_len)
+			break;
+		stack_buf_len -= written;
+		cur += written;
+
+		if (!reliable)
+			continue;
+
+		unwind_next_frame(&state);
+	}
+}
+#endif
+
+static int bhv_domain_batch_op(bhv_domain_batched_entry_arg_t *arg)
+{
+	int rc = 0;
+	uint32_t dest;
+	uint32_t invalid_state = BHV_VAS_DOMAIN_BATCH_STATE_INVALID;
+	unsigned long flags = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg->state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	dest = bhv_domain_batch_get_slot();
+
+	// disable interrupts for the completion of the entry.
+	local_irq_save(flags);
+	BUG_ON(atomic_read(&batch_area->entries[dest].state) !=
+	       BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	memcpy(&batch_area->entries[dest], arg,
+	       sizeof(bhv_domain_batched_entry_arg_t));
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+	bhv_domain_add_stack_trace(&batch_area->entries[dest]);
+#endif
+
+	if (!atomic_try_cmpxchg(&batch_area->entries[dest].state,
+				&invalid_state,
+				BHV_VAS_DOMAIN_BATCH_STATE_VALID)) {
+		panic("Could not set valid state!");
+	}
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+
+static int bhv_domain_batch_map(uint32_t op, struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages, bool read, bool write,
+				bool exec, bool kernel)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = op;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.map.read = read;
+	arg.map.write = write;
+	arg.map.exec = exec;
+	arg.map.kernel = kernel;
+	arg.map.pfn.pfn = pfn;
+	arg.map.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec)
+{
+	if (!isolate)
+		return 0;
+
+	return bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pfn, nr_pages,
+				    read, write, exec, true);
+}
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(pte))
+		return;
+
+	if (pte_special(pte) && !pte_exec(pte)) {
+		bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), true);
+	} else {
+		uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+		if (pte_present(*ptep))
+			bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+		bhv_domain_batch_map(bhv_domain_op, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), false);
+	}
+}
+EXPORT_SYMBOL(bhv_domain_set_pte_at);
+
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte)) {
+		set_pte(ptep, pte);
+		return;
+	}
+#endif
+
+	bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+			     pte_read(pte), pte_write(pte), pte_exec(pte),
+			     true);
+
+	set_pte(ptep, pte);
+}
+
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(pmd))
+		return;
+
+	if (!pmd_large(pmd))
+		return;
+
+	if (pmd_present(*pmdp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pmd_pfn(pmd), 512,
+			     pmd_read(pmd), pmd_write(pmd), pmd_exec(pmd),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pmd_at);
+
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(pud))
+		return;
+
+	if (!pud_large(pud))
+		return;
+
+	if (pud_present(*pudp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pud_pfn(pud), 512 * 512,
+			     pud_read(pud), pud_write(pud), pud_exec(pud),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pud_at);
+
+static int bhv_domain_unmap_pte(struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_UNMAP;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.unmap.pfn.pfn = pfn;
+	arg.unmap.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(*ptep))
+		return;
+
+	bhv_domain_unmap_pte(mm, pte_pfn(*ptep), 1);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pte);
+
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(*pmdp))
+		return;
+
+	if (!pmd_large(*pmdp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pmd_pfn(*pmdp), 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pmd);
+
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(*pudp))
+		return;
+
+	if (!pud_large(*pudp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pud_pfn(*pudp), 512 * 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pud);
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	int rc = 0;
+	bhv_domain_debug_arg_t arg = {};
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	arg.msg_type = BHV_VAS_DOMAIN_DEBUG_DESTROY_PGD;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	rc = BHV_DOMAIN_DEBUG_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV an error occurred during DEBUG destroy pgd hypercall\n",
+		       __FUNCTION__);
+	}
+}
+#endif
+
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_PGD_DESTROY;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	bhv_domain_batch_op(&arg);
+}
+
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg = { 0 };
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return 0;
+
+	rc = populate_event_context(&arg.report.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context", __FUNCTION__);
+	}
+
+	arg.report.domain_src.id = bhv_get_domain(t);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.report.domain_src.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(t->active_mm->pgd));
+	arg.report.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.report.domain_target.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.report.range.start = vma->vm_start;
+	arg.report.range.end = vma->vm_end;
+	arg.report.write =
+		((gup_flags & FOLL_WRITE) || (gup_flags & FOLL_FORCE));
+
+	rc = BHV_DOMAIN_REPORT_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+	if (arg.report.block)
+		rc = -EPERM;
+
+#if 0
+	pr_info("[-BHV-] %s: dom[%llu] -> dom[%llu] write=%d block=%d", __FUNCTION__, arg.report.domain_src.id, arg.report.domain_target.id, arg.report.write, arg.report.block);
+#endif
+
+	return rc;
+}
+
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+				     bool foreign)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg = { 0 };
+	struct mm_struct *mm_target = vma->vm_mm;
+
+	if (!bhv_domain_is_active())
+		return true;
+
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return true;
+
+	if (!disallow_forced_mem_access)
+		return true;
+
+	rc = populate_event_context(&arg.report.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context", __FUNCTION__);
+	}
+
+	arg.report.domain_src.id = bhv_get_domain(current);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.report.domain_src.pgd = bhv_virt_to_phys(
+		bhv_domain_get_user_pgd(current->active_mm->pgd));
+	arg.report.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.report.domain_target.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.report.range.start = vma->vm_start;
+	arg.report.range.end = vma->vm_end;
+	arg.report.write = write;
+
+	rc = BHV_DOMAIN_REPORT_FORCED_MEM_ACCESS_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return false;
+	}
+
+	pr_info("[-BHV-] %s: vma @ [0x%lx:0x%lx] write=%d", __FUNCTION__,
+		vma->vm_start, vma->vm_end, write);
+
+	return !arg.report.block;
+}
+
+static void __init bhv_domain_init(void)
+{
+	int rc;
+	bool pti_active = false;
+	// We use the CPU region before the initialization!
+	// This is important, otherwise this would overwrite nr_entries!!!
+	bhv_domain_arg_t *arg = (bhv_domain_arg_t *)&_batch_area;
+
+	if (bhv_domain_initialized)
+		return;
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		pti_active = 1;
+#endif
+
+	arg->config.pti = (uint64_t)pti_active;
+	arg->config.batched_region = bhv_virt_to_phys(&_batch_area);
+
+	rc = BHV_DOMAIN_CONFIGURE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot configure StrongIsolation\n",
+		       __FUNCTION__);
+		return;
+	}
+
+	isolate = !!arg->config.isolate;
+	disallow_forced_mem_access = !arg->config.allow_forced_mem_access;
+
+	bhv_domain_initialized = true;
+}
+
+int __init bhv_domain_mm_init(void)
+{
+	int rc;
+	int cpu;
+	uint32_t i;
+
+	if (!bhv_domain_is_enabled())
+		return -EPERM;
+
+	rc = xa_reserve_bh(&xa_domids, BHV_INIT_DOMAIN, GFP_KERNEL);
+	if (rc) {
+		bhv_fail("%s: BHV: cannot reserve domain ID %lu", __FUNCTION__,
+			 BHV_INIT_DOMAIN);
+		return -EFAULT;
+	}
+
+	bhv_domain_init();
+
+	for_each_possible_cpu (cpu) {
+		per_cpu(bhv_domain_current_domain, cpu) = 0;
+	}
+
+	/* Reserve the ID of the first, static memory namespace. */
+
+	atomic64_set(&_batch_area.info, 0);
+
+	for (i = 0; i < BHV_DOMAIN_MAX_ENTRIES; i++) {
+		atomic_set(&_batch_area.entries[i].state,
+			   BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_MEM_NS
+__initcall(bhv_domain_mm_init);
+#endif
+
+#endif /* CONFIG_MEM_NS */
diff --git security/bhv/file_protection.c security/bhv/file_protection.c
new file mode 100644
index 000000000..136e72b73
--- /dev/null
+++ security/bhv/file_protection.c
@@ -0,0 +1,131 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/file_protection.h>
+
+#include <bhv/file_protection.h>
+
+bhv_file_protection_config_t bhv_file_protection_config __ro_after_init = { 0 };
+
+struct kmem_cache *bhv_file_protection_violation_cache;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void)
+{
+	unsigned long r;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	// Create slab cache
+	bhv_file_protection_violation_cache = kmem_cache_create(
+		"bhv_file_protection_violation_cache",
+		sizeof(bhv_file_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_file_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+
+#ifndef VASKM // inside kernel tree
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION,
+			      BHV_VAS_FILE_PROTECTION_OP_INIT,
+			      &bhv_file_protection_config);
+	if (r) {
+		pr_err("File protection init failed");
+		return;
+	}
+
+#else // out of tree
+	bhv_file_protection_config_t *bhv_file_protection_config_tmp_ptr =
+		(bhv_file_protection_config_t *)kmalloc(
+			sizeof(bhv_file_protection_config_t), GFP_KERNEL);
+	if (!bhv_file_protection_config_tmp_ptr) {
+		pr_err("BHV: cannot allocate bhv_file_protection_config\n");
+		return;
+	}
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION,
+			      BHV_VAS_FILE_PROTECTION_OP_INIT,
+			      bhv_file_protection_config_tmp_ptr);
+	if (r) {
+		pr_err("File protection init failed");
+		kfree(bhv_file_protection_config_tmp_ptr);
+		return;
+	}
+
+	bhv_file_protection_config = *bhv_file_protection_config_tmp_ptr;
+
+	kfree(bhv_file_protection_config_tmp_ptr);
+#endif // VASKM
+}
+/***********************************************************************/
+
+bool bhv_block_read_only_file_write(const char *target, bool dirtycred)
+{
+	unsigned long r;
+	bool rv;
+	bhv_file_protection_violation_t *volatile violation = NULL;
+	u32 bhv_hypercall_op;
+
+	// Prepare hypercall
+	if (!dirtycred) {
+		bhv_hypercall_op =
+			BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION;
+	} else {
+		bhv_hypercall_op =
+			BHV_VAS_FILE_PROTECTION_OP_VIOLATION_DIRTYCRED;
+	}
+
+	violation = kmem_cache_alloc(bhv_file_protection_violation_cache,
+				     GFP_KERNEL);
+
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate file protection violation");
+		return true;
+	}
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	// Setup arg
+	violation->name_len = strlen(target) + 1 /* NULL terminator */;
+
+	violation->name = bhv_virt_to_phys((void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION, bhv_hypercall_op,
+			      violation);
+	if (r) {
+		pr_err("file protection violation hypercall failed");
+		kmem_cache_free(bhv_file_protection_violation_cache, violation);
+		return true;
+	}
+
+	/* Note: in case of dirtycred, "block" halts the guest with a panic */
+	// Read block and free
+	rv = (bool)violation->block;
+	kmem_cache_free(bhv_file_protection_violation_cache, violation);
+	return rv;
+}
\ No newline at end of file
diff --git security/bhv/fileops_protection.c security/bhv/fileops_protection.c
new file mode 100644
index 000000000..d96685ba0
--- /dev/null
+++ security/bhv/fileops_protection.c
@@ -0,0 +1,396 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <uapi/linux/magic.h>
+#include <linux/sort.h>
+#include <linux/bsearch.h>
+#include <linux/moduleparam.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/file_protection.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/file_protection.h>
+
+#include <bhv/fileops_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/guestlog.h>
+#include <bhv/interface/confserver.h>
+
+#include <asm/sections.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+static bool bhv_strict_fileops __ro_after_init = false;
+
+bool bhv_strict_fileops_enforced(void)
+{
+	return bhv_strict_fileops;
+}
+
+struct kmem_cache *bhv_fileops_protection_violation_cache;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_fileops_protection(void)
+{
+	if (!bhv_fileops_file_protection_is_enabled())
+		return;
+
+	// Create slab cache for violation data being set by host
+	bhv_fileops_protection_violation_cache = kmem_cache_create(
+		"bhv_fileops_protection_violation_cache",
+		sizeof(bhv_fileops_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_fileops_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache.");
+		return;
+	}
+}
+/***********************************************************************/
+
+u8 bhv_fileops_type(u32 fs_magic)
+{
+	u8 op;
+	switch (fs_magic) {
+#if defined CONFIG_EXT4_FS || defined VASKM
+	case EXT4_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_EXT4;
+		break;
+#endif
+#if defined CONFIG_XFS_FS || defined VASKM
+	case XFS_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_XFS;
+		break;
+#endif
+	case TMPFS_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_TMPFS;
+		break;
+	case PROC_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_PROC;
+		break;
+	case CGROUP2_SUPER_MAGIC:
+		fallthrough;
+	case SYSFS_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_SYSFS;
+		break;
+	case DEBUGFS_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_DEBUGFS;
+		break;
+	default:
+		op = BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED;
+	}
+
+	return op;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod && ((unsigned long)mod->mem[MOD_RODATA].base <= addr) &&
+	       ((unsigned long)mod->mem[MOD_RODATA].base +
+			mod->mem[MOD_RODATA].size >
+		addr);
+}
+#else
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.text_size <=
+		addr) &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.ro_size >
+		addr);
+}
+#endif
+
+bool bhv_fileops_is_ro(u64 f_op)
+{
+	struct module *mod;
+	if (f_op >= KLN_SYM(__start_rodata) && f_op < KLN_SYM(__end_rodata))
+		return true;
+
+	preempt_disable();
+	mod = __module_address(f_op);
+	preempt_enable();
+	if (mod == NULL)
+		return false;
+
+	if (_is_module_ro_data(f_op, mod))
+		return true;
+
+	return false;
+}
+
+bool bhv_block_fileops(const char *target, u8 fops_type, bool is_dir,
+		       const void *fops_ptr)
+{
+	unsigned long err;
+	bool retval;
+	bhv_fileops_protection_violation_t *volatile violation;
+	size_t path_sz;
+	int rv;
+
+	if (!bhv_fileops_file_protection_is_enabled())
+		return false;
+
+	violation = kmem_cache_alloc(bhv_fileops_protection_violation_cache,
+				     GFP_KERNEL);
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate fileops protection violation");
+		return true;
+	}
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		kmem_cache_free(bhv_fileops_protection_violation_cache,
+				violation);
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= BHV_VAS_FILEOPS_PATH_MAX_SZ) {
+		path_sz = BHV_VAS_FILEOPS_PATH_MAX_SZ - 1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->fops_type = fops_type;
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+	violation->fops_ptr = (uint64_t)fops_ptr;
+
+	// pr_err("Bad fops %d %s %d %px %pS\n", violation->fops_type, violation->path_name, violation->is_dir, (void*)violation->fops_ptr, (void*)violation->fops_ptr);
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = bhv_hypercall_vas(
+		BHV_VAS_BACKEND_FILE_PROTECTION,
+		BHV_VAS_FILE_PROTECTION_OP_VIOLATION_FILEOPS_PROTECTION,
+		violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		kmem_cache_free(bhv_fileops_protection_violation_cache,
+				violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	kmem_cache_free(bhv_fileops_protection_violation_cache, violation);
+
+	return retval;
+}
+
+#ifndef VASKM // inside kernel tree
+
+const fops_t fileops_map[] __section(".rodata") = {
+#define FOPS_MAP(_, idx, file_ops, dir_ops) [idx] = { &file_ops, &dir_ops },
+#define FOPS_MAP_DIRNULL(_, idx, file_ops) [idx] = { &file_ops, NULL },
+#include <bhv/fileops_internal_fopsmap.h>
+};
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[] __ro_after_init = {
+#define FOPS(_)
+#define FOPS_PROC(sym) &sym,
+#include <bhv/fileops_internal_symlist.h>
+};
+
+#define init_fileops_data()
+
+#else // out of tree
+fops_t *fileops_map;
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[0
+#define FOPS(_)
+#define FOPS_PROC(_) +1
+#include <bhv/fileops_internal_symlist.h>
+] __ro_after_init = {};
+
+static void init_fileops_data(void)
+{
+	static_assert(sizeof(fops_t) * BHV_VAS_FILEOPS_PROTECTION_MAP_LENGTH <=
+		      PAGE_SIZE);
+	fileops_map = (fops_t *)vzalloc(PAGE_SIZE);
+	BUG_ON(!fileops_map);
+	BUG_ON(!PAGE_ALIGNED(fileops_map));
+
+#define KFOPS(sym) KLN_SYMBOL_P(const struct file_operations *, sym)
+#define FOPS_MAP(_, idx, file_ops, dir_ops)                                    \
+	fileops_map[idx][0] = KFOPS(file_ops);                                 \
+	fileops_map[idx][1] = KFOPS(dir_ops);
+#define FOPS_MAP_DIRNULL(_, idx, file_ops)                                     \
+	fileops_map[idx][0] = KFOPS(file_ops);                                 \
+	fileops_map[idx][1] = NULL;
+#include <bhv/fileops_internal_fopsmap.h>
+
+	int c = 0;
+#define FOPS(_)
+#define FOPS_PROC(sym) proc_fops[c++] = KFOPS(sym);
+#include <bhv/fileops_internal_symlist.h>
+	BUG_ON(c != ARRAY_SIZE(proc_fops));
+}
+#endif // VASKM
+
+int cmp_fileops(const void *a_ptr, const void *b_ptr)
+{
+	u64 a = *(u64 *)a_ptr;
+	u64 b = *(u64 *)b_ptr;
+	return ((a == b) ? 0 : (a > b ? 1 : -1));
+}
+
+/***************************************************************
+ * init
+ ***************************************************************/
+void __init bhv_init_fileops(void)
+{
+	bhv_confserver_req_strict_fileops_t *bhv_arg_ptr;
+
+#ifndef VASKM // inside kernel tree
+	static bhv_confserver_req_strict_fileops_t bhv_arg;
+	bhv_arg_ptr = &bhv_arg;
+
+#else // out of tree
+	bhv_arg_ptr = (bhv_confserver_req_strict_fileops_t *)kmalloc(
+		sizeof(bhv_confserver_req_strict_fileops_t), GFP_KERNEL);
+
+	if (!bhv_arg_ptr) {
+		panic("BHV: cannot allocate bhv_arg\n");
+	}
+#endif // VASKM
+
+	init_fileops_data();
+
+	BHV_CONFSERVER_STRICTFILEOPS_HYP((void *)bhv_arg_ptr);
+	bhv_strict_fileops = bhv_arg_ptr->strict_fileops;
+
+#ifdef VASKM // out of tree
+	kfree(bhv_arg_ptr);
+#endif // VASKM
+
+	if (bhv_file_protection_is_enabled()) {
+		sort(proc_fops, ARRAY_SIZE(proc_fops), sizeof(proc_fops[0]),
+		     cmp_fileops, NULL);
+	}
+}
+/***************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **fop_ptr)
+{
+	if (bsearch(fop_ptr, proc_fops, ARRAY_SIZE(proc_fops),
+		    sizeof(proc_fops[0]), cmp_fileops))
+		return true;
+
+	return false;
+}
+
+#ifndef VASKM // inside kernel tree
+extern int full_proxy_release(struct inode *inode, struct file *filp);
+extern loff_t full_proxy_llseek(struct file *, loff_t, int);
+extern ssize_t full_proxy_read(struct file *, char __user *, size_t, loff_t *);
+extern ssize_t full_proxy_write(struct file *, const char __user *, size_t,
+				loff_t *);
+extern __poll_t full_proxy_poll(struct file *, struct poll_table_struct *);
+extern long full_proxy_unlocked_ioctl(struct file *, unsigned int,
+				      unsigned long);
+#endif
+
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr)
+{
+	size_t i;
+
+	if (fop_ptr == NULL)
+		return false;
+
+	// Check whether the fop points to one of three proxy fops
+	// see fs/debugfs/file.c
+	if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    debugfs_noop_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_open_proxy_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_full_proxy_file_operations))
+		return true;
+
+	// No. This should be a dynamically allocated fops struct.
+	// We will validate the pointer within fops.
+	// See __full_proxy_fops_init in fs/debugfs/file.c
+	if (fop_ptr->llseek != NULL &&
+	    (u64)fop_ptr->llseek != KLN_SYM(full_proxy_llseek))
+		return false;
+
+	if (fop_ptr->read != NULL &&
+	    (u64)fop_ptr->read != KLN_SYM(full_proxy_read))
+		return false;
+
+	if (fop_ptr->write != NULL &&
+	    (u64)fop_ptr->write != KLN_SYM(full_proxy_write))
+		return false;
+
+	if (fop_ptr->read_iter != NULL)
+		return false;
+
+	if (fop_ptr->write_iter != NULL)
+		return false;
+
+	if (fop_ptr->iopoll != NULL)
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 5, 0)
+	if (fop_ptr->iterate != NULL)
+		return false;
+#endif
+
+	if (fop_ptr->iterate_shared != NULL)
+		return false;
+
+	if (fop_ptr->poll != NULL &&
+	    (u64)fop_ptr->poll != KLN_SYM(full_proxy_poll))
+		return false;
+
+	if (fop_ptr->unlocked_ioctl != NULL &&
+	    (u64)fop_ptr->unlocked_ioctl != KLN_SYM(full_proxy_unlocked_ioctl))
+		return false;
+
+	if (fop_ptr->compat_ioctl != NULL)
+		return false;
+
+	if (fop_ptr->mmap != NULL)
+		return false;
+
+	if (fop_ptr->open != NULL)
+		return false;
+
+	if (fop_ptr->flush != NULL)
+		return false;
+
+	if ((u64)fop_ptr->release != KLN_SYM(full_proxy_release))
+		return false;
+
+	// Remainder of pointers must be NULL as well
+	for (i = offsetof(struct file_operations, fsync);
+	     i < sizeof(struct file_operations); i++) {
+		if (((uint8_t *)fop_ptr)[i] != '\0') {
+			return false;
+		}
+	}
+
+	return true;
+}
diff --git security/bhv/guestconn.c security/bhv/guestconn.c
new file mode 100644
index 000000000..4977aabe4
--- /dev/null
+++ security/bhv/guestconn.c
@@ -0,0 +1,279 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#define bhv_guestconn_configured true
+#else // out of tree
+static bool bhv_guestconn_configured = false;
+#endif // VASKM
+
+typedef struct {
+	struct list_head list;
+	size_t to_send;
+	GuestConnABI__Header__T *msg;
+} bhv_guestconn_send_item_t;
+
+uint32_t bhv_guestconn_cid __ro_after_init = 0;
+uint32_t bhv_guestconn_port __ro_after_init = 0;
+
+static struct socket *vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+static LIST_HEAD(bhv_guestconn_msg_list);
+static DEFINE_RAW_SPINLOCK(bhv_guestconn_msg_lock);
+
+static struct workqueue_struct *bhv_guestconn_workqueue = NULL;
+static struct work_struct bhv_guestconn_work_struct;
+static struct delayed_work bhv_guestconn_delayed_work_struct;
+static struct kmem_cache *bhv_guestconn_send_item_cache;
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void)
+{
+	_Static_assert(GuestConnABI__MAX_MSG_SZ <= PAGE_SIZE,
+		       "GuestConnABI__MAX_MSG_SZ > PAGE_SIZE");
+	GuestConnABI__Header__T *rv =
+		(GuestConnABI__Header__T *)__get_free_page(GFP_KERNEL);
+	memset(rv, 0, PAGE_SIZE);
+	return rv;
+}
+
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg)
+{
+	free_page((unsigned long)msg);
+}
+
+static inline size_t bhv_send(void *data, size_t size, size_t to_send)
+{
+	int r;
+	struct kvec vec;
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+		r = kernel_sendmsg(vsock, &msghdr, &vec, 1, vec.iov_len);
+		if (r == -EAGAIN) {
+			return to_send;
+		} else if (r < 0) {
+			pr_err("BHV GuestLog: Send Failed (%d)", r);
+			return 0;
+		}
+		to_send -= r;
+	}
+	return 0;
+}
+
+static void bhv_guestconn_sendmsg(struct work_struct *ws)
+{
+	bhv_guestconn_send_item_t *item;
+	unsigned long flags;
+
+	while (true) {
+		raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+		if (list_empty(&bhv_guestconn_msg_list)) {
+			raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock,
+						   flags);
+			return;
+		}
+		item = list_first_entry(&bhv_guestconn_msg_list,
+					bhv_guestconn_send_item_t, list);
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+		item->to_send =
+			bhv_send(item->msg, item->msg->sz, item->to_send);
+
+		if (item->to_send == 0 ||
+		    unlikely(atomic_read(&reboot_in_progress))) {
+			raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+			list_del(&(item->list));
+			raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock,
+						   flags);
+			bhv_guestconn_free_msg(item->msg);
+			kmem_cache_free(bhv_guestconn_send_item_cache, item);
+		} else {
+			queue_delayed_work(bhv_guestconn_workqueue,
+					   &bhv_guestconn_delayed_work_struct,
+					   msecs_to_jiffies(1000));
+			return;
+		}
+	}
+}
+
+int bhv_guestconn_send(uint16_t backend, GuestConnABI__Header__T *msg,
+		       size_t size)
+{
+	bhv_guestconn_send_item_t *cur;
+	unsigned long flags;
+
+	if (!bhv_guestconn_configured)
+		return 0;
+
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+
+	pr_debug("BHV GuestConn: Queuing msg for backend %u with size %lu",
+		 backend, size);
+
+	BUG_ON(!bhv_guestconn_send_item_cache);
+
+	cur = kmem_cache_alloc(bhv_guestconn_send_item_cache, GFP_ATOMIC);
+	if (cur == NULL) {
+		bhv_fail("BHV: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	cur->msg = msg;
+
+	cur->msg->backend = backend;
+	cur->msg->sz = GuestConnABI__Header__SZ + size;
+
+	cur->to_send = cur->msg->sz;
+
+	raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+		kmem_cache_free(bhv_guestconn_send_item_cache, cur);
+		bhv_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	list_add_tail(&(cur->list), &bhv_guestconn_msg_list);
+	raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+	if (atomic_read(&workqueue_ready))
+		queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+
+	return 0;
+}
+
+static int bhv_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	int expected = 0;
+	if (atomic_read(&workqueue_ready) && atomic_try_cmpxchg(&reboot_in_progress, &expected, 1)) {
+		// Cancel pending work.
+		cancel_delayed_work_sync(&bhv_guestconn_delayed_work_struct);
+
+		// Drain the workqueue
+		drain_workqueue(bhv_guestconn_workqueue);
+
+		// We assume all messages are gone now and we shut down the socket
+		sock_release(vsock);
+		vsock = NULL;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bhv_guestconn_reboot_notifier = {
+	.notifier_call = bhv_guestconn_reboot,
+	.priority = 0,
+};
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_guestconn(void)
+{
+	int err;
+	struct sockaddr_vm addr;
+
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	vsock_addr_init(&addr, bhv_guestconn_cid, bhv_guestconn_port);
+	pr_info("bhv guestconn started with cid %u, port %u", bhv_guestconn_cid,
+		bhv_guestconn_port);
+#ifdef VASKM // out of tree
+	bhv_guestconn_configured = true;
+#endif // VASKM
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not create kernel socket (%d)", err);
+		return;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not connect to host (%d)", err);
+		return;
+	}
+
+	// Initialize work queue
+	INIT_WORK(&bhv_guestconn_work_struct, bhv_guestconn_sendmsg);
+	INIT_DELAYED_WORK(&bhv_guestconn_delayed_work_struct,
+			  bhv_guestconn_sendmsg);
+	bhv_guestconn_workqueue =
+		alloc_workqueue("bhv_guestconn_workqueue", WQ_UNBOUND, 1);
+	// queue = create_singlethread_workqueue("bhv_guestlog_work_queue");
+	if (bhv_guestconn_workqueue == NULL) {
+		bhv_fail("BHV: Could not allocate work queue!");
+		kmem_cache_destroy(bhv_guestconn_send_item_cache);
+		return;
+	}
+	atomic_inc(&workqueue_ready);
+
+	register_reboot_notifier(&bhv_guestconn_reboot_notifier);
+
+	queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+}
+/**********************************************************/
+
+/**********************************************************
+ * mm_init
+ **********************************************************/
+void __init bhv_mm_init_guestconn(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	// Create cache
+	bhv_guestconn_send_item_cache = kmem_cache_create(
+		"bhv_guestconn_send_item_cache",
+		sizeof(bhv_guestconn_send_item_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_guestconn_send_item_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+}
+/**********************************************************/
+
+/**********************************************************
+ * init
+ **********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	bhv_guestconn_cid = cid;
+	bhv_guestconn_port = port;
+	return 0;
+}
+/**********************************************************/
diff --git security/bhv/guestlog.c security/bhv/guestlog.c
new file mode 100644
index 000000000..c1baa4fa5
--- /dev/null
+++ security/bhv/guestlog.c
@@ -0,0 +1,549 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/types.h>
+
+#include <linux/cgroup.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/time_namespace.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <net/net_namespace.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init = { 0,
+								       false };
+
+#define BHV_GUESTLOG_MAX_PAYLOAD_SZ                                            \
+	BHV_GUESTCONN_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__Message__SZ
+
+#define BHV_GUESTLOG_MAX_BUF_SZ(EVT_T)                                         \
+	BHV_GUESTLOG_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__##EVT_T##__SZ
+
+static inline uint16_t bhv_guestlog_calc_msg_sz(uint16_t event_id,
+						size_t buf_sz)
+{
+	size_t size = GuestConnABI__GuestLog__Message__SZ;
+	switch (event_id) {
+	case GuestConnABI__GuestLog__StringMsg__EVT_ID:
+		size += GuestConnABI__GuestLog__StringMsg__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessFork__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessFork__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessExec__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__ProcessExit__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExit__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriverLoad__EVT_ID:
+		size += GuestConnABI__GuestLog__DriverLoad__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelAccess__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelAccess__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__FopsUnknown__EVT_ID:
+		size += GuestConnABI__GuestLog__FopsUnknown__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelExec__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__CgroupCreate__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupCreate__SZ;
+		break;
+	case GuestConnABI__GuestLog__CgroupDestroy__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupDestroy__SZ;
+		break;
+	case GuestConnABI__GuestLog__NamespaceChange__EVT_ID:
+		size += GuestConnABI__GuestLog__NamespaceChange__SZ;
+		break;
+	default:
+		BUG();
+	}
+	size += buf_sz;
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+	return size;
+}
+
+#define ALLOC_MSG(EVT, EVT_T)                                                  \
+	GuestConnABI__Header__T *__gc_hdr;                                     \
+	GuestConnABI__GuestLog__Message__T *__msg;                             \
+	GuestConnABI__GuestLog__##EVT_T##__T *EVT;                             \
+                                                                               \
+	__gc_hdr = bhv_guestconn_alloc_msg();                                  \
+	if (!__gc_hdr)                                                         \
+		return -ENOMEM;                                                \
+                                                                               \
+	__msg = (GuestConnABI__GuestLog__Message__T *)__gc_hdr->payload;       \
+	EVT = (GuestConnABI__GuestLog__##EVT_T##__T *)__msg->payload
+
+#define SEND_MSG(EVT, EVT_T, BUF_SIZE, FULL_PATH)                              \
+	__msg->header.evt = GuestConnABI__GuestLog__##EVT_T##__EVT_ID;         \
+	__msg->header.sz = bhv_guestlog_calc_msg_sz(                           \
+		GuestConnABI__GuestLog__##EVT_T##__EVT_ID, BUF_SIZE);          \
+                                                                               \
+	if (populate_event_context(&__msg->context, FULL_PATH)) {              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}                                                                      \
+                                                                               \
+	return bhv_guestconn_send(GuestConnABI__GuestLog__BACKEND_ID,          \
+				  __gc_hdr, __msg->header.sz)
+
+#define SEND_MSG_STRLEN(EVT, EVT_T, FULL_PATH)                                 \
+	SEND_MSG(EVT, EVT_T,                                                   \
+		 strnlen(EVT->buf, BHV_GUESTLOG_MAX_BUF_SZ(EVT_T)) + 1,        \
+		 FULL_PATH);
+
+int bhv_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+
+	ALLOC_MSG(evt, StringMsg);
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(evt->buf, BHV_GUESTLOG_MAX_BUF_SZ(StringMsg), fmt,
+			 args);
+	va_end(args);
+
+	SEND_MSG(evt, StringMsg, len, true);
+}
+
+static int _bhv_get_cgroup_info(struct task_struct *tsk, u64 *cgrp_id,
+				char *cgrp_name, size_t cgrp_name_sz)
+{
+	int r;
+	struct cgroup *cgrp;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	*cgrp_id = cgroup_id(cgrp);
+	r = cgroup_name(cgrp, cgrp_name, cgrp_name_sz);
+	rcu_read_unlock();
+	return r;
+}
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+
+static void _bhv_get_ns_inums(struct task_struct *tsk, u32 *cgroup_ns_inum,
+			      u32 *ipc_ns_inum, u32 *mnt_ns_inum,
+			      u32 *net_ns_inum, u32 *pid_ns_inum,
+			      u32 *pid_for_children_ns_inum, u32 *time_ns_inum,
+			      u32 *time_for_children_ns_inum, u32 *user_ns_inum,
+			      u32 *uts_ns_inum)
+{
+	*cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+	*ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+	*mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+	*net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+	*pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+	*pid_for_children_ns_inum = NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+	*time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+	*time_for_children_ns_inum =
+		NS_DEREF(tsk->nsproxy->time_ns_for_children);
+	*user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+	*uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+}
+
+static void _bhv_get_incoming_ns_inums(
+	struct task_struct *tsk, struct nsset *nsset, u32 *cgroup_ns_inum,
+	u32 *ipc_ns_inum, u32 *mnt_ns_inum, u32 *net_ns_inum, u32 *pid_ns_inum,
+	u32 *pid_for_children_ns_inum, u32 *time_ns_inum,
+	u32 *time_for_children_ns_inum, u32 *user_ns_inum, u32 *uts_ns_inum)
+{
+	if (nsset->nsproxy) {
+		*cgroup_ns_inum = NS_DEREF(nsset->nsproxy->cgroup_ns);
+		*ipc_ns_inum = NS_DEREF(nsset->nsproxy->ipc_ns);
+		*mnt_ns_inum = from_mnt_ns(nsset->nsproxy->mnt_ns)->inum;
+		*net_ns_inum = NS_DEREF(nsset->nsproxy->net_ns);
+		*time_ns_inum = NS_DEREF(nsset->nsproxy->time_ns);
+		*time_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->time_ns_for_children);
+		*uts_ns_inum = NS_DEREF(nsset->nsproxy->uts_ns);
+		*pid_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->pid_ns_for_children);
+	} else {
+		*cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		*ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		*mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		*net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		*time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		*time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		*uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+		*pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+	}
+
+	*pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+	if (nsset->flags & CLONE_NEWUSER) {
+		*user_ns_inum = NS_DEREF(nsset->cred->user_ns);
+	} else {
+		*user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+	}
+}
+
+#undef NS_DEREF
+
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm)
+{
+	int r;
+	ALLOC_MSG(evt, ProcessFork);
+
+	r = _bhv_get_cgroup_info(current, &(evt->cgroup_id), evt->cgroup_name,
+				 GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ);
+	if (!r)
+		evt->cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(current, &(evt->cgroup_ns_inum), &(evt->ipc_ns_inum),
+			  &(evt->mnt_ns_inum), &(evt->net_ns_inum),
+			  &(evt->pid_ns_inum), &(evt->pid_for_children_ns_inum),
+			  &(evt->time_ns_inum),
+			  &(evt->time_for_children_ns_inum),
+			  &(evt->user_ns_inum), &(evt->uts_ns_inum));
+
+	evt->child_pid = child_pid;
+	evt->parent_pid = parent_pid;
+	evt->child_comm_offset = 0;
+	strscpy(evt->buf, child_comm, TASK_COMM_LEN);
+	evt->parent_comm_offset = strnlen(evt->buf, TASK_COMM_LEN) + 1;
+	strscpy(&(evt->buf[evt->parent_comm_offset]), parent_comm,
+		TASK_COMM_LEN);
+
+	SEND_MSG(evt, ProcessFork,
+		 evt->parent_comm_offset +
+			 strnlen(&evt->buf[evt->parent_comm_offset],
+				 TASK_COMM_LEN) +
+			 1,
+		 true);
+}
+
+static inline struct page *bhv_get_page(struct linux_binprm *bprm,
+					unsigned long addr)
+{
+	struct page *page;
+#ifdef CONFIG_MMU
+	/*
+		 * This is called at execve() time in order to dig around
+		 * in the argv/environment of the new proceess
+		 * (represented by bprm).  'current' is the process doing
+		 * the execve().
+		 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0)
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL) <=
+	    0)
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL,
+				  NULL) <= 0)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+		return NULL;
+#else /* CONFIG_MMU */
+	page = bprm->page[addr / PAGE_SIZE];
+#endif /* CONFIG_MMU */
+	return page;
+}
+
+static uint32_t bhv_get_args_env(struct linux_binprm *bprm,
+				 uint32_t start_offset, uint32_t count,
+				 char *dst, size_t dst_sz)
+{
+	uint32_t rv = 0;
+	char *kaddr;
+	struct page *page;
+	int cur = 0;
+	unsigned long i, j;
+	unsigned long pos = bprm->p + start_offset;
+	unsigned int offset = pos % PAGE_SIZE;
+
+	if (dst_sz <= 0)
+		return 0;
+
+	if (count <= 0) {
+		dst[0] = '\0';
+		return 1;
+	}
+
+	page = bhv_get_page(bprm, pos);
+	if (page == NULL) {
+		pr_err("BHV: unable to find user page\n");
+		return rv;
+	}
+	kaddr = kmap_atomic(page);
+
+	for (i = 0, j = 0; i < dst_sz; i++) {
+		dst[i] = *(char *)(kaddr + (offset + j));
+
+		if (dst[i] == '\0') {
+			cur++;
+			if (cur == count) {
+				rv = i + 1;
+				break;
+			}
+			dst[i] = ' ';
+		}
+
+		if ((offset + j + 1) >= PAGE_SIZE) {
+			kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+			put_page(page);
+#endif
+
+			page = bhv_get_page(bprm, pos + i);
+			if (page == NULL) {
+				pr_err("BHV: unable to find user page\n");
+				dst[dst_sz - 1] = '\0';
+				return i + 1;
+			}
+			kaddr = kmap_atomic(page);
+
+			offset = 0;
+			j = 0;
+		} else {
+			j++;
+		}
+	}
+	dst[dst_sz - 1] = '\0';
+
+	kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+	put_page(page);
+#endif
+	return rv;
+}
+
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm)
+{
+	int r;
+	uint32_t env_offset;
+
+	ALLOC_MSG(evt, ProcessExec);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	strscpy(evt->name, comm, TASK_COMM_LEN);
+	env_offset =
+		bhv_get_args_env(bprm, 0, bprm->argc, evt->args,
+				 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+	bhv_get_args_env(bprm, env_offset, bprm->envc, evt->env,
+			 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+
+	r = _bhv_get_cgroup_info(current, &(evt->cgroup_id), evt->cgroup_name,
+				 GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ);
+	if (!r)
+		evt->cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(current, &(evt->cgroup_ns_inum), &(evt->ipc_ns_inum),
+			  &(evt->mnt_ns_inum), &(evt->net_ns_inum),
+			  &(evt->pid_ns_inum), &(evt->pid_for_children_ns_inum),
+			  &(evt->time_ns_inum),
+			  &(evt->time_for_children_ns_inum),
+			  &(evt->user_ns_inum), &(evt->uts_ns_inum));
+
+	SEND_MSG(evt, ProcessExec, 0, true);
+}
+
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	ALLOC_MSG(evt, ProcessExit);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	strscpy(evt->buf, comm, TASK_COMM_LEN);
+
+	SEND_MSG_STRLEN(evt, ProcessExit, false);
+}
+
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupCreate);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupCreate, 0, true);
+}
+
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupDestroy);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupDestroy, 0, true);
+}
+
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset)
+{
+	int r;
+	ALLOC_MSG(evt, NamespaceChange);
+
+	evt->target_task_pid = tsk->pid;
+	strncpy(evt->target_task_name, tsk->comm,
+		GuestConnABI__GuestLog__PROC_COMM_SZ);
+
+	r = _bhv_get_cgroup_info(tsk, &(evt->cgroup_id), evt->cgroup_name,
+				 GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ);
+	if (!r)
+		evt->cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(tsk, &(evt->cgroup_ns_inum), &(evt->ipc_ns_inum),
+			  &(evt->mnt_ns_inum), &(evt->net_ns_inum),
+			  &(evt->pid_ns_inum), &(evt->pid_for_children_ns_inum),
+			  &(evt->time_ns_inum),
+			  &(evt->time_for_children_ns_inum),
+			  &(evt->user_ns_inum), &(evt->uts_ns_inum));
+
+	_bhv_get_incoming_ns_inums(
+		tsk, nsset, &(evt->incoming_cgroup_ns_inum),
+		&(evt->incoming_ipc_ns_inum), &(evt->incoming_mnt_ns_inum),
+		&(evt->incoming_net_ns_inum), &(evt->incoming_pid_ns_inum),
+		&(evt->incoming_pid_for_children_ns_inum),
+		&(evt->incoming_time_ns_inum),
+		&(evt->incoming_time_for_children_ns_inum),
+		&(evt->incoming_user_ns_inum), &(evt->incoming_uts_ns_inum));
+
+	SEND_MSG(evt, NamespaceChange, 0, true);
+}
+
+int bhv_guestlog_log_driver_load(const char *name)
+{
+	ALLOC_MSG(evt, DriverLoad);
+
+	strscpy(evt->buf, name, BHV_GUESTLOG_MAX_BUF_SZ(DriverLoad));
+
+	SEND_MSG_STRLEN(evt, DriverLoad, true);
+}
+
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t type)
+{
+	ALLOC_MSG(evt, KernelAccess);
+
+	evt->address = addr;
+	evt->type = type;
+
+	SEND_MSG(evt, KernelAccess, 0, true);
+}
+
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint64_t type, const void *fops_ptr)
+{
+	ALLOC_MSG(evt, FopsUnknown);
+
+	evt->magic = (u64)magic;
+	evt->type = type;
+	evt->address = (u64)fops_ptr;
+	strscpy(evt->buf, pathname, BHV_GUESTLOG_MAX_BUF_SZ(FopsUnknown));
+
+	SEND_MSG_STRLEN(evt, FopsUnknown, true);
+}
+
+static void bhv_guestlog_char_array_into_buf(char *dst, size_t dst_size,
+					     char **arr)
+{
+	char *cur;
+	unsigned int i;
+	ssize_t cur_len = 0;
+	ssize_t total_len = 0;
+
+	BUG_ON(dst == NULL);
+
+	if (arr == NULL) {
+		if (dst_size > 0)
+			dst[0] = '\0';
+
+		return;
+	}
+
+	for (i = 0, cur = arr[0]; cur != NULL; i++, cur = arr[i]) {
+		if (i != 0) {
+			// Replace NULL of previous round with a space
+			dst[total_len] = ' ';
+			// Increase len since strscpy returns len without NULL
+			total_len++;
+		}
+
+		cur_len = strscpy(&dst[total_len], cur, dst_size);
+
+		// No more room. We are done
+		if (cur_len == -E2BIG)
+			return;
+
+		// Update size
+		dst_size -= cur_len;
+		total_len += cur_len;
+
+		// No more room. We are done. The last character is '\0'
+		if (dst_size <= 1)
+			return;
+	}
+}
+
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp)
+{
+	ALLOC_MSG(evt, KernelExec);
+
+	strscpy(evt->path, path, sizeof(evt->path));
+	bhv_guestlog_char_array_into_buf(evt->args, sizeof(evt->args), argv);
+	bhv_guestlog_char_array_into_buf(evt->env, sizeof(evt->env), envp);
+
+	SEND_MSG(evt, KernelExec, 0, true);
+}
+
+/*****************************************************************
+ * init
+ *****************************************************************/
+int __init bhv_init_guestlog()
+{
+	HypABI__Guestlog__Init__arg__T *glc = &bhv_guestlog_config;
+
+	if (!bhv_guestlog_enabled())
+		return 0;
+
+	if (HypABI__Guestlog__Init__arg__slab)
+		glc = HypABI__Guestlog__Init__arg__ALLOC();
+
+	if (HypABI__Guestlog__Init__hypercall_noalloc(glc)) {
+		bhv_fail("BHV: guestlog init failed");
+		if (glc != &bhv_guestlog_config)
+			HypABI__Guestlog__Init__arg__FREE(glc);
+		return -EFAULT;
+	}
+
+	if (glc != &bhv_guestlog_config) {
+		bhv_guestlog_config = *glc;
+		HypABI__Guestlog__Init__arg__FREE(glc);
+	}
+
+	return 0;
+}
+/***************************************************************/
diff --git security/bhv/init/init.c security/bhv/init/init.c
new file mode 100644
index 000000000..d3b2a35e1
--- /dev/null
+++ security/bhv/init/init.c
@@ -0,0 +1,254 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/syscall.h>
+#include <linux/kmod.h>
+#include <linux/mm.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv_print.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/creds.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/init/init.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+
+extern uint8_t __bhv_text_start[];
+extern uint8_t __bhv_text_end[];
+#endif // VASKM
+
+bool __bhv_init_done __ro_after_init = false;
+
+static inline void
+bhv_section_run_ctor(HypABI__Init__Init__BHVSectionRun__T *curr_item,
+		     HypABI__Init__Init__BHVSectionRun__T *prev_item,
+		     uint64_t gpa_start, uint64_t size, uint8_t type)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (HypABI__Init__Init__BHVSectionRun__T){
+		.gpa_start = gpa_start,
+		.size = size,
+		.type = type,
+		.next = BHV_INVALID_PHYS_ADDR,
+	};
+
+	if (prev_item)
+		prev_item->next = bhv_virt_to_phys(curr_item);
+}
+
+static int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size)
+{
+	unsigned long r;
+	unsigned int region_counter = 0;
+
+	struct {
+		HypABI__Init__Init__arg__T init_arg;
+		bhv_mem_region_t mem_regions[BHV_INIT_MAX_REGIONS];
+		HypABI__Init__Init__BHVSectionRun__T bhv_section_runs[];
+	} *arg = bhv_data;
+	BUG_ON((void *)&arg->bhv_section_runs[2] - bhv_data > +bhv_data_size);
+
+#ifndef VASKM // inside kernel tree
+#define BI_ALIGN_START(start) (unsigned long)(start)
+#else // out of tree
+#define BI_ALIGN_START(start) round_down((unsigned long)(start), PAGE_SIZE)
+#endif // VASKM
+
+#define BI_ALIGN_START_SIZE(start, end)                                        \
+	(bhv_virt_to_phys((void *)BI_ALIGN_START(start))),                     \
+		(round_up((unsigned long)(end), PAGE_SIZE) -                   \
+		 BI_ALIGN_START(start))
+#define BI_ALIGN_START_SIZE_KLN(start, end)                                    \
+	BI_ALIGN_START_SIZE(KLN_SYM(start), KLN_SYM(end))
+#define BI_LL_FIRST(ll) &(ll)[region_counter], NULL
+#define BI_LL_NEXT(ll) &(ll)[region_counter], &(ll)[region_counter - 1]
+
+	bhv_mem_region_create_ctor(BI_LL_FIRST(arg->mem_regions),
+				   BI_ALIGN_START_SIZE_KLN(_stext, _etext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL TEXT SECTION");
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sinittext, _einittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL INIT TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sexittext, _eexittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL EXIT TEXT SECTION");
+	region_counter++;
+#endif // VASKM
+
+	bhv_init_hyp_arch(arg->mem_regions, &region_counter);
+
+	region_counter = 0;
+	BUG_ON((unsigned long)bhv_data & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_FIRST(arg->bhv_section_runs), bhv_virt_to_phys(bhv_data),
+		bhv_data_size,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA);
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		BI_ALIGN_START_SIZE(__bhv_text_start, __bhv_text_end),
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__VAULT);
+	region_counter++;
+
+#else // out of tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		vmalloc_to_phys(__bhv_text_start), PAGE_SIZE,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__VAULT);
+
+	for (uint8_t *p = __bhv_text_start + PAGE_SIZE; p < __bhv_text_end;
+	     p += PAGE_SIZE) {
+		phys_addr_t phy = vmalloc_to_phys(p);
+		if (phy == arg->bhv_section_runs[region_counter].gpa_start +
+				   arg->bhv_section_runs[region_counter].size) {
+			arg->bhv_section_runs[region_counter].size += PAGE_SIZE;
+		} else {
+			region_counter++;
+			BUG_ON((uint64_t)&arg->bhv_section_runs[region_counter +
+								1] -
+				       (uint64_t)arg >
+			       bhv_data_size);
+
+			bhv_section_run_ctor(
+				BI_LL_NEXT(arg->bhv_section_runs), phy,
+				PAGE_SIZE,
+				HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__VAULT);
+		}
+	}
+	region_counter++;
+#endif // VASKM
+
+#ifdef BHV_CONST_MODPROBE_PATH
+	arg->init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	arg->init_arg.modprobe_path = bhv_virt_to_phys((void *)&modprobe_path);
+#else
+	arg->init_arg.modprobe_path_sz = 0;
+	arg->init_arg.modprobe_path = BHV_INVALID_PHYS_ADDR;
+#endif /* BHV_CONST_MODPROBE_PATH */
+
+	arg->init_arg.owner = 0;
+	arg->init_arg.region_head = bhv_virt_to_phys(&arg->mem_regions);
+	arg->init_arg.bhv_region_head =
+		bhv_virt_to_phys(&arg->bhv_section_runs);
+
+	r = HypABI__Init__Init__hypercall_noalloc(&arg->init_arg);
+	if (r)
+		return -EINVAL;
+	return 0;
+}
+
+void __init bhv_init_platform(void)
+{
+	int rv;
+	uint32_t cid, port;
+	void *bhv_data_ptr = NULL;
+	HypABI__Init__Init__BHVData__T *data_ptr = NULL;
+	static_assert(sizeof(uint64_t) ==
+		      sizeof(unsigned long)); //for pointer cast below
+
+	bhv_init_arch();
+
+#ifndef VASKM // inside kernel tree
+	rv = bhv_init_hyp(__bhv_data_start, __bhv_data_end - __bhv_data_start);
+	bhv_data_ptr = __bhv_data_start;
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+#else // out of tree
+	bhv_data_ptr = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	// no kfree on purpose
+	if (!bhv_data_ptr) {
+		pr_err("BHV: cannot allocate bhv_data\n");
+		return;
+	}
+
+	rv = bhv_init_hyp(bhv_data_ptr, PAGE_SIZE);
+#endif // VASKM
+
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return;
+	}
+
+	bhv_initialized = true;
+	data_ptr = bhv_data_ptr;
+	bhv_configuration_bitmap =
+		(unsigned long *)&data_ptr
+			->config_bitmap; // see static_cast above
+	cid = data_ptr->vsocket_cid;
+	port = data_ptr->vsocket_port;
+
+	rv = bhv_init_guestconn(cid, port);
+	if (rv) {
+		bhv_fail(
+			"BHV: Cannot configure the BHV guest connection subsystem");
+		return;
+	}
+
+	rv = bhv_init_guestlog();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV logging subsystem");
+		return;
+	}
+
+	rv = bhv_init_cred();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV creds subsystem");
+		return;
+	}
+
+	bhv_init_fileops();
+
+#ifndef VASKM // inside kernel tree
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+#endif // VASKM
+
+	__bhv_init_done = true;
+}
diff --git security/bhv/init/late_start.c security/bhv/init/late_start.c
new file mode 100644
index 000000000..f7eb50a63
--- /dev/null
+++ security/bhv/init/late_start.c
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+#include <bhv/init/late_start.h>
+
+void bhv_late_start(void)
+{
+	int r = bhv_late_start_init_ptpg();
+	if (r) {
+		bhv_fail("ptpg init failed");
+	}
+}
diff --git security/bhv/init/mm_init.c security/bhv/init/mm_init.c
new file mode 100644
index 000000000..177124a71
--- /dev/null
+++ security/bhv/init/mm_init.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/acl.h>
+#endif // VASKM
+
+#include <bhv/init/mm_init.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+void __init bhv_mm_init(void)
+{
+	HypABI__init_slabs();
+
+	bhv_mm_init_integrity();
+#ifndef VASKM // inside kernel tree
+	bhv_mm_init_acl();
+#endif // VASKM
+	bhv_mm_init_guestconn();
+	bhv_mm_init_cred();
+	bhv_mm_init_file_protection();
+	bhv_mm_init_fileops_protection();
+}
diff --git security/bhv/init/start.c security/bhv/init/start.c
new file mode 100644
index 000000000..b9209e28f
--- /dev/null
+++ security/bhv/init/start.c
@@ -0,0 +1,176 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <linux/types.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/guestconn.h>
+
+#include <bhv/init/start.h>
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_SECURITY_SELINUX
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#endif // VASKM
+
+
+static inline void do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	HypABI__Init__Start__arg__T *config =
+		(HypABI__Init__Start__arg__T *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = HypABI__Init__Start__hypercall_noalloc(config);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (HypABI__Init__Start__arg__T *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = HypABI__Init__Start__hypercall_noalloc(config);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#if !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		if ((sizeof(HypABI__Init__Start__arg__T) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#else // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		bhv_fail("guest policy available without target LSM");
+#endif // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+bool __init_km bhv_start(void)
+{
+	int rc;
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_node_t *n[2];
+#endif // VASKM
+
+	if (!is_bhv_initialized())
+		return false;
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_ptpg();
+
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return false;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region.remove));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+#endif // VASKM
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_start_delete_alternatives();
+	}
+
+	bhv_start_guestconn();
+#endif // VASKM
+
+	do_start();
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_sysfs();
+	}
+
+	if (bhv_reg_protect_is_enabled()) {
+		bhv_start_reg_protect();
+	}
+
+	return true;
+}
diff --git security/bhv/inode.c security/bhv/inode.c
new file mode 100644
index 000000000..51333a666
--- /dev/null
+++ security/bhv/inode.c
@@ -0,0 +1,396 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fs.h>
+#include <linux/siphash.h>
+#include <linux/uidgid.h>
+
+#include <bhv/creds.h>
+#include <bhv/event.h>
+#include <bhv/inode.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/inode.h>
+
+#define BHV_INODE_HYP(op, arg) bhv_hypercall_vas(BHV_VAS_BACKEND_INODE, op, arg)
+
+#define BHV_INODE_REGISTER_HYP(arg)                                            \
+	BHV_INODE_HYP(BHV_VAS_INODE_OP_REGISTER, arg)
+#define BHV_INODE_UPDATE_HYP(arg) BHV_INODE_HYP(BHV_VAS_INODE_OP_UPDATE, arg)
+#define BHV_INODE_RELEASE_HYP(arg) BHV_INODE_HYP(BHV_VAS_INODE_OP_RELEASE, arg)
+#define BHV_INODE_VERIFY_HYP(arg) BHV_INODE_HYP(BHV_VAS_INODE_OP_VERIFY, arg)
+#define BHV_INODE_LOG_HYP(arg) BHV_INODE_HYP(BHV_VAS_INODE_OP_LOG, arg)
+
+#define BHV_INODE_DEBUG 0
+
+static bool bhv_inode_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_inode_is_active(void)
+{
+	if (!bhv_inode_initialized)
+		return false;
+
+	return bhv_inode_is_enabled();
+}
+
+static size_t collect_inode_invariants(char *buf, const struct inode *inode,
+				       size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t i_addr = (uint64_t)inode;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(umode_t) +
+				       sizeof(kuid_t) + sizeof(kgid_t) +
+				       sizeof(unsigned long);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	/*
+	 * Consider tracking the integrity of the remaining fields as well (not
+	 * just the below). Determine, which of them are subject to change
+	 * during the inode's life time. Otherwise, tracking the integrity of
+	 * those would require an inode tag update every time the fields change.
+	 *
+	 * NOTE: The difficulty in tracking these fields is that we need to make
+	 * sure that we track every time any of the below fields change at
+	 * run-time.
+	 */
+
+	_buf = memcpy(_buf, &i_addr, sizeof(i_addr));
+	_buf += sizeof(i_addr);
+
+	_buf = memcpy(_buf, &inode->i_mode, sizeof(umode_t));
+	_buf += sizeof(umode_t);
+
+	_buf = memcpy(_buf, &inode->i_uid, sizeof(kuid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_gid, sizeof(kgid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_ino, sizeof(unsigned long));
+	_buf += sizeof(unsigned long);
+
+	return buf_size;
+}
+
+static uint64_t siphash_inode_state(const struct inode *inode)
+{
+#define MAX_BUF_SIZE                                                           \
+	sizeof(uint64_t) + sizeof(umode_t) + sizeof(kuid_t) + sizeof(kgid_t) + \
+		sizeof(unsigned long)
+
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_inode_invariants(buf, inode, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static inline bool __is_setuid(struct cred *new, const struct cred *old)
+{
+	return !uid_eq(new->euid, old->uid);
+}
+
+static inline bool __is_setgid(struct cred *new, const struct cred *old)
+{
+	return !gid_eq(new->egid, old->gid);
+}
+
+static inline void bhv_inode_register(struct inode *inode)
+{
+	int rc = 0;
+	bhv_inode_arg_t arg = { 0 };
+
+	if (!inode)
+		return;
+
+	arg.inode_register.inode.addr = (uint64_t)inode;
+	arg.inode_register.inode.hmac = siphash_inode_state(inode);
+
+	rc = BHV_INODE_REGISTER_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Register inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_register.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_update(struct inode *inode)
+{
+	int rc = 0;
+	bhv_inode_arg_t arg = { 0 };
+
+	if (!inode)
+		return;
+
+	arg.inode_update.inode.addr = (uint64_t)inode;
+	arg.inode_update.inode.hmac = siphash_inode_state(inode);
+
+	rc = BHV_INODE_UPDATE_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the UPDATE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Update inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_update.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_release(struct inode *inode)
+{
+	int rc = 0;
+	bhv_inode_arg_t arg = { 0 };
+
+	if (!inode)
+		return;
+
+	arg.inode_release.inode.addr = (uint64_t)inode;
+
+	rc = BHV_INODE_RELEASE_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the RELEASE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu\n", __FUNCTION__, inode->i_ino);
+#endif
+}
+
+static inline int bhv_inode_verify(struct inode *inode, struct file *file)
+{
+	int rc = 0;
+	bhv_inode_arg_t arg = { 0 };
+	enum event_type type = EVENT_NONE;
+
+	if (!inode)
+		return 0;
+
+	arg.inode_verify.inode.addr = (uint64_t)inode;
+	arg.inode_verify.inode.hmac = siphash_inode_state(inode);
+
+	rc = BHV_INODE_VERIFY_HYP(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Verify inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_verify.inode.hmac);
+#endif
+
+	type = arg.inode_verify.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		populate_event_context(&arg.inode_log.context, true);
+		rc = get_file_path(&file->f_path, arg.inode_log.file_path,
+				   GuestConnABI__GuestLog__Context__MAX_PATH_SZ);
+		if (rc) {
+			return rc;
+		}
+
+		arg.inode_log.event_type = type;
+		arg.inode_log.inode_addr = (uint64_t)(inode);
+		arg.inode_log.inode_uid = (uint32_t)(__kuid_val(inode->i_uid));
+		arg.inode_log.inode_gid = (uint32_t)(__kgid_val(inode->i_gid));
+		arg.inode_log.inode_mode = (uint16_t)(inode->i_mode);
+
+		rc = BHV_INODE_LOG_HYP(&arg);
+		if (rc) {
+			pr_err("[-BHV-] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			return rc;
+		}
+
+		return (arg.inode_log.block) ? -EPERM : 0;
+	}
+
+	return rc;
+}
+
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
+{
+	int rc = 0;
+	bool is_setxid = false;
+	const struct cred *old = current_cred();
+	struct cred *new = bprm->cred;
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return 0;
+
+	is_setxid = __is_setuid(new, old) || __is_setgid(new, old);
+	if (!is_setxid)
+		return 0;
+
+	inode = d_backing_inode(file->f_path.dentry);
+
+	/*
+	 * We consider the following cases if is_setxid == true:
+	 * - The new executable has the SXID bits set: verify its inode.
+	 * - The new executable does not have the SXID bits set: verify
+	 *   credentials of the current process.
+	 */
+
+	inode_lock(inode);
+
+	if (is_sxid(inode->i_mode)) {
+		rc = bhv_inode_verify(inode, file);
+		inode_unlock(inode);
+	} else {
+		inode_unlock(inode);
+		rc = bhv_cred_verify(current);
+	}
+
+	return rc;
+}
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+/* Caller holds semaphore inode->i_rwsem. */
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid,
+			    umode_t old_mode)
+{
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return;
+
+	/*
+	 * Unfortunately, we cannot verify the current task's credentials at
+	 * this point. This function is sometimes called after having
+	 * temporarily overwritten the current tasks' credentials via
+	 * override_creds; this is done e.g., by the overlay fs to temporarily
+	 * elevate permissions to the filesystem's owner so allow ops on the
+	 * filesystem. As long as we do not track the temporarily overwritten
+	 * credentials, we cannot verify the current task's credentials.
+	 */
+
+	/* We are interested only in mode, UID, and GID changes. */
+	if (!(ia_valid & (ATTR_MODE | ATTR_UID | ATTR_GID)))
+		return;
+
+	/* Note that the caller of this function locked the inode. */
+	inode = d_backing_inode(dentry);
+
+	/* Mode changes should be updated only if the SXID bits are set. */
+	if ((old_mode & (S_ISUID | S_ISGID)) ==
+	    (inode->i_mode & ((S_ISUID | S_ISGID)))) {
+		if (!is_sxid(inode->i_mode))
+			return;
+	}
+
+	/* Register inode if the previous mode did not have SXID bits set. */
+	if (!is_sxid(old_mode) && is_sxid(inode->i_mode)) {
+		bhv_inode_register(inode);
+		return;
+	}
+
+	/* Release tracked inode if setattr removed SXID bits. */
+	if (is_sxid(old_mode) && !is_sxid(inode->i_mode)) {
+		bhv_inode_release(inode);
+		return;
+	}
+
+	/* In any other case, the inode is tracked: update the changes */
+	bhv_inode_update(inode);
+}
+
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+	if (!is_sxid(inode->i_mode))
+		return;
+
+	/*
+	 * Note: we could check whether the inode has any existing dentry
+	 * aliases, via d_find_any_alias, and if so return. Yet, we chose to do
+	 * this check from within BHV to ensure that an attacker cannot modify
+	 * this value to skip inode registrations.
+	 */
+
+	spin_lock(&inode->i_lock);
+	bhv_inode_register(inode);
+	spin_unlock(&inode->i_lock);
+}
+
+/* Caller holds the spinlock inode->i_lock. */
+void bhv_inode_iput_final(struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu i_count=%d i_nlink=%d\n",
+		__FUNCTION__, inode->i_ino, atomic_read(&inode->i_count),
+		inode->i_nlink);
+#endif
+
+	/*
+	 * Note: when releasing inodes, we unconditionally call into BHV (i.e.,
+	 * disregarding whether the inode is privileged or not) to avoid
+	 * potential reuse attacks.  Otherwise, the attacker could maliciously
+	 * modify a privileged (and by BHV registered) inode, cause the kernel
+	 * to drop the inode in  the kernel, and then re-allocate an
+	 * unprivileged file, the meta data of which can be adjusted to match
+	 * the previous meta information that were incorporated into the HMAC of
+	 * the previously privileged inode.
+	 *
+	 * XXX: Consider conditionally calling into BHV at this point if the
+	 * above renders to be not critical (every new inode allocation receives
+	 * a new inode->i_ino). This would reduce the number of hypercalls.
+	 */
+
+	bhv_inode_release(inode);
+}
+
+int __init bhv_inode_init(void)
+{
+	if (!bhv_inode_is_enabled())
+		return -EPERM;
+
+	if (bhv_inode_initialized)
+		return -EPERM;
+
+	bhv_inode_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/integrity.c security/bhv/integrity.c
new file mode 100644
index 000000000..2d608b3e4
--- /dev/null
+++ security/bhv/integrity.c
@@ -0,0 +1,215 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/filter.h>
+#include <linux/module.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+/************************************************************/
+
+int bhv_integrity_freeze_events(uint64_t flags)
+{
+	if (HypABI__Integrity__Freeze__HYPERCALL(.flags = flags))
+		return -EINVAL;
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+extern struct mutex module_mutex;
+extern struct list_head modules;
+#else // out of tree
+#include <kln.h>
+#endif //VASKM
+
+static void lock_all_modules(void)
+{
+	static bool all_modules_locked = false;
+	struct module *mptr;
+
+	if (all_modules_locked)
+		return;
+
+	mutex_lock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+
+	list_for_each_entry (mptr, KLN_SYMBOL_P(struct list_head *, modules),
+			     list) {
+		if (mptr != THIS_MODULE) {
+			if (!try_module_get(mptr)) {
+				printk(KERN_WARNING
+				       "%s: Cannot lock module %s\n",
+				       __FUNCTION__, mptr->name);
+			}
+		}
+	}
+
+	mutex_unlock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+	all_modules_locked = true;
+	return;
+}
+
+bool bhv_allow_kmod_loads = true;
+bool bhv_allow_patch = true;
+bool bhv_integrity_freeze_create_currently_frozen = false;
+bool bhv_integrity_freeze_update_currently_frozen = false;
+bool bhv_integrity_freeze_remove_currently_frozen = false;
+bool bhv_integrity_freeze_patch_currently_frozen = false;
+
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks)
+{
+	int ret;
+	if (!skip_locks) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_kmod_loads = false;
+			lock_all_modules();
+		}
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_patch = false;
+		}
+	} else {
+		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+		       __FUNCTION__);
+	}
+
+	ret = bhv_integrity_freeze_events(flags);
+
+	if (!ret) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE)
+			bhv_integrity_freeze_create_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE)
+			bhv_integrity_freeze_update_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE)
+			bhv_integrity_freeze_remove_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH)
+			bhv_integrity_freeze_patch_currently_frozen = true;
+	}
+
+	return ret;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Create__HYPERCALL(.owner = owner,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Update__HYPERCALL(.region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) false,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) true,
+						    .owner = owner);
+}
+
+static uint64_t _ptpg_pgd_offset __ro_after_init;
+static uint64_t _ptpg_pgd_value __ro_after_init;
+static atomic_t _ptpg_ready __ro_after_init = ATOMIC_INIT(0);
+
+/**************************************************************
+ * start
+ **************************************************************/
+void __init_km bhv_start_ptpg(void)
+{
+	_ptpg_pgd_offset = 0;
+	_ptpg_pgd_value = 0;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+
+	bhv_start_get_pt_protect_pgd_data(&_ptpg_pgd_offset, &_ptpg_pgd_value);
+
+	atomic_inc(&_ptpg_ready);
+}
+/**************************************************************/
+
+/**************************************************************
+ * late_start
+ **************************************************************/
+int bhv_late_start_init_ptpg(void)
+{
+	unsigned long r;
+	HypABI__Integrity__PtpgInit__arg__T *arg;
+
+	if (!bhv_integrity_pt_prot_is_enabled() || !atomic_read(&_ptpg_ready)) {
+		return 0;
+	}
+
+	BUG_ON(sizeof(HypABI__Integrity__PtpgInit__arg__T) > PAGE_SIZE);
+
+	arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+
+	bhv_late_start_get_pt_protect_data(arg);
+
+	r = HypABI__Integrity__PtpgInit__hypercall_noalloc(arg);
+
+	HypABI__Integrity__PtpgInit__arg__FREE(arg);
+
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+/**************************************************************/
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+	unsigned long r;
+	bool success;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+	success = bhv_pt_protect_check_pgd_arch(mm, _ptpg_pgd_offset,
+						_ptpg_pgd_value);
+	if (!success) {
+		r = HypABI__Integrity__PtpgReport__hypercall_noalloc();
+		if (r) {
+			pr_err("BHV: error reporting pt violation to host!");
+		}
+	}
+}
diff --git security/bhv/keyring.c security/bhv/keyring.c
new file mode 100644
index 000000000..a62cfc2ee
--- /dev/null
+++ security/bhv/keyring.c
@@ -0,0 +1,204 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/key-type.h>
+#include <linux/siphash.h>
+
+#include <bhv/keyring.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/keyring.h>
+
+#define BHV_KEYRING_HYP(op, arg)                                               \
+	bhv_hypercall_vas(BHV_VAS_BACKEND_KEYRING, op, arg)
+
+#define BHV_KEYRING_REGISTER_HYP(arg)                                          \
+	BHV_KEYRING_HYP(BHV_VAS_KEYRING_OP_REGISTER, arg)
+#define BHV_KEYRING_VERIFY_HYP(arg)                                            \
+	BHV_KEYRING_HYP(BHV_VAS_KEYRING_OP_VERIFY, arg)
+#define BHV_KEYRING_LOG_HYP(arg) BHV_KEYRING_HYP(BHV_VAS_KEYRING_OP_LOG, arg)
+
+#define BHV_KEYRING_DEBUG 0
+
+static bool bhv_keyring_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_keyring_is_active(void)
+{
+	if (!bhv_keyring_initialized)
+		return false;
+
+	return bhv_keyring_is_enabled();
+}
+
+static size_t collect_keyring_invariants(char *buf, const struct key *keyring,
+					 uint64_t anchor, size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct key keyring_copy;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct key);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&keyring_copy, keyring, sizeof(struct key));
+
+	/* Exclude mutable fields from the keyring to be hashed. */
+
+	refcount_set(&keyring_copy.usage, 0);
+	memset(&keyring_copy.graveyard_link, 0, sizeof(struct list_head));
+	memset(&keyring_copy.serial_node, 0, sizeof(struct rb_node));
+#ifdef CONFIG_KEY_NOTIFICATIONS
+	keyring_copy.watchers = NULL;
+#endif
+	memset(&keyring_copy.sem, 0, sizeof(struct rw_semaphore));
+	keyring_copy.last_used_at = 0;
+	keyring_copy.datalen = 0;
+	memset(&keyring_copy.payload, 0, sizeof(union key_payload));
+
+	bound_context = (uint64_t)keyring ^ anchor;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &keyring_copy, sizeof(struct key));
+
+	return buf_size;
+}
+
+static uint64_t siphash_keyring_state(const struct key *keyring,
+				      uint64_t anchor)
+{
+#define MAX_BUF_SIZE sizeof(uint64_t) + sizeof(struct key)
+	char buf[MAX_BUF_SIZE];
+	size_t size =
+		collect_keyring_invariants(buf, keyring, anchor, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	bhv_keyring_arg_t arg = { 0 };
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	arg.keyring_verify.keyring.addr = (uint64_t)anchor;
+	arg.keyring_verify.keyring.hmac =
+		siphash_keyring_state(keyring, (uint64_t)anchor);
+
+	rc = BHV_KEYRING_VERIFY_HYP(&arg);
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	type = arg.keyring_verify.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		populate_event_context(&arg.keyring_log.context, true);
+
+		arg.keyring_log.event_type = type;
+		arg.keyring_log.addr = (uint64_t)(keyring);
+		arg.keyring_log.uid = (uint32_t)(__kuid_val(keyring->uid));
+		arg.keyring_log.gid = (uint32_t)(__kgid_val(keyring->gid));
+		arg.keyring_log.perm = (uint32_t)(keyring->perm);
+		arg.keyring_log.serial = (uint32_t)(keyring->serial);
+
+		strncpy(arg.keyring_log.desc, keyring->description,
+			GuestConnABI__GuestLog__Context__MAX_PATH_SZ);
+
+		rc = BHV_KEYRING_LOG_HYP(&arg);
+		if (rc) {
+			pr_err("[BHV] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			return rc;
+		}
+
+		return (arg.keyring_log.block) ? -EPERM : 0;
+	}
+
+	return rc;
+}
+
+int bhv_keyring_verify(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	down_read(&keyring->sem);
+	rc = bhv_keyring_verify_locked(keyring, anchor);
+	up_read(&keyring->sem);
+
+	return rc;
+}
+
+static int bhv_keyring_register(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	bhv_keyring_arg_t arg = { 0 };
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	/* Note: we index system-trusted keyrings by it's global anchor. */
+	arg.keyring_register.keyring.addr = (uint64_t)anchor;
+	arg.keyring_register.keyring.hmac =
+		siphash_keyring_state(keyring, (uint64_t)anchor);
+
+	rc = BHV_KEYRING_REGISTER_HYP(&arg);
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	return (arg.keyring_register.block) ? -EPERM : 0;
+}
+
+int bhv_keyring_register_system_trusted(struct key **k)
+{
+	void *anchor = k;
+	return bhv_keyring_register(*k, anchor);
+}
+
+int __init bhv_init_keyring(void)
+{
+	if (!bhv_keyring_is_enabled())
+		return -EPERM;
+
+	if (bhv_keyring_initialized)
+		return -EPERM;
+
+	bhv_keyring_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/lsm.c security/bhv/lsm.c
new file mode 100644
index 000000000..51fa6eef8
--- /dev/null
+++ security/bhv/lsm.c
@@ -0,0 +1,687 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fdtable.h>
+#include <linux/kernel.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <uapi/linux/magic.h>
+#include <linux/dcache.h>
+#include <linux/module.h>
+
+#include <bhv/fileops_internal.h>
+#include <bhv/acl.h>
+#include <bhv/bhv.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/inode.h>
+#include <bhv/integrity.h>
+#include <bhv/guestlog.h>
+#include <bhv/sysfs_fops.h>
+
+#ifndef VASKM // inside kernel tree
+#include <linux/lsm_hooks.h>
+#include <bhv/kernel-kln.h>
+#define static_vk static
+
+#else // out of tree
+#include <module.h>
+#include <kln.h>
+#include <fsreg.h>
+#define static_vk
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+static int bhv_read_file(struct file *file, enum kernel_read_file_id id,
+			 bool whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+
+static int bhv_load_data(enum kernel_load_data_id id, bool contents)
+{
+	const char *origin = kernel_read_file_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			bhv_guestlog_log_driver_load("[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+#endif // VASKM
+
+static_vk int bhv_task_alloc(struct task_struct *target,
+			     long unsigned int clone_flags)
+{
+	pr_debug("[bhv] TASK CREATE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_fork(target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+static_vk void bhv_task_free(struct task_struct *target)
+{
+	pr_debug("[bhv] TASK FREE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exit(target->pid, target->parent->pid,
+					      target->comm);
+	}
+}
+
+static_vk int bhv_bprm_check_security(struct linux_binprm *bprm)
+{
+	int rv = 0;
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+	if (bhv_acl_is_proc_acl_enabled()) {
+		const char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&bprm->file->f_path, filename_buf, PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+
+		// check executed filename (name on cli)
+		if (bhv_block_process(bprm->filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		// check underlying file (e.g., busybox or interpreter)
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exec(bprm, current->pid,
+					      current->parent->pid,
+					      bprm->filename);
+	}
+
+	return rv;
+}
+
+static const inline char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+static bool try_update_bhv_fops_map(u8 bhv_fops, bool is_dir,
+				    const struct file_operations *fops_ptr)
+{
+	BUG_ON(fileops_map[bhv_fops][is_dir == true ? 1 : 0]);
+
+	if (!bhv_allow_update_fileops_map)
+		return false;
+
+	if (is_module_ro_data((unsigned long)fops_ptr)) {
+		fileops_map[bhv_fops][is_dir == true ? 1 : 0] = fops_ptr;
+		pr_info("%s: Added fops %d %d\n", __FUNCTION__, bhv_fops,
+			is_dir);
+		return true;
+	}
+	return false;
+}
+#endif // defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+
+static bool bhv_perform_check_fileops(struct file *file, u8 bhv_fops,
+				      bool is_dir)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+	bool block = false;
+	uint64_t dtype = 0;
+
+	if (bhv_fops != BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED) {
+		if (file->f_op ==
+		    fileops_map[bhv_fops][is_dir == true ? 1 : 0]) {
+			// fops matches
+			return false;
+		}
+	}
+
+	switch (bhv_fops) {
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS // out of tree
+	case BHV_VAS_FILEOPS_PROTECTION_EXT4:
+	case BHV_VAS_FILEOPS_PROTECTION_XFS:
+		if (!fileops_map[bhv_fops][is_dir == true ? 1 : 0])
+			if (try_update_bhv_fops_map(bhv_fops, is_dir,
+						    file->f_op))
+				return false;
+		break;
+#endif // VASKM
+	case BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED: {
+		if (bhv_strict_fileops_enforced()) {
+			/*
+			 * strict mode configured or forced by kernel boot option
+			 * continue to report and optionally block
+			 */
+			break;
+		}
+
+		/*
+		 * fallback in case of unknown fops:
+		 * check if pointer points to ro section
+		 */
+		if (bhv_fileops_is_ro((u64)file->f_op)) {
+			if (!bhv_guestlog_log_unknown_fileops()) {
+				return false;
+			}
+
+			// set the type for logging
+			if (d_is_reg(file->f_path.dentry)) {
+				dtype = 1;
+			} else if (d_can_lookup(file->f_path.dentry)) {
+				dtype = 2;
+			} else if (d_is_special(file->f_path.dentry)) {
+				// save i_rdev which contains major and minor
+				dtype = ((u64)(file->f_inode->i_rdev) << 0x10) |
+					(MINORBITS << 8) | 3;
+			}
+
+			// log info about unsupported
+			pathname_buf = kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ,
+					       GFP_KERNEL);
+			pathname = get_pathname(file, pathname_buf,
+						BHV_VAS_FILEOPS_PATH_MAX_SZ);
+
+			bhv_guestlog_log_fops_unknown(
+				file->f_inode->i_sb->s_magic, pathname, dtype,
+				file->f_op);
+
+			pathname = NULL;
+			kfree(pathname_buf);
+			// OK
+			return false;
+		}
+
+		break;
+	}
+	// additional check for empty dir ops
+	case BHV_VAS_FILEOPS_PROTECTION_SYSFS:
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       empty_dir_operations)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	// additional check for dummy fops
+	case BHV_VAS_FILEOPS_PROTECTION_TTY:
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       hung_up_tty_fops)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	case BHV_VAS_FILEOPS_PROTECTION_NULL:
+	case BHV_VAS_FILEOPS_PROTECTION_URANDOM:
+	case BHV_VAS_FILEOPS_PROTECTION_RANDOM:
+	case BHV_VAS_FILEOPS_PROTECTION_CONSOLE:
+	case BHV_VAS_FILEOPS_PROTECTION_KMSG:
+	case BHV_VAS_FILEOPS_PROTECTION_MEM:
+	case BHV_VAS_FILEOPS_PROTECTION_ZERO:
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       def_chr_fops)) {
+			// we're all set
+			return false;
+		}
+		break;
+	case BHV_VAS_FILEOPS_PROTECTION_PROC:
+		// additional check for proc fops:
+		if (is_valid_proc_fop(&(file->f_op)))
+			return false;
+		break;
+	case BHV_VAS_FILEOPS_PROTECTION_DEBUGFS:
+		if (is_valid_debugfs_fop(file->f_op))
+			return false;
+		break;
+	default:
+		break;
+	}
+
+	// by now we have decided that the fops ptr is bad
+
+	pathname_buf = kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ, GFP_KERNEL);
+	pathname =
+		get_pathname(file, pathname_buf, BHV_VAS_FILEOPS_PATH_MAX_SZ);
+	if (bhv_block_fileops(pathname, bhv_fops, is_dir, file->f_op)) {
+		// block file operation
+		block = true;
+	}
+
+	pathname = NULL;
+	kfree(pathname_buf);
+
+	return block;
+}
+
+static bool bhv_check_fileops(struct file *file)
+{
+	// set up fops check data
+	u8 bhv_fops = BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED;
+	bool is_dir = false;
+
+	unsigned dev_major = imajor(file->f_inode);
+	unsigned dev_minor = iminor(file->f_inode);
+
+	if (d_can_lookup(file->f_path.dentry)) {
+		/* directory S_ISDIR(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+		is_dir = true;
+	} else if (d_is_reg(file->f_path.dentry)) {
+		/* regular file  S_ISREG(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+	} else if (d_is_special(file->f_path.dentry)) {
+		// DCACHE_SPECIAL_TYPE
+		if (S_ISSOCK(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == SOCKFS_MAGIC
+			bhv_fops = BHV_VAS_FILEOPS_PROTECTION_SOCKET;
+		} else if (S_ISFIFO(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == PIPEFS_MAGIC
+			bhv_fops = BHV_VAS_FILEOPS_PROTECTION_PIPE;
+		} else if (S_ISCHR(file->f_inode->i_mode)) {
+			// character device
+			if (dev_major == 1) {
+				// mem
+				switch (dev_minor) {
+				case 1: // DEVMEM_MINOR
+					// /dev/mem
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_MEM;
+					break;
+				case 3:
+					// /dev/null
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_NULL;
+					break;
+				case 4:
+					// /dev/port
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_PORT;
+					break;
+				case 5:
+					// /dev/zero
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_ZERO;
+					break;
+				case 7:
+					// /dev/full
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_FULL;
+					break;
+				case 8:
+					// /dev/random
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_RANDOM;
+					break;
+				case 9:
+					// /dev/urandom
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_URANDOM;
+					break;
+				case 11:
+					// /dev/kmsg
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_KMSG;
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 5) {
+				switch (dev_minor) {
+				case 0:
+					// /dev/tty
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_TTY;
+					break;
+				case 1:
+					// /dev/console
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_CONSOLE;
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 4 && dev_minor == 64) {
+				// /dev/ttyS0
+				bhv_fops = BHV_VAS_FILEOPS_PROTECTION_TTY;
+			} else if (dev_major == 229 && dev_minor == 0) {
+				// /dev/hvc0
+				bhv_fops = BHV_VAS_FILEOPS_PROTECTION_TTY;
+			}
+		}
+	}
+
+#ifdef DEBUG
+	if (bhv_fops == BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED) {
+		char *pathname_buf =
+			kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ, GFP_KERNEL);
+		const char *pathname = get_pathname(
+			file, pathname_buf, BHV_VAS_FILEOPS_PATH_MAX_SZ);
+		pr_debug(
+			"name: %s magic: 0x%lx dentry_type: 0x%x stat: 0o%o path: %s fops: 0x%px",
+			file->f_inode->i_sb->s_type->name,
+			file->f_inode->i_sb->s_magic,
+			file->f_path.dentry->d_flags & DCACHE_ENTRY_TYPE,
+			(file->f_inode->i_mode & S_IFMT), pathname, file->f_op);
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+#endif
+
+	// perform fops check
+	return bhv_perform_check_fileops(file, bhv_fops, is_dir);
+}
+
+static_vk int bhv_check_files_dirty_pipe(const void *address_space,
+					 struct file *file, unsigned int number)
+{
+	char *filename_buf = NULL;
+	const char *filename = NULL;
+	int rv;
+
+	struct inode *ipipe =
+		address_space != NULL ?
+			((struct address_space *)address_space)->host :
+			NULL;
+	struct inode *ifile = d_real_inode(file->f_path.dentry);
+
+	if (ipipe == NULL || ifile == NULL || !virt_addr_valid(ipipe))
+		return 0;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (ifile->i_ino == ipipe->i_ino && ifile->i_sb == ipipe->i_sb &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Allocate memory
+		filename_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+
+		if (filename_buf == NULL) {
+			pr_err("Could not allocate file buffer");
+			filename = "UNKNOWN";
+		} else {
+			// Get Path of the file we are trying to write
+			filename = d_path(&file->f_path, filename_buf,
+					  BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+			if (IS_ERR(filename)) {
+				pr_err("Could not retrieve file name (%ld)",
+				       PTR_ERR(filename));
+				filename = "UNKNOWN";
+			}
+		}
+
+		// Ask the HOST whether we should block this attempt.
+		rv = bhv_block_read_only_file_write(filename, 0);
+
+		if (filename_buf != NULL) {
+			kfree(filename_buf);
+		}
+
+		return rv;
+	}
+
+	return 0;
+}
+
+/*
+ * Dirty cred detection
+ * When a write happens this checks whether the struct file object is opened for
+ * writing. If this is not the case notify and optionally stop the write
+ * attempt immediately.
+ */
+void bhv_check_file_dirty_cred(struct file *file, int mask)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+
+	if (!bhv_dirtycred_file_protection_is_enabled())
+		return;
+
+	// No need to continue if no write attempt is ongoing.
+	if (!(mask & MAY_WRITE))
+		return;
+
+	if (!(file->f_mode & FMODE_WRITE) ||
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		pathname_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+		pathname = get_pathname(file, pathname_buf,
+					BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+		pr_err("Write to read-only file \"%s\" detected!", pathname);
+		// At this point a struct file object was already placed illegitimately
+		// at the location of another. When we're be called from a LSM file
+		// permission hook, we could still bail out gracefully. However, as
+		// illegitimately placing struct file objects often happens after
+		// permission and LSM hooks, we need to check deep inside write attempts
+		// (e.g. in mm/filemap.c:generic_perform_write()). There, we do not have
+		// much choice as the attempts might have already corrupted the
+		// read-only file. Hence, on a block, we panic after having informed the
+		// HOST.
+		if (bhv_block_read_only_file_write(pathname, 1)) {
+			panic("possible dirtycred compromise detected");
+		}
+	}
+}
+
+static_vk int bhv_file_open(struct file *file)
+{
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	return 0;
+}
+
+static_vk int bhv_file_permission(struct file *file, int mask)
+{
+	struct pipe_inode_info *info;
+	struct pipe_buffer *buf;
+
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	/* check for dirty cred inside LSM hook (e.g. for aio_write()) */
+	bhv_check_file_dirty_cred(file, mask);
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return 0;
+
+	/*
+	 * Dirty pipe detection. Whenever we write to a file and this file is
+	 * a pipe, we are going to check whether this pipe points to a read-only
+	 * file. This check happens in `bhv_check_files_dirty_pipe` above.
+	 */
+	if ((mask & MAY_WRITE) == MAY_WRITE &&
+	    (info = get_pipe_info(file, false))) {
+		// Check current buffer in the pipe for dirty pipe
+		buf = &info->bufs[(info->head - 1) & (info->ring_size - 1)];
+		// Iterate over all open files and see whether the pipe points to the same file.
+		if (buf && buf->page && current->files &&
+		    virt_addr_valid(buf->page->mapping)) {
+			if (iterate_fd(current->files, 0,
+				       bhv_check_files_dirty_pipe,
+				       buf->page->mapping)) {
+				return -EACCES;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static_vk int bhv_cgroup_mkdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_create(cgrp);
+	return 0;
+}
+
+static_vk void bhv_cgroup_rmdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_destroy(cgrp);
+}
+
+static_vk int bhv_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+static_vk int bhv_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+static void bhv_module_loaded(struct module *mod)
+{
+	if (bhv_guestlog_log_driver_events()) {
+		bhv_guestlog_log_driver_load(mod->name);
+	}
+}
+
+static struct security_hook_list bhv_hooks[] __lsm_ro_after_init = {
+	LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+	LSM_HOOK_INIT(kernel_load_data, bhv_load_data), // init_module
+	LSM_HOOK_INIT(task_alloc, bhv_task_alloc), // fork
+	LSM_HOOK_INIT(task_free, bhv_task_free), // exit
+	LSM_HOOK_INIT(bprm_check_security, bhv_bprm_check_security), // execve
+	LSM_HOOK_INIT(file_permission, bhv_file_permission), // file read/write
+	LSM_HOOK_INIT(file_open, bhv_file_open), // file open
+	LSM_HOOK_INIT(module_loaded,
+		      bhv_module_loaded), // module has successfully loaded
+
+	/* Inode protection */
+	LSM_HOOK_INIT(bprm_creds_from_file, bhv_inode_bprm_creds_from_file),
+	LSM_HOOK_INIT(task_fix_setuid, bhv_inode_task_fix_setuid),
+	LSM_HOOK_INIT(task_fix_setgid, bhv_inode_task_fix_setgid),
+	LSM_HOOK_INIT(d_instantiate, bhv_inode_d_instantiate),
+
+	/* Container visibility */
+	LSM_HOOK_INIT(cgroup_mkdir, bhv_cgroup_mkdir),
+	LSM_HOOK_INIT(cgroup_rmdir, bhv_cgroup_rmdir),
+	LSM_HOOK_INIT(unshare, bhv_unshare), LSM_HOOK_INIT(setns, bhv_setns)
+
+};
+
+static int __init bhv_lsm_init(void)
+{
+	pr_info("[bhv] LSM active");
+	security_add_hooks(bhv_hooks, ARRAY_SIZE(bhv_hooks), "bhv");
+	return 0;
+}
+
+DEFINE_LSM(bhv) = {
+	.name = "bhv",
+	.init = bhv_lsm_init,
+};
+#endif // VASKM
diff --git security/bhv/memory_freeze.c security/bhv/memory_freeze.c
new file mode 100644
index 000000000..6999f0119
--- /dev/null
+++ security/bhv/memory_freeze.c
@@ -0,0 +1,91 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/printk.h>
+#include <linux/umh.h>
+
+#include <bhv/integrity.h>
+#include <bhv/guestlog.h>
+#include <bhv/interface/confserver.h>
+
+static int __maybe_unused bhv_memory_freeze_main(void *_)
+{
+	static const __section(".rodata") char *const argv[] = {
+		"/usr/bin/systemctl", "is-system-running", NULL
+	};
+	static const __section(".rodata") char *const envp[] = {
+		"HOME=/", "PATH=/sbin:/bin:/usr/sbin:/usr/bin", NULL
+	};
+	int ret;
+
+	while (true) {
+		msleep(500);
+
+		ret = call_usermodehelper(
+			argv[0], (char **)argv, (char **)envp,
+			UMH_WAIT_PROC /* wait for the process to complete */);
+
+		if (ret == -ENOENT) {
+			pr_info("%s: Non-systemd filesystem\n", __FUNCTION__);
+			msleep(120000);
+			pr_info("%s: Assuming system is up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+
+		if (ret == 0) {
+			pr_info("%s: System up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+	}
+}
+
+void bhv_memory_freeze_init(void)
+{
+	bhv_confserver_req_freeze_memory_after_boot_t arg;
+
+	if (!is_bhv_initialized())
+		return;
+
+	BHV_CONFSERVER_FREEZEMEMORY_HYP((void *)&arg);
+
+	if (arg.freeze_memory_after_boot) {
+#ifdef CONFIG_STATIC_USERMODEHELPER
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.");
+#endif
+#ifdef CONFIG_BPF_JIT
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_BPF_JIT.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_BPF_JIT.");
+#endif
+
+#ifndef NO_FREEZE
+		if (ERR_PTR(-ENOMEM) == kthread_run(bhv_memory_freeze_main,
+						    NULL, "memory_freeze")) {
+			panic("%s: Could not create memory_freeze thread",
+			      __FUNCTION__);
+		}
+#endif
+	}
+}
\ No newline at end of file
diff --git security/bhv/module.c security/bhv/module.c
new file mode 100644
index 000000000..858d3d9d2
--- /dev/null
+++ security/bhv/module.c
@@ -0,0 +1,624 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/bhv_print.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t, const char *);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags, label);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t type,
+				    uint64_t flags, const char *unused2)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3, const char *unused4)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags,
+				  label);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags, label);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags,
+			       const char *label)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, label,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	type &= ~HypABI__Integrity__MemFlags__MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, "INVALID",
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size,
+				HypABI__Integrity__MemType__UNKNOWN,
+				HypABI__Integrity__MemFlags__NONE, "INVALID",
+				_bhv_link_node_op_remove);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static void bhv_prepare_mod_init(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_INIT_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_INIT_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static void bhv_prepare_mod_core(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_TEXT].base,
+				   mod->mem[MOD_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_RODATA].base,
+				   mod->mem[MOD_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (mod->mem[MOD_RO_AFTER_INIT].size) {
+		bhv_create_section(head, mod->mem[MOD_RO_AFTER_INIT].base,
+				   mod->mem[MOD_RO_AFTER_INIT].size,
+				   HypABI__Integrity__MemType__DATA,
+				   base_flags |
+					   HypABI__Integrity__MemFlags__MUTABLE,
+				   "MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_DATA].base,
+				   mod->mem[MOD_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout,
+				   unsigned long base_flags)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   HypABI__Integrity__MemType__CODE_PATCHABLE,
+			   base_flags, "MODULE TEXT SECTION");
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			HypABI__Integrity__MemType__DATA,
+			base_flags | HypABI__Integrity__MemFlags__MUTABLE,
+			"MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static inline void bhv_prepare_mod_init(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout, base_flags);
+}
+
+static inline void bhv_prepare_mod_core(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->core_layout, base_flags);
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_init(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+
+#ifndef VASKM // inside kernel tree
+	bhv_prepare_mod_core(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	/* Prepare the module region's .text section. */
+	if (mod->mem[MOD_INIT_TEXT].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size);
+
+	/* Prepare the module region's .rodata section. */
+	if (mod->mem[MOD_INIT_RODATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size);
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size);
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	const struct module_layout *layout = &mod->init_layout;
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_complete_free_init(const struct module *mod)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	_bhv_complete_free_init(mod, &bhv_region_list_head);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod,
+				     unsigned long base_flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+	void *base = mod->mem[MOD_RO_AFTER_INIT].base;
+	unsigned int size = mod->mem[MOD_RO_AFTER_INIT].size;
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   HypABI__Integrity__MemType__DATA_READ_ONLY,
+			   base_flags);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region.update);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#ifndef VASKM // inside kernel tree
+	bhv_update_ro_after_init(mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_update_ro_after_init(mod,
+					 HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_update_ro_after_init(
+			mod, HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+	bhv_complete_free_init(mod);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags,
+			   "BPF SECTION");
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__CODE_PATCHABLE,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_add_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT,
+			       ((size + PAGE_SIZE - 1) >> PAGE_SHIFT));
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_rm_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+	bhv_release_memory_by_owner((uint64_t)base);
+}
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+	BUG_ON(owner == 0);
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	bhv_create_section(&bhv_region_list_head, base, size, type, flags,
+			   description);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+#endif //VASKM
diff --git security/bhv/patch_alternative.c security/bhv/patch_alternative.c
new file mode 100644
index 000000000..184bf8ebb
--- /dev/null
+++ security/bhv/patch_alternative.c
@@ -0,0 +1,207 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/kversion.h>
+
+#include <asm/bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+/**************************************************/
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = HypABI__Vault__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		pr_err("BHV: %s: Unknown module!\n", __FUNCTION__);
+		rv = -EACCES;
+		goto out;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+out:
+	// Close vault.
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(&search, arch,
+					  bhv_alternatives_find_by_alt);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) */
diff --git security/bhv/patch_bpf.c security/bhv/patch_bpf.c
new file mode 100644
index 000000000..a38ab5ec0
--- /dev/null
+++ security/bhv/patch_bpf.c
@@ -0,0 +1,196 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com> 
+ */
+
+void init_xmem(void);
+
+#include <linux/version.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+struct bhv_bpf_code_range {
+	uint64_t start_pfn;
+	size_t num_pages;
+	struct list_head next;
+};
+
+static DEFINE_MUTEX(bhv_bpf_mutex);
+static LIST_HEAD(bhv_bpf_code_ranges_head);
+
+static void __always_inline bhv_bpf_lock(void)
+{
+	mutex_lock(&bhv_bpf_mutex);
+}
+
+static void __always_inline bhv_bpf_unlock(void)
+{
+	mutex_unlock(&bhv_bpf_mutex);
+}
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages)
+{
+	struct bhv_bpf_code_range *n =
+		kzalloc(sizeof(struct bhv_bpf_code_range), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->start_pfn = pfn;
+	n->num_pages = num_pages;
+
+	bhv_bpf_lock();
+	list_add(&(n->next), &bhv_bpf_code_ranges_head);
+	bhv_bpf_unlock();
+}
+
+void bhv_rm_bpf_code_range(uint64_t pfn)
+{
+	struct bhv_bpf_code_range *i, *tmp;
+
+	bhv_bpf_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_bpf_code_ranges_head, next) {
+		if (i->start_pfn == pfn) {
+			list_del(&(i->next));
+			kfree(i);
+			break;
+		}
+	}
+	bhv_bpf_unlock();
+}
+
+static bool __bhv_text bhv_bpf_check_write(void *dst, size_t sz)
+{
+	struct bhv_bpf_code_range *i;
+	uint64_t start_pfn = ((uint64_t)dst) >> PAGE_SHIFT;
+	size_t num_pages = ((sz + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	uint64_t end_pfn = (start_pfn + num_pages) - 1;
+
+	list_for_each_entry(i, &bhv_bpf_code_ranges_head, next) {
+		if (start_pfn >= i->start_pfn &&
+		    start_pfn < (i->start_pfn + i->num_pages)) {
+			if (end_pfn >= i->start_pfn &&
+			    end_pfn < (i->start_pfn + i->num_pages))
+				return true;
+		}
+	}
+
+	return false;
+}
+
+static int __bhv_text bhv_bpf_write_vault(void *dst, void *src, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+	rv = HypABI__Vault__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	rv = bhv_patch_hypercall(dst, src, sz, false);
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+	return rv;
+}
+
+static int __bhv_text bhv_bpf_invalidate_vault(void *dst, uint8_t b, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+	rv = HypABI__Vault__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change
+	}
+
+	rv = bhv_patch_hypercall_memset(dst, sz, b, false);
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+	if (HypABI__Vault__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+	return rv;
+}
+
+int bhv_bpf_write(void *dst, void *src, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_write_vault(dst, src, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_invalidate_vault(dst, b, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
\ No newline at end of file
diff --git security/bhv/patch_jump_label.c security/bhv/patch_jump_label.c
new file mode 100644
index 000000000..9b9952383
--- /dev/null
+++ security/bhv/patch_jump_label.c
@@ -0,0 +1,299 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static void __always_inline bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static void __always_inline bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+enum jump_label_type __bhv_text bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+typedef enum {
+	FAIL = 0,
+	SKIP,
+	SUCCESS,
+} jump_label_validation_t;
+
+jump_label_validation_t __bhv_text validate_jmp_labels(struct jump_entry *entry,
+						       const void *opcode,
+						       size_t len,
+						       char *error_msg)
+{
+	struct bhv_static_key_mod *i;
+	unsigned long addr = (unsigned long)jump_entry_code(entry);
+	struct jump_entry *iter;
+	unsigned long tmp_addr;
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		// We should only modify the kernel text section
+		if (!kernel_text_address(addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				addr);
+			return FAIL;
+		}
+
+		if (!bhv_jump_label_validate_opcode(
+			    entry, bhv_jump_label_type(entry), opcode, len)) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Jump label opcode validation failed");
+			return FAIL;
+		}
+
+		// Search for overlapping jump labels.
+		for (iter = KLN_SYMBOL(struct jump_entry *,
+				       __start___jump_table);
+		     iter <
+		     KLN_SYMBOL(struct jump_entry *, __stop___jump_table);
+		     iter++) {
+			if (iter == entry)
+				continue;
+
+			tmp_addr = (unsigned long)jump_entry_code(iter);
+
+			if (addr <= tmp_addr && tmp_addr < addr + len) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label (0x%lx) overlaps with jump label (0x%lx)",
+					(unsigned long)(entry),
+					(unsigned long)(iter));
+				return FAIL;
+			}
+		}
+
+		return SUCCESS;
+	}
+
+	list_for_each_entry(i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			struct module *tmp;
+
+			if (i->mod->state != MODULE_STATE_COMING &&
+			    i->mod->state != MODULE_STATE_LIVE) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Invalid module state for jump label (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			if ((jump_entry_is_init(entry) &&
+			     i->mod->state != MODULE_STATE_COMING)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Trying to apply jump label in incorrect module state (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			// Module entries should only point to the text section
+			// of the module
+			// Note: We use the kernel API to check this. We could perform
+			// the check manually but using the kernel API seems more stable.
+			tmp = __module_text_address(addr);
+			if (tmp == NULL) {
+				// No module found
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label trying to write address outside of module (0x%lx)",
+					addr);
+				return FAIL;
+			}
+
+			if (tmp != i->mod) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label (0x%lx<->0x%lx)",
+					(unsigned long)tmp,
+					(unsigned long)i->mod);
+				return FAIL;
+			}
+
+			if (!bhv_jump_label_validate_opcode(
+				    entry, bhv_jump_label_type(entry), opcode,
+				    len)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label");
+				return FAIL;
+			}
+
+			// Search for overlapping jump labels.
+			for (iter = i->entries_start; iter < i->entries_stop;
+			     iter++) {
+				if (iter == entry)
+					continue;
+
+				tmp_addr = (unsigned long)jump_entry_code(iter);
+
+				if (addr <= tmp_addr && tmp_addr < addr + len) {
+					snprintf(
+						error_msg,
+						HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+						"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+						(unsigned long)entry,
+						(unsigned long)iter);
+					return FAIL;
+				}
+			}
+
+			return SUCCESS;
+		}
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+	return FAIL;
+}
+
+int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+					  const void *opcode, size_t len)
+{
+	int rv = 0;
+	unsigned long r;
+	jump_label_validation_t validation_ok;
+	void *dest_virt_addr = (void *)jump_entry_code(entry);
+	char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+
+	rv = HypABI__Vault__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (HypABI__Vault__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return -E2BIG;
+	}
+
+	validation_ok = validate_jmp_labels(entry, opcode, len, message);
+	if (validation_ok == SKIP) {
+		if (HypABI__Vault__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return 0;
+	}
+
+	if (validation_ok == FAIL) {
+		if (bhv_patch_violation_hypercall(dest_virt_addr, message)) {
+			// Block this patch
+			if (HypABI__Vault__Close__hypercall())
+				pr_err("%s: could not close vault",
+				       __FUNCTION__);
+			return -EACCES;
+		}
+
+		// The violation should only be logged. Thus we continue.
+	}
+
+	r = bhv_patch_hypercall(dest_virt_addr, opcode, (uint64_t)len, true);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+	return rv;
+}
diff --git security/bhv/reg_protect.c security/bhv/reg_protect.c
new file mode 100644
index 000000000..6c11be74a
--- /dev/null
+++ security/bhv/reg_protect.c
@@ -0,0 +1,44 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/reg_protect.h>
+
+int bhv_reg_protect_freeze(uint64_t reg_selector, uint64_t freeze_bitfield)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_reg_protect_t *bhv_arg =
+		kmalloc(sizeof(bhv_reg_protect_t), GFP_KERNEL);
+	BUG_ON(!bhv_arg);
+
+	bhv_arg->bhv_reg_protect_freeze.register_selector = reg_selector;
+	bhv_arg->bhv_reg_protect_freeze.freeze_bitfield = freeze_bitfield;
+
+	printk(KERN_INFO "%s: Sending %16phN\n", __FUNCTION__, bhv_arg);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_REGISTER_PROTECTION,
+			      BHV_VAS_REGPROTECT_OP_FREEEZE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kfree(bhv_arg);
+	return rv;
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void)
+{
+	bhv_start_reg_protect_arch();
+}
+/***************************************************/
diff --git security/bhv/sysfs.c security/bhv/sysfs.c
new file mode 100644
index 000000000..066682a7f
--- /dev/null
+++ security/bhv/sysfs.c
@@ -0,0 +1,67 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef CONFIG_SYSFS
+#error CONFIG_SYSFS required!
+#endif
+
+#include <linux/kobject.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/sysfs_reg_protect.h>
+#include <bhv/sysfs_version.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+
+/**********************************************************
+ * start
+ **********************************************************/
+void __init_km bhv_start_sysfs(void)
+{
+	struct kobject *bhv_kobj;
+	struct kobject *version_kobj;
+	struct kobject *integrity_kobj;
+	struct kobject *integrity_freeze_kobj;
+	struct kobject *register_kobj;
+	struct kobject *register_freeze_kobj;
+	struct kobject *fops_kobj;
+	struct kobject *fopsstatus_kobj;
+
+#define CREATE_KOBJ(kobj, name, parent)                                        \
+	kobj = kobject_create_and_add(name, parent);                           \
+	BUG_ON(!kobj);
+
+#ifndef CONFIG_SYS_HYPERVISOR
+	struct kobject *hypervisor_kobj;
+	CREATE_KOBJ(hypervisor_kobj, "hypervisor", NULL);
+#endif // CONFIG_SYS_HYPERVISOR
+
+	CREATE_KOBJ(bhv_kobj, "bhv", hypervisor_kobj);
+	
+	CREATE_KOBJ(version_kobj, "version", bhv_kobj);
+	bhv_start_sysfs_version(version_kobj);
+
+	CREATE_KOBJ(integrity_kobj, "integrity", bhv_kobj);
+	CREATE_KOBJ(integrity_freeze_kobj, "freeze", integrity_kobj);
+	bhv_start_sysfs_integrity_freeze(integrity_freeze_kobj);
+
+	if (bhv_reg_protect_is_enabled()) {
+		CREATE_KOBJ(register_kobj, "register", bhv_kobj);
+		CREATE_KOBJ(register_freeze_kobj, "freeze", register_kobj);
+
+		bhv_start_sysfs_reg_protect(register_freeze_kobj);
+	}
+
+	if (bhv_fileops_file_protection_is_enabled()) {
+		CREATE_KOBJ(fops_kobj, "fileops_protection", bhv_kobj);
+		CREATE_KOBJ(fopsstatus_kobj, "status", fops_kobj);
+
+		bhv_start_sysfs_fileops_protection(fops_kobj, fopsstatus_kobj);
+	}
+}
+/**********************************************************/
diff --git security/bhv/sysfs_fops.c security/bhv/sysfs_fops.c
new file mode 100644
index 000000000..cc006d738
--- /dev/null
+++ security/bhv/sysfs_fops.c
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kobject.h>
+#include <linux/printk.h>
+#include <asm-generic/set_memory.h>
+
+#include <bhv/bhv.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/module.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf);
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf);
+
+struct bhv_fopsstatus_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint8_t param;
+} static const __section(".rodata") _bhv_fopsstatus_sysfs_data[] = {
+#define FOPS_MAP(name, idx, _, __)                                             \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected,     \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FOPS_MAP_DIRNULL(name, idx, _)                                         \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected_dn,  \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FILEOPS_INTERNAL_FOPSMAP_ALL
+#include <bhv/fileops_internal_fopsmap.h>
+};
+#define attr_to_bsd(ptr)                                                       \
+	container_of((ptr), struct bhv_fopsstatus_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c %c\n",
+			 fileops_map[data->param][0] ? '1' : '0',
+			 fileops_map[data->param][1] ? '1' : '0');
+}
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n",
+			 fileops_map[data->param][0] ? '1' : '0');
+}
+#undef attr_to_bsd
+
+#ifdef VASKM // out of tree
+
+bool bhv_allow_update_fileops_map = true;
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf);
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count);
+
+static char _bhv_fopsfreeze_flag;
+struct bhv_fopsfreeze_sysfs_data {
+	const struct kobj_attribute attr;
+} static _bhv_fopsfreeze_sysfs_data[] = {
+	{
+		.attr = __ATTR(freeze, 0644, _bhv_intfr_sysfs_show_freeze,
+			       _bhv_fops_sysfs_store_freeze),
+	},
+};
+
+int bhv_freeze_fops_map(void)
+{
+	int rc;
+	bhv_allow_update_fileops_map = false;
+	rc = set_memory_ro((unsigned long)fileops_map, 1);
+	if (rc)
+		return rc;
+	bhv_protect_generic_memory((unsigned long)THIS_MODULE, fileops_map,
+				   PAGE_SIZE,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "FILEOPS_MAP");
+	_bhv_fopsfreeze_flag = '1';
+	return 0;
+}
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%c\n", _bhv_fopsfreeze_flag);
+}
+
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1')) {
+		if (_bhv_fopsfreeze_flag == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((_bhv_fopsfreeze_flag == '0') && (buf[0] == '1')) {
+			int ret = bhv_freeze_fops_map();
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_fileops_protection(struct kobject *fops,
+						  struct kobject *status)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsstatus_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsstatus_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(status, attr_array));
+
+#ifdef VASKM // out of tree
+	static_assert(ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) >=
+		      ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data));
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsfreeze_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(fops, attr_array));
+#endif // VASKM
+}
+/***************************************************/
diff --git security/bhv/sysfs_integrity_freeze.c security/bhv/sysfs_integrity_freeze.c
new file mode 100644
index 000000000..fbb72aacf
--- /dev/null
+++ security/bhv/sysfs_integrity_freeze.c
@@ -0,0 +1,102 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count);
+
+struct bhv_intfr_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t param;
+	const bool *const flagp;
+} static const __section(".rodata") _bhv_intfr_sysfs_data[] = {
+	{ .attr = __ATTR(create, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__CREATE,
+	  .flagp = &bhv_integrity_freeze_create_currently_frozen },
+	{ .attr = __ATTR(update, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__UPDATE,
+	  .flagp = &bhv_integrity_freeze_update_currently_frozen },
+	{ .attr = __ATTR(remove, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+	  .flagp = &bhv_integrity_freeze_remove_currently_frozen },
+	{ .attr = __ATTR(patch, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__PATCH,
+	  .flagp = &bhv_integrity_freeze_patch_currently_frozen },
+};
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_intfr_sysfs_data, attr)
+#define cur_flag_val(bisdp) (*((bisdp)->flagp) ? '1' : '0')
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n", cur_flag_val(data));
+}
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1' || buf[0] == '2')) {
+		struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+		if (cur_flag_val(data) == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((cur_flag_val(data) == '0') &&
+			   (buf[0] == '1' || buf[0] == '2')) {
+			int ret = bhv_enable_integrity_freeze_flag(
+				data->param, buf[0] == '2');
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_integrity_freeze(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_intfr_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_intfr_sysfs_data); ++i)
+		attr_array[i] = &_bhv_intfr_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_reg_protect.c security/bhv/sysfs_reg_protect.c
new file mode 100644
index 000000000..d031d8736
--- /dev/null
+++ security/bhv/sysfs_reg_protect.c
@@ -0,0 +1,106 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/reg_protect.h>
+#include <bhv/interface/reg_protect.h>
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count);
+
+struct bhv_rp_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t reg;
+} static const __section(".rodata") _bhv_rp_sysfs_data[] = {
+#define Q(regn)                                                                \
+	{ .attr = __ATTR(regn, 0640, _bhv_rp_sysfs_show, _bhv_rp_sysfs_store), \
+	  .reg = BHV_REG_PROTECT_REG_##regn },
+	BHV_FREEZABLE_REGISTERS
+#undef Q
+};
+
+struct bhv_rp_sysfs_data_var {
+	uint64_t curr_status;
+} static _bhv_rp_sysfs_data_var[] = {
+#define Q(regn) { .curr_status = 0UL },
+	BHV_FREEZABLE_REGISTERS
+#undef Q
+};
+
+static_assert(ARRAY_SIZE(_bhv_rp_sysfs_data) ==
+	      ARRAY_SIZE(_bhv_rp_sysfs_data_var));
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_rp_sysfs_data, attr)
+#define attr_to_bsd_var(ptr)                                                   \
+	_bhv_rp_sysfs_data_var + (attr_to_bsd(ptr) - _bhv_rp_sysfs_data)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static DEFINE_MUTEX(_bhv_rp_sysfs_lock);
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int b;
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	b = scnprintf(buf, PAGE_SIZE, "%016llx\n", data_var->curr_status);
+        mutex_unlock(&_bhv_rp_sysfs_lock);
+	return b;
+}
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	uint64_t nval;
+	int rv = 0;
+	int i = sscanf(buf, "%llx", &nval);
+	if (i != 1) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	nval |= data_var->curr_status;
+
+	rv = bhv_reg_protect_freeze(data->reg, nval);
+
+	if (!rv)
+		data_var->curr_status = nval;
+        mutex_unlock(&_bhv_rp_sysfs_lock);
+
+	if (rv < 0) {
+		return rv;
+	} else {
+		return count;
+	}
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_reg_protect(struct kobject *kobj)
+{
+	int i;
+	const struct attribute *attr_array[BHV_NUM_FREEZABLE_REGISTERS + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_rp_sysfs_data); ++i)
+		attr_array[i] = &_bhv_rp_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_version.c security/bhv/sysfs_version.c
new file mode 100644
index 000000000..ea76ca1ae
--- /dev/null
+++ security/bhv/sysfs_version.c
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_version.h>
+
+#define __BHV_VERSION(a, b, c) a, b, c
+#define __BHV_VAS_ABI_VERSION(a, b, c, d) a, b, c, d
+#include <bhv/version.h>
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf);
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf);
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#endif // VASKM
+
+
+struct bhv_version_sysfs_data {
+	const struct kobj_attribute attr;
+} static const __section(".rodata") _bhv_version_sysfs_data[] = {
+
+	{
+		.attr = __ATTR(bhv_version, 0600,
+			       _bhv_version_sysfs_show_bhv_version, NULL),
+	},
+	{
+		.attr = __ATTR(bhv_abi_version, 0600,
+			       _bhv_version_sysfs_show_bhv_abi_version, NULL),
+	},
+#ifndef VASKM // inside kernel tree
+	{
+		.attr = __ATTR(linux_commit, 0600,
+			       _bhv_version_sysfs_show_linux_commit, NULL),
+	},
+#else // out of tree
+	{
+		.attr = __ATTR(vaskm_commit, 0600,
+			       _bhv_version_sysfs_show_vaskm_commit, NULL),
+	},
+#endif // VASKM
+};
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02d.%02d.%d\n", BHV_VERSION);
+}
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02u.%02u.%02u:%05u\n", HypABI__ABI_VERSION);
+}
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", KERNEL_COMMIT_HASH);
+}
+
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", VASKM_COMMIT_HASH);
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_version(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_version_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_version_sysfs_data); ++i)
+		attr_array[i] = &_bhv_version_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/vmalloc_to_page.c security/bhv/vmalloc_to_page.c
new file mode 100644
index 000000000..a83c30219
--- /dev/null
+++ security/bhv/vmalloc_to_page.c
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copied from kernel v6.1, mm/vmalloc.c
+/*
+ *  Copyright (C) 1993  Linus Torvalds
+ *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
+ *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
+ *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002
+ *  Numa awareness, Christoph Lameter, SGI, June 2005
+ *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+
+#ifdef VASKM // out of tree
+#include <kln.h>
+#undef pgd_offset_k
+#define pgd_offset_k(address)                                                  \
+	pgd_offset(KLN_SYMBOL(struct mm_struct *, init_mm), (address))
+#endif // VASKM
+
+
+
+/*
+ * Walk a vmap address to the struct page it maps. Huge vmap mappings will
+ * return the tail page that corresponds to the base page address, which
+ * matches small vmap mappings.
+ */
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr)
+{
+	unsigned long addr = (unsigned long) vmalloc_addr;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	/*
+	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
+	 * architectures that do not vmalloc module space
+	 */
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (pgd_none(*pgd))
+		return NULL;
+	if (WARN_ON_ONCE(pgd_leaf(*pgd)))
+		return NULL; /* XXX: no allowance for huge pgd */
+	if (WARN_ON_ONCE(pgd_bad(*pgd)))
+		return NULL;
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return NULL;
+	if (p4d_leaf(*p4d))
+		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(p4d_bad(*p4d)))
+		return NULL;
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return NULL;
+	if (pud_leaf(*pud))
+		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return NULL;
+	if (pmd_leaf(*pmd))
+		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pmd_bad(*pmd)))
+		return NULL;
+
+	ptep = pte_offset_map(pmd, addr);
+	pte = *ptep;
+	if (pte_present(pte))
+		page = pte_page(pte);
+	pte_unmap(ptep);
+
+	return page;
+}
\ No newline at end of file
diff --git security/integrity/digsig.c security/integrity/digsig.c
index de442af7b..f141fc961 100644
--- security/integrity/digsig.c
+++ security/integrity/digsig.c
@@ -17,6 +17,8 @@
 #include <crypto/public_key.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
+
 #include "integrity.h"
 
 static struct key *keyring[INTEGRITY_KEYRING_MAX];
@@ -44,6 +46,7 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 		return ERR_PTR(-EINVAL);
 
 	if (!keyring[id]) {
+		int rc = 0;
 		keyring[id] =
 			request_key(&key_type_keyring, keyring_name[id], NULL);
 		if (IS_ERR(keyring[id])) {
@@ -52,6 +55,19 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 			keyring[id] = NULL;
 			return ERR_PTR(err);
 		}
+
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets registered
+		 * directly in set_platform_trusted_keys.
+		 */
+		rc = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
+
+	} else {
+		int rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
 	}
 
 	return keyring[id];
@@ -109,6 +125,19 @@ static int __init __integrity_init_keyring(const unsigned int id,
 			keyring_name[id], err);
 		keyring[id] = NULL;
 	} else {
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets also registered
+		 * in set_platform_trusted_keys to allow passing the keyring
+		 * directly and indirectly (through INTEGRITY_KEYRING_PLATFORM)
+		 * to verify_pkcs7_message_sig.
+		 */
+		err = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (err) {
+			key_put(keyring[id]);
+			keyring[id] = NULL;
+			return err;
+		}
+
 		if (id == INTEGRITY_KEYRING_PLATFORM)
 			set_platform_trusted_keys(keyring[id]);
 	}
@@ -156,6 +185,10 @@ int __init integrity_add_key(const unsigned int id, const void *data,
 	if (!keyring[id])
 		return -EINVAL;
 
+	rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+	if (rc)
+		return rc;
+
 	key = key_create_or_update(make_key_ref(keyring[id], 1), "asymmetric",
 				   NULL, data, size, perm,
 				   KEY_ALLOC_NOT_IN_QUOTA);
diff --git security/integrity/ima/ima_mok.c security/integrity/ima/ima_mok.c
index 95cc31525..87c7d9703 100644
--- security/integrity/ima/ima_mok.c
+++ security/integrity/ima/ima_mok.c
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
 
 struct key *ima_blacklist_keyring;
 
@@ -44,6 +45,10 @@ static __init int ima_mok_init(void)
 
 	if (IS_ERR(ima_blacklist_keyring))
 		panic("Can't allocate IMA blacklist keyring.");
+	else
+		if (bhv_keyring_register_system_keyring(&ima_blacklist_keyring))
+			panic("Can't register IMA blacklist keyring.");
+
 	return 0;
 }
 device_initcall(ima_mok_init);
diff --git security/security.c security/security.c
index 0bbcb100b..a028c59f1 100644
--- security/security.c
+++ security/security.c
@@ -2629,3 +2629,28 @@ int security_perf_event_write(struct perf_event *event)
 	return call_int_hook(perf_event_write, 0, event);
 }
 #endif /* CONFIG_PERF_EVENTS */
+
+void security_module_loaded(struct module *mod)
+{
+	call_void_hook(module_loaded, mod);
+}
+
+int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(unshare, 0, tsk, nsset);
+}
+
+int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(setns, 0, tsk, nsset);
+}
+
+int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return call_int_hook(cgroup_mkdir, 0, cgrp);
+}
+
+void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+	call_void_hook(cgroup_rmdir, cgrp);
+}
diff --git security/selinux/hooks.c security/selinux/hooks.c
index 46c00a68b..dfe4ce793 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -125,7 +125,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index d893c2280..23e42ceca 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -611,9 +615,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
 	struct selinux_load_state load_state;
@@ -667,6 +671,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2246,6 +2276,40 @@ static int __init init_sel_fs(void)
 
 __initcall(init_sel_fs);
 
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinuxfs_mount->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&fsi->state->policy_mutex);
+
+	rv = security_load_policy(fsi->state, data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(fsi->state, &load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(fsi->state, &load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&fsi->state->policy_mutex);
+	return rv;
+}
+#endif
+
 #ifdef CONFIG_SECURITY_SELINUX_DISABLE
 void exit_sel_fs(void)
 {
