diff --git arch/Kconfig arch/Kconfig
index bd9f095d69..230f17dff0 100644
--- arch/Kconfig
+++ arch/Kconfig
@@ -70,6 +70,7 @@ config GENERIC_ENTRY
 config KPROBES
 	bool "Kprobes"
 	depends on HAVE_KPROBES
+	depends on !BHV_LOCKDOWN
 	select KALLSYMS
 	select EXECMEM
 	select NEED_TASKS_RCU
diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index 5bfbf7d79c..d21e14e59a 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,6 +3,7 @@ obj-y			+= kernel/ mm/ net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
 obj-$(subst m,y,$(CONFIG_HYPERV))	+= hyperv/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
 
 # for cleaning
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 0000000000..38bc8ae73a
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,17 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+ccflags-y+=-Werror
+
+obj-$(CONFIG_BHV_VAS)		:= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/arm64/bhv/init/init.c arch/arm64/bhv/init/init.c
new file mode 100644
index 0000000000..cb2a09ef88
--- /dev/null
+++ arch/arm64/bhv/init/init.c
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+#include <asm/sysreg.h>
+
+#include <bhv/integrity.h>
+#include <bhv/init/init.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
+
+bool __init bhv_init_arch(void)
+{
+
+#define MRS(__id, __val) asm("mrs %0, "#__id : "=r" (__val))
+    long reg_val;
+    MRS(ID_AA64AFR0_EL1, reg_val);
+    if (!memcmp(&reg_val, "BHV.", 4)) {
+        printk(KERN_INFO "Running on BHV");
+        return true;
+    }
+
+    return false;
+
+}
diff --git arch/arm64/bhv/init/start.c arch/arm64/bhv/init/start.c
new file mode 100644
index 0000000000..6bad1f4aea
--- /dev/null
+++ arch/arm64/bhv/init/start.c
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/integrity.h>
+#include <bhv/init/late_start.h>
+#include <bhv/init/start.h>
+
+int bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
+
+int bhv_late_start_arch(void)
+{
+	return bhv_late_start_integrity_arch();
+}
\ No newline at end of file
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 0000000000..d50b974fab
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,128 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+extern char vdso_start[], vdso_end[];
+
+/************************************************************
+ * start until 6.11, late_start from 6.12
+ ************************************************************/
+static int bhv_start_integrity_add_ro(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 3
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON((unsigned long)vdso_start < (unsigned long)__start_rodata ||
+	       (unsigned long)vdso_start >= (unsigned long)__end_rodata);
+	BUG_ON((unsigned long)vdso_end < (unsigned long)__start_rodata ||
+	       (unsigned long)vdso_end >= (unsigned long)__end_rodata);
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 */
+	bhv_mem_region_create_ctor(
+		&n[0]->region, NULL,
+		bhv_virt_to_phys_single((void *)__start_rodata),
+		(unsigned long)vdso_start - (unsigned long)__start_rodata,
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[1]->region, &n[0]->region,
+		bhv_virt_to_phys_single((void *)vdso_end),
+		(unsigned long)__end_rodata - (unsigned long)vdso_end,
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[2]->region, &n[1]->region,
+		bhv_virt_to_phys_single((void *)vdso_start),
+		(unsigned long)vdso_end - (unsigned long)vdso_start,
+		HypABI__Integrity__MemType__VDSO,
+		HypABI__Integrity__MemFlags__NONE, "KERNEL VDSO SECTION");
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+#undef NUM_BHV_MEM_REGION_NODES
+}
+/************************************************************/
+
+/************************************************************
+ * start
+ ************************************************************/
+void bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	*pgd_offset = 0;
+	*pgd_value = 0;
+}
+
+int bhv_start_integrity_arch(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 11, 0)
+	return 0;
+#else // LINUX_VERSION_CODE < 6.11
+	return bhv_start_integrity_add_ro();
+#endif // LINUX_VERSION_CODE <> 6.11
+}
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	init_ptpg_arg->init_pgd = 0;
+	init_ptpg_arg->pt_levels = 0;
+	init_ptpg_arg->num_ranges = 0;
+}
+
+int bhv_late_start_integrity_arch(void) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 11, 0)
+	return bhv_start_integrity_add_ro();
+#else // LINUX_VERSION_CODE < 6.11
+	return 0;
+#endif // LINUX_VERSION_CODE <> 6.11
+}
+/************************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	return true;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 0000000000..158c6a02bd
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,388 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+#include <asm/bhv/patch.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#include <asm/cacheflush.h>
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_patch_hypercall((void *)&updptr[i], (uint8_t *)&insn,
+				    sizeof(insn), false);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+#define ALT_CAP(a) ((a)->cpufeature & ~ARM64_CB_BIT)
+#define ALT_HAS_CB(a) ((a)->cpufeature & ARM64_CB_BIT)
+
+extern DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(
+			 ctr_el0, CTR_EL0_DminLine_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+		int cap = ALT_CAP(alt);
+
+		if (!test_bit(cap, feature_mask))
+			continue;
+
+		if (!cpus_have_cap(cap))
+			continue;
+
+		if (ALT_HAS_CB(alt))
+			BUG_ON(alt->alt_len != 0);
+		else
+			BUG_ON(alt->alt_len != alt->orig_len);
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (ALT_HAS_CB(alt)) {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		} else {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	/*
+	 * The core module code takes care of cache maintenance in
+	 * flush_module_icache().
+	 */
+	if (!mod->arch.is_module) {
+		dsb(ish);
+		icache_inval_all_pou();
+		isb();
+
+		/* Ignore ARM64_CB bit from feature mask */
+		bitmap_or(applied_alternatives, applied_alternatives,
+			  feature_mask, ARM64_NCAPS);
+		bitmap_and(applied_alternatives, applied_alternatives,
+			   cpu_hwcaps, ARM64_NCAPS);
+	}
+
+	return 0;
+}
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 0000000000..318a8a6033
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+__always_inline u32 bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/reg_protect.c arch/arm64/bhv/reg_protect.c
new file mode 100644
index 0000000000..10233d22aa
--- /dev/null
+++ arch/arm64/bhv/reg_protect.c
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+}
+/***************************************************/
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index 00d97b8a75..23eafe6ea7 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -10,6 +10,8 @@
 #include <linux/types.h>
 #include <linux/stddef.h>
 
+#include <bhv/interface/abi_base_autogen.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
diff --git arch/arm64/include/asm/bhv/domain.h arch/arm64/include/asm/bhv/domain.h
new file mode 100644
index 0000000000..ae5486a479
--- /dev/null
+++ arch/arm64/include/asm/bhv/domain.h
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __ASM_BRS_DOMAIN_H__
+#define __ASM_BRS_DOMAIN_H__
+
+#ifdef CONFIG_BRS_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define brs_domain_arch_get_user_pgd(pgd) pgd
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_RDONLY);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pte_read(pmd_pte(pmd));
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pte_read(pud_pte(pud));
+}
+
+static inline bool pte_exec(pte_t pte)
+{
+	return !!(pte_val(pte) & (PTE_PXN | PTE_UXN));
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return pte_exec(pmd_pte(pmd));
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return pte_exec(pud_pte(pud));
+}
+
+static inline bool pmd_large(pmd_t pmd)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 10, 0)
+	return pmd_thp_or_huge(pmd);
+#else // LINUX_VERSION_CODE >= 6.10
+	return pmd_leaf(pmd);
+#endif // LINUX_VERSION_CODE <> 6.10
+}
+
+static inline bool pud_large(pud_t pud)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 10, 0)
+	return pud_huge(pud);
+#else // LINUX_VERSION_CODE >= 6.10
+	return pud_leaf(pud);
+#endif // LINUX_VERSION_CODE <> 6.10
+}
+
+static inline bool brs_domain_is_user_pte(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_USER);
+}
+
+static inline bool brs_domain_is_user_pmd(pmd_t pmd)
+{
+	return brs_domain_is_user_pte(pmd_pte(pmd));
+}
+
+static inline bool brs_domain_is_user_pud(pud_t pud)
+{
+	return brs_domain_is_user_pte(pud_pte(pud));
+}
+
+#endif
+
+#endif /* __ASM_BRS_DOMAIN_H__ */
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 0000000000..7c9d75eb2f
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 0000000000..779ea118ce
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,44 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#else /* !CONFIG_BHV_VAS */
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 66d93e320e..078918bc3f 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -100,6 +100,10 @@ alternative_cb_end
 #include <asm/kvm_host.h>
 #include <asm/kvm_nested.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
 void kvm_compute_layout(void);
diff --git arch/arm64/include/asm/runtime-const.h arch/arm64/include/asm/runtime-const.h
index be5915669d..df378367e9 100644
--- arch/arm64/include/asm/runtime-const.h
+++ arch/arm64/include/asm/runtime-const.h
@@ -7,6 +7,8 @@
 /* Sigh. You can still run arm64 in BE mode */
 #include <asm/byteorder.h>
 
+#include <bhv/patch_base.h>
+
 #define runtime_const_ptr(sym) ({				\
 	typeof(sym) __ret;					\
 	asm_inline("1:\t"					\
@@ -41,12 +43,13 @@
 } while (0)
 
 /* 16-bit immediate for wide move (movz and movk) in bits 5..20 */
-static inline void __runtime_fixup_16(__le32 *p, unsigned int val)
+static __always_inline void __runtime_fixup_16(__le32 *p, unsigned int val)
 {
 	u32 insn = le32_to_cpu(*p);
 	insn &= 0xffe0001f;
 	insn |= (val & 0xffff) << 5;
-	*p = cpu_to_le32(insn);
+	__le32 insn_le = cpu_to_le32(insn);
+	bhv_patch(p, &insn_le, sizeof(__le32));
 }
 
 static inline void __runtime_fixup_caches(void *where, unsigned int insns)
@@ -55,7 +58,7 @@ static inline void __runtime_fixup_caches(void *where, unsigned int insns)
 	caches_clean_inval_pou(va, va + 4*insns);
 }
 
-static inline void __runtime_fixup_ptr(void *where, unsigned long val)
+static __always_inline void __runtime_fixup_ptr(void *where, unsigned long val)
 {
 	__le32 *p = lm_alias(where);
 	__runtime_fixup_16(p, val);
@@ -66,18 +69,21 @@ static inline void __runtime_fixup_ptr(void *where, unsigned long val)
 }
 
 /* Immediate value is 6 bits starting at bit #16 */
-static inline void __runtime_fixup_shift(void *where, unsigned long val)
+static __always_inline void __runtime_fixup_shift(void *where,
+						  unsigned long val)
 {
 	__le32 *p = lm_alias(where);
 	u32 insn = le32_to_cpu(*p);
 	insn &= 0xffc0ffff;
 	insn |= (val & 63) << 16;
-	*p = cpu_to_le32(insn);
+	__le32 insn_le = cpu_to_le32(insn);
+	bhv_patch(p, &insn_le, sizeof(__le32));
 	__runtime_fixup_caches(where, 1);
 }
 
-static inline void runtime_const_fixup(void (*fn)(void *, unsigned long),
-	unsigned long val, s32 *start, s32 *end)
+static __always_inline void
+runtime_const_fixup(void (*fn)(void *, unsigned long), unsigned long val,
+		    s32 *start, s32 *end)
 {
 	while (start < end) {
 		fn(*start + (void *)start, val);
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 8ff6610af4..6d67c6b2fa 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -20,6 +20,11 @@
 #include <asm/vdso.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+
 #define __ALT_PTR(a, f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -30,13 +35,23 @@
 /* Volatile, as we may be patching the guts of READ_ONCE() */
 static volatile int all_alternatives_applied;
 
-static DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
 
 struct alt_region {
 	struct alt_instr *begin;
 	struct alt_instr *end;
 };
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+
+void bhv_init_alternatives(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+#endif
+
 bool alternative_is_applied(u16 cpucap)
 {
 	if (WARN_ON(cpucap >= ARM64_NCAPS))
@@ -48,6 +63,7 @@ bool alternative_is_applied(u16 cpucap)
 /*
  * Check if the target PC is within an alternative block.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline bool branch_insn_requires_update(struct alt_instr *alt, unsigned long pc)
 {
 	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
@@ -56,6 +72,7 @@ static __always_inline bool branch_insn_requires_update(struct alt_instr *alt, u
 
 #define align_down(x, a)	((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr, __le32 *altinsnptr)
 {
 	u32 insn;
@@ -101,7 +118,8 @@ static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
 	return insn;
 }
 
-static noinstr void patch_alternative(struct alt_instr *alt,
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static __bhv_noinstr void patch_alternative(struct alt_instr *alt,
 			      __le32 *origptr, __le32 *updptr, int nr_inst)
 {
 	__le32 *replptr;
@@ -112,7 +130,21 @@ static noinstr void patch_alternative(struct alt_instr *alt,
 		u32 insn;
 
 		insn = get_alt_insn(alt, origptr + i, replptr + i);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key)) {
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+			continue;
+		}
+
+		if (bhv_integrity_is_enabled()) {
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+		}
+#else
 		updptr[i] = cpu_to_le32(insn);
+#endif
 	}
 }
 
@@ -121,6 +153,7 @@ static noinstr void patch_alternative(struct alt_instr *alt,
  * accidentally call into the cache.S code, which is patched by us at
  * runtime.
  */
+#ifndef CONFIG_BHV_VAULT_SPACES
 static noinstr void clean_dcache_range_nopatch(u64 start, u64 end)
 {
 	u64 cur, d_size, ctr_el0;
@@ -138,7 +171,9 @@ static noinstr void clean_dcache_range_nopatch(u64 start, u64 end)
 		asm volatile("dc civac, %0" : : "r" (cur) : "memory");
 	} while (cur += d_size, cur < end);
 }
+#endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __apply_alternatives(const struct alt_region *region,
 				 bool is_module,
 				 unsigned long *cpucap_mask)
@@ -147,6 +182,16 @@ static void __apply_alternatives(const struct alt_region *region,
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* !CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 		int cap = ALT_CAP(alt);
@@ -173,10 +218,19 @@ static void __apply_alternatives(const struct alt_region *region,
 
 		alt_cb(alt, origptr, updptr, nr_inst);
 
+		/*
+		 * With spaces-based vaults, it is the task of BHV to take care
+		 * of d-cache flushing after patching. Given that the Linux
+		 * kernel's .text segment is write-protected, writing back any
+		 * dirty cache entries targeting the .text segment will result
+		 * in data abort violations.
+		 */
+#ifndef CONFIG_BHV_VAULT_SPACES
 		if (!is_module) {
 			clean_dcache_range_nopatch((u64)origptr,
 						   (u64)(origptr + nr_inst));
 		}
+#endif
 	}
 
 	/*
@@ -195,7 +249,8 @@ static void __apply_alternatives(const struct alt_region *region,
 	}
 }
 
-static void __init apply_alternatives_vdso(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void apply_alternatives_vdso(void)
 {
 	struct alt_region region;
 	const struct elf64_hdr *hdr;
@@ -216,8 +271,18 @@ static void __init apply_alternatives_vdso(void)
 		.end	= (void *)hdr + alt->sh_offset + alt->sh_size,
 	};
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 false);
+	}
+#endif
+#endif
+
 	__apply_alternatives(&region, false, &all_capabilities[0]);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives_vdso);
 
 static const struct alt_region kernel_alternatives __initconst = {
 	.begin	= (struct alt_instr *)__alt_instructions,
@@ -228,7 +293,8 @@ static const struct alt_region kernel_alternatives __initconst = {
  * We might be patching the stop_machine state machine, so implement a
  * really simple polling protocol here.
  */
-static int __init __apply_alternatives_multi_stop(void *unused)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __apply_alternatives_multi_stop(void *unused)
 {
 	/* We always have a CPU 0 at this point (__init) */
 	if (smp_processor_id()) {
@@ -250,6 +316,7 @@ static int __init __apply_alternatives_multi_stop(void *unused)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_alternatives_multi_stop);
 
 void __init apply_alternatives_all(void)
 {
@@ -265,6 +332,14 @@ void __init apply_alternatives_all(void)
  * a feature detect on the boot CPU). No need to worry about other CPUs
  * here.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline void __apply_boot_alternatives(void)
+{
+	__apply_alternatives(&kernel_alternatives, false,
+			     &boot_cpucaps[0]);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_boot_alternatives);
+
 void __init apply_boot_alternatives(void)
 {
 	/* If called on non-boot cpu things could go wrong */
@@ -272,11 +347,11 @@ void __init apply_boot_alternatives(void)
 
 	pr_info("applying boot alternatives\n");
 
-	__apply_alternatives(&kernel_alternatives, false,
-			     &boot_cpucaps[0]);
+	__apply_boot_alternatives();
 }
 
 #ifdef CONFIG_MODULES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void apply_alternatives_module(void *start, size_t length)
 {
 	struct alt_region region = {
@@ -287,14 +362,51 @@ void apply_alternatives_module(void *start, size_t length)
 
 	bitmap_fill(all_capabilities, ARM64_NCAPS);
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
+#endif
+#endif
+
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives_module);
 #endif
 
-noinstr void alt_cb_patch_nops(struct alt_instr *alt, __le32 *origptr,
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+__bhv_noinstr void alt_cb_patch_nops(struct alt_instr *alt, __le32 *origptr,
 			       __le32 *updptr, int nr_inst)
 {
-	for (int i = 0; i < nr_inst; i++)
+	for (int i = 0; i < nr_inst; i++) {
+#ifdef CONFIG_BHV_VAS
+		u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key)) {
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+			return;
+		}
+#endif
+
+		if (bhv_integrity_is_enabled()) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+#else
+			bhv_patch_hypercall((void *)&updptr[i],
+					    (uint8_t *)&insn, sizeof(insn),
+					    false);
+#endif
+		} else {
+			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alt_cb_patch_nops);
 EXPORT_SYMBOL(alt_cb_patch_nops);
diff --git arch/arm64/kernel/entry-common.c arch/arm64/kernel/entry-common.c
index d23315ef7b..5607da2382 100644
--- arch/arm64/kernel/entry-common.c
+++ arch/arm64/kernel/entry-common.c
@@ -165,6 +165,12 @@ static __always_inline void exit_to_user_mode_prepare(struct pt_regs *regs)
 	local_irq_disable();
 
 	flags = read_thread_flags();
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	// Make sure we are on the current domain before exiting to userspace
+	brs_domain_enter(current);
+#endif
+
 	if (unlikely(flags & _TIF_WORK_MASK))
 		do_notify_resume(regs, flags);
 
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index f63ea915d6..b8fd327078 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -11,10 +11,27 @@
 #include <asm/insn.h>
 #include <asm/patching.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+
+void bhv_init_jump_label(void)
+{
+        if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+#endif
+
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool arch_jump_label_transform_queue(struct jump_entry *entry,
 				     enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -25,6 +42,19 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return true;
+	}
+#endif
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return true;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 	return true;
 }
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index 31eaf15d20..625b5327d6 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -32,6 +32,14 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -577,7 +585,8 @@ static enum mitigation_state spectre_v4_enable_hw_mitigation(void)
  * Patch a branch over the Spectre-v4 mitigation code with a NOP so that
  * we fallthrough and check whether firmware needs to be called on this CPU.
  */
-void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 						  __le32 *origptr,
 						  __le32 *updptr, int nr_inst)
 {
@@ -589,17 +598,35 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+					       sizeof(insn));
+#else
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+#endif
+
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
  * Patch a NOP in the Spectre-v4 mitigation code with an SMC/HVC instruction
  * to call into firmware to adjust the mitigation state.
  */
-void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst)
 {
 	u32 insn;
 
@@ -616,7 +643,22 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1106,30 +1148,69 @@ bool is_spectre_bhb_fw_mitigated(void)
 }
 
 /* Patched to NOP when enabled */
-void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
-						     __le32 *origptr,
-						      __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
+						            __le32 *origptr,
+						            __le32 *updptr,
+						            int nr_inst)
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_LOOP, &system_bhb_mitigations))
+	if (test_bit(BHB_LOOP, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+					       sizeof(insn));
+#else
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+#endif
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to NOP when enabled */
-void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
-						   __le32 *origptr,
-						   __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
+						           __le32 *origptr,
+						           __le32 *updptr,
+						           int nr_inst)
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_FW, &system_bhb_mitigations))
+	if (test_bit(BHB_FW, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+					       sizeof(insn));
+#else
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+#endif
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to correct the immediate */
-void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
+				               __le32 *origptr, __le32 *updptr,
+				               int nr_inst)
 {
 	u8 rd;
 	u32 insn;
@@ -1144,12 +1225,28 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, max_bhb_k, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to mov WA3 when supported */
-void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_noinstr spectre_bhb_patch_wa3(struct alt_instr *alt, __le32 *origptr,
+				   __le32 *updptr, int nr_inst)
 {
 	u8 rd;
 	u32 insn;
@@ -1170,20 +1267,63 @@ void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 	if (WARN_ON_ONCE(insn == AARCH64_BREAK_FAULT))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to NOP when not supported */
-void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init spectre_bhb_patch_clearbhb(struct alt_instr *alt, __le32 *origptr,
+				           __le32 *updptr, int nr_inst)
 {
 	BUG_ON(nr_inst != 2);
 
 	if (test_bit(BHB_INSN, &system_bhb_mitigations))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+		updptr++;
+
+		insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index 87f61fd678..6db6c689c2 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -54,6 +54,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/init/init.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -321,6 +323,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT)) {
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index e73326bd3f..2c0fc0e151 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -181,7 +181,11 @@ SECTIONS
 			LOCK_TEXT
 			KPROBES_TEXT
 			HYPERVISOR_TEXT
+			BHV_TEXT
 			*(.gnu.warning)
+#ifdef CONFIG_BHV_VAULT_SPACES
+			BHV_VAULT_TEXT(jump_label)
+#endif
 	}
 
 	. = ALIGN(SEGMENT_ALIGN);
@@ -230,17 +234,34 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
 		__alt_instructions = .;
 		*(.altinstructions)
 		__alt_instructions_end = .;
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 	}
 
 	UNWIND_DATA_SECTIONS
@@ -313,6 +334,34 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+    . = ALIGN(PAGE_SIZE);
+    .bhv.vault.comm : AT(ADDR(.bhv.vault.comm) - LOAD_OFFSET) {
+        __bhv_vault_comm_start = .;
+        . += PAGE_SIZE;
+        . = ALIGN(PAGE_SIZE);
+        __bhv_vault_comm_end = .;
+    }
+    . = ALIGN(PAGE_SIZE);
+    .bhv.vault.data : AT(ADDR(.bhv.vault.data) - LOAD_OFFSET) {
+        BHV_VAULT_DATA(jump_label)
+    }
+    . = ALIGN(PAGE_SIZE);
+    .bhv.vault.rodata : AT(ADDR(.bhv.vault.rodata) - LOAD_OFFSET) {
+        BHV_VAULT_RO_DATA(jump_label)
+    }
+#endif
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index 91b22a0146..a0c29ba88e 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -13,6 +13,11 @@
 #include <asm/kvm_mmu.h>
 #include <asm/memory.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -151,8 +156,23 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_patch_hypercall((void *)dest, (uint8_t *)&le32_insn,
+			    sizeof(le32_insn));
+#else
+	bhv_patch_hypercall((void *)dest, (uint8_t *)&le32_insn,
+			    sizeof(le32_insn), false);
+#endif
+}
+#endif /* CONFIG_BHV_VAS */
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+
 {
 	int i;
 
@@ -170,7 +190,16 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (cpus_have_cap(ARM64_HAS_VIRT_HOST_EXTN) || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop());
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* !CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -181,12 +210,20 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
-void kvm_patch_vector_branch(struct alt_instr *alt,
-			     __le32 *origptr, __le32 *updptr, int nr_inst)
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst)
 {
 	u64 addr;
 	u32 insn;
@@ -217,15 +254,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -233,12 +284,28 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst)
@@ -257,7 +324,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 16) & 0xffff), lsl #16 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -265,7 +340,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 16,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -273,7 +356,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 48) & 0xffff), lsl #48 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -281,7 +372,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 48,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 void kvm_get_kimage_voffset(struct alt_instr *alt,
diff --git arch/arm64/mm/fault.c arch/arm64/mm/fault.c
index 850307b49b..339da6b763 100644
--- arch/arm64/mm/fault.c
+++ arch/arm64/mm/fault.c
@@ -44,6 +44,10 @@
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 struct fault_info {
 	int	(*fn)(unsigned long far, unsigned long esr,
 		      struct pt_regs *regs);
@@ -389,12 +393,19 @@ static void __do_kernel_fault(unsigned long addr, unsigned long esr,
 	}
 
 	if (is_el1_permission_fault(addr, esr, regs)) {
-		if (esr & ESR_ELx_WNR)
+		uint8_t type;
+		if (esr & ESR_ELx_WNR) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
 			msg = "write to read-only memory";
-		else if (is_el1_instruction_abort(esr))
+		} else if (is_el1_instruction_abort(esr)) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
 			msg = "execute from non-executable memory";
-		else
+		} else {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
 			msg = "read from unreadable memory";
+		}
+
+		security_kaccess((uint64_t)addr, type);
 	} else if (addr < PAGE_SIZE) {
 		msg = "NULL pointer dereference";
 	} else {
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 93ba66de16..643327312c 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -48,6 +48,9 @@
 #include <asm/alternative.h>
 #include <asm/xen/swiotlb-xen.h>
 
+#include <bhv/init/start.h>
+#include <bhv/vault.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -81,6 +84,25 @@ phys_addr_t __ro_after_init arm64_dma_phys_limit;
 #define ARM64_MEMSTART_SHIFT		PMD_SHIFT
 #endif
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+static void __ref bhv_vault_release_memory(void)
+{
+        int rc;
+        HypABI__Wagner__Delete__arg__T vault;
+
+        if (!bhv_vault_is_enabled())
+                return;
+
+        vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+        vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+        rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+        if (rc) {
+                bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+                return;
+        }
+}
+#endif
+
 /*
  * sparsemem vmemmap imposes an additional requirement on the alignment of
  * memstart_addr, due to the fact that the base of the vmemmap region
@@ -410,6 +432,10 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_vault_release_memory();
+#endif
+	bhv_start();
 	void *lm_init_begin = lm_alias(__init_begin);
 	void *lm_init_end = lm_alias(__init_end);
 
diff --git arch/arm64/net/bpf_jit_comp.c arch/arm64/net/bpf_jit_comp.c
index 515c411c2c..d2060be579 100644
--- arch/arm64/net/bpf_jit_comp.c
+++ arch/arm64/net/bpf_jit_comp.c
@@ -26,6 +26,9 @@
 
 #include "bpf_jit.h"
 
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 #define TMP_REG_1 (MAX_BPF_JIT_REG + 0)
 #define TMP_REG_2 (MAX_BPF_JIT_REG + 1)
 #define TCCNT_PTR (MAX_BPF_JIT_REG + 2)
@@ -249,6 +252,12 @@ static void jit_fill_hole(void *area, unsigned int size)
 
 int bpf_arch_text_invalidate(void *dst, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		return bhv_bpf_invalidate(dst, 0xcc, len);
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	if (!aarch64_insn_set(dst, AARCH64_BREAK_FAULT, len))
 		return -EINVAL;
 
@@ -2015,6 +2024,14 @@ bool bpf_jit_supports_kfunc_call(void)
 
 void *bpf_arch_text_copy(void *dst, void *src, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		if (bhv_bpf_write(dst, src, len))
+			return ERR_PTR(-EINVAL);
+		return dst;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	if (!aarch64_insn_copy(dst, src, len))
 		return ERR_PTR(-EINVAL);
 	return dst;
diff --git arch/x86/Kbuild arch/x86/Kbuild
index cf0ad89f56..2ca057f32d 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -15,6 +15,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 0000000000..78f04a3480
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,18 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+ccflags-y+=-Werror
+
+obj-$(CONFIG_BHV_VAS)		:= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/x86/bhv/init/init.c arch/x86/bhv/init/init.c
new file mode 100644
index 0000000000..a07c068686
--- /dev/null
+++ arch/x86/bhv/init/init.c
@@ -0,0 +1,137 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/types.h> // This is here so that hypervisor.h knows bool
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#include <vdso/datapage.h>
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+#include <asm/vvar.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_64.data),
+				   vdso_image_64.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 64");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_x32.data),
+				   vdso_image_x32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE X32");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_32.data),
+				   vdso_image_32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 32");
+	(*region_counter)++;
+
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static __always_inline void
+bhv_init_add_vvar(bhv_mem_region_t *init_phys_mem_regions,
+		  unsigned int *region_counter)
+{
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   __pa_symbol(&__vvar_page), PAGE_SIZE,
+				   HypABI__Integrity__MemType__VVAR, HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL VVAR PAGE");
+	(*region_counter)++;
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vvar(init_phys_mem_regions, region_counter);
+}
+
+static uint32_t __init bhv_detect(void)
+{
+	uint32_t rv = 0;
+	if (boot_cpu_data.cpuid_level < 0)
+		return rv; /* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		if (hypervisor_cpuid_base("BHV.VMM.VAS.", 0)) {
+			_brs_is_standalone = false;
+			rv = 1;
+		}
+
+	return rv;
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform
+};
+
+
+bool __init bhv_init_arch(void)
+{
+	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+    return true;
+}
diff --git arch/x86/bhv/init/start.c arch/x86/bhv/init/start.c
new file mode 100644
index 0000000000..a8acbb41f1
--- /dev/null
+++ arch/x86/bhv/init/start.c
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <bhv/init/start.h>
+#include <bhv/init/late_start.h>
+#include <bhv/integrity.h>
+
+int bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
+
+int bhv_late_start_arch(void)
+{
+	return bhv_late_start_integrity_arch();
+}
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 0000000000..d34f88734e
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,585 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/pgtable.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_EFI
+#include <linux/efi.h>
+#endif /* CONFIG_EFI */
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <linux/pagewalk.h>
+
+typedef struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} table_data_t;
+
+static table_data_t table_data __ro_after_init = { 0 };
+
+typedef struct {
+	uint64_t start_addr;
+	uint64_t size;
+	const char *label;
+	uint32_t mem_type;
+} ro_region_t;
+
+/**********************************************************
+ * start
+ **********************************************************/
+static int bhv_start_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = bhv_virt_to_phys_single((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(
+		head, addr, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE, "IDT");
+}
+
+static int bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_start_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region.create);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+static inline int bhv_start_rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int bhv_start_rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int bhv_start_rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = bhv_start_rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+static void bhv_start_integrity_add_vdso_common(
+	uint64_t start_addr, uint64_t size, const char *label,
+	ro_region_t *range, unsigned int *cur_entry, size_t range_sz)
+{
+	unsigned int i;
+	uint64_t end = start_addr + size;
+	uint64_t cur_end = 0;
+
+	BUG_ON(size == 0);
+	BUG_ON((*cur_entry) >= range_sz);
+	BUG_ON(start_addr < range[0].start_addr);
+
+	// Check for overlaps
+	for (i = 0; i < (*cur_entry); i++) {
+		cur_end = range[i].start_addr + range[i].size;
+
+		// No overlap. Nothing to do.
+		if (end <= range[i].start_addr || start_addr >= cur_end)
+			continue;
+
+		// No range that we add should be exactly the same as an
+		// existing one.
+		if (start_addr == range[i].start_addr && end == cur_end) {
+			BUG(); // Range already exists
+		}
+
+		// Overlap. Split range.
+		// Case 1 (overlap left): New range starts before current range
+		//                        with/before the current range.
+		//                        Update the new range to end when cur starts.
+		//                        Thus creating range A and B.
+		//  -----------------------
+		// |      A      ##########B##########
+		// |     new     #         |   cur   #
+		// |             ##########|##########
+		// ------------------------
+		if (start_addr < range[i].start_addr && end <= cur_end) {
+			size = range[i].start_addr - start_addr;
+			continue;
+		}
+
+		// Case 2 (overlap right): New range starts within current range
+		//                         and ends with/after the current range.
+		//                         Update cur to end when new starts.
+		//                         Thus creating range A and B.
+		//                         -----------------------
+		//              #####A####|##########  B          |
+		//              #   cur   |         #     new     |
+		//              ##########|##########             |
+		//                        ------------------------
+		if (range[i].start_addr < start_addr && cur_end <= end) {
+			range[i].size = start_addr - range[i].start_addr;
+			continue;
+		}
+
+		// Case 3: New range fully encompasses current range.
+		//         Create three ranges A, B, C.
+		//  --------------------------------------------
+		// |     A     ##########B##########     C      |
+		// |    new    #        cur        #    new     |
+		// |           #####################            |
+		// ---------------------------------------------
+		if (start_addr <= range[i].start_addr && cur_end <= end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				// No. Add new Range A.
+				range[(*cur_entry)].start_addr = start_addr;
+				range[(*cur_entry)].size =
+					range[i].start_addr - start_addr;
+				range[(*cur_entry)].label = label;
+				range[(*cur_entry)].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Update new to be C. It will be added after the loop.
+				// B is already in our range list.
+				start_addr = cur_end;
+				size = end - cur_end;
+			} else {
+				// Remaining ranges are the same. Just update B with new label/type.
+				// Since a range can only have one label, it will take the
+				// label of the new range. This generally seems to make sense
+				// since we add the entire read-only section and then split it
+				// with smaller sections.
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+			continue;
+		}
+
+		// Case 4: Current range fully encompasses new range.
+		//         Create three ranges A, B, C.
+		// ##############################################
+		// #     A      ---------B---------      C      #
+		// #    cur    |        new        |    cur     #
+		// #            -------------------             #
+		// ##############################################
+		if (range[i].start_addr <= start_addr && end <= cur_end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				range[(*cur_entry)].start_addr =
+					range[i].start_addr;
+				range[(*cur_entry)].size =
+					start_addr - range[i].start_addr;
+				range[(*cur_entry)].label = range[i].label;
+				range[(*cur_entry)].mem_type =
+					range[i].mem_type;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Create C. B will be added after the loop.
+				range[i].start_addr = end;
+				range[i].size = cur_end - end;
+			} else {
+				// Remaining ranges are the same.
+				// Shrink the current range to B. We also update its label/type.
+				// This generally seems to make sense since we add the
+				// entire read-only section and then split it with smaller
+				// sections.
+				range[i].start_addr = start_addr;
+				range[i].size = size;
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+
+			continue;
+		}
+
+		BUG(); // "Unexpected case"
+	}
+
+	if (size != 0) {
+		range[(*cur_entry)].start_addr = start_addr;
+		range[(*cur_entry)].size = size;
+		range[(*cur_entry)].label = label;
+		range[(*cur_entry)].mem_type = HypABI__Integrity__MemType__VDSO;
+		(*cur_entry)++;
+	}
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_64_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start = bhv_virt_to_phys_single(vdso_image_64.data);
+	uint64_t size = vdso_image_64.size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 64",
+					    range, cur_entry, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_x32_to_range(ro_region_t *range,
+						unsigned int *cur_entry,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start = bhv_virt_to_phys_single(vdso_image_x32->data);
+	uint64_t size = vdso_image_x32->size;
+	bhv_start_integrity_add_vdso_common(start, size,
+					    "KERNEL VDSO IMAGE X32", range,
+					    cur_entry, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_32_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start = bhv_virt_to_phys_single(vdso_image_32.data);
+	uint64_t size = vdso_image_32.size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 32",
+					    range, cur_entry, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void bhv_start_integrity_get_ro_ranges(ro_region_t *range,
+							unsigned int *cur_entry,
+							size_t range_sz)
+{
+	BUG_ON(range_sz < 1);
+
+	range[(*cur_entry)].start_addr = bhv_virt_to_phys_single(
+		(void *)((unsigned long)__start_rodata & PAGE_MASK));
+	range[(*cur_entry)].size = PAGE_ALIGN((unsigned long)__end_rodata) -
+				   ((unsigned long)__start_rodata & PAGE_MASK);
+	range[(*cur_entry)].label = "KERNEL READ-ONLY DATA SECTION";
+	range[(*cur_entry)].mem_type =
+		HypABI__Integrity__MemType__DATA_READ_ONLY;
+	(*cur_entry)++;
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, cur_entry,
+						       range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, cur_entry,
+							range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, cur_entry,
+						       range_sz);
+}
+
+/************************************************************
+ * start until 6.11, late_start from 6.12
+ ************************************************************/
+static int bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+	unsigned int i;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	unsigned int nr_ro_ranges = 0;
+	ro_region_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_MAX_RO_RANGES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, &nr_ro_ranges,
+					  BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   nr_ro_ranges, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < nr_ro_ranges; i++) {
+		bhv_mem_region_create_ctor(
+			&n[i]->region, (prev ? &prev->region : NULL),
+			ro_ranges[i].start_addr, ro_ranges[i].size,
+			ro_ranges[i].mem_type,
+			HypABI__Integrity__MemFlags__NONE, ro_ranges[i].label);
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, nr_ro_ranges, (void **)&n);
+
+	return rc;
+}
+/************************************************************/
+
+#ifdef CONFIG_EFI
+static int bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		bhv_mem_region_create_ctor(&n->region, NULL, md->phys_addr,
+					   (md->num_pages) << PAGE_SHIFT,
+					   HypABI__Integrity__MemType__CODE,
+					   HypABI__Integrity__MemFlags__NONE,
+					   "EFI REGION");
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1,
+							 &(n->region.create));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+#else /* CONFIG_EFI */
+static int bhv_start_integrity_add_efi_regions(void)
+{
+	return 0;
+}
+#endif /* CONFIG_EFI */
+
+int bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 11, 0)
+	return 0;
+#else // LINUX_VERSION_CODE < 6.11
+	return bhv_start_integrity_add_ro();
+#endif // LINUX_VERSION_CODE <> 6.11
+}
+/**********************************************************/
+
+#define BHV_DIR_PTR_ENTRY_MASK 0x800FFFFFFFFFF89D
+
+/**********************************************************
+ * start
+ **********************************************************/
+struct bhv_pw_data {
+	bool set;
+	uint64_t pgd_offset;
+	uint64_t pgd_value;
+};
+
+static int bhv_start_pgd_entry(pgd_t *pgd, unsigned long addr,
+					 unsigned long next,
+					 struct mm_walk *walk)
+{
+	struct bhv_pw_data *data = (struct bhv_pw_data *)walk->private;
+	pr_info("%s: pgd=%llx (%llx) addr=%lx next=%lx\n", __FUNCTION__,
+		(uint64_t)pgd, bhv_virt_to_phys_single(pgd), addr, next);
+	BUG_ON(data->set);
+	data->pgd_offset = ((uint64_t)pgd & 0xFFF);
+	data->pgd_value = *((uint64_t *)pgd);
+	data->set = true;
+	return 1;
+}
+
+static const struct mm_walk_ops bhv_start_walk_ops = {
+	.pgd_entry = bhv_start_pgd_entry,
+	.p4d_entry = NULL,
+	.pud_entry = NULL,
+	.pmd_entry = NULL,
+	.pte_entry = NULL,
+	.pte_hole = NULL,
+	.hugetlb_entry = NULL,
+	.test_walk = NULL,
+	.pre_vma = NULL,
+	.post_vma = NULL
+};
+
+void bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	struct bhv_pw_data text_pw_data = { 0 };
+	struct bhv_pw_data ro_pw_data = { 0 };
+
+	mmap_write_lock(&init_mm);
+	walk_page_range_novma(&init_mm, (unsigned long)_stext,
+			      (unsigned long)_etext, &bhv_start_walk_ops,
+			      init_mm.pgd, &text_pw_data);
+	walk_page_range_novma(&init_mm, (unsigned long)__start_rodata,
+			      (unsigned long)__end_rodata, &bhv_start_walk_ops,
+			      init_mm.pgd, &ro_pw_data);
+	mmap_write_unlock(&init_mm);
+
+	BUG_ON(text_pw_data.pgd_offset != ro_pw_data.pgd_offset ||
+	       text_pw_data.pgd_value != ro_pw_data.pgd_value);
+
+	*pgd_offset = text_pw_data.pgd_offset;
+	*pgd_value = (text_pw_data.pgd_value & BHV_DIR_PTR_ENTRY_MASK);
+}
+/**********************************************************/
+
+/**********************************************************
+ * late_start
+ **********************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	BUG_ON(CONFIG_PGTABLE_LEVELS < 4 || CONFIG_PGTABLE_LEVELS > 5);
+
+	init_ptpg_arg->init_pgd =
+		(uint64_t)bhv_virt_to_phys_single(&init_mm.pgd);
+	init_ptpg_arg->pt_levels = pgtable_l5_enabled() ? 5 : 4;
+	init_ptpg_arg->num_ranges = 2;
+
+	init_ptpg_arg->ranges[0] = (uint64_t)(unsigned long)_stext;
+	init_ptpg_arg->ranges[1] = (uint64_t)(unsigned long)_etext;
+	init_ptpg_arg->ranges[2] = (uint64_t)(unsigned long)__start_rodata;
+	init_ptpg_arg->ranges[3] = (uint64_t)(unsigned long)__end_rodata;
+}
+
+int bhv_late_start_integrity_arch(void) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 11, 0)
+	return bhv_start_integrity_add_ro();
+#else // LINUX_VERSION_CODE < 6.11
+	return 0;
+#endif // LINUX_VERSION_CODE <> 6.11
+}
+/**********************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	uint64_t value = *((uint64_t *)(((uint8_t *)mm->pgd) + pgd_offset));
+	return ((value & BHV_DIR_PTR_ENTRY_MASK) == pgd_value);
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 0000000000..ea6659b7f9
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,833 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/kversion.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#define NOPS x86_nops
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end,
+				      const s32 *locks_begin,
+				      const s32 *locks_end, u8 *text_begin,
+				      u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len, bool patch)
+{
+	size_t total_length = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) > sizeof(buf))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(buf + total_length, NOPS[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, NOPS[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_patch_hypercall(insns, buf, total_length, false);
+	}
+}
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off, bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(u8 *target,
+								   bool lock)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target,
+				    "Invalid altinst smp unlock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch.
+		}
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target, "Invalid altinst smp lock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch
+		}
+
+		opcode = unlock_opcode;
+	}
+
+	r = bhv_patch_hypercall((void *)target, &opcode, 1, false);
+
+	if (r) {
+		panic("BHV patch hypercall failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+		    ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+// CONFIG_MITIGATION_RETPOLINE is used in 6.12 and CONFIG_RETPOLINE in prev
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) &&     \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+		bytes[i++] = BYTES_NOP1;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (invalid op)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+
+	// Retpolines may be disabled or there is another error
+	// this is not an attack.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr,
+			    "Invalid altinst retpoline (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_optimize_nops(bytes, len, true);
+	bhv_patch_hypercall((void *)addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+// CONFIG_MITIGATION_RETHUNK is used in 6.12 and CONFIG_RETHUNK in prev
+#if (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK))
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		return;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+	    WARN_ONCE(dest != &__x86_return_thunk,
+		      "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+		      addr))
+		return;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	// Feature may be disabled.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_patch_hypercall(addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK)) */
+
+#endif /* defined(CONFIG_RETPOLINE)*/
+
+#if defined CONFIG_PARAVIRT && LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0)
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, false);
+
+	bhv_patch_hypercall((void *)(p->instr), insn_buff, p->len, false);
+
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_PARAVIRT && LINUX_VERSION_CODE < 6.8 */
+
+// CONFIG_MITIGATION_RETPOLINE is used in 6.12 and CONFIG_RETPOLINE in prev
+#if (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) &&     \
+	defined(CONFIG_PARAVIRT) &&                                            \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL)) &&       \
+	LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0)
+
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETPOLINE && CONFIG_PARAVIRT &&
+	(CONFIG_STACK_VALIDATION || CONFIG_OBJTOOL) &&
+	LINUX_VERSION_CODE < 6.8 */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		if (bhv_patch_violation_hypercall(
+			    instr, "Invalid altinst patch (too big)")) {
+			// Block attempt.
+			return -EACCES;
+		}
+
+		// Allow patch.
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		// This can happen legitmately. We just return.
+		return -EINVAL;
+	}
+
+#if defined(BHV_KVERS_6_1)
+	if (!boot_cpu_has(a->cpuid & ~ALTINSTR_FLAG_INV) ==
+	    !(a->cpuid & ALTINSTR_FLAG_INV)) {
+#elif defined(BHV_KVERS_ATLEAST_6_12)
+	if (!boot_cpu_has(a->cpuid) == !(a->flags & ALT_FLAG_NOT)) {
+#endif
+		bhv_optimize_nops(instr, a->instrlen, true);
+		return 0;
+	}
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+
+	if (insn_buff_sz >= HypABI__Patch__MAX_PATCH_SZ)
+		panic("Instruction buffer size too small!");
+
+	rv = bhv_patch_hypercall((void *)instr, insn_buff, insn_buff_sz, false);
+
+	bhv_optimize_nops(instr, a->instrlen, true);
+
+	return rv;
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a);
+		else
+			bhv_alternatives_patch_vault(a);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+	    cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __bhv_text bhv_apply_ibt_endbr_vault(s32 *s)
+{
+	int ret;
+	u32 endbr, poison;
+	void *addr;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	poison = gen_endbr_poison();
+	addr = (void *)s + *s;
+
+	if (get_kernel_nofault(endbr, addr))
+		goto out;
+
+	if (!is_endbr(endbr))
+		goto out;
+
+	bhv_patch_hypercall(addr, (uint8_t *)&poison, 4, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __init_or_module bhv_apply_ibt_endbr(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_ibt_endbr_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 0000000000..5b7ab21eac
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,77 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+	if (len == 2) {
+		CHECK_NOP(x86_nops[2]);
+	} else if (len == 5) {
+		CHECK_NOP(x86_nops[5]);
+	}
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
+#endif
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 0000000000..1a3e33df3e
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,139 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <linux/version.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static __always_inline void bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static __always_inline void bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+extern void __static_call_return(void);
+
+static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
+{
+	u8 ret = 0;
+
+	if (insn[0] == 0x0f) {
+		u8 tmp = insn[1];
+		if ((tmp & 0xf0) == 0x80)
+			ret = tmp;
+	}
+
+	return ret;
+}
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func)
+{
+	int ret;
+	const void *emulate = NULL;
+	int size = CALL_INSN_SIZE;
+	const void *code;
+	u8 buf[6];
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+		break;
+
+	case NOP:
+		code = (const uint8_t *const *)x86_nops[5];
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+		break;
+	case JCC:
+		if (!func) {
+			func = (void *)__static_call_return;
+			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+				func = __x86_return_thunk;
+		}
+
+		buf[0] = 0x0f;
+		__text_gen_insn(buf + 1, __is_Jcc(insn), insn + 1, func, 5);
+		code = buf;
+		size = 6;
+
+		break;
+	}
+
+	if (memcmp(insn, code, size) == 0) {
+		if (bhv_patch_violation_hypercall(insn,
+						  "Invalid static call")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow patch
+	}
+
+	if (size > HypABI__Patch__MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_patch_hypercall((void *)insn, code, size, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func)
+{
+	unsigned long flags;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/reg_protect.c arch/x86/bhv/reg_protect.c
new file mode 100644
index 0000000000..5a9f52936e
--- /dev/null
+++ arch/x86/bhv/reg_protect.c
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#include <asm/processor-flags.h>
+
+#define BHV_CR0_FREEZE X86_CR0_WP
+extern const unsigned long cr4_pinned_mask;
+#define BHV_CR4_FREEZE cr4_pinned_mask
+#define BHV_EFER_FREEZE 0x0ULL
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+	int r;
+
+	if (BHV_CR0_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR0,
+			BHV_CR0_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR0!\n");
+	}
+
+	if (BHV_CR4_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR4,
+			BHV_CR4_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR4!\n");
+	}
+
+	if (BHV_EFER_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__EFER,
+			BHV_EFER_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze EFER!\n");
+	}
+}
+/***************************************************/
diff --git arch/x86/entry/common.c arch/x86/entry/common.c
index 51efd2da4d..e1e6fc65cf 100644
--- arch/x86/entry/common.c
+++ arch/x86/entry/common.c
@@ -37,6 +37,8 @@
 #include <asm/syscall.h>
 #include <asm/irq_stack.h>
 
+#include <bhv/integrity.h>
+
 #ifdef CONFIG_X86_64
 
 static __always_inline bool do_syscall_x64(struct pt_regs *regs, int nr)
@@ -78,6 +80,8 @@ __visible noinstr bool do_syscall_64(struct pt_regs *regs, int nr)
 	add_random_kstack_offset();
 	nr = syscall_enter_from_user_mode(regs, nr);
 
+	bhv_pt_protect_check_pgd(current->active_mm);
+
 	instrumentation_begin();
 
 	if (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {
diff --git arch/x86/entry/entry_64.S arch/x86/entry/entry_64.S
index 9c6a110a52..ecabb65804 100644
--- arch/x86/entry/entry_64.S
+++ arch/x86/entry/entry_64.S
@@ -186,6 +186,11 @@ SYM_FUNC_START(__switch_to_asm)
 	pushq	%r14
 	pushq	%r15
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	movq	PER_CPU_VAR(brs_domain_current_domain), %r12
+	pushq	%r12
+#endif
+
 	/* switch stack */
 	movq	%rsp, TASK_threadsp(%rdi)
 	movq	TASK_threadsp(%rsi), %rsp
@@ -204,6 +209,17 @@ SYM_FUNC_START(__switch_to_asm)
 	 */
 	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	// Save arguments in r12 and r13
+	movq	%rdi, %r12
+	movq	%rsi, %r13
+	popq	%rdi
+	callq	brs_domain_switch
+	// Restore original arguments
+	movq	%r12, %rdi
+	movq	%r13, %rsi
+#endif
+
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
diff --git arch/x86/include/asm/bhv/domain.h arch/x86/include/asm/bhv/domain.h
new file mode 100644
index 0000000000..07803c2faf
--- /dev/null
+++ arch/x86/include/asm/bhv/domain.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __ASM_BRS_DOMAIN_H__
+#define __ASM_BRS_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define brs_domain_arch_get_user_pgd(pgd) kernel_to_user_pgdp(pgd)
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return pte_present(pte);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pmd_present(pmd);
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pud_present(pud);
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return !(pgprot_val(pmd_pgprot(pmd)) & _PAGE_NX);
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return !(pgprot_val(pud_pgprot(pud)) & _PAGE_NX);
+}
+
+static inline bool brs_domain_is_user_pte(pte_t pte)
+{
+	return !!(pgprot_val(pte_pgprot(pte)) & _PAGE_USER);
+}
+
+static inline bool brs_domain_is_user_pmd(pmd_t pmd)
+{
+	return !!(pgprot_val(pmd_pgprot(pmd)) & _PAGE_USER);
+}
+
+static inline bool brs_domain_is_user_pud(pud_t pud)
+{
+	return !!(pgprot_val(pud_pgprot(pud)) & _PAGE_USER);
+}
+
+#endif
+
+#endif /* __ASM_BRS_DOMAIN_H__ */
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 0000000000..99a1e49556
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 0000000000..f7bc6f2444
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+#else /* !CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 0000000000..2abda93432
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,97 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+#include <bhv/kversion.h>
+#include <bhv/vault.h>
+
+#include <linux/version.h>
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+// CONFIG_MITIGATION_RETPOLINE is used in 6.12 and CONFIG_RETPOLINE in prev
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL is used in 6.1
+#if (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) &&     \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __bhv_text bhv_apply_retpolines_vault(s32 *s);
+// CONFIG_MITIGATION_RETHUNK is used in 6.12 and CONFIG_RETHUNK in prev
+#if (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK))
+void __bhv_text bhv_apply_returns_vault(s32 *s);
+#endif /* (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK)) */
+#endif /* (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) */
+
+#if defined(CONFIG_PARAVIRT) && (LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0))
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+	JCC = 4,
+};
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func);
+
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func, bool modinit)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec4..591e48b050 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/include/asm/pgtable.h arch/x86/include/asm/pgtable.h
index 4c2d080d26..e2035d2b4c 100644
--- arch/x86/include/asm/pgtable.h
+++ arch/x86/include/asm/pgtable.h
@@ -24,6 +24,8 @@
 #include <asm-generic/pgtable_uffd.h>
 #include <linux/page_table_check.h>
 
+#include <bhv/domain_pt.h>
+
 extern pgd_t early_top_pgt[PTRS_PER_PGD];
 bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
@@ -1281,6 +1283,7 @@ static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
 			      pmd_t *pmdp, pmd_t pmd)
 {
 	page_table_check_pmd_set(mm, pmdp, pmd);
+	brs_domain_set_pmd_at(mm, addr, pmdp, pmd);
 	set_pmd(pmdp, pmd);
 }
 
@@ -1288,6 +1291,7 @@ static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
 			      pud_t *pudp, pud_t pud)
 {
 	page_table_check_pud_set(mm, pudp, pud);
+	brs_domain_set_pud_at(mm, addr, pudp, pud);
 	native_set_pud(pudp, pud);
 }
 
@@ -1317,7 +1321,9 @@ extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
-	pte_t pte = native_ptep_get_and_clear(ptep);
+	pte_t pte;
+	brs_domain_clear_pte(mm, addr, ptep, *ptep);
+	pte = native_ptep_get_and_clear(ptep);
 	page_table_check_pte_clear(mm, pte);
 	return pte;
 }
@@ -1333,6 +1339,7 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
 		 * Full address destruction in progress; paravirt does not
 		 * care about updates and native needs no locking
 		 */
+		brs_domain_clear_pte(mm, addr, ptep, pte);
 		pte = native_local_ptep_get_and_clear(ptep);
 		page_table_check_pte_clear(mm, pte);
 	} else {
@@ -1356,6 +1363,7 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm,
 	do {
 		new_pte = pte_wrprotect(old_pte);
 	} while (!try_cmpxchg((long *)&ptep->pte, (long *)&old_pte, *(long *)&new_pte));
+	brs_domain_set_pte_at(mm, addr, ptep, *ptep);
 }
 
 #define flush_tlb_fix_spurious_fault(vma, address, ptep) do { } while (0)
@@ -1385,7 +1393,9 @@ extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
 static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pmd_t *pmdp)
 {
-	pmd_t pmd = native_pmdp_get_and_clear(pmdp);
+	pmd_t pmd;
+	brs_domain_clear_pmd(mm, addr, pmdp, *pmdp);
+	pmd = native_pmdp_get_and_clear(pmdp);
 
 	page_table_check_pmd_clear(mm, pmd);
 
@@ -1396,7 +1406,11 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long
 static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 					unsigned long addr, pud_t *pudp)
 {
-	pud_t pud = native_pudp_get_and_clear(pudp);
+	pud_t pud;
+
+	brs_domain_clear_pud(mm, addr, pudp, *pudp);
+
+	pud = native_pudp_get_and_clear(pudp);
 
 	page_table_check_pud_clear(mm, pud);
 
@@ -1414,6 +1428,8 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 	 */
 	pmd_t old_pmd, new_pmd;
 
+	brs_domain_set_pmd_at(mm, addr, pmdp, *pmdp);
+
 	old_pmd = READ_ONCE(*pmdp);
 	do {
 		new_pmd = pmd_wrprotect(old_pmd);
diff --git arch/x86/include/asm/runtime-const.h arch/x86/include/asm/runtime-const.h
index 6652ebddfd..8bedcff799 100644
--- arch/x86/include/asm/runtime-const.h
+++ arch/x86/include/asm/runtime-const.h
@@ -2,6 +2,8 @@
 #ifndef _ASM_RUNTIME_CONST_H
 #define _ASM_RUNTIME_CONST_H
 
+#include <bhv/patch_base.h>
+
 #define runtime_const_ptr(sym) ({				\
 	typeof(sym) __ret;					\
 	asm_inline("mov %1,%0\n1:\n"				\
@@ -39,14 +41,14 @@
  * when the text section hasn't been marked RO, and before the text
  * has ever been executed.
  */
-static inline void __runtime_fixup_ptr(void *where, unsigned long val)
+static __always_inline void __runtime_fixup_ptr(void *where, unsigned long val)
 {
-	*(unsigned long *)where = val;
+	bhv_patch((void *)where, (void *)&val, sizeof(unsigned long));
 }
 
-static inline void __runtime_fixup_shift(void *where, unsigned long val)
+static __always_inline void __runtime_fixup_shift(void *where, unsigned long val)
 {
-	*(unsigned char *)where = val;
+	bhv_patch((void *)where, (void *)&val, sizeof(unsigned char));
 }
 
 static inline void runtime_const_fixup(void (*fn)(void *, unsigned long),
diff --git arch/x86/include/asm/switch_to.h arch/x86/include/asm/switch_to.h
index 7524854640..4814f36fcb 100644
--- arch/x86/include/asm/switch_to.h
+++ arch/x86/include/asm/switch_to.h
@@ -21,6 +21,9 @@ __visible void ret_from_fork(struct task_struct *prev, struct pt_regs *regs,
  * order of the fields must match the code in __switch_to_asm().
  */
 struct inactive_task_frame {
+#if defined(CONFIG_DOMAIN_SPACES)
+	unsigned long domain;
+#endif
 #ifdef CONFIG_X86_64
 	unsigned long r15;
 	unsigned long r14;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 6ab96bc764..bd71790895 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -35,6 +35,13 @@
 #include <asm/ibt.h>
 #include <asm/set_memory.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/domain.h>
+
+#include <bhv/vault.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
@@ -48,6 +55,18 @@ EXPORT_SYMBOL_GPL(alternatives_patched);
 #define DA_ENDBR	0x08
 #define DA_SMP		0x10
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, sync_core);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled, X86_FEATURE_RETPOLINE);
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled,
+			    X86_FEATURE_RETPOLINE_LFENCE);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
 static unsigned int debug_alternative;
 
 static int __init debug_alt(char *str)
@@ -271,6 +290,16 @@ u8 *its_static_thunk(int reg)
  * in the .altinstr_replacement section.
  */
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_alternatives(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_alternatives);
+#endif
+
 /*
  * Fill the buffer with a single effective instruction of size @len.
  *
@@ -280,6 +309,7 @@ u8 *its_static_thunk(int reg)
  * each single-byte NOPs). If @len to fill out is > ASM_NOP_MAX, pad with INT3 and
  * *jump* over instead of executing long and daft NOPs.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void add_nop(u8 *buf, unsigned int len)
 {
 	u8 *target = buf + len;
@@ -314,6 +344,7 @@ void text_poke_early(void *addr, const void *opcode, size_t len);
 /*
  * Matches NOP and NOPL, not any of the other possible NOPs.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool insn_is_nop(struct insn *insn)
 {
 	/* Anything NOP, but no REP NOP */
@@ -334,6 +365,7 @@ static bool insn_is_nop(struct insn *insn)
  * Find the offset of the first non-NOP instruction starting at @offset
  * but no further than @len.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int skip_nops(u8 *buf, int offset, int len)
 {
 	struct insn insn;
@@ -353,6 +385,7 @@ static int skip_nops(u8 *buf, int offset, int len)
  * "noinline" to cause control flow change and thus invalidate I$ and
  * cause refetch after modification.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void noinline optimize_nops(const u8 * const instr, u8 *buf, size_t len)
 {
 	for (int next, i = 0; i < len; i = next) {
@@ -580,13 +613,23 @@ static inline u8 * instr_va(struct alt_instr *i)
  * Marked "noinline" to cause control flow change and thus insn cache
  * to refetch changed I$ lines.
  */
-void __init_or_module noinline apply_alternatives(struct alt_instr *start,
-						  struct alt_instr *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_alternatives(struct alt_instr *start,
+						      struct alt_instr *end)
 {
 	u8 insn_buff[MAX_PATCH_LEN];
 	u8 *instr, *replacement;
 	struct alt_instr *a, *b;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif
+#endif
+
 	DPRINTK(ALT, "alt table %px, -> %px", start, end);
 
 	/*
@@ -619,7 +662,8 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 		 */
 		for (b = a+1; b < end && instr_va(b) == instr_va(a); b++) {
 			u8 len = max(a->instrlen, b->instrlen);
-			a->instrlen = b->instrlen = len;
+			bhv_patch(&a->instrlen, &len, sizeof(a->instrlen));
+			bhv_patch(&b->instrlen, &len, sizeof(b->instrlen));
 		}
 
 		instr = instr_va(a);
@@ -635,6 +679,9 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 		 */
 		if (!boot_cpu_has(a->cpuid) == !(a->flags & ALT_FLAG_NOT)) {
 			memcpy(insn_buff, instr, a->instrlen);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			insn_buff_sz = a->instrlen;
+#endif
 			optimize_nops(instr, insn_buff, a->instrlen);
 			text_poke_early(instr, insn_buff, a->instrlen);
 			continue;
@@ -664,12 +711,21 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 		DUMP_BYTES(ALT, replacement, a->replacementlen, "%px:   rpl_insn: ", replacement);
 		DUMP_BYTES(ALT, insn_buff, insn_buff_sz, "%px: final_insn: ", instr);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(instr, insn_buff, insn_buff_sz);
+		else
+			text_poke_early(instr, insn_buff, insn_buff_sz);
+#else
 		text_poke_early(instr, insn_buff, insn_buff_sz);
+#endif
 	}
 
 	kasan_enable_current();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool is_jcc32(struct insn *insn)
 {
 	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
@@ -681,6 +737,7 @@ static inline bool is_jcc32(struct insn *insn)
 /*
  * CALL/JMP *%\reg
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int emit_indirect(int op, int reg, u8 *bytes)
 {
 	int i = 0;
@@ -808,6 +865,7 @@ static bool cpu_wants_indirect_its_thunk_at(unsigned long addr, int reg)
  *
  * It also tries to inline spectre_v2=retpoline,lfence when size permits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 {
 	retpoline_thunk_t *target;
@@ -823,11 +881,16 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE() &&
+	    !bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE())
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
-	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+#endif
+	{
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
 			return emit_call_track_retpoline(addr, insn, reg, bytes);
-
 		return -1;
 	}
 
@@ -860,7 +923,11 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/*
 	 * For RETPOLINE_LFENCE: prepend the indirect CALL/JMP with an LFENCE.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE()) {
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+#endif
 		bytes[i++] = 0x0f;
 		bytes[i++] = 0xae;
 		bytes[i++] = 0xe8; /* LFENCE */
@@ -898,10 +965,23 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 /*
  * Generated by 'objtool --retpoline'.
  */
-void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
+
+/* XXX: Consider moving module_finalze in module.c into the vault. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_retpolines(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -939,10 +1019,18 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 			optimize_nops(addr, bytes, len);
 			DUMP_BYTES(RETPOLINE, ((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(RETPOLINE, ((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_retpolines);
 
 #ifdef CONFIG_MITIGATION_RETHUNK
 
@@ -972,6 +1060,7 @@ bool cpu_wants_rethunk_at(void *addr)
  *
  *   RET
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 {
 	int i = 0;
@@ -990,10 +1079,21 @@ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 	return i;
 }
 
-void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __bhv_init_or_module __apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_returns(s);
+		return;
+	}
+#endif /* !CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	if (cpu_wants_rethunk())
 		static_call_force_reinit();
 
@@ -1026,10 +1126,23 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 		if (len == insn.length) {
 			DUMP_BYTES(RET, ((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(RET, ((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_returns);
+
+void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+{
+	__apply_returns(start, end);
+}
 #else
 void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 #endif /* CONFIG_MITIGATION_RETHUNK */
@@ -1073,18 +1186,45 @@ static void __init_or_module poison_endbr(void *addr, bool warn)
  * Seal the functions for indirect calls by clobbering the ENDBR instructions
  * and the kCFI hash value.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void __init_or_module noinline apply_seal_endbr(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_ibt_endbr(s);
+		return;
+	}
+#endif //defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+
+#if defined(CONFIG_BHV_VAS) && defined(CONFIG_BHV_VAULT_SPACES)
+	u32 endbr;
+#endif //defined(CONFIG_BHV_VAS) && defined(CONFIG_BHV_VAULT_SPACES)
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 
+#if defined(CONFIG_BHV_VAS) && defined(CONFIG_BHV_VAULT_SPACES)
+		// Check data before overwriting it
+		if (get_kernel_nofault(endbr, addr)){
+			pr_err("Cannot check endbr\n");
+			return;
+		}
+
+		if (!is_endbr(endbr)){
+			pr_err("endbr is not endbr\n");
+			return;
+		}
+#endif //defined(CONFIG_BHV_VAS) && defined(CONFIG_BHV_VAULT_SPACES)
+
 		poison_endbr(addr, true);
 		if (IS_ENABLED(CONFIG_FINEIBT))
 			poison_cfi(addr - 16);
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_seal_endbr);
 
 #else
 
@@ -1570,6 +1710,7 @@ static void poison_cfi(void *addr)
 		break;
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_ibt_endbr);
 
 /*
  * regs->ip points to a UD2 instruction, return true and fill out target and
@@ -1620,8 +1761,9 @@ void apply_fineibt(s32 *start_retpoline, s32 *end_retpoline,
 }
 
 #ifdef CONFIG_SMP
-static void alternatives_smp_lock(const s32 *start, const s32 *end,
-				  u8 *text, u8 *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void alternatives_smp_lock(const s32 *start, const s32 *end, u8 *text,
+				  u8 *text_end)
 {
 	const s32 *poff;
 
@@ -1631,11 +1773,21 @@ static void alternatives_smp_lock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn DS segment override prefix into lock prefix */
-		if (*ptr == 0x3e)
-			text_poke(ptr, ((unsigned char []){0xf0}), 1);
+		if (*ptr == 0x3e) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0xf0 }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#endif
+		}
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 				    u8 *text, u8 *text_end)
 {
@@ -1647,11 +1799,19 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn lock prefix into DS segment override prefix */
-		if (*ptr == 0xf0)
-			text_poke(ptr, ((unsigned char []){0x3E}), 1);
+		if (*ptr == 0xf0) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0x3E }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#endif
+		}
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -1670,14 +1830,27 @@ struct smp_alt_module {
 static LIST_HEAD(smp_alt_modules);
 static bool uniproc_patched = false;	/* protected by text_mutex */
 
-void __init_or_module alternatives_smp_module_add(struct module *mod,
-						  char *name,
-						  void *locks, void *locks_end,
-						  void *text,  void *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_add(struct module *mod,
+						      char *name, void *locks,
+						      void *locks_end,
+						      void *text,
+						      void *text_end)
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	if (!uniproc_patched)
 		goto unlock;
 
@@ -1702,16 +1875,44 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
-	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end,
+							 locks, locks_end, text,
+							 text_end);
+			// Apply
+			smp_lock = false;
+			p.locks_begin = locks;
+			p.locks_end = locks_end;
+			bhv_alternatives_apply_custom_filter(
+				&p, &smp_lock, bhv_alternatives_find_by_lock);
+		}
+	} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
 unlock:
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_add);
 
-void __init_or_module alternatives_smp_module_del(struct module *mod)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_del(struct module *mod)
 {
 	struct smp_alt_module *item;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	list_for_each_entry(item, &smp_alt_modules, next) {
 		if (mod != item->mod)
 			continue;
@@ -1719,9 +1920,15 @@ void __init_or_module alternatives_smp_module_del(struct module *mod)
 		kfree(item);
 		break;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_del);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void alternatives_enable_smp(void)
 {
 	struct smp_alt_module *mod;
@@ -1729,25 +1936,50 @@ void alternatives_enable_smp(void)
 	/* Why bother if there are no other CPUs? */
 	BUG_ON(num_possible_cpus() == 1);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (uniproc_patched) {
 		pr_info("switching to SMP code\n");
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
-			alternatives_smp_lock(mod->locks, mod->locks_end,
-					      mod->text, mod->text_end);
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+		}
 		uniproc_patched = false;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_enable_smp);
 
 /*
  * Return 1 if the address range is reserved for SMP-alternatives.
  * Must hold text_mutex.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int alternatives_text_reserved(void *start, void *end)
 {
 	struct smp_alt_module *mod;
@@ -1770,6 +2002,7 @@ int alternatives_text_reserved(void *start, void *end)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_text_reserved);
 #endif /* CONFIG_SMP */
 
 /*
@@ -1788,6 +2021,7 @@ int alternatives_text_reserved(void *start, void *end)
  * convention such that we can 'call' it from assembly.
  */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 extern void int3_magic(unsigned int *ptr); /* defined in asm */
 
 asm (
@@ -1884,7 +2118,8 @@ static noinline void __init alt_reloc_selftest(void)
 	);
 }
 
-void __init alternative_instructions(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __alternative_instructions(void)
 {
 	u64 ibt;
 
@@ -1963,6 +2198,12 @@ void __init alternative_instructions(void)
 
 	alt_reloc_selftest();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __alternative_instructions);
+
+void __init alternative_instructions(void)
+{
+	__alternative_instructions();
+}
 
 /**
  * text_poke_early - Update instructions on a live kernel at boot time
@@ -1976,11 +2217,25 @@ void __init alternative_instructions(void)
  * instructions. And on the local CPU you need to be protected against NMI or
  * MCE handlers seeing an inconsistent instruction while you patch.
  */
-void __init_or_module text_poke_early(void *addr, const void *opcode,
-				      size_t len)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module text_poke_early(void *addr, const void *opcode,
+					  size_t len)
 {
 	unsigned long flags;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	if (boot_cpu_has(X86_FEATURE_NX) &&
 	    is_module_text_address((unsigned long)addr)) {
 		/*
@@ -1992,7 +2247,11 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 	} else {
 		local_irq_save(flags);
 		memcpy(addr, opcode, len);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_sync_core();
+#else
 		sync_core();
+#endif
 		local_irq_restore(flags);
 
 		/*
@@ -2001,6 +2260,7 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 		 */
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_early);
 
 typedef struct {
 	struct mm_struct *mm;
@@ -2019,6 +2279,7 @@ typedef struct {
  *          loaded, thereby preventing interrupt handler bugs from overriding
  *          the kernel memory protection.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 {
 	temp_mm_state_t temp_state;
@@ -2035,6 +2296,9 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 	switch_mm_irqs_off(NULL, mm, current);
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -2053,10 +2317,14 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 	return temp_state;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_enter(prev_state.mm == NULL ? NULL : prev_state.mm->owner);
+#endif
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
@@ -2069,11 +2337,13 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 __ro_after_init struct mm_struct *poking_mm;
 __ro_after_init unsigned long poking_addr;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_memcpy(void *dst, const void *src, size_t len)
 {
 	memcpy(dst, src, len);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_memset(void *dst, const void *src, size_t len)
 {
 	int c = *(const int *)src;
@@ -2083,6 +2353,7 @@ static void text_poke_memset(void *dst, const void *src, size_t len)
 
 typedef void text_poke_f(void *dst, const void *src, size_t len);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t len)
 {
 	bool cross_page_boundary = offset_in_page(addr) + len > PAGE_SIZE;
@@ -2148,7 +2419,22 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
 	prev = use_temporary_mm(poking_mm);
 
 	kasan_disable_current();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives((u8 *)poking_addr + offset_in_page(addr),
+				       src, len);
+	} else {
+		if (bhv_integrity_is_enabled())
+			bhv_apply_alternatives((u8 *)poking_addr +
+						       offset_in_page(addr),
+					       src, len);
+		else
+			func((u8 *)poking_addr + offset_in_page(addr), src,
+			     len);
+	}
+#else
 	func((u8 *)poking_addr + offset_in_page(addr), src, len);
+#endif
 	kasan_enable_current();
 
 	/*
@@ -2188,6 +2474,7 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
 	pte_unmap_unlock(ptep, ptl);
 	return addr;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __text_poke);
 
 /**
  * text_poke - Update instructions on a live kernel
@@ -2205,6 +2492,7 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
  * by registering a module notifier, and ordering module removal and patching
  * through a mutex.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *text_poke(void *addr, const void *opcode, size_t len)
 {
 	lockdep_assert_held(&text_mutex);
@@ -2303,11 +2591,17 @@ void *text_poke_set(void *addr, int c, size_t len)
 	return addr;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_sync_core(void *info)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_sync_core();
+#else
 	sync_core();
+#endif
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void text_poke_sync(void)
 {
 	on_each_cpu(do_sync_core, NULL, 1);
@@ -2337,6 +2631,7 @@ struct bp_patching_desc {
 
 static struct bp_patching_desc bp_desc;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline
 struct bp_patching_desc *try_get_desc(void)
 {
@@ -2348,6 +2643,7 @@ struct bp_patching_desc *try_get_desc(void)
 	return desc;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void put_desc(void)
 {
 	struct bp_patching_desc *desc = &bp_desc;
@@ -2356,11 +2652,13 @@ static __always_inline void put_desc(void)
 	raw_atomic_dec(&desc->refs);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void *text_poke_addr(struct text_poke_loc *tp)
 {
 	return _stext + tp->rel_addr;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline int patch_cmp(const void *key, const void *elt)
 {
 	struct text_poke_loc *tp = (struct text_poke_loc *) elt;
@@ -2455,7 +2753,9 @@ noinstr int poke_int3_handler(struct pt_regs *regs)
 }
 
 #define TP_VEC_MAX (PAGE_SIZE / sizeof(struct text_poke_loc))
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct text_poke_loc tp_vec[TP_VEC_MAX];
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static int tp_vec_nr;
 
 /**
@@ -2479,6 +2779,7 @@ static int tp_vec_nr;
  *		  replacing opcode
  *	- sync cores
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries)
 {
 	unsigned char int3 = INT3_INSN_OPCODE;
@@ -2612,6 +2913,7 @@ static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries
 		atomic_cond_read_acquire(&bp_desc.refs, !VAL);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
 			       const void *opcode, size_t len, const void *emulate)
 {
@@ -2691,6 +2993,7 @@ static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
  * We hard rely on the tp_vec being ordered; ensure this is so by flushing
  * early if needed.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool tp_order_fail(void *addr)
 {
 	struct text_poke_loc *tp;
@@ -2708,6 +3011,7 @@ static bool tp_order_fail(void *addr)
 	return false;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_flush(void *addr)
 {
 	if (tp_vec_nr == TP_VEC_MAX || tp_order_fail(addr)) {
@@ -2716,20 +3020,46 @@ static void text_poke_flush(void *addr)
 	}
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void text_poke_finish(void)
+{
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		text_poke_flush(NULL);
+	}
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_finish);
+#else
 void text_poke_finish(void)
 {
 	text_poke_flush(NULL);
 }
+#endif
 
-void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc *tp;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	text_poke_flush(addr);
 
 	tp = &tp_vec[tp_vec_nr++];
 	text_poke_loc_init(tp, addr, opcode, len, emulate);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_queue);
 
 /**
  * text_poke_bp() -- update instructions on live kernel on SMP
@@ -2742,10 +3072,25 @@ void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const voi
  * dynamically allocated memory. This function should be used when it is
  * not possible to allocate memory.
  */
-void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc tp;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	text_poke_loc_init(&tp, addr, opcode, len, emulate);
 	text_poke_bp_batch(&tp, 1);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_bp);
diff --git arch/x86/kernel/cpu/common.c arch/x86/kernel/cpu/common.c
index 976545ec8f..bec8db3d57 100644
--- arch/x86/kernel/cpu/common.c
+++ arch/x86/kernel/cpu/common.c
@@ -400,8 +400,9 @@ static __always_inline void setup_umip(struct cpuinfo_x86 *c)
 }
 
 /* These bits should not change their value after CPU init is finished. */
-static const unsigned long cr4_pinned_mask = X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_UMIP |
-					     X86_CR4_FSGSBASE | X86_CR4_CET | X86_CR4_FRED;
+const unsigned long cr4_pinned_mask = X86_CR4_SMEP | X86_CR4_SMAP |
+				      X86_CR4_UMIP | X86_CR4_FSGSBASE |
+				      X86_CR4_CET | X86_CR4_FRED;
 static DEFINE_STATIC_KEY_FALSE_RO(cr_pinning);
 static unsigned long cr4_pinned_bits __ro_after_init;
 
@@ -2397,6 +2398,23 @@ void arch_smt_update(void)
 	apic_smt_update();
 }
 
+#ifdef CONFIG_X86_64
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void arch_cpu_finalize_init_runtime_const_init(void)
+{
+	unsigned long USER_PTR_MAX = TASK_SIZE_MAX;
+
+	/*
+	 * Enable this when LAM is gated on LASS support
+	if (cpu_feature_enabled(X86_FEATURE_LAM))
+		USER_PTR_MAX = (1ul << 63) - PAGE_SIZE;
+	 */
+	runtime_const_init(ptr, USER_PTR_MAX);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label,
+			  arch_cpu_finalize_init_runtime_const_init);
+#endif // CONFIG_X86_64
+
 void __init arch_cpu_finalize_init(void)
 {
 	struct cpuinfo_x86 *c = this_cpu_ptr(&cpu_info);
@@ -2449,14 +2467,7 @@ void __init arch_cpu_finalize_init(void)
 	alternative_instructions();
 
 	if (IS_ENABLED(CONFIG_X86_64)) {
-		unsigned long USER_PTR_MAX = TASK_SIZE_MAX;
-
-		/*
-		 * Enable this when LAM is gated on LASS support
-		if (cpu_feature_enabled(X86_FEATURE_LAM))
-			USER_PTR_MAX = (1ul << 63) - PAGE_SIZE;
-		 */
-		runtime_const_init(ptr, USER_PTR_MAX);
+		arch_cpu_finalize_init_runtime_const_init();
 
 		/*
 		 * Make sure the first 2MB area is not mapped by huge pages
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3a..20bfa373b0 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index f445bec516..b72bc81f4a 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -12,6 +12,7 @@
 #include <asm/hw_irq.h>
 #include <asm/ia32.h>
 #include <asm/idtentry.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -311,6 +312,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index f5b8ef02d1..603e973c2f 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -16,22 +16,52 @@
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
 #include <asm/insn.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
 
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER2_NORET(void, insn_decode_kernel, struct insn *, insn, void *, entry);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_jump_label(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_jump_label);
+#endif
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int arch_jump_entry_size(struct jump_entry *entry)
 {
 	struct insn insn = {};
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_insn_decode_kernel(&insn, (void *)jump_entry_code(entry));
+#else
 	insn_decode_kernel(&insn, (void *)jump_entry_code(entry));
+#endif
 	BUG_ON(insn.length != 2 && insn.length != 5);
 
 	return insn.length;
 }
 
+#if !defined(CONFIG_BHV_VAS) || !defined(CONFIG_BHV_VAULT_SPACES)
 struct jump_label_patch {
 	const void *code;
 	int size;
 };
+#endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static struct jump_label_patch
 __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 {
@@ -78,14 +108,33 @@ __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 
 	return (struct jump_label_patch){.code = code, .size = size};
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_patch);
 
-static __always_inline void
-__jump_label_transform(struct jump_entry *entry,
-		       enum jump_label_type type,
-		       int init)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type,
+					  int init)
 {
 	const struct jump_label_patch jlp = __jump_label_patch(entry, type);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+#else
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+#endif
+
 	/*
 	 * As long as only a single processor is running and the code is still
 	 * not marked as RO, text_poke_early() can be used; Checking that
@@ -105,21 +154,32 @@ __jump_label_transform(struct jump_entry *entry,
 	text_poke_bp((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
 }
 
-static void __ref jump_label_transform(struct jump_entry *entry,
-				       enum jump_label_type type,
-				       int init)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void jump_label_transform(struct jump_entry *entry,
+				 enum jump_label_type type,
+				 int init)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	__jump_label_transform(entry, type, init);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
 	jump_label_transform(entry, type, 0);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool arch_jump_label_transform_queue(struct jump_entry *entry,
 				     enum jump_label_type type)
 {
@@ -133,16 +193,65 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		return true;
 	}
 
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#else /* !CONFIG_BHV_VAULT_SPACES */
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_SPACES */
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	jlp = __jump_label_patch(entry, type);
 	text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void arch_jump_label_transform_apply(void)
+{
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		bhv_wrapper_mutex_lock(&text_mutex);
+#else
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+#endif
+		text_poke_finish();
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_mutex_unlock(&text_mutex);
+#else
+		mutex_unlock(&text_mutex);
+#endif
+	}
+}
+#else /* !CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index 1e231dac61..1813185dde 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -230,6 +233,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*calls = NULL, *cfi = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".altinstructions", secstrings + s->sh_name))
 			alt = s;
@@ -280,9 +289,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = me->mem[MOD_TEXT].base;
+		arch.text_end = me->mem[MOD_TEXT].base + me->mem[MOD_TEXT].size;
+	}
+#endif
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (calls || alt) {
diff --git arch/x86/kernel/paravirt.c arch/x86/kernel/paravirt.c
index 0c1b915d7e..6504d09c8a 100644
--- arch/x86/kernel/paravirt.c
+++ arch/x86/kernel/paravirt.c
@@ -34,6 +34,8 @@
 #include <asm/io_bitmap.h>
 #include <asm/gsseg.h>
 
+#include <bhv/vault.h>
+
 /* stub always returning 0. */
 DEFINE_ASM_FUNC(paravirt_ret0, "xor %eax,%eax", .entry.text);
 
diff --git arch/x86/kernel/process.c arch/x86/kernel/process.c
index 4c9c98c5de..1117bd373e 100644
--- arch/x86/kernel/process.c
+++ arch/x86/kernel/process.c
@@ -53,6 +53,8 @@
 #include <asm/mmu_context.h>
 #include <asm/shstk.h>
 
+#include <bhv/domain.h>
+
 #include "process.h"
 
 /*
@@ -217,6 +219,10 @@ int copy_thread(struct task_struct *p, const struct kernel_clone_args *args)
 	if (IS_ERR_VALUE(new_ssp))
 		return PTR_ERR((void *)new_ssp);
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	frame->domain = brs_get_domain(p);
+#endif
+
 	fpu_clone(p, clone_flags, args->fn, new_ssp);
 
 	/* Kernel thread ? */
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index aae909d4ed..abbba0c2a9 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,6 +4,19 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(void, mutex_unlock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(bool, cpu_feature_enabled, u16, feature);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_VAULT_SPACES)
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
@@ -11,21 +24,36 @@ enum insn_type {
 	RET = 3,  /* tramp / site cond-tail-call */
 	JCC = 4,
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
  * that there is no false-positive trampoline identification while also being a
  * speculation stop.
  */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 tramp_ud[] = { 0x0f, 0xb9, 0xcc };
 
 /*
  * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
  */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
 
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_static_call(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_static_call);
+#endif
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
 {
 	u8 ret = 0;
@@ -50,8 +78,9 @@ asm (".global __static_call_return\n\t"
      "ret; int3\n\t"
      ".size __static_call_return, . - __static_call_return \n\t");
 
-static void __ref __static_call_transform(void *insn, enum insn_type type,
-					  void *func, bool modinit)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void __static_call_transform(void *insn, enum insn_type type,
+				    void *func, bool modinit)
 {
 	const void *emulate = NULL;
 	int size = CALL_INSN_SIZE;
@@ -61,6 +90,13 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	if ((type == JMP || type == RET) && (op = __is_Jcc(insn)))
 		type = JCC;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func, modinit);
+		return;
+	}
+#endif
+
 	switch (type) {
 	case CALL:
 		func = callthunks_translate_call_dest(func);
@@ -105,12 +141,21 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	if (memcmp(insn, code, size) == 0)
 		return;
 
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(insn, code, size);
+		return;
+	}
+#endif
+
 	if (system_state == SYSTEM_BOOTING || modinit)
 		return text_poke_early(insn, code, size);
 
 	text_poke_bp(insn, code, size, emulate);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_call_validate(u8 *insn, bool tail, bool tramp)
 {
 	u8 opcode = insn[0];
@@ -139,6 +184,7 @@ static void __static_call_validate(u8 *insn, bool tail, bool tramp)
 	BUG();
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline enum insn_type __sc_insn(bool null, bool tail)
 {
 	/*
@@ -154,9 +200,14 @@ static inline enum insn_type __sc_insn(bool null, bool tail)
 	return 2*tail + null;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (tramp) {
 		__static_call_validate(tramp, true, true);
@@ -168,8 +219,13 @@ void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 		__static_call_transform(site, __sc_insn(!func, tail), func, false);
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, arch_static_call_transform);
 EXPORT_SYMBOL_GPL(arch_static_call_transform);
 
 noinstr void __static_call_update_early(void *tramp, void *func)
@@ -192,6 +248,7 @@ noinstr void __static_call_update_early(void *tramp, void *func)
  * This means that __static_call_transform() above can have overwritten the
  * return trampoline and we now need to fix things up to be consistent.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool __static_call_fixup(void *tramp, u8 op, void *dest)
 {
 	unsigned long addr = (unsigned long)tramp;
@@ -212,11 +269,27 @@ bool __static_call_fixup(void *tramp, u8 op, void *dest)
 		return false;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
+
 	if (op == RET_INSN_OPCODE || dest == &__x86_return_thunk)
 		__static_call_transform(tramp, RET, NULL, true);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 
 	return true;
 }
+/*
+ * XXX: REMOVE ENTRY POINT AS SOON AS WE ADD ALTERNATIVE INSTRUCTIONS INTO THE
+ * VAULT.
+ */
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_fixup);
+
 #endif
diff --git arch/x86/kernel/traps.c arch/x86/kernel/traps.c
index 243f3bc9b4..99b4e8f7d1 100644
--- arch/x86/kernel/traps.c
+++ arch/x86/kernel/traps.c
@@ -30,6 +30,7 @@
 #include <linux/errno.h>
 #include <linux/kexec.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/timer.h>
 #include <linux/init.h>
@@ -80,6 +81,16 @@
 
 DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
+__always_inline static bool is_vault(unsigned long addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
+
 __always_inline int is_valid_bugaddr(unsigned long addr)
 {
 	if (addr < TASK_SIZE_MAX)
@@ -286,6 +297,11 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 	int ud_type, ud_len;
 	s32 ud_imm;
 
+	if (is_vault(regs->ip)){
+		show_regs(regs);
+		panic("[%s] BUG in vault at %pS\n", __FUNCTION__, (void*)regs->ip);
+	}
+
 	ud_type = decode_bug(regs->ip, &ud_imm, &ud_len);
 	if (ud_type == BUG_NONE)
 		return handled;
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index e2567c8f6b..bd2bc471a9 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -129,8 +129,16 @@ SECTIONS
 		KPROBES_TEXT
 		SOFTIRQENTRY_TEXT
 #ifdef CONFIG_MITIGATION_RETPOLINE
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+		__indirect_thunk_start = .;
+#endif
 		*(.text..__x86.indirect_thunk)
 		*(.text..__x86.return_thunk)
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+		__indirect_thunk_end = .;
+#endif
 #endif
 		STATIC_CALL_TEXT
 
@@ -147,8 +155,13 @@ SECTIONS
 		*(.text..__x86.rethunk_safe)
 #endif
 		ALIGN_ENTRY_TEXT_END
+		BHV_TEXT
 		*(.gnu.warning)
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+		BHV_VAULT_TEXT(jump_label)
+#endif
+
 	} :text = 0xcccccccc
 
 	/* End of text section, which should occupy whole number of pages */
@@ -245,8 +258,16 @@ SECTIONS
 	 *
 	 * See static_cpu_has() for an example.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+#endif
 	.altinstr_aux : AT(ADDR(.altinstr_aux) - LOAD_OFFSET) {
+		__altinstr_aux_start = .;
 		*(.altinstr_aux)
+		__altinstr_aux_end = .;
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 	}
 
 	INIT_DATA_SECTION(16)
@@ -272,18 +293,29 @@ SECTIONS
 	 * __x86_indirect_thunk_*(). These instructions can be patched along
 	 * with alternatives, after which the section can be freed.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {
 		__retpoline_sites = .;
 		*(.retpoline_sites)
 		__retpoline_sites_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	.return_sites : AT(ADDR(.return_sites) - LOAD_OFFSET) {
 		__return_sites = .;
 		*(.return_sites)
 		__return_sites_end = .;
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 	}
 
 	. = ALIGN(8);
@@ -317,11 +349,18 @@ SECTIONS
 	 * "Alternative instructions for different CPU types or capabilities"
 	 * Think locking instructions on spinlocks.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	.altinstructions : AT(ADDR(.altinstructions) - LOAD_OFFSET) {
 		__alt_instructions = .;
 		*(.altinstructions)
 		__alt_instructions_end = .;
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 	}
 
 	/*
@@ -340,15 +379,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -385,6 +438,34 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.comm : AT(ADDR(.bhv.vault.comm) - LOAD_OFFSET) {
+		__bhv_vault_comm_start = .;
+		. += PAGE_SIZE;
+		. = ALIGN(PAGE_SIZE);
+		__bhv_vault_comm_end = .;
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.data : AT(ADDR(.bhv.vault.data) - LOAD_OFFSET) {
+		BHV_VAULT_DATA(jump_label)
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.rodata : AT(ADDR(.bhv.vault.rodata) - LOAD_OFFSET) {
+		BHV_VAULT_RO_DATA(jump_label)
+	}
+#endif
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/lib/inat.c arch/x86/lib/inat.c
index b0f3b2a62a..7d5351f4f3 100644
--- arch/x86/lib/inat.c
+++ arch/x86/lib/inat.c
@@ -6,15 +6,19 @@
  */
 #include <asm/insn.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 /* Attribute tables are generated from opcode map */
 #include "inat-tables.c"
 
 /* Attribute search APIs */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
 {
 	return inat_primary_table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int inat_get_last_prefix_id(insn_byte_t last_pfx)
 {
 	insn_attr_t lpfx_attr;
@@ -23,6 +27,7 @@ int inat_get_last_prefix_id(insn_byte_t last_pfx)
 	return inat_last_prefix_id(lpfx_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 				      insn_attr_t esc_attr)
 {
@@ -42,6 +47,7 @@ insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 	return table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 				     insn_attr_t grp_attr)
 {
@@ -62,6 +68,7 @@ insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 	       inat_group_common_attribute(grp_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
 				   insn_byte_t vex_p)
 {
diff --git arch/x86/lib/insn.c arch/x86/lib/insn.c
index 6ffb931b9f..e7dc0176b2 100644
--- arch/x86/lib/insn.c
+++ arch/x86/lib/insn.c
@@ -20,6 +20,8 @@
 
 #include <asm/emulate_prefix.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 #define leXX_to_cpu(t, r)						\
 ({									\
 	__typeof__(t) v;						\
@@ -58,6 +60,7 @@
  * @buf_len:	length of the insn buffer at @kaddr
  * @x86_64:	!0 for 64-bit kernel or 64-bit app
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 {
 	/*
@@ -82,6 +85,7 @@ void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 static const insn_byte_t xen_prefix[] = { __XEN_EMULATE_PREFIX };
 static const insn_byte_t kvm_prefix[] = { __KVM_EMULATE_PREFIX };
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __insn_get_emulate_prefix(struct insn *insn,
 				     const insn_byte_t *prefix, size_t len)
 {
@@ -101,6 +105,7 @@ static int __insn_get_emulate_prefix(struct insn *insn,
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void insn_get_emulate_prefix(struct insn *insn)
 {
 	if (__insn_get_emulate_prefix(insn, xen_prefix, sizeof(xen_prefix)))
@@ -121,6 +126,7 @@ static void insn_get_emulate_prefix(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_prefixes(struct insn *insn)
 {
 	struct insn_field *prefixes = &insn->prefixes;
@@ -270,6 +276,7 @@ int insn_get_prefixes(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_opcode(struct insn *insn)
 {
 	struct insn_field *opcode = &insn->opcode;
@@ -357,6 +364,7 @@ int insn_get_opcode(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_modrm(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -403,6 +411,7 @@ int insn_get_modrm(struct insn *insn)
  * If necessary, first collects the instruction up to and including the
  * ModRM byte.  No effect if @insn->x86_64 is 0.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_rip_relative(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -432,6 +441,7 @@ int insn_rip_relative(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_sib(struct insn *insn)
 {
 	insn_byte_t modrm;
@@ -473,6 +483,7 @@ int insn_get_sib(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_displacement(struct insn *insn)
 {
 	insn_byte_t mod, rm, base;
@@ -533,6 +544,7 @@ int insn_get_displacement(struct insn *insn)
 }
 
 /* Decode moffset16/32/64. Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_moffset(struct insn *insn)
 {
 	switch (insn->addr_bytes) {
@@ -558,6 +570,7 @@ static int __get_moffset(struct insn *insn)
 }
 
 /* Decode imm v32(Iz). Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv32(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -579,6 +592,7 @@ static int __get_immv32(struct insn *insn)
 }
 
 /* Decode imm v64(Iv/Ov), Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -604,6 +618,7 @@ static int __get_immv(struct insn *insn)
 }
 
 /* Decode ptr16:16/32(Ap) */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immptr(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -640,6 +655,7 @@ static int __get_immptr(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_immediate(struct insn *insn)
 {
 	int ret;
@@ -713,6 +729,7 @@ int insn_get_immediate(struct insn *insn)
  *  - 0 on success
  *  - < 0 on error
 */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_length(struct insn *insn)
 {
 	int ret;
@@ -731,6 +748,7 @@ int insn_get_length(struct insn *insn)
 }
 
 /* Ensure this instruction is decoded completely */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static inline int insn_complete(struct insn *insn)
 {
 	return insn->opcode.got && insn->modrm.got && insn->sib.got &&
@@ -748,6 +766,7 @@ static inline int insn_complete(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_decode(struct insn *insn, const void *kaddr, int buf_len, enum insn_mode m)
 {
 	int ret;
diff --git arch/x86/lib/memcpy_64.S arch/x86/lib/memcpy_64.S
index 0ae2e1712e..ae6b8b5c11 100644
--- arch/x86/lib/memcpy_64.S
+++ arch/x86/lib/memcpy_64.S
@@ -1,6 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
 /* Copyright 2002 Andi Kleen */
 
+#include <bhv/vault.h>
 #include <linux/export.h>
 #include <linux/linkage.h>
 #include <linux/cfi_types.h>
@@ -8,7 +9,18 @@
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE jump_label
+#else
 .section .noinstr.text, "ax"
+#endif
+
+/*
+ * We build a jump to memcpy_orig by default which gets NOPped out on
+ * the majority of x86 CPUs which set REP_GOOD. In addition, CPUs which
+ * have the enhanced REP MOVSB/STOSB feature (ERMS), change those NOPs
+ * to a jmp to memcpy_erms which does the REP; MOVSB mem copy.
+ */
 
 /*
  * memcpy - Copy a memory block.
@@ -170,3 +182,6 @@ SYM_FUNC_START_LOCAL(memcpy_orig)
 	RET
 SYM_FUNC_END(memcpy_orig)
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_ASM_POP_SECTION
+#endif
diff --git arch/x86/lib/memset_64.S arch/x86/lib/memset_64.S
index 0199d56cb4..5da9b6b915 100644
--- arch/x86/lib/memset_64.S
+++ arch/x86/lib/memset_64.S
@@ -1,6 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /* Copyright 2002 Andi Kleen, SuSE Labs */
 
+#include <bhv/vault.h>
 #include <linux/export.h>
 #include <linux/linkage.h>
 #include <asm/cpufeatures.h>
@@ -8,6 +9,8 @@
 
 .section .noinstr.text, "ax"
 
+BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE jump_label
+
 /*
  * ISO C memset - set a memory block to a byte value. This function uses fast
  * string to get better performance than the original function. The code is
@@ -115,3 +118,5 @@ SYM_FUNC_START_LOCAL(memset_orig)
 	jmp .Lafter_bad_alignment
 .Lfinal:
 SYM_FUNC_END(memset_orig)
+
+BHV_ASM_POP_SECTION
diff --git arch/x86/mm/fault.c arch/x86/mm/fault.c
index ac52255fab..56d921194f 100644
--- arch/x86/mm/fault.c
+++ arch/x86/mm/fault.c
@@ -38,6 +38,8 @@
 #include <asm/fred.h>
 #include <asm/sev.h>			/* snp_dump_hva_rmpentry()	*/
 
+#include <bhv/interface/abi_base_autogen.h>
+
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
@@ -510,6 +512,27 @@ static void show_ldttss(const struct desc_ptr *gdt, const char *name, u16 index)
 static void
 show_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
+	// is kernel?
+	if (!(error_code & X86_PF_USER)) {
+		// is perm violation?
+		if ((error_code & X86_PF_PROT) &&
+		    !(error_code & X86_PF_RSVD) &&
+		    !(error_code & X86_PF_PK)) {
+			uint8_t type;
+			if (error_code &
+			    X86_PF_INSTR) { // is instr fetch?
+				type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+			} else if (error_code &
+				   X86_PF_WRITE) { // is write?
+				type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+			} else {
+				type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+			}
+			security_kaccess((uint64_t)address, type);
+		}
+	}
+
+
 	if (!oops_may_print())
 		return;
 
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index 9cbc1e6057..e9e703de46 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -37,6 +37,9 @@
 
 #include "mm_internal.h"
 
+#include <bhv/init/start.h>
+#include <bhv/vault.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -971,12 +974,72 @@ void free_kernel_image_pages(const char *what, void *begin, void *end)
 		set_memory_np_noalias(begin_ul, len_pages);
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __altinstr_aux_start[];
+extern char __altinstr_aux_end[];
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+static void __ref bhv_vault_release_memory(void)
+{
+	int rc;
+	HypABI__Wagner__Delete__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return;
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+#ifdef CONFIG_MITIGATION_RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end - (unsigned long)__retpoline_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end - (unsigned long)__return_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+#endif
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__altinstr_aux_start);
+	vault.mem.size = (unsigned long)__altinstr_aux_end - (unsigned long)__altinstr_aux_start;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+}
+
+#endif
+
 void __ref free_initmem(void)
 {
 	e820__reallocate_tables();
 
 	mem_encrypt_free_decrypted_mem();
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_vault_release_memory();
+#endif
+
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git arch/x86/mm/pat/memtype.c arch/x86/mm/pat/memtype.c
index d721cc19ad..3c550265b1 100644
--- arch/x86/mm/pat/memtype.c
+++ arch/x86/mm/pat/memtype.c
@@ -278,7 +278,7 @@ void __init pat_bp_init(void)
 		 * NOTE: When WC or WP is used, it is redirected to UC- per
 		 * the default setup in __cachemode2pte_tbl[].
 		 */
-		pat_msr_val = PAT_VALUE(WB, WT, UC_MINUS, UC, WB, WT, UC_MINUS, UC);
+		pat_msr_val = PAT_VALUE(WB, WT, WC, UC, WB, WT, UC_MINUS, UC);
 	}
 
 	/*
diff --git arch/x86/mm/pgtable.c arch/x86/mm/pgtable.c
index 5745a354a2..c23246e38b 100644
--- arch/x86/mm/pgtable.c
+++ arch/x86/mm/pgtable.c
@@ -7,6 +7,10 @@
 #include <asm/fixmap.h>
 #include <asm/mtrr.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 phys_addr_t physical_mask __ro_after_init = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;
 EXPORT_SYMBOL(physical_mask);
@@ -505,8 +509,12 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 {
 	int changed = !pte_same(*ptep, entry);
 
-	if (changed && dirty)
+	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		brs_domain_set_pte_at(vma->vm_mm, address, ptep, entry);
+#endif
 		set_pte(ptep, entry);
+	}
 
 	return changed;
 }
@@ -521,6 +529,9 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		brs_domain_set_pmd_at(vma->vm_mm, address, pmdp, entry);
+#endif
 		set_pmd(pmdp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pmd
@@ -541,6 +552,9 @@ int pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,
 	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		brs_domain_set_pud_at(vma->vm_mm, address, pudp, entry);
+#endif
 		set_pud(pudp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pud
diff --git arch/x86/mm/tlb.c arch/x86/mm/tlb.c
index 8629d90fdc..3cd523ea83 100644
--- arch/x86/mm/tlb.c
+++ arch/x86/mm/tlb.c
@@ -22,6 +22,8 @@
 #include <asm/perf_event.h>
 #include <asm/tlb.h>
 
+#include <bhv/domain.h>
+
 #include "mm_internal.h"
 
 #ifdef CONFIG_PARAVIRT
@@ -327,6 +329,9 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	local_irq_save(flags);
 	switch_mm_irqs_off(NULL, next, tsk);
 	local_irq_restore(flags);
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_enter(next == NULL ? NULL : next->owner);
+#endif
 }
 
 /*
diff --git arch/x86/net/bpf_jit_comp.c arch/x86/net/bpf_jit_comp.c
index ccb2f7703c..91300e4d3f 100644
--- arch/x86/net/bpf_jit_comp.c
+++ arch/x86/net/bpf_jit_comp.c
@@ -21,6 +21,9 @@
 
 static bool all_callee_regs_used[4] = {true, true, true, true};
 
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 static u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)
 {
 	if (len == 1)
@@ -303,6 +306,11 @@ static void jit_fill_hole(void *area, unsigned int size)
 
 int bpf_arch_text_invalidate(void *dst, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		return bhv_bpf_invalidate(dst, 0xcc, len);
+	}
+#endif /* CONFIG_BHV_VAS */
 	return IS_ERR_OR_NULL(text_poke_set(dst, 0xcc, len));
 }
 
@@ -3562,6 +3570,13 @@ bool bpf_jit_supports_kfunc_call(void)
 
 void *bpf_arch_text_copy(void *dst, void *src, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		if (bhv_bpf_write(dst, src, len))
+			return ERR_PTR(-EINVAL);
+		return dst;
+	}
+#endif /* CONFIG_BHV_VAS */
 	if (text_poke_copy(dst, src, len) == NULL)
 		return ERR_PTR(-EINVAL);
 	return dst;
diff --git certs/blacklist.c certs/blacklist.c
index 675dd7a8f0..fbc678b962 100644
--- certs/blacklist.c
+++ certs/blacklist.c
@@ -32,6 +32,8 @@
 static const char tbs_prefix[] = "tbs";
 static const char bin_prefix[] = "bin";
 
+#include <bhv/keyring.h>
+
 static struct key *blacklist_keyring;
 
 #ifdef CONFIG_SYSTEM_REVOCATION_LIST
@@ -287,7 +289,9 @@ int add_key_to_revocation_list(const char *data, size_t size)
  */
 int is_key_on_revocation_list(struct pkcs7_message *pkcs7)
 {
-	int ret;
+	int ret = bhv_keyring_verify(blacklist_keyring, &blacklist_keyring);
+	if (ret)
+		return -EPERM;
 
 	ret = pkcs7_validate_trust(pkcs7, blacklist_keyring);
 
@@ -350,6 +354,10 @@ static int __init blacklist_init(void)
 	for (bl = blacklist_hashes; *bl; bl++)
 		if (mark_raw_hash_blacklisted(*bl) < 0)
 			pr_err("- blacklisting failed\n");
+
+	if (bhv_keyring_register_system_trusted(&blacklist_keyring))
+		panic("Can't register system blacklist keyring\n");
+
 	return 0;
 }
 
diff --git certs/system_keyring.c certs/system_keyring.c
index 9de610bf1f..81960794f1 100644
--- certs/system_keyring.c
+++ certs/system_keyring.c
@@ -17,6 +17,8 @@
 #include <keys/system_keyring.h>
 #include <crypto/pkcs7.h>
 
+#include <bhv/keyring.h>
+
 static struct key *builtin_trusted_keys;
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 static struct key *secondary_trusted_keys;
@@ -47,6 +49,11 @@ int restrict_link_by_builtin_trusted(struct key *dest_keyring,
 				     const union key_payload *payload,
 				     struct key *restriction_key)
 {
+	int rc = bhv_keyring_verify_locked(builtin_trusted_keys,
+					   &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  builtin_trusted_keys);
 }
@@ -90,6 +97,8 @@ int restrict_link_by_builtin_and_secondary_trusted(
 	const union key_payload *payload,
 	struct key *restrict_key)
 {
+	int rc = 0;
+
 	/* If we have a secondary trusted keyring, then that contains a link
 	 * through to the builtin keyring and the search will follow that link.
 	 */
@@ -99,6 +108,12 @@ int restrict_link_by_builtin_and_secondary_trusted(
 		/* Allow the builtin keyring to be added to the secondary */
 		return 0;
 
+	rc = bhv_keyring_verify_locked(secondary_trusted_keys,
+				       &secondary_trusted_keys);
+	if (rc)
+		return rc;
+
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  secondary_trusted_keys);
 }
@@ -190,6 +205,9 @@ void __init set_machine_trusted_keys(struct key *keyring)
 
 	if (key_link(secondary_trusted_keys, machine_trusted_keys) < 0)
 		panic("Can't link (machine) trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&machine_trusted_keys))
+		panic("Can't register machine trusted keyring\n");
 }
 
 /**
@@ -237,6 +255,9 @@ static __init int system_trusted_keyring_init(void)
 	if (IS_ERR(builtin_trusted_keys))
 		panic("Can't allocate builtin trusted keyring\n");
 
+	if (bhv_keyring_register_system_trusted(&builtin_trusted_keys))
+		panic("Can't register builtin trusted keyring\n");
+
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 	secondary_trusted_keys =
 		keyring_alloc(".secondary_trusted_keys",
@@ -252,6 +273,9 @@ static __init int system_trusted_keyring_init(void)
 
 	if (key_link(secondary_trusted_keys, builtin_trusted_keys) < 0)
 		panic("Can't link trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&secondary_trusted_keys))
+		panic("Can't register secondary trusted keyring\n");
 #endif
 
 	return 0;
@@ -280,6 +304,10 @@ static __init int load_system_certificate_list(void)
 {
 	const u8 *p;
 	unsigned long size;
+	int rc = bhv_keyring_verify(builtin_trusted_keys,
+				    &builtin_trusted_keys);
+	if (rc)
+		return rc;
 
 	pr_notice("Loading compiled-in X.509 certificates\n");
 
@@ -337,15 +365,35 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 	}
 
 	if (!trusted_keys) {
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 	} else if (trusted_keys == VERIFY_USE_SECONDARY_KEYRING) {
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
+		ret = bhv_keyring_verify(secondary_trusted_keys,
+					 &secondary_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = secondary_trusted_keys;
 #else
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 #endif
 	} else if (trusted_keys == VERIFY_USE_PLATFORM_KEYRING) {
 #ifdef CONFIG_INTEGRITY_PLATFORM_KEYRING
+		ret = bhv_keyring_verify(platform_trusted_keys,
+					 &platform_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = platform_trusted_keys;
 #else
 		trusted_keys = NULL;
@@ -356,6 +404,7 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 			goto error;
 		}
 	}
+
 	ret = pkcs7_validate_trust(pkcs7, trusted_keys);
 	if (ret < 0) {
 		if (ret == -ENOKEY)
@@ -424,5 +473,7 @@ EXPORT_SYMBOL_GPL(verify_pkcs7_signature);
 void __init set_platform_trusted_keys(struct key *keyring)
 {
 	platform_trusted_keys = keyring;
+	if (bhv_keyring_register_system_trusted(&platform_trusted_keys))
+		panic("Can't register platform trusted keyring\n");
 }
 #endif
diff --git drivers/block/drbd/drbd_int.h drivers/block/drbd/drbd_int.h
index e21492981f..d47f4e965d 100644
--- drivers/block/drbd/drbd_int.h
+++ drivers/block/drbd/drbd_int.h
@@ -47,7 +47,11 @@ extern int drbd_fault_rate;
 #endif
 
 extern unsigned int drbd_minor_count;
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+extern const char drbd_usermode_helper[];
+#else
 extern char drbd_usermode_helper[];
+#endif
 extern int drbd_proc_details;
 
 
diff --git drivers/block/drbd/drbd_main.c drivers/block/drbd/drbd_main.c
index 5bbd312c3e..98a0aa8769 100644
--- drivers/block/drbd/drbd_main.c
+++ drivers/block/drbd/drbd_main.c
@@ -96,9 +96,14 @@ module_param_named(proc_details, drbd_proc_details, int, 0644);
 unsigned int drbd_minor_count = DRBD_MINOR_COUNT_DEF;
 /* Module parameter for setting the user mode helper program
  * to run. Default is /sbin/drbdadm */
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+const char drbd_usermode_helper[] __section(".rodata") = "/sbin/drbdadm";
+#else
 char drbd_usermode_helper[80] = "/sbin/drbdadm";
-module_param_named(minor_count, drbd_minor_count, uint, 0444);
 module_param_string(usermode_helper, drbd_usermode_helper, sizeof(drbd_usermode_helper), 0644);
+#endif
+
+module_param_named(minor_count, drbd_minor_count, uint, 0444);
 
 /* in 2.6.x, our device mapping and config info contains our virtual gendisks
  * as member "struct gendisk *vdisk;"
diff --git drivers/block/zram/zram_drv.c drivers/block/zram/zram_drv.c
index 76b326ddd7..d648cce8e2 100644
--- drivers/block/zram/zram_drv.c
+++ drivers/block/zram/zram_drv.c
@@ -921,7 +921,7 @@ static ssize_t read_block_state(struct file *file, char __user *buf,
 	return written;
 }
 
-static const struct file_operations proc_zram_block_state_op = {
+const struct file_operations proc_zram_block_state_op __section(".rodata") = {
 	.open = simple_open,
 	.read = read_block_state,
 	.llseek = default_llseek,
diff --git drivers/char/mem.c drivers/char/mem.c
index 169eed162a..15334c4e9d 100644
--- drivers/char/mem.c
+++ drivers/char/mem.c
@@ -633,7 +633,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define splice_write_zero	splice_write_null
 #define open_mem	open_port
 
-static const struct file_operations __maybe_unused mem_fops = {
+const struct file_operations __maybe_unused mem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
@@ -646,7 +646,7 @@ static const struct file_operations __maybe_unused mem_fops = {
 	.fop_flags	= FOP_UNSIGNED_OFFSET,
 };
 
-static const struct file_operations null_fops = {
+const struct file_operations null_fops __section(".rodata") = {
 	.llseek		= null_lseek,
 	.read		= read_null,
 	.write		= write_null,
@@ -657,7 +657,7 @@ static const struct file_operations null_fops = {
 };
 
 #ifdef CONFIG_DEVPORT
-static const struct file_operations port_fops = {
+const struct file_operations port_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_port,
 	.write		= write_port,
@@ -665,7 +665,7 @@ static const struct file_operations port_fops = {
 };
 #endif
 
-static const struct file_operations zero_fops = {
+const struct file_operations zero_fops __section(".rodata") = {
 	.llseek		= zero_lseek,
 	.write		= write_zero,
 	.read_iter	= read_iter_zero,
@@ -680,7 +680,7 @@ static const struct file_operations zero_fops = {
 #endif
 };
 
-static const struct file_operations full_fops = {
+const struct file_operations full_fops __section(".rodata") = {
 	.llseek		= full_lseek,
 	.read_iter	= read_iter_zero,
 	.write		= write_full,
diff --git drivers/char/random.c drivers/char/random.c
index 23ee76bbb4..7c5d49b857 100644
--- drivers/char/random.c
+++ drivers/char/random.c
@@ -1560,7 +1560,7 @@ static int random_fasync(int fd, struct file *filp, int on)
 	return fasync_helper(fd, filp, on, &fasync);
 }
 
-const struct file_operations random_fops = {
+const struct file_operations random_fops __section(".rodata") = {
 	.read_iter = random_read_iter,
 	.write_iter = random_write_iter,
 	.poll = random_poll,
@@ -1572,7 +1572,7 @@ const struct file_operations random_fops = {
 	.splice_write = iter_file_splice_write,
 };
 
-const struct file_operations urandom_fops = {
+const struct file_operations urandom_fops __section(".rodata") = {
 	.read_iter = urandom_read_iter,
 	.write_iter = random_write_iter,
 	.unlocked_ioctl = random_ioctl,
diff --git drivers/tty/tty_io.c drivers/tty/tty_io.c
index dcb1769c36..09c3ca8b48 100644
--- drivers/tty/tty_io.c
+++ drivers/tty/tty_io.c
@@ -458,7 +458,7 @@ static void tty_show_fdinfo(struct seq_file *m, struct file *file)
 		tty->ops->show_fdinfo(tty, m);
 }
 
-static const struct file_operations tty_fops = {
+const struct file_operations tty_fops __section(".rodata") = {
 	.read_iter	= tty_read,
 	.write_iter	= tty_write,
 	.splice_read	= copy_splice_read,
@@ -472,7 +472,7 @@ static const struct file_operations tty_fops = {
 	.show_fdinfo	= tty_show_fdinfo,
 };
 
-static const struct file_operations console_fops = {
+const struct file_operations console_fops __section(".rodata") = {
 	.read_iter	= tty_read,
 	.write_iter	= redirected_tty_write,
 	.splice_read	= copy_splice_read,
@@ -485,7 +485,7 @@ static const struct file_operations console_fops = {
 	.fasync		= tty_fasync,
 };
 
-static const struct file_operations hung_up_tty_fops = {
+const struct file_operations hung_up_tty_fops __section(".rodata") = {
 	.read_iter	= hung_up_tty_read,
 	.write_iter	= hung_up_tty_write,
 	.poll		= hung_up_tty_poll,
diff --git drivers/video/fbdev/uvesafb.c drivers/video/fbdev/uvesafb.c
index 5d52fd0080..376f46f0e6 100644
--- drivers/video/fbdev/uvesafb.c
+++ drivers/video/fbdev/uvesafb.c
@@ -34,7 +34,12 @@ static struct cb_id uvesafb_cn_id = {
 	.idx = CN_IDX_V86D,
 	.val = CN_VAL_V86D_UVESAFB
 };
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static const char v86d_path[] __section(".rodata") = "/sbin/v86d";
+#else
 static char v86d_path[PATH_MAX] = "/sbin/v86d";
+#endif
 static char v86d_started;	/* has v86d been started by uvesafb? */
 
 static const struct fb_fix_screeninfo uvesafb_fix = {
@@ -118,7 +123,7 @@ static int uvesafb_helper_start(void)
 	};
 
 	char *argv[] = {
-		v86d_path,
+		(char *)v86d_path,
 		NULL,
 	};
 
@@ -1864,6 +1869,9 @@ static ssize_t v86d_show(struct device_driver *dev, char *buf)
 	return snprintf(buf, PAGE_SIZE, "%s\n", v86d_path);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static DRIVER_ATTR_RO(v86d);
+#else
 static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 		size_t count)
 {
@@ -1871,6 +1879,7 @@ static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 	return count;
 }
 static DRIVER_ATTR_RW(v86d);
+#endif
 
 static int uvesafb_init(void)
 {
@@ -1991,8 +2000,11 @@ MODULE_PARM_DESC(mode_option,
 module_param(vbemode, ushort, 0);
 MODULE_PARM_DESC(vbemode,
 	"VBE mode number to set, overrides the 'mode' option");
+
+#ifndef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
 module_param_string(v86d, v86d_path, PATH_MAX, 0660);
 MODULE_PARM_DESC(v86d, "Path to the v86d userspace helper.");
+#endif
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Michal Januszewski <spock@gentoo.org>");
diff --git fs/aio.c fs/aio.c
index e8920178b5..f496d96bc1 100644
--- fs/aio.c
+++ fs/aio.c
@@ -163,6 +163,8 @@ struct kioctx {
 	struct folio		*internal_folios[AIO_RING_PAGES];
 	struct file		*aio_ring_file;
 
+	struct mm_struct        *owner;
+
 	unsigned		id;
 };
 
@@ -767,6 +769,8 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
 	if (!ctx)
 		return ERR_PTR(-ENOMEM);
 
+	ctx->owner = mm;
+
 	ctx->max_reqs = max_reqs;
 
 	spin_lock_init(&ctx->ctx_lock);
@@ -1125,6 +1129,9 @@ static void aio_complete(struct aio_kiocb *iocb)
 	struct io_event	*ev_page, *event;
 	unsigned tail, pos, head, avail;
 	unsigned long	flags;
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain;
+#endif
 
 	/*
 	 * Add a completion event to the ring buffer. Must be done holding
@@ -1142,8 +1149,17 @@ static void aio_complete(struct aio_kiocb *iocb)
 	ev_page = folio_address(ctx->ring_folios[pos / AIO_EVENTS_PER_PAGE]);
 	event = ev_page + pos % AIO_EVENTS_PER_PAGE;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	domain = brs_get_active_domain();
+	brs_domain_enter(ctx->owner->owner);
+#endif
+
 	*event = iocb->ki_res;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_switch(domain);
+#endif
+
 	flush_dcache_folio(ctx->ring_folios[pos / AIO_EVENTS_PER_PAGE]);
 
 	pr_debug("%p[%u]: %p: %p %Lx %Lx %Lx\n", ctx, tail, iocb,
@@ -1370,10 +1386,10 @@ static long read_events(struct kioctx *ctx, long min_nr, long nr,
  *	Create an aio_context capable of receiving at least nr_events.
  *	ctxp must not point to an aio_context that already exists, and
  *	must be initialized to 0 prior to the call.  On successful
- *	creation of the aio_context, *ctxp is filled in with the resulting 
+ *	creation of the aio_context, *ctxp is filled in with the resulting
  *	handle.  May fail with -EINVAL if *ctxp is not initialized,
- *	if the specified nr_events exceeds internal limits.  May fail 
- *	with -EAGAIN if the specified nr_events exceeds the user's limit 
+ *	if the specified nr_events exceeds internal limits.  May fail
+ *	with -EAGAIN if the specified nr_events exceeds the user's limit
  *	of available events.  May fail with -ENOMEM if insufficient kernel
  *	resources are available.  May fail with -EFAULT if an invalid
  *	pointer is passed for ctxp.  Will fail with -ENOSYS if not
@@ -1443,7 +1459,7 @@ COMPAT_SYSCALL_DEFINE2(io_setup, unsigned, nr_events, u32 __user *, ctx32p)
 #endif
 
 /* sys_io_destroy:
- *	Destroy the aio_context specified.  May cancel any outstanding 
+ *	Destroy the aio_context specified.  May cancel any outstanding
  *	AIOs and block on completion.  Will fail with -ENOSYS if not
  *	implemented.  May fail with -EINVAL if the context pointed to
  *	is invalid.
diff --git fs/attr.c fs/attr.c
index c04d19b58f..ea331fce1f 100644
--- fs/attr.c
+++ fs/attr.c
@@ -17,6 +17,8 @@
 #include <linux/filelock.h>
 #include <linux/security.h>
 
+#include <bhv/inode.h>
+
 /**
  * setattr_should_drop_sgid - determine whether the setgid bit needs to be
  *                            removed
@@ -507,6 +509,8 @@ int notify_change(struct mnt_idmap *idmap, struct dentry *dentry,
 	if (!error) {
 		fsnotify_change(dentry, ia_valid);
 		security_inode_post_setattr(idmap, dentry, ia_valid);
+		/* XXX: Make an LSM hook out of me! */
+		bhv_inode_post_setattr(dentry, ia_valid, mode);
 	}
 
 	return error;
diff --git fs/binfmt_elf.c fs/binfmt_elf.c
index 47335a0f4a..ca09059f08 100644
--- fs/binfmt_elf.c
+++ fs/binfmt_elf.c
@@ -50,6 +50,9 @@
 #include <asm/param.h>
 #include <asm/page.h>
 
+#include <bhv/config.h>
+#include <bhv/guestlog.h>
+
 #ifndef ELF_COMPAT
 #define ELF_COMPAT 0
 #endif
@@ -922,14 +925,25 @@ static int load_elf_binary(struct linux_binprm *bprm)
 		goto out_free_ph;
 	}
 
+	if (brs__is__GuestKernelConfig__userspace_force_nx_stack())
+		executable_stack = EXSTACK_DISABLE_X;
+
 	elf_ppnt = elf_phdata;
 	for (i = 0; i < elf_ex->e_phnum; i++, elf_ppnt++)
 		switch (elf_ppnt->p_type) {
 		case PT_GNU_STACK:
-			if (elf_ppnt->p_flags & PF_X)
-				executable_stack = EXSTACK_ENABLE_X;
-			else
+			if (elf_ppnt->p_flags & PF_X) {
+				int err = security_elf_load_exec_stack(bprm);
+				if (err < 0) {
+					retval = err;
+					goto out_free_dentry;
+				}
+				if (!brs__is__GuestKernelConfig__userspace_force_nx_stack()) {
+					executable_stack = EXSTACK_ENABLE_X;
+				}
+			} else {
 				executable_stack = EXSTACK_DISABLE_X;
+			}
 			break;
 
 		case PT_LOPROC ... PT_HIPROC:
diff --git fs/coredump.c fs/coredump.c
index 64894ba6ef..3168f38273 100644
--- fs/coredump.c
+++ fs/coredump.c
@@ -56,6 +56,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/config.h>
+
 static bool dump_vma_snapshot(struct coredump_params *cprm);
 static void free_vma_snapshot(struct coredump_params *cprm);
 
@@ -72,6 +74,12 @@ static void free_vma_snapshot(struct coredump_params *cprm);
 static int core_uses_pid;
 static unsigned int core_pipe_limit;
 static unsigned int core_sort_vma;
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define CORE_PATTERN brs_policy_get_core_pattern()
+#else
+#define CORE_PATTERN core_pattern
+#endif
+
 static char core_pattern[CORENAME_MAX_SIZE] = "core";
 static int core_name_size = CORENAME_MAX_SIZE;
 unsigned int core_file_note_size_limit = CORE_FILE_NOTE_SIZE_DEFAULT;
@@ -217,7 +225,7 @@ static int format_corename(struct core_name *cn, struct coredump_params *cprm,
 			   size_t **argv, int *argc)
 {
 	const struct cred *cred = current_cred();
-	const char *pat_ptr = core_pattern;
+	const char *pat_ptr = CORE_PATTERN;
 	int ispipe = (*pat_ptr == '|');
 	bool was_space = false;
 	int pid_in_pattern = 0;
@@ -1027,7 +1035,7 @@ EXPORT_SYMBOL(dump_align);
 void validate_coredump_safety(void)
 {
 	if (suid_dumpable == SUID_DUMP_ROOT &&
-	    core_pattern[0] != '/' && core_pattern[0] != '|') {
+	    CORE_PATTERN[0] != '/' && CORE_PATTERN[0] != '|') {
 
 		coredump_report_failure("Unsafe core_pattern used with fs.suid_dumpable=2: "
 			"pipe handler or fully qualified core dump path required. "
@@ -1038,7 +1046,11 @@ void validate_coredump_safety(void)
 static int proc_dostring_coredump(const struct ctl_table *table, int write,
 		  void *buffer, size_t *lenp, loff_t *ppos)
 {
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	int error = proc_docore_pattern(table, write, buffer, lenp, ppos);
+#else
 	int error = proc_dostring(table, write, buffer, lenp, ppos);
+#endif
 
 	if (!error)
 		validate_coredump_safety();
@@ -1060,7 +1072,11 @@ static struct ctl_table coredump_sysctls[] = {
 		.procname	= "core_pattern",
 		.data		= core_pattern,
 		.maxlen		= CORENAME_MAX_SIZE,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+#else
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring_coredump,
 	},
 	{
diff --git fs/dcache.c fs/dcache.c
index 0f6b16ba30..0ae26214af 100644
--- fs/dcache.c
+++ fs/dcache.c
@@ -37,6 +37,8 @@
 
 #include <asm/runtime-const.h>
 
+#include <bhv/vault.h>
+
 /*
  * Usage:
  * dcache->d_inode->i_lock protects:
@@ -3151,7 +3153,9 @@ static int __init set_dhash_entries(char *str)
 }
 __setup("dhash_entries=", set_dhash_entries);
 
-static void __init dcache_init_early(void)
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void dcache_init_early(void)
 {
 	/* If hashes are distributed across NUMA nodes, defer
 	 * hash allocation until vmalloc space is available.
@@ -3174,8 +3178,10 @@ static void __init dcache_init_early(void)
 	runtime_const_init(shift, d_hash_shift);
 	runtime_const_init(ptr, dentry_hashtable);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, dcache_init_early);
 
-static void __init dcache_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void dcache_init(void)
 {
 	/*
 	 * A constructor could be added for stable state like the lists,
@@ -3205,6 +3211,7 @@ static void __init dcache_init(void)
 	runtime_const_init(shift, d_hash_shift);
 	runtime_const_init(ptr, dentry_hashtable);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, dcache_init);
 
 /* SLAB cache for __getname() consumers */
 struct kmem_cache *names_cachep __ro_after_init;
diff --git fs/debugfs/file.c fs/debugfs/file.c
index 67299e8b73..3667238cde 100644
--- fs/debugfs/file.c
+++ fs/debugfs/file.c
@@ -38,12 +38,13 @@ static ssize_t default_write_file(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations debugfs_noop_file_operations = {
-	.read =		default_read_file,
-	.write =	default_write_file,
-	.open =		simple_open,
-	.llseek =	noop_llseek,
-};
+const struct file_operations
+	debugfs_noop_file_operations __section(".rodata") = {
+		.read = default_read_file,
+		.write = default_write_file,
+		.open = simple_open,
+		.llseek = noop_llseek,
+	};
 
 #define F_DENTRY(filp) ((filp)->f_path.dentry)
 
@@ -293,28 +294,29 @@ static int open_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_open_proxy_file_operations = {
-	.open = open_proxy_open,
-};
+const struct file_operations
+	debugfs_open_proxy_file_operations __section(".rodata") = {
+		.open = open_proxy_open,
+	};
 
 #define PROTO(args...) args
 #define ARGS(args...) args
 
-#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)		\
-static ret_type full_proxy_ ## name(proto)				\
-{									\
-	struct dentry *dentry = F_DENTRY(filp);			\
-	const struct file_operations *real_fops;			\
-	ret_type r;							\
-									\
-	r = debugfs_file_get(dentry);					\
-	if (unlikely(r))						\
-		return r;						\
-	real_fops = debugfs_real_fops(filp);				\
-	r = real_fops->name(args);					\
-	debugfs_file_put(dentry);					\
-	return r;							\
-}
+#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)                     \
+	ret_type full_proxy_##name(proto)                                      \
+	{                                                                      \
+		struct dentry *dentry = F_DENTRY(filp);                        \
+		const struct file_operations *real_fops;                       \
+		ret_type r;                                                    \
+                                                                               \
+		r = debugfs_file_get(dentry);                                  \
+		if (unlikely(r))                                               \
+			return r;                                              \
+		real_fops = debugfs_real_fops(filp);                           \
+		r = real_fops->name(args);                                     \
+		debugfs_file_put(dentry);                                      \
+		return r;                                                      \
+	}
 
 FULL_PROXY_FUNC(llseek, loff_t, filp,
 		PROTO(struct file *filp, loff_t offset, int whence),
@@ -334,8 +336,7 @@ FULL_PROXY_FUNC(unlocked_ioctl, long, filp,
 		PROTO(struct file *filp, unsigned int cmd, unsigned long arg),
 		ARGS(filp, cmd, arg));
 
-static __poll_t full_proxy_poll(struct file *filp,
-				struct poll_table_struct *wait)
+__poll_t full_proxy_poll(struct file *filp, struct poll_table_struct *wait)
 {
 	struct dentry *dentry = F_DENTRY(filp);
 	__poll_t r = 0;
@@ -350,7 +351,7 @@ static __poll_t full_proxy_poll(struct file *filp,
 	return r;
 }
 
-static int full_proxy_release(struct inode *inode, struct file *filp)
+int full_proxy_release(struct inode *inode, struct file *filp)
 {
 	const struct dentry *dentry = F_DENTRY(filp);
 	const struct file_operations *real_fops = debugfs_real_fops(filp);
@@ -451,9 +452,10 @@ static int full_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_full_proxy_file_operations = {
-	.open = full_proxy_open,
-};
+const struct file_operations
+	debugfs_full_proxy_file_operations __section(".rodata") = {
+		.open = full_proxy_open,
+	};
 
 ssize_t debugfs_attr_read(struct file *file, char __user *buf,
 			size_t len, loff_t *ppos)
diff --git fs/exec.c fs/exec.c
index d607943729..bdac03b044 100644
--- fs/exec.c
+++ fs/exec.c
@@ -46,6 +46,7 @@
 #include <linux/personality.h>
 #include <linux/binfmts.h>
 #include <linux/utsname.h>
+#include <linux/mem_namespace.h>
 #include <linux/pid_namespace.h>
 #include <linux/module.h>
 #include <linux/namei.h>
@@ -78,6 +79,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -291,6 +294,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 		goto err;
 
 	mm->stack_vm = mm->total_vm = 1;
+
 	mmap_write_unlock(mm);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
@@ -660,6 +664,9 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 {
 	int len = strnlen(arg, MAX_ARG_STRLEN) + 1 /* terminating NUL */;
 	unsigned long pos = bprm->p;
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain;
+#endif
 
 	if (len == 0)
 		return -EFAULT;
@@ -684,8 +691,16 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 		page = get_arg_page(bprm, pos, 1);
 		if (!page)
 			return -E2BIG;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+		domain = brs_get_active_domain();
+		brs_domain_enter(bprm->mm->owner);
+#endif
 		flush_arg_page(bprm, pos & PAGE_MASK, page);
 		memcpy_to_page(page, offset_in_page(pos), arg, bytes_to_copy);
+#if defined(CONFIG_DOMAIN_SPACES)
+		brs_domain_switch(domain);
+#endif
 		put_arg_page(page);
 	}
 
@@ -832,8 +847,10 @@ int setup_arg_pages(struct linux_binprm *bprm,
 #endif
 	current->mm->start_stack = bprm->p;
 	ret = expand_stack_locked(vma, stack_base);
-	if (ret)
+	if (ret) {
 		ret = -EFAULT;
+		goto out_unlock;
+	}
 
 out_unlock:
 	mmap_write_unlock(mm);
diff --git fs/ext4/dir.c fs/ext4/dir.c
index b278b5703c..d0499d847d 100644
--- fs/ext4/dir.c
+++ fs/ext4/dir.c
@@ -680,7 +680,7 @@ static int ext4_dir_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations ext4_dir_operations = {
+const struct file_operations ext4_dir_operations __section(".rodata") = {
 	.open		= ext4_dir_open,
 	.llseek		= ext4_dir_llseek,
 	.read		= generic_read_dir,
diff --git fs/ext4/file.c fs/ext4/file.c
index 6c692151b0..7f3e20a717 100644
--- fs/ext4/file.c
+++ fs/ext4/file.c
@@ -921,7 +921,7 @@ loff_t ext4_llseek(struct file *file, loff_t offset, int whence)
 	return vfs_setpos(file, offset, maxbytes);
 }
 
-const struct file_operations ext4_file_operations = {
+const struct file_operations ext4_file_operations __section(".rodata") = {
 	.llseek		= ext4_llseek,
 	.read_iter	= ext4_file_read_iter,
 	.write_iter	= ext4_file_write_iter,
diff --git fs/file.c fs/file.c
index b6fb6d18ac..4f75587194 100644
--- fs/file.c
+++ fs/file.c
@@ -1331,6 +1331,7 @@ static int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)
 {
 	int err = -EBADF;
 	struct file *file;
+	struct file *sec_file;
 	struct files_struct *files = current->files;
 
 	if ((flags & ~O_CLOEXEC) != 0)
@@ -1342,21 +1343,37 @@ static int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)
 	if (newfd >= rlimit(RLIMIT_NOFILE))
 		return -EBADF;
 
+	// Get reference to file.
+	// Keep the reference after the check to avoid
+	// TOCTOU.
+	sec_file = fget_raw(oldfd);
+	if (unlikely(!sec_file))
+		return -EBADF;
+	err = security_fd_dup(newfd, sec_file);
+	if (err) {
+		fput(sec_file);
+		return err;
+	}
+
 	spin_lock(&files->file_lock);
 	err = expand_files(files, newfd);
 	file = files_lookup_fd_locked(files, oldfd);
-	if (unlikely(!file))
+	if (unlikely(!file) || unlikely(sec_file != file))
 		goto Ebadf;
 	if (unlikely(err < 0)) {
 		if (err == -EMFILE)
 			goto Ebadf;
 		goto out_unlock;
 	}
+	// Free reference to sec_file
+	fput(sec_file);
 	return do_dup2(files, file, newfd, flags);
 
 Ebadf:
 	err = -EBADF;
 out_unlock:
+	// Free reference to sec_file
+	fput(sec_file);
 	spin_unlock(&files->file_lock);
 	return err;
 }
@@ -1404,11 +1421,20 @@ int f_dupfd(unsigned int from, struct file *file, unsigned flags)
 {
 	unsigned long nofile = rlimit(RLIMIT_NOFILE);
 	int err;
+	int sec_err;
 	if (from >= nofile)
 		return -EINVAL;
 	err = alloc_fd(from, nofile, flags);
 	if (err >= 0) {
 		get_file(file);
+
+		sec_err = security_fd_dup(err, file);
+		if (sec_err) {
+			put_unused_fd(err);
+			fput(file);
+			return sec_err;
+		}
+
 		fd_install(err, file);
 	}
 	return err;
diff --git fs/inode.c fs/inode.c
index 8dabb224f9..d40720c57c 100644
--- fs/inode.c
+++ fs/inode.c
@@ -24,6 +24,8 @@
 #include <trace/events/writeback.h>
 #include "internal.h"
 
+#include <bhv/inode.h>
+
 /*
  * Inode locking rules:
  *
@@ -145,6 +147,8 @@ static int no_open(struct inode *inode, struct file *file)
 	return -ENXIO;
 }
 
+const struct file_operations no_open_fops = {.open = no_open};
+
 /**
  * inode_init_always_gfp - perform inode structure initialisation
  * @sb: superblock inode belongs to
@@ -158,7 +162,6 @@ static int no_open(struct inode *inode, struct file *file)
 int inode_init_always_gfp(struct super_block *sb, struct inode *inode, gfp_t gfp)
 {
 	static const struct inode_operations empty_iops;
-	static const struct file_operations no_open_fops = {.open = no_open};
 	struct address_space *const mapping = &inode->i_data;
 
 	inode->i_sb = sb;
@@ -1900,6 +1903,7 @@ void iput(struct inode *inode)
 			mark_inode_dirty_sync(inode);
 			goto retry;
 		}
+		bhv_inode_iput_final(inode);
 		iput_final(inode);
 	}
 }
diff --git fs/libfs.c fs/libfs.c
index 3cb49463a8..bf19aa4ef0 100644
--- fs/libfs.c
+++ fs/libfs.c
@@ -226,7 +226,7 @@ ssize_t generic_read_dir(struct file *filp, char __user *buf, size_t siz, loff_t
 }
 EXPORT_SYMBOL(generic_read_dir);
 
-const struct file_operations simple_dir_operations = {
+const struct file_operations simple_dir_operations __section(".rodata") = {
 	.open		= dcache_dir_open,
 	.release	= dcache_dir_close,
 	.llseek		= dcache_dir_lseek,
@@ -1743,7 +1743,7 @@ static int empty_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-static const struct file_operations empty_dir_operations = {
+const struct file_operations empty_dir_operations __section(".rodata") = {
 	.llseek		= empty_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= empty_dir_readdir,
diff --git fs/nfsd/nfs4recover.c fs/nfsd/nfs4recover.c
index 1c8fcb04b3..e697b04cf2 100644
--- fs/nfsd/nfs4recover.c
+++ fs/nfsd/nfs4recover.c
@@ -1687,11 +1687,15 @@ static const struct nfsd4_client_tracking_ops nfsd4_cld_tracking_ops_v2 = {
 };
 
 #ifdef CONFIG_NFSD_LEGACY_CLIENT_TRACKING
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static const char cltrack_prog[] __section(".rodata") = "/sbin/nfsdcltrack";
+#else
 /* upcall via usermodehelper */
 static char cltrack_prog[PATH_MAX] = "/sbin/nfsdcltrack";
 module_param_string(cltrack_prog, cltrack_prog, sizeof(cltrack_prog),
 			S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(cltrack_prog, "Path to the nfsdcltrack upcall program");
+#endif
 
 static bool cltrack_legacy_disable;
 module_param(cltrack_legacy_disable, bool, S_IRUGO|S_IWUSR);
@@ -1850,9 +1854,13 @@ nfsd4_umh_cltrack_upcall(char *cmd, char *arg, char *env0, char *env1)
 	 */
 	if (ret == -ENOENT || ret == -EACCES) {
 		dprintk("NFSD: %s was not found or isn't executable (%d). "
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+			cltrack_prog, ret);
+#else
 			"Setting cltrack_prog to blank string!",
 			cltrack_prog, ret);
 		cltrack_prog[0] = '\0';
+#endif
 	}
 	dprintk("%s: %s return value: %d\n", __func__, cltrack_prog, ret);
 
diff --git fs/notify/inotify/inotify_user.c fs/notify/inotify/inotify_user.c
index 0794dcaf1e..75e72f7aac 100644
--- fs/notify/inotify/inotify_user.c
+++ fs/notify/inotify/inotify_user.c
@@ -354,7 +354,7 @@ static long inotify_ioctl(struct file *file, unsigned int cmd,
 	return ret;
 }
 
-static const struct file_operations inotify_fops = {
+const struct file_operations inotify_fops __section(".rodata") = {
 	.show_fdinfo	= inotify_show_fdinfo,
 	.poll		= inotify_poll,
 	.read		= inotify_read,
diff --git fs/proc/array.c fs/proc/array.c
index 5e4f7b411f..414cb068ce 100644
--- fs/proc/array.c
+++ fs/proc/array.c
@@ -813,7 +813,7 @@ static int children_seq_open(struct inode *inode, struct file *file)
 	return seq_open(file, &children_seq_ops);
 }
 
-const struct file_operations proc_tid_children_operations = {
+const struct file_operations proc_tid_children_operations __section(".rodata") = {
 	.open    = children_seq_open,
 	.read    = seq_read,
 	.llseek  = seq_lseek,
diff --git fs/proc/base.c fs/proc/base.c
index a2541f5204..17e9942f22 100644
--- fs/proc/base.c
+++ fs/proc/base.c
@@ -409,7 +409,7 @@ static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_pid_cmdline_ops = {
+const struct file_operations proc_pid_cmdline_ops __section(".rodata") = {
 	.read	= proc_pid_cmdline_read,
 	.llseek	= generic_file_llseek,
 };
@@ -573,7 +573,7 @@ static ssize_t lstats_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_lstats_operations = {
+const struct file_operations proc_lstats_operations __section(".rodata") = {
 	.open		= lstats_open,
 	.read		= seq_read,
 	.write		= lstats_write,
@@ -821,7 +821,7 @@ static int proc_single_open(struct inode *inode, struct file *filp)
 	return single_open(filp, proc_single_show, inode);
 }
 
-static const struct file_operations proc_single_file_operations = {
+const struct file_operations proc_single_file_operations __section(".rodata") = {
 	.open		= proc_single_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -982,7 +982,7 @@ static int mem_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_mem_operations = {
+const struct file_operations proc_mem_operations __section(".rodata") = {
 	.llseek		= mem_lseek,
 	.read		= mem_read,
 	.write		= mem_write,
@@ -1059,7 +1059,7 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_environ_operations = {
+const struct file_operations proc_environ_operations __section(".rodata") = {
 	.open		= environ_open,
 	.read		= environ_read,
 	.llseek		= generic_file_llseek,
@@ -1086,7 +1086,7 @@ static ssize_t auxv_read(struct file *file, char __user *buf,
 				       nwords * sizeof(mm->saved_auxv[0]));
 }
 
-static const struct file_operations proc_auxv_operations = {
+const struct file_operations proc_auxv_operations __section(".rodata") = {
 	.open		= auxv_open,
 	.read		= auxv_read,
 	.llseek		= generic_file_llseek,
@@ -1245,7 +1245,7 @@ static ssize_t oom_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_adj_operations = {
+const struct file_operations proc_oom_adj_operations __section(".rodata") = {
 	.read		= oom_adj_read,
 	.write		= oom_adj_write,
 	.llseek		= generic_file_llseek,
@@ -1295,7 +1295,7 @@ static ssize_t oom_score_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_score_adj_operations = {
+const struct file_operations proc_oom_score_adj_operations __section(".rodata") = {
 	.read		= oom_score_adj_read,
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
@@ -1363,7 +1363,7 @@ static ssize_t proc_loginuid_write(struct file * file, const char __user * buf,
 	return count;
 }
 
-static const struct file_operations proc_loginuid_operations = {
+const struct file_operations proc_loginuid_operations __section(".rodata") = {
 	.read		= proc_loginuid_read,
 	.write		= proc_loginuid_write,
 	.llseek		= generic_file_llseek,
@@ -1385,7 +1385,7 @@ static ssize_t proc_sessionid_read(struct file * file, char __user * buf,
 	return simple_read_from_buffer(buf, count, ppos, tmpbuf, length);
 }
 
-static const struct file_operations proc_sessionid_operations = {
+const struct file_operations proc_sessionid_operations __section(".rodata") = {
 	.read		= proc_sessionid_read,
 	.llseek		= generic_file_llseek,
 };
@@ -1440,7 +1440,7 @@ static ssize_t proc_fault_inject_write(struct file * file,
 	return count;
 }
 
-static const struct file_operations proc_fault_inject_operations = {
+const struct file_operations proc_fault_inject_operations __section(".rodata") = {
 	.read		= proc_fault_inject_read,
 	.write		= proc_fault_inject_write,
 	.llseek		= generic_file_llseek,
@@ -1481,7 +1481,7 @@ static ssize_t proc_fail_nth_read(struct file *file, char __user *buf,
 	return simple_read_from_buffer(buf, count, ppos, numbuf, len);
 }
 
-static const struct file_operations proc_fail_nth_operations = {
+const struct file_operations proc_fail_nth_operations __section(".rodata") = {
 	.read		= proc_fail_nth_read,
 	.write		= proc_fail_nth_write,
 };
@@ -1530,7 +1530,7 @@ static int sched_open(struct inode *inode, struct file *filp)
 	return single_open(filp, sched_show, inode);
 }
 
-static const struct file_operations proc_pid_sched_operations = {
+const struct file_operations proc_pid_sched_operations __section(".rodata") = {
 	.open		= sched_open,
 	.read		= seq_read,
 	.write		= sched_write,
@@ -1604,7 +1604,7 @@ static int sched_autogroup_open(struct inode *inode, struct file *filp)
 	return ret;
 }
 
-static const struct file_operations proc_pid_sched_autogroup_operations = {
+const struct file_operations proc_pid_sched_autogroup_operations __section(".rodata") = {
 	.open		= sched_autogroup_open,
 	.read		= seq_read,
 	.write		= sched_autogroup_write,
@@ -1707,7 +1707,7 @@ static int timens_offsets_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timens_offsets_show, inode);
 }
 
-static const struct file_operations proc_timens_offsets_operations = {
+const struct file_operations proc_timens_offsets_operations __section(".rodata") = {
 	.open		= timens_offsets_open,
 	.read		= seq_read,
 	.write		= timens_offsets_write,
@@ -1765,7 +1765,7 @@ static int comm_open(struct inode *inode, struct file *filp)
 	return single_open(filp, comm_show, inode);
 }
 
-static const struct file_operations proc_pid_set_comm_operations = {
+const struct file_operations proc_pid_set_comm_operations __section(".rodata") = {
 	.open		= comm_open,
 	.read		= seq_read,
 	.write		= comm_write,
@@ -2486,7 +2486,7 @@ proc_map_files_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-static const struct file_operations proc_map_files_operations = {
+const struct file_operations proc_map_files_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_map_files_readdir,
 	.llseek		= generic_file_llseek,
@@ -2585,7 +2585,7 @@ static int proc_timers_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_timers_operations = {
+const struct file_operations proc_timers_operations __section(".rodata") = {
 	.open		= proc_timers_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -2678,7 +2678,7 @@ static int timerslack_ns_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timerslack_ns_show, inode);
 }
 
-static const struct file_operations proc_pid_set_timerslack_ns_operations = {
+const struct file_operations proc_pid_set_timerslack_ns_operations __section(".rodata") = {
 	.open		= timerslack_ns_open,
 	.read		= seq_read,
 	.write		= timerslack_ns_write,
@@ -2851,7 +2851,7 @@ static ssize_t proc_pid_attr_write(struct file * file, const char __user * buf,
 	return rv;
 }
 
-static const struct file_operations proc_pid_attr_operations = {
+const struct file_operations proc_pid_attr_operations __section(".rodata") = {
 	.open		= proc_pid_attr_open,
 	.read		= proc_pid_attr_read,
 	.write		= proc_pid_attr_write,
@@ -2927,7 +2927,7 @@ static int proc_attr_dir_readdir(struct file *file, struct dir_context *ctx)
 				   attr_dir_stuff, ARRAY_SIZE(attr_dir_stuff));
 }
 
-static const struct file_operations proc_attr_dir_operations = {
+const struct file_operations proc_attr_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_attr_dir_readdir,
 	.llseek		= generic_file_llseek,
@@ -3019,7 +3019,7 @@ static ssize_t proc_coredump_filter_write(struct file *file,
 	return count;
 }
 
-static const struct file_operations proc_coredump_filter_operations = {
+const struct file_operations proc_coredump_filter_operations __section(".rodata") = {
 	.read		= proc_coredump_filter_read,
 	.write		= proc_coredump_filter_write,
 	.llseek		= generic_file_llseek,
@@ -3154,7 +3154,7 @@ static int proc_projid_map_open(struct inode *inode, struct file *file)
 	return proc_id_map_open(inode, file, &proc_projid_seq_operations);
 }
 
-static const struct file_operations proc_uid_map_operations = {
+const struct file_operations proc_uid_map_operations __section(".rodata") = {
 	.open		= proc_uid_map_open,
 	.write		= proc_uid_map_write,
 	.read		= seq_read,
@@ -3162,7 +3162,7 @@ static const struct file_operations proc_uid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_gid_map_operations = {
+const struct file_operations proc_gid_map_operations __section(".rodata") = {
 	.open		= proc_gid_map_open,
 	.write		= proc_gid_map_write,
 	.read		= seq_read,
@@ -3170,7 +3170,7 @@ static const struct file_operations proc_gid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_projid_map_operations = {
+const struct file_operations proc_projid_map_operations __section(".rodata") = {
 	.open		= proc_projid_map_open,
 	.write		= proc_projid_map_write,
 	.read		= seq_read,
@@ -3221,7 +3221,7 @@ static int proc_setgroups_release(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static const struct file_operations proc_setgroups_operations = {
+const struct file_operations proc_setgroups_operations __section(".rodata") = {
 	.open		= proc_setgroups_open,
 	.write		= proc_setgroups_write,
 	.read		= seq_read,
@@ -3300,7 +3300,7 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 /*
  * Thread groups
  */
-static const struct file_operations proc_task_operations;
+const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
 static const struct pid_entry tgid_base_stuff[] = {
@@ -3425,7 +3425,7 @@ static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
 				   tgid_base_stuff, ARRAY_SIZE(tgid_base_stuff));
 }
 
-static const struct file_operations proc_tgid_base_operations = {
+const struct file_operations proc_tgid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3771,7 +3771,7 @@ static struct dentry *proc_tid_base_lookup(struct inode *dir, struct dentry *den
 				  tid_base_stuff + ARRAY_SIZE(tid_base_stuff));
 }
 
-static const struct file_operations proc_tid_base_operations = {
+const struct file_operations proc_tid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3998,7 +3998,7 @@ static const struct inode_operations proc_task_inode_operations = {
 	.permission	= proc_pid_permission,
 };
 
-static const struct file_operations proc_task_operations = {
+const struct file_operations proc_task_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_task_readdir,
 	.llseek		= proc_dir_llseek,
diff --git fs/proc/fd.c fs/proc/fd.c
index 5e391cbca7..cada16ed47 100644
--- fs/proc/fd.c
+++ fs/proc/fd.c
@@ -105,7 +105,7 @@ static const struct inode_operations proc_fdinfo_file_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
-static const struct file_operations proc_fdinfo_file_operations = {
+const struct file_operations proc_fdinfo_file_operations __section(".rodata") = {
 	.open		= seq_fdinfo_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -317,7 +317,7 @@ static int proc_fd_iterate(struct file *file, struct dir_context *ctx)
 	return proc_readfd_common(file, ctx, proc_fd_instantiate);
 }
 
-const struct file_operations proc_fd_operations = {
+const struct file_operations proc_fd_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_fd_iterate,
 	.llseek		= generic_file_llseek,
@@ -419,7 +419,7 @@ const struct inode_operations proc_fdinfo_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
-const struct file_operations proc_fdinfo_operations = {
+const struct file_operations proc_fdinfo_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_fdinfo_iterate,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/generic.c fs/proc/generic.c
index 3431b083f7..2c8a7084bd 100644
--- fs/proc/generic.c
+++ fs/proc/generic.c
@@ -337,7 +337,7 @@ int proc_readdir(struct file *file, struct dir_context *ctx)
  * use the in-memory "struct proc_dir_entry" tree to parse
  * the /proc directory.
  */
-static const struct file_operations proc_dir_operations = {
+const struct file_operations proc_dir_operations __section(".rodata") = {
 	.llseek			= generic_file_llseek,
 	.read			= generic_read_dir,
 	.iterate_shared		= proc_readdir,
diff --git fs/proc/inode.c fs/proc/inode.c
index 3604b61631..b25f4ac05c 100644
--- fs/proc/inode.c
+++ fs/proc/inode.c
@@ -552,7 +552,7 @@ static int proc_reg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_reg_file_ops = {
+const struct file_operations proc_reg_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -564,7 +564,7 @@ static const struct file_operations proc_reg_file_ops = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops = {
+const struct file_operations proc_iter_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.write		= proc_reg_write,
@@ -578,7 +578,7 @@ static const struct file_operations proc_iter_file_ops = {
 };
 
 #ifdef CONFIG_COMPAT
-static const struct file_operations proc_reg_file_ops_compat = {
+const struct file_operations proc_reg_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -591,7 +591,7 @@ static const struct file_operations proc_reg_file_ops_compat = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops_compat = {
+const struct file_operations proc_iter_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.splice_read	= copy_splice_read,
diff --git fs/proc/namespaces.c fs/proc/namespaces.c
index 8e159fc78c..df908ffc83 100644
--- fs/proc/namespaces.c
+++ fs/proc/namespaces.c
@@ -11,7 +11,6 @@
 #include <linux/user_namespace.h>
 #include "internal.h"
 
-
 static const struct proc_ns_operations *ns_entries[] = {
 #ifdef CONFIG_NET_NS
 	&netns_operations,
@@ -23,8 +22,7 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&ipcns_operations,
 #endif
 #ifdef CONFIG_PID_NS
-	&pidns_operations,
-	&pidns_for_children_operations,
+	&pidns_operations,    &pidns_for_children_operations,
 #endif
 #ifdef CONFIG_USER_NS
 	&userns_operations,
@@ -34,8 +32,10 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&cgroupns_operations,
 #endif
 #ifdef CONFIG_TIME_NS
-	&timens_operations,
-	&timens_for_children_operations,
+	&timens_operations,   &timens_for_children_operations,
+#endif
+#ifdef CONFIG_MEM_NS
+	&memns_operations,
 #endif
 };
 
@@ -142,7 +142,7 @@ static int proc_ns_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-const struct file_operations proc_ns_dir_operations = {
+const struct file_operations proc_ns_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_ns_dir_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/proc_net.c fs/proc/proc_net.c
index 52f0b75cbc..074d97baad 100644
--- fs/proc/proc_net.c
+++ fs/proc/proc_net.c
@@ -339,7 +339,7 @@ static int proc_tgid_net_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-const struct file_operations proc_net_operations = {
+const struct file_operations proc_net_operations __section(".rodata") = {
 	.llseek		= generic_file_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_net_readdir,
diff --git fs/proc/proc_sysctl.c fs/proc/proc_sysctl.c
index e785db5fa4..189b3fab7b 100644
--- fs/proc/proc_sysctl.c
+++ fs/proc/proc_sysctl.c
@@ -24,9 +24,9 @@
 	for (size_t i = 0 ; i < header->ctl_table_size; ++i, entry++)
 
 static const struct dentry_operations proc_sys_dentry_operations;
-static const struct file_operations proc_sys_file_operations;
+const struct file_operations proc_sys_file_operations;
 static const struct inode_operations proc_sys_inode_operations;
-static const struct file_operations proc_sys_dir_file_operations;
+const struct file_operations proc_sys_dir_file_operations;
 static const struct inode_operations proc_sys_dir_operations;
 
 /*
@@ -849,7 +849,7 @@ static int proc_sys_getattr(struct mnt_idmap *idmap,
 	return 0;
 }
 
-static const struct file_operations proc_sys_file_operations = {
+const struct file_operations proc_sys_file_operations __section(".rodata") = {
 	.open		= proc_sys_open,
 	.poll		= proc_sys_poll,
 	.read_iter	= proc_sys_read,
@@ -859,7 +859,7 @@ static const struct file_operations proc_sys_file_operations = {
 	.llseek		= default_llseek,
 };
 
-static const struct file_operations proc_sys_dir_file_operations = {
+const struct file_operations proc_sys_dir_file_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_sys_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/root.c fs/proc/root.c
index 06a297a27b..c8782d3aff 100644
--- fs/proc/root.c
+++ fs/proc/root.c
@@ -345,7 +345,7 @@ static int proc_root_readdir(struct file *file, struct dir_context *ctx)
  * <pid> directories. Thus we don't use the generic
  * directory handling functions for that..
  */
-static const struct file_operations proc_root_operations = {
+const struct file_operations proc_root_operations __section(".rodata") = {
 	.read		 = generic_read_dir,
 	.iterate_shared	 = proc_root_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/task_mmu.c fs/proc/task_mmu.c
index 72a58681f0..a5dd93b1bd 100644
--- fs/proc/task_mmu.c
+++ fs/proc/task_mmu.c
@@ -616,7 +616,7 @@ static long procfs_procmap_ioctl(struct file *file, unsigned int cmd, unsigned l
 	}
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1338,14 +1338,14 @@ static int smaps_rollup_release(struct inode *inode, struct file *file)
 	return single_release(inode, file);
 }
 
-const struct file_operations proc_pid_smaps_operations = {
+const struct file_operations proc_pid_smaps_operations __section(".rodata") = {
 	.open		= pid_smaps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
 	.release	= proc_map_release,
 };
 
-const struct file_operations proc_pid_smaps_rollup_operations = {
+const struct file_operations proc_pid_smaps_rollup_operations __section(".rodata") = {
 	.open		= smaps_rollup_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1608,7 +1608,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations proc_clear_refs_operations = {
+const struct file_operations proc_clear_refs_operations __section(".rodata") = {
 	.write		= clear_refs_write,
 	.llseek		= noop_llseek,
 };
@@ -2823,7 +2823,7 @@ static long do_pagemap_cmd(struct file *file, unsigned int cmd,
 	}
 }
 
-const struct file_operations proc_pagemap_operations = {
+const struct file_operations proc_pagemap_operations __section(".rodata") = {
 	.llseek		= mem_lseek, /* borrow this */
 	.read		= pagemap_read,
 	.open		= pagemap_open,
@@ -3093,7 +3093,7 @@ static int pid_numa_maps_open(struct inode *inode, struct file *file)
 				sizeof(struct numa_maps_private));
 }
 
-const struct file_operations proc_pid_numa_maps_operations = {
+const struct file_operations proc_pid_numa_maps_operations __section(".rodata") = {
 	.open		= pid_numa_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc/task_nommu.c fs/proc/task_nommu.c
index bce6745330..f92667dad0 100644
--- fs/proc/task_nommu.c
+++ fs/proc/task_nommu.c
@@ -287,7 +287,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return maps_open(inode, file, &proc_pid_maps_ops);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc_namespace.c fs/proc_namespace.c
index e133b507dd..373f63662a 100644
--- fs/proc_namespace.c
+++ fs/proc_namespace.c
@@ -314,7 +314,7 @@ static int mountstats_open(struct inode *inode, struct file *file)
 	return mounts_open_common(inode, file, show_vfsstat);
 }
 
-const struct file_operations proc_mounts_operations = {
+const struct file_operations proc_mounts_operations __section(".rodata") = {
 	.open		= mounts_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= copy_splice_read,
@@ -323,7 +323,7 @@ const struct file_operations proc_mounts_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountinfo_operations = {
+const struct file_operations proc_mountinfo_operations __section(".rodata") = {
 	.open		= mountinfo_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= copy_splice_read,
@@ -332,7 +332,7 @@ const struct file_operations proc_mountinfo_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountstats_operations = {
+const struct file_operations proc_mountstats_operations __section(".rodata") = {
 	.open		= mountstats_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= copy_splice_read,
diff --git fs/read_write.c fs/read_write.c
index befec0b5c5..459b2c0550 100644
--- fs/read_write.c
+++ fs/read_write.c
@@ -25,6 +25,8 @@
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
 
+#include "bhv/file_protection.h"
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read_iter	= generic_file_read_iter,
@@ -683,6 +685,9 @@ ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_
 		ret = new_sync_write(file, buf, count, pos);
 	else
 		ret = -EINVAL;
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_modify(file);
 		add_wchar(current, ret);
@@ -954,6 +959,9 @@ ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
 
 	kiocb_start_write(iocb);
 	ret = file->f_op->write_iter(iocb, iter);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret != -EIOCBQUEUED)
 		kiocb_end_write(iocb);
 	if (ret > 0)
@@ -986,6 +994,9 @@ ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
 
 	file_start_write(file);
 	ret = do_iter_readv_writev(file, iter, ppos, WRITE, flags);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0)
 		fsnotify_modify(file);
 	file_end_write(file);
@@ -1069,6 +1080,9 @@ static ssize_t vfs_writev(struct file *file, const struct iovec __user *vec,
 	file_end_write(file);
 out:
 	kfree(iov);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	return ret;
 }
 
@@ -1651,6 +1665,9 @@ ssize_t vfs_copy_file_range(struct file *file_in, loff_t pos_in,
 	ret = do_splice_direct(file_in, &pos_in, file_out, &pos_out,
 			       min_t(size_t, len, MAX_RW_COUNT), 0);
 done:
+
+	bhv_check_file_dirty_cred(file_out, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_access(file_in);
 		add_rchar(current, ret);
diff --git fs/xfs/xfs_file.c fs/xfs/xfs_file.c
index aba54e3c58..4a7756fc58 100644
--- fs/xfs/xfs_file.c
+++ fs/xfs/xfs_file.c
@@ -1583,7 +1583,7 @@ xfs_file_mmap(
 	return 0;
 }
 
-const struct file_operations xfs_file_operations = {
+const struct file_operations xfs_file_operations __section(".rodata") = {
 	.llseek		= xfs_file_llseek,
 	.read_iter	= xfs_file_read_iter,
 	.write_iter	= xfs_file_write_iter,
@@ -1606,7 +1606,7 @@ const struct file_operations xfs_file_operations = {
 			  FOP_BUFFER_WASYNC | FOP_DIO_PARALLEL_WRITE,
 };
 
-const struct file_operations xfs_dir_file_operations = {
+const struct file_operations xfs_dir_file_operations __section(".rodata") = {
 	.open		= xfs_dir_open,
 	.read		= generic_read_dir,
 	.iterate_shared	= xfs_file_readdir,
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index c768de6f19..cdf01b57d7 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifdef CONFIG_HAVE_FUNCTION_DESCRIPTORS
 void *dereference_function_descriptor(void *ptr);
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 23b358a127..99fde191cf 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -355,9 +355,13 @@
 	*(.data..once)							\
 	__end_once = .;							\
 	STRUCT_ALIGN();							\
+	. = ALIGN(PAGE_SIZE);						\
+	__start_bhv_tp_vault = .;					\
 	*(__tracepoints)						\
+	__end_bhv_tp_vault = .;						\
+	. = ALIGN(PAGE_SIZE);						\
 	/* implement dynamic printk debug */				\
-	. = ALIGN(8);							\
+	/*. = ALIGN(8);*/							\
 	BOUNDED_SECTION_BY(__dyndbg_classes, ___dyndbg_classes)		\
 	BOUNDED_SECTION_BY(__dyndbg, ___dyndbg)				\
 	CODETAG_SECTIONS()						\
@@ -403,6 +407,7 @@
 	__end_init_stack = .;
 
 #define JUMP_TABLE_DATA							\
+	. = ALIGN(PAGE_SIZE);           				\
 	. = ALIGN(8);							\
 	BOUNDED_SECTION_BY(__jump_table, ___jump_table)
 
@@ -410,7 +415,8 @@
 #define STATIC_CALL_DATA						\
 	. = ALIGN(8);							\
 	BOUNDED_SECTION_BY(.static_call_sites, _static_call_sites)	\
-	BOUNDED_SECTION_BY(.static_call_tramp_key, _static_call_tramp_key)
+	BOUNDED_SECTION_BY(.static_call_tramp_key, _static_call_tramp_key) \
+	. = ALIGN(PAGE_SIZE);
 #else
 #define STATIC_CALL_DATA
 #endif
@@ -536,17 +542,99 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_start = .;						\
+	*(.bhv.text)							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_end = .;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define BHV_VAULT_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_start = .;			\
+	*(.bhv.vault.text.##vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_end = .;
+
+#define BHV_VAULT_REF_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_start = .;			\
+	*(.ref.text.bhv.vault.text.##vault_name)			\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_end = .;
+
+#define BHV_VAULT_SHARED_TEXT(vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_start = .;		\
+	*(.bhv.vault.shared.text.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_end = .;
+
+#define BHV_VAULT_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_data_##vault_name##_start = .; 			\
+	*(.bhv.vault.data.##vault_name)					\
+	. = ALIGN(PAGE_SIZE); 						\
+	__bhv_vault_data_##vault_name##_end = .;
+
+#define BHV_VAULT_RO_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_start = .;			\
+	*(.bhv.vault.rodata.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_##vault_name##_eps_start = .;			\
+	*(.bhv.vault.##vault_name##.eps)				\
+	__bhv_vault_##vault_name##_eps_end = .;				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_end = .;
+
+#define BHV_VAULT_ENTRIES()	. = ALIGN(8);				\
+			__start_vault_entries = .;			\
+			KEEP(*(.vault_entries))				\
+			__stop_vault_entries = .;			\
+			__start_vault_return_sites = .;			\
+			KEEP(*(.vault_sites))				\
+			__stop_vault_return_sites = .;			\
+			__start_vault_rethunk_sites = .;		\
+			KEEP(*(.vault_rethunks))			\
+			__stop_vault_rethunk_sites = .;
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define BHV_VAULT_TEXT(vault_name)
+#define BHV_VAULT_DATA(vault_name)
+#define BHV_VAULT_RO_DATA(vault_name)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else /* !CONFIG_BHV_VAS */
+
+#define BHV_TEXT
+
+#endif /* CONFIG_BHV_VAS */
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#define BHV_VAULT_ENTRIES()
+#define BHV_VAULT_REF_TEXT(vault_name)
+#define BHV_VAULT_SHARED_TEXT(vault_name)
+
+#endif
+
 
 /*
  * Non-instrumentable text section
  */
 #define NOINSTR_TEXT							\
-		ALIGN_FUNCTION();					\
+		. = ALIGN(PAGE_SIZE);					\
 		__noinstr_text_start = .;				\
 		*(.noinstr.text)					\
 		__cpuidle_text_start = .;				\
 		*(.cpuidle.text)					\
 		__cpuidle_text_end = .;					\
+		. = ALIGN(PAGE_SIZE);                                   \
 		__noinstr_text_end = .;
 
 /*
@@ -565,7 +653,10 @@
 		*(.text.unknown .text.unknown.*)			\
 		NOINSTR_TEXT						\
 		*(.ref.text)						\
-		*(.text.asan.* .text.tsan.*)
+		BHV_VAULT_REF_TEXT(jump_label)				\
+		*(.text.asan.* .text.tsan.*)				\
+		BHV_VAULT_SHARED_TEXT(jump_label)			\
+
 
 
 /* sched.text is aling to function alignment to secure we have same
@@ -674,6 +765,7 @@
 	*(.init.data .init.data.*)					\
 	KERNEL_CTORS()							\
 	MCOUNT_REC()							\
+	BHV_VAULT_ENTRIES()                             \
 	*(.init.rodata .init.rodata.*)					\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 0000000000..badca4db6b
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,66 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#if defined CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_acl(void);
+/************************************************************/
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__ProcessACL__enabled();
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__DriverACL__enabled();
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else // defined CONFIG_BHV_VAS
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_block_driver(const char *target)
+{
+	return false;
+}
+
+static inline bool bhv_block_process(const char *target)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/base.h include/bhv/base.h
new file mode 100644
index 0000000000..f31d62df1a
--- /dev/null
+++ include/bhv/base.h
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BASE_H__
+#define __BHV_BASE_H__
+
+#if defined CONFIG_BHV_VAS
+extern bool _brs_is_standalone;
+extern bool bhv_initialized;
+
+static inline bool brs_is_standalone(void)
+{
+	return _brs_is_standalone;
+}
+
+static inline bool is_bhv_initialized(void)
+{
+	return !brs_is_standalone() && bhv_initialized;
+}
+
+#elif defined CONFIG_BRS
+static inline bool brs_is_standalone(void)
+{
+	return true;
+}
+
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+
+#else /* CONFIG_* */
+static inline bool brs_is_standalone(void)
+{
+	return false;
+}
+
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+#endif /* CONFIG_* */
+
+#endif /* __BHV_BASE_H__ */
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 0000000000..ff46365ddc
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,138 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <linux/preempt.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#include <bhv/base.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define __bhv_text
+#else
+#define __bhv_text __section(".bhv.text") noinline
+#endif // CONFIG_BHV_VAULT_SPACES
+
+#define __bhv_data __section(".bhv.data") noinline
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr);
+
+extern bool __bhv_init_done;
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys_single(void *address)
+{
+	BUG_ON(!address);
+
+#if defined(__aarch64__)
+	/*
+	 * Note: in contrast to x86, the kernel, including its .text, .rodata,
+	 * and .data segments, may overlap with the vmalloc arena on Arm.
+	 * Without an explicit check, the following code will try to use
+	 * vmalloc_to_page() to translate the given address. This, however,
+	 * would fail dramatically.
+	 *
+	 * XXX: note that the following check might become insufficient in the
+	 * future. By then, consider adding the missing segment into the check,
+	 * or rework the BHV address translation logic.
+	 */
+	if (__is_kernel_text((uint64_t)address) ||
+	    is_kernel_rodata((uint64_t)address))
+		return virt_to_phys(address);
+#endif
+
+	if (__bhv_init_done && is_vmalloc_or_module_addr(address)) {
+		struct page *p = NULL;
+		uint64_t offset = 0;
+
+		rcu_read_lock_sched_notrace();
+		p = bhv_vmalloc_to_page(address);
+		rcu_read_unlock_sched_notrace();
+
+		BUG_ON(!p);
+		BUG_ON(PageCompound(p));
+
+		offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	}
+
+	return virt_to_phys(address);
+}
+
+static inline phys_addr_t bhv_virt_to_phys(void *addressp, size_t size)
+{
+	uint64_t first_pfn;
+	uint64_t last_pfn;
+	const uint64_t address = (uint64_t)addressp;
+	uint64_t address_last_byte;
+	phys_addr_t first_byte_gpa;
+	uint64_t pfn;
+
+	BUG_ON(!address || !size);
+	// Get address of last relevant byte, BUG on overflow.
+	BUG_ON(check_add_overflow((uint64_t)address, (uint64_t)size - 1,
+				  &address_last_byte));
+
+	first_pfn = address >> PAGE_SHIFT;
+	last_pfn = address_last_byte >> PAGE_SHIFT;
+
+	first_byte_gpa = bhv_virt_to_phys_single(addressp);
+
+	for (pfn = first_pfn + 1; pfn <= last_pfn; pfn++) {
+		uint64_t this_gpa =
+			bhv_virt_to_phys_single((void *)(pfn << PAGE_SHIFT));
+		BUG_ON(this_gpa - first_byte_gpa !=
+		       (pfn << PAGE_SHIFT) - (uint64_t)address);
+	}
+
+	return first_byte_gpa;
+}
+
+#define BHV_VIRT_TO_PHYS_SIZEOF(obj)                                           \
+	({                                                                     \
+		/* this will emit "warning: dereferencing 'void *' pointer" */ \
+		/* if obj is void */                                           \
+		(void)(obj)[42];                                               \
+		bhv_virt_to_phys(obj, sizeof(*(obj)));                         \
+	})
+
+#else /* !CONFIG_BHV_VAS */
+static inline phys_addr_t bhv_virt_to_phys_single(void *address)
+{
+	return 0;
+}
+
+static inline phys_addr_t bhv_virt_to_phys(void *address, size_t size)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 0000000000..adb104f6c3
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BRS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BRS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/bhv_trace.h include/bhv/bhv_trace.h
new file mode 100644
index 0000000000..bee38e4950
--- /dev/null
+++ include/bhv/bhv_trace.h
@@ -0,0 +1,50 @@
+#ifdef CONFIG_BHV_TRACEPOINTS
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM bhv
+
+#if !defined(__BHV_TRACE_H__) || defined(TRACE_HEADER_MULTI_READ)
+#define __BHV_TRACE_H__
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(bhv_trace_class,
+	TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+	TP_ARGS(bhv_target, bhv_backend, op),
+
+	TP_STRUCT__entry(
+		__field(uint32_t, bhv_target)
+		__field(uint32_t, bhv_backend)
+		__field(uint32_t, op)
+	),
+
+	TP_fast_assign(
+		__entry->bhv_target = bhv_target;
+		__entry->bhv_backend = bhv_backend;
+		__entry->op = op;
+	),
+
+	TP_printk("BHV hypercall: target=%u backend=%u op=%u",
+		  __entry->bhv_target, __entry->bhv_backend, __entry->op)
+);
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_start,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_end,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+#endif /* __BHV_TRACE_H__ */
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+
+#define TRACE_INCLUDE_PATH ../../include/bhv
+#define TRACE_INCLUDE_FILE bhv_trace
+
+#include <trace/define_trace.h>
+
+#endif /* CONFIG_BHV_TRACEPOINTS */
diff --git include/bhv/brpol_enums_autogen.h include/bhv/brpol_enums_autogen.h
new file mode 100644
index 0000000000..2227e4ecad
--- /dev/null
+++ include/bhv/brpol_enums_autogen.h
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_policy_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+enum BRPOL__AllowDeny {
+        BRPOL__AllowDeny__ALLOW = 0,
+        BRPOL__AllowDeny__DENY = 1,
+        BRPOL__AllowDeny__MAX = 1,
+};
+
+inline const char* BRPOL__AllowDeny_2str(enum BRPOL__AllowDeny val) {
+        return
+                val == BRPOL__AllowDeny__ALLOW ? "ALLOW" :
+                val == BRPOL__AllowDeny__DENY  ? "DENY" :
+                "??";
+}
+
+
+#ifdef CONFIG_X86_64
+enum BRPOL__RegisterProtect__Register {
+        BRPOL__RegisterProtect__Register__CR0 = 0,
+        BRPOL__RegisterProtect__Register__CR3 = 1,
+        BRPOL__RegisterProtect__Register__CR4 = 2,
+        BRPOL__RegisterProtect__Register__EFER = 3,
+        BRPOL__RegisterProtect__Register__MAX = 3,
+};
+
+inline const char* BRPOL__RegisterProtect__Register_2str(enum BRPOL__RegisterProtect__Register val) {
+        return
+                val == BRPOL__RegisterProtect__Register__CR0  ? "CR0" :
+                val == BRPOL__RegisterProtect__Register__CR3  ? "CR3" :
+                val == BRPOL__RegisterProtect__Register__CR4  ? "CR4" :
+                val == BRPOL__RegisterProtect__Register__EFER ? "EFER" :
+                "??";
+}
+#endif
+
+
+#ifdef CONFIG_ARM64
+enum BRPOL__RegisterProtect__Register {
+        BRPOL__RegisterProtect__Register__TTBR0_EL1 = 0,
+        BRPOL__RegisterProtect__Register__TTBR1_EL1 = 1,
+        BRPOL__RegisterProtect__Register__TCR_EL1 = 2,
+        BRPOL__RegisterProtect__Register__SCTLR_EL1 = 3,
+        BRPOL__RegisterProtect__Register__VBAR_EL1 = 4,
+        BRPOL__RegisterProtect__Register__MAX = 4,
+};
+
+inline const char* BRPOL__RegisterProtect__Register_2str(enum BRPOL__RegisterProtect__Register val) {
+        return
+                val == BRPOL__RegisterProtect__Register__TTBR0_EL1 ? "TTBR0_EL1" :
+                val == BRPOL__RegisterProtect__Register__TTBR1_EL1 ? "TTBR1_EL1" :
+                val == BRPOL__RegisterProtect__Register__TCR_EL1   ? "TCR_EL1" :
+                val == BRPOL__RegisterProtect__Register__SCTLR_EL1 ? "SCTLR_EL1" :
+                val == BRPOL__RegisterProtect__Register__VBAR_EL1  ? "VBAR_EL1" :
+                "??";
+}
+#endif
+
+
diff --git include/bhv/brs.h include/bhv/brs.h
new file mode 100644
index 0000000000..fb1fc44642
--- /dev/null
+++ include/bhv/brs.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BR_H__
+#define __BHV_BR_H__
+
+#include <linux/preempt.h>
+#include <linux/slab.h>
+
+#include <bhv/base.h>
+
+static __always_inline void *brs_aa_kmalloc(size_t size)
+{
+	return in_atomic() ? kmalloc(size, GFP_ATOMIC) :
+			     kmalloc(size, GFP_KERNEL);
+}
+
+#endif /* __BHV_BR_H__ */
diff --git include/bhv/brs_fs.h include/bhv/brs_fs.h
new file mode 100644
index 0000000000..d5b92cc68b
--- /dev/null
+++ include/bhv/brs_fs.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 BlueRock Security, Inc.
+ * All rights reserved.
+ */
+#ifndef __BHV_BRS_FS_H__
+#define __BHV_BRS_FS_H__
+
+#include <linux/dcache.h>
+
+ssize_t brs_fs_relay_write(void *data, size_t size, bool sync);
+ssize_t brs_fs_relay_write_sync(void *data, size_t size, uint64_t id);
+struct dentry *brs_fs_dir(void);
+void brs_fs_policy_update(void);
+
+int __init brs_fs_relay_init(void);
+void brs_fs_relay_uninit(void);
+
+#endif /* __BHV_BRS_FS_H__ */
diff --git include/bhv/brs_policy.h include/bhv/brs_policy.h
new file mode 100644
index 0000000000..c64f8e54d2
--- /dev/null
+++ include/bhv/brs_policy.h
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 BlueRock Security, Inc.
+ * All rights reserved.
+ */
+#ifndef __BHV_BRS_POLICY_H__
+#define __BHV_BRS_POLICY_H__
+
+#define BRS_POLICY_FILENAME "policy"
+#define BRS_MAX_POLICY_SIZE (0x100000)
+
+int __init brs_policy_init(void);
+void brs_policy_deinit(void);
+
+struct task_struct;
+bool brs_current_can_configure(void);
+void brs_configurator_process_terminated(struct task_struct *task);
+int brs_is_configurator_process(struct task_struct *task);
+
+#endif
diff --git include/bhv/capability.h include/bhv/capability.h
new file mode 100644
index 0000000000..5e4fdcd5ff
--- /dev/null
+++ include/bhv/capability.h
@@ -0,0 +1,63 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+#ifndef __BHV_CAPABILITY_H__
+#define __BHV_CAPABILITY_H__
+
+#include <linux/capability.h>
+
+static const char *const _cap2str[] = {
+	[CAP_CHOWN] = "CAP_CHOWN", // 0
+	[CAP_DAC_OVERRIDE] = "CAP_DAC_OVERRIDE", // 1
+	[CAP_DAC_READ_SEARCH] = "CAP_DAC_READ_SEARCH", // 2
+	[CAP_FOWNER] = "CAP_FOWNER", // 3
+	[CAP_FSETID] = "CAP_FSETID", // 4
+	[CAP_KILL] = "CAP_KILL", // 5
+	[CAP_SETGID] = "CAP_SETGID", // 6
+	[CAP_SETUID] = "CAP_SETUID", // 7
+	[CAP_SETPCAP] = "CAP_SETPCAP", // 8
+	[CAP_LINUX_IMMUTABLE] = "CAP_LINUX_IMMUTABLE", // 9
+	[CAP_NET_BIND_SERVICE] = "CAP_NET_BIND_SERVICE", // 10
+	[CAP_NET_BROADCAST] = "CAP_NET_BROADCAST", // 11
+	[CAP_NET_ADMIN] = "CAP_NET_ADMIN", // 12
+	[CAP_NET_RAW] = "CAP_NET_RAW", // 13
+	[CAP_IPC_LOCK] = "CAP_IPC_LOCK", // 14
+	[CAP_IPC_OWNER] = "CAP_IPC_OWNER", // 15
+	[CAP_SYS_MODULE] = "CAP_SYS_MODULE", // 16
+	[CAP_SYS_RAWIO] = "CAP_SYS_RAWIO", // 17
+	[CAP_SYS_CHROOT] = "CAP_SYS_CHROOT", // 18
+	[CAP_SYS_PTRACE] = "CAP_SYS_PTRACE", // 19
+	[CAP_SYS_PACCT] = "CAP_SYS_PACCT", // 20
+	[CAP_SYS_ADMIN] = "CAP_SYS_ADMIN", // 21
+	[CAP_SYS_BOOT] = "CAP_SYS_BOOT", // 22
+	[CAP_SYS_NICE] = "CAP_SYS_NICE", // 23
+	[CAP_SYS_RESOURCE] = "CAP_SYS_RESOURCE", // 24
+	[CAP_SYS_TIME] = "CAP_SYS_TIME", // 25
+	[CAP_SYS_TTY_CONFIG] = "CAP_SYS_TTY_CONFIG", // 26
+	[CAP_MKNOD] = "CAP_MKNOD", // 27
+	[CAP_LEASE] = "CAP_LEASE", // 28
+	[CAP_AUDIT_WRITE] = "CAP_AUDIT_WRITE", // 29
+	[CAP_AUDIT_CONTROL] = "CAP_AUDIT_CONTROL", // 30
+	[CAP_SETFCAP] = "CAP_SETFCAP", // 31
+	[CAP_MAC_OVERRIDE] = "CAP_MAC_OVERRIDE", // 32
+	[CAP_MAC_ADMIN] = "CAP_MAC_ADMIN", // 33
+	[CAP_SYSLOG] = "CAP_SYSLOG", // 34
+	[CAP_WAKE_ALARM] = "CAP_WAKE_ALARM", // 35
+	[CAP_BLOCK_SUSPEND] = "CAP_BLOCK_SUSPEND", // 36
+	[CAP_AUDIT_READ] = "CAP_AUDIT_READ", // 37
+	[CAP_PERFMON] = "CAP_PERFMON", // 38
+	[CAP_BPF] = "CAP_BPF", // 39
+	[CAP_CHECKPOINT_RESTORE] = "CAP_CHECKPOINT_RESTORE", // 40
+};
+
+static_assert(CAP_LAST_CAP == 40, "Unexpected number of caps!");
+
+static inline const char *cap2str(int cap)
+{
+	BUG_ON(!cap_valid(cap));
+	return _cap2str[cap];
+}
+
+#endif /* __BHV_CAPABILITY_H__ */
diff --git include/bhv/config.h include/bhv/config.h
new file mode 100644
index 0000000000..13d0ddcee7
--- /dev/null
+++ include/bhv/config.h
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CONFIG_H__
+#define __BHV_CONFIG_H__
+
+#ifdef CONFIG_BRS
+#include <linux/init.h>
+#include <linux/sched.h>
+
+#include <linux/refcount.h>
+#include <linux/rcupdate.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+
+#include <bhv/policy_sks_autogen.h>
+
+#include <bhv/flast.h>
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init brs_mm_init_config(void);
+/************************************************************/
+
+struct brs_policy {
+	flast_buf flast; // The flast object
+	refcount_t refcount; // How many events reference this object.
+	uint32_t id; // Id of the policy. This will be send as part of every message.
+};
+
+extern struct brs_policy *brs_policy;
+
+static inline void brs_put_policy(struct brs_policy *policy)
+{
+	if (likely(policy != NULL) &&
+	    refcount_dec_and_test(&policy->refcount)) {
+		// Last reference. Free the object.
+		vfree(policy->flast.buf);
+		kfree(policy);
+	}
+}
+
+static inline void _brs_get_policy(struct brs_policy *policy)
+{
+	if (likely(policy != NULL))
+		BUG_ON(!refcount_inc_not_zero(&policy->refcount));
+}
+
+static inline struct brs_policy *brs_get_policy(void)
+{
+	struct brs_policy *p;
+
+	rcu_read_lock();
+	p = rcu_dereference(brs_policy);
+	_brs_get_policy(p);
+	rcu_read_unlock();
+	return p;
+}
+
+#ifdef CONFIG_BHV_VAS
+void bhv_policy_update(void);
+#endif // defined CONFIG_BHV_VAS
+bool brs_policy_update(void *policy, size_t sz);
+
+const char *brs_policy_get_modprobe_path(void);
+const char *brs_policy_get_poweroff_cmd(void);
+const char *brs_policy_get_core_pattern(void);
+
+bool brs_policy_reverse_shell_is_interpreter(struct brs_policy *policy,
+					     const char *path);
+
+bool brs_policy_guestlog_protected_unix_sockets_is_empty(
+	struct brs_policy *policy);
+typedef bool (*brs_protected_unix_socket_filter_t)(const char *, void *);
+bool brs_policy_guestlog_match_each_protected_unix_sockets(
+	struct brs_policy *policy, brs_protected_unix_socket_filter_t f,
+	void *arg);
+
+struct task_struct;
+bool brs_is_cgroup_exempt(struct brs_policy *policy, struct task_struct *tsk);
+bool brs_is_process_excluded_from_trace(struct brs_policy *policy, const struct task_struct *task);
+
+#else // defined CONFIG_BRS
+static inline bool brs__is__GuestKernelConfig__userspace_force_nx_stack(void)
+{
+	return false;
+}
+#endif // defined CONFIG_BRS
+#endif /* __BHV_CONFIG_H__ */
diff --git include/bhv/config_structs.h include/bhv/config_structs.h
new file mode 100644
index 0000000000..03f08f6c93
--- /dev/null
+++ include/bhv/config_structs.h
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso@bluerock.io>
+ */
+
+#ifndef __BHV_CONFIG_STRUCTS_H__
+#define __BHV_CONFIG_STRUCTS_H__
+
+#ifdef CONFIG_BRS
+
+#include <linux/cache.h>
+
+#include <bhv/flast.h>
+
+extern flast_buf _boot_policy __section(".bhv.data");
+extern flast_buf _runtime_policy __section(".bhv.data");
+
+#endif // defined CONFIG_BRS
+#endif /* __BHV_CONFIG_STRUCTS_H__ */
diff --git include/bhv/context.h include/bhv/context.h
new file mode 100644
index 0000000000..2ebb4917b2
--- /dev/null
+++ include/bhv/context.h
@@ -0,0 +1,280 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - 2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CONTEXT_H__
+#define __BHV_CONTEXT_H__
+
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/time_namespace.h>
+#include <linux/types.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+
+#include <bhv/brs.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+#define BHV_DEBUG_PRINT_EVENTS 0
+
+static inline int get_file_path(struct path *f_path, char *buf, size_t max_size)
+{
+	char *tmp_buf = NULL;
+	char *path = NULL;
+
+	BUG_ON(f_path == NULL);
+	BUG_ON(buf == NULL);
+
+	/* Avoid allocating this buffer on the stack. */
+	tmp_buf = brs_aa_kmalloc(HypABI__Context__MAX_PATH_SZ);
+	if (tmp_buf == NULL)
+		return -ENOMEM;
+
+	path = d_path(f_path, tmp_buf, HypABI__Context__MAX_PATH_SZ);
+	if (IS_ERR(path)) {
+		kfree(tmp_buf);
+		return PTR_ERR(path);
+	}
+
+	strncpy(buf, path, max_size);
+	buf[max_size - 1] = '\0';
+
+	kfree(tmp_buf);
+
+	return 0;
+}
+
+static inline void get_comm_from_task(struct task_struct *task, char *buf,
+				      size_t buf_sz)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+	BUG_ON(buf_sz == 0);
+
+	strncpy(buf, task->comm, buf_sz);
+	buf[buf_sz - 1] = '\0';
+}
+
+static inline int get_path_from_task(struct task_struct *task, char *buf,
+				     size_t buf_sz, bool get_full_path)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+	BUG_ON(buf_sz == 0);
+
+	if (!get_full_path || task->active_mm == NULL ||
+	    task->active_mm->exe_file == NULL) {
+		strncpy(buf, "UNKNOWN", buf_sz);
+		buf[buf_sz - 1] = '\0';
+		return 0;
+	}
+
+	return get_file_path(&task->active_mm->exe_file->f_path, buf, buf_sz);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	u64 val = current_real_cred()->cap_effective.val;
+	memcpy(&context->cap_effective, &val, sizeof(context->cap_effective));
+	val = current_real_cred()->cap_permitted.val;
+	memcpy(&context->cap_permitted, &val, sizeof(context->cap_permitted));
+}
+#else
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	memcpy(&context->cap_effective,
+	       &current_real_cred()->cap_effective.cap[0],
+	       sizeof(context->cap_effective));
+	memcpy(&context->cap_permitted,
+	       &current_real_cred()->cap_permitted.cap[0],
+	       sizeof(context->cap_permitted));
+}
+#endif
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _bhv_get_ns_inums(struct task_struct *tsk,
+			      HypABI__Context__Inums__T *inums)
+{
+	if (tsk->nsproxy == NULL) {
+		memset(inums, 0, sizeof(HypABI__Context__Inums__T));
+	} else {
+		inums->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		inums->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+		inums->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		rcu_read_lock();
+		inums->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+		inums->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+	}
+}
+#undef NS_DEREF
+
+static int _bhv_get_cgroup_info(struct task_struct *tsk,
+				HypABI__Context__CGroupInfo__T *cgroup)
+{
+	int r;
+	struct cgroup *cgrp;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	cgroup->cgroup_id = cgroup_id(cgrp);
+	r = cgroup_path(cgrp, cgroup->cgroup_name,
+			HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+	rcu_read_unlock();
+	if (r >= HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ)
+		pr_err("BRU: cgroup path being truncated in event! (sz=%d)\n",
+		       r);
+	return r;
+}
+
+static bool is_system_daemon_in_task_hierarchy(struct task_struct *t)
+{
+	bool is_sys_daemon = false;
+	struct task_struct *ancestor = NULL;
+
+	if (!t)
+		return false;
+
+#if BHV_DEBUG_PRINT_EVENTS
+	pr_info("[BHV] %s: [%s] pid=%d\n", __FUNCTION__, t->comm, t->tgid);
+#endif
+
+	/* Only user space processes can be daemons. */
+	if (!t->mm)
+		return false;
+
+	/* Daemons do not have a controlling tty. */
+	if (t->signal->tty)
+		return false;
+
+	/* We consider init a system daemon. */
+	if (is_global_init(t)) {
+#if BHV_DEBUG_PRINT_EVENTS
+		pr_info("[BHV] %s: \t[*] task [%s] pid=%d\n", __FUNCTION__,
+			t->comm, t->tgid);
+#endif
+		return true;
+	}
+
+	/* Check if the process' ancestors have a direct descendant of init. */
+
+	ancestor = t;
+
+	/*
+	 * XXX: Taking the rcu_read_lock leads to a deadlock warning of the
+	 * LOCKDEP mechanism.
+	 */
+	rcu_read_lock();
+
+	while (ancestor && task_tgid_nr(ancestor) != 0) {
+#if BHV_DEBUG_PRINT_EVENTS
+		pr_info("[BHV] %s: \t[*] ancestor [%s] pid=%d tid=%d ppid=%d "
+			"vs ppid=%d (session leader=%d)\n",
+			__FUNCTION__, ancestor->comm, ancestor->tgid,
+			ancestor->pid, ancestor->parent->tgid,
+			task_ppid_nr(ancestor), ancestor->signal->leader);
+#endif
+
+		if (ancestor->signal->leader) {
+			if (task_tgid_nr(ancestor) == 1 ||
+			    task_ppid_nr(ancestor) == 1) {
+				is_sys_daemon = true;
+				break;
+			}
+
+			/*
+			 * The process cannot be a typical system daemon, if we
+			 * find a process along the line of ancestry, which is
+			 * a session lead, yet, its real PPID is not 1. In this
+			 * case, the process was not spawned as a system service.
+			 */
+
+#if BHV_DEBUG_PRINT_EVENTS
+			pr_info("[BHV] %s: \t[*] ancestor->parent [%s] pid=%d "
+				"tid=%d (session leader=%d)\n",
+				__FUNCTION__, ancestor->parent->comm,
+				ancestor->parent->tgid, ancestor->parent->pid,
+				ancestor->parent->signal->leader);
+#endif
+
+			break;
+		}
+
+		ancestor = rcu_dereference(ancestor->real_parent);
+	}
+
+	rcu_read_unlock();
+
+	return is_sys_daemon;
+}
+
+static inline int populate_context(HypABI__Context__T *context,
+				   bool get_full_path)
+{
+	int rv;
+
+	BUG_ON(context == NULL);
+
+	context->vcpu_id = raw_smp_processor_id();
+	context->uid = current_uid().val;
+	context->euid = current_euid().val;
+	context->gid = current_gid().val;
+	context->egid = current_egid().val;
+	_copy_cap(context);
+	context->pid = current->tgid;
+
+	rv = get_path_from_task(current, context->path,
+				HypABI__Context__MAX_PATH_SZ, get_full_path);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	get_comm_from_task(current, context->comm,
+			   HypABI__Context__MAX_PATH_SZ);
+
+	if (current->real_parent != NULL) {
+		context->parent_pid = current->real_parent->tgid;
+
+		get_comm_from_task(current->real_parent, context->parent_comm,
+				   HypABI__Context__MAX_PATH_SZ);
+	} else {
+		context->parent_pid = 0;
+		context->parent_comm[0] = '\0';
+	}
+
+	rv = _bhv_get_cgroup_info(current, &context->cgroup);
+	if (rv <= 0 && rv != -E2BIG)
+		context->cgroup.cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(current, &context->inums);
+
+	context->sys_daemon = (u8)is_system_daemon_in_task_hierarchy(current);
+
+	context->valid = true;
+	return 0;
+}
+
+#endif /* __BHV_CONTEXT_H__ */
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 0000000000..25eaa2840b
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__CredProtection__enabled();
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+/******************************************************************
+ * init
+ ******************************************************************/
+int __init bhv_init_cred(void);
+/******************************************************************/
+
+/******************************************************************
+ * mm_init
+ ******************************************************************/
+void __init bhv_mm_init_cred(void);
+/******************************************************************/
+
+#else /* !CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/domain.h include/bhv/domain.h
new file mode 100644
index 0000000000..04864f6bf7
--- /dev/null
+++ include/bhv/domain.h
@@ -0,0 +1,226 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - 2025 BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __BRS_DOMAIN_H__
+#define __BRS_DOMAIN_H__
+
+#include <linux/mem_namespace.h>
+#include <linux/sched.h>
+#include <linux/sched/task.h>
+#include <linux/mm_types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/domain_pt.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#include <asm/bhv/domain.h>
+#endif
+
+#define BRS_INIT_DOMAIN 0UL
+#define BRS_INVALID_DOMAIN (-1UL)
+
+#ifdef CONFIG_MEM_NS
+
+#if defined(CONFIG_X86_64)
+// #define BRS_VAS_DOMAIN_DEBUG 1
+#endif
+
+#if defined(CONFIG_DOMAIN_SPACES)
+DECLARE_PER_CPU(uint64_t, brs_domain_current_domain);
+#endif
+
+extern bool brs_domain_initialized;
+
+/*
+ * As long as the memory namespaces are not part of the official Linux mainline
+ * sources, they re-purpose the PID namespace instantiation request for their
+ * own creation. Once user space tools become aware of Linux memory namespaces,
+ * we will have to remove this function; otherwise, flags requesting both memory
+ * and PID namespaces would create two memory namespaces.
+ */
+static inline bool brs_check_memns_enable_flags(unsigned long flags)
+{
+	if (flags & (CLONE_NEWMEM | CLONE_NEWPID))
+		return true;
+
+	return false;
+}
+
+static inline bool brs_domain_is_enabled(void)
+{
+	if (!is_bhv_initialized() && !brs_is_standalone())
+		return false;
+
+	return brs__is__StrongIsolation__enabled();
+}
+
+// Initialized and enabled
+static inline bool brs_domain_is_active(void)
+{
+	if (!brs_domain_initialized)
+		return false;
+
+	/*
+	 * XXX: At the moment, the unplugged BRS setting requires memory
+	 * namespaces to be able to allocate and use BRS domain IDs, independent
+	 * of whether or not StrongIsolation was enabled by the policy. The
+	 * reason for this is that the policy will be initialized only after the
+	 * system has booted. At the same time, if the respective
+	 * StrongIsolation-related data structures were not allocated and
+	 * initialized already during boot, StrongIsolation will not be able to
+	 * function correctly because the created processes, will not use valid
+	 * domain IDs.
+	 *
+	 * TODO: To avoid unnecessary resource allocations, e.g., when
+	 * StrongIsolation remains disabled during run-time, consider porting
+	 * the hook-based Strong Isolation functionality to PID namespaces. PID
+	 * namespaces are always present, and will not consume additional
+	 * resources, even if the hook-based StrongIsolation remains disabled.
+	 */
+
+	return brs_is_standalone() || brs_domain_is_enabled();
+}
+
+int brs_domain_mm_init(void);
+
+int brs_domain_create(uint64_t *domid);
+void brs_domain_destroy(uint64_t domid);
+
+#if defined(CONFIG_DOMAIN_SPACES)
+int brs_domain_switch(uint64_t domid);
+#endif
+int brs_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags);
+void brs_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte);
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static inline uint64_t brs_get_active_domain(void)
+{
+	return this_cpu_read(brs_domain_current_domain);
+}
+
+static inline uint64_t brs_get_domain(const struct task_struct *task)
+{
+	return memns_of_task(task)->domain;
+}
+
+static inline void brs_domain_enter(const struct task_struct *next)
+{
+	uint64_t domain_next = brs_get_domain(next);
+	brs_domain_switch(domain_next);
+}
+
+int brs_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns);
+int brs_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec);
+void brs_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm);
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+static inline pgd_t *brs_domain_get_user_pgd(const pgd_t *pgd)
+{
+	pgd_t *pgd_normalized = (pgd_t *)pgd;
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	/*
+	 * We need to ensure that the kernel was not configured to disable KPTI,
+	 * despite CONFIG_PAGE_TABLE_ISOLATION being set. Only then, we can
+	 * normalize the PGD pointer. The normalized PGD pointer ensures that
+	 * BRASS becomes able to always associate both user and kernel memory
+	 * accesses with the virtual address space, and the domain it belongs
+	 * to.
+	 */
+	if (static_cpu_has(X86_FEATURE_PTI))
+		pgd_normalized = brs_domain_arch_get_user_pgd(pgd_normalized);
+#endif
+	return pgd_normalized;
+}
+
+#ifdef BRS_VAS_DOMAIN_DEBUG
+void brs_domain_debug_destroy_pgd(struct task_struct *tsk,
+				  struct mm_struct *mm);
+#else
+static inline void brs_domain_debug_destroy_pgd(struct task_struct *tsk,
+						struct mm_struct *mm)
+{
+}
+#endif
+
+#else /* !CONFIG_MEM_NS */
+
+static inline bool brs_check_memns_enable_flags(unsigned long flags)
+{
+	return false;
+}
+
+static inline bool brs_domain_is_enabled(void)
+{
+	return false;
+}
+
+static inline int brs_domain_create(uint64_t *domid)
+{
+	return 0;
+}
+
+static inline int brs_domain_destroy(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int brs_domain_report(const struct task_struct *t,
+				    const struct mm_struct *mm_target,
+				    const struct vm_area_struct *vma,
+				    unsigned int gup_flags)
+{
+	return 0;
+}
+
+static inline void brs_domain_enter(const struct task_struct *next)
+{
+}
+
+static inline void brs_domain_set_pte_at_kernel(struct mm_struct *mm,
+						unsigned long addr, pte_t *ptep,
+						pte_t pte)
+{
+}
+#endif /* CONFIG_MEM_NS */
+
+#if !defined(CONFIG_MEM_NS) || !defined(CONFIG_DOMAIN_SPACES)
+
+
+static inline uint64_t brs_get_domain(const struct task_struct *task)
+{
+	return 0;
+}
+
+static inline void brs_domain_destroy_pgd(struct task_struct *tsk,
+					  struct mm_struct *mm)
+{
+}
+
+static inline int brs_domain_map_kernel(struct mm_struct *mm, uint64_t pfn,
+					uint64_t nr_pages, bool read,
+					bool write, bool exec)
+{
+	return 0;
+}
+
+static inline int brs_domain_transfer_mm(struct mm_struct *const mm,
+					 struct nsproxy *const old_ns,
+					 struct nsproxy *const new_ns)
+{
+	return 0;
+}
+
+#endif /* !defined(CONFIG_MEM_NS) || !defined(CONFIG_DOMAIN_SPACES) */
+
+#endif /* __BRS_DOMAIN_H__ */
diff --git include/bhv/domain_pt.h include/bhv/domain_pt.h
new file mode 100644
index 0000000000..81fd461133
--- /dev/null
+++ include/bhv/domain_pt.h
@@ -0,0 +1,64 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __BRS_DOMAIN_PT_H__
+#define __BRS_DOMAIN_PT_H__
+
+#include <asm/page.h>
+
+#if defined(CONFIG_DOMAIN_SPACES)
+
+void brs_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte);
+void brs_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd);
+void brs_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud);
+
+void brs_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte);
+void brs_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd);
+void brs_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud);
+
+#else //  !!defined(CONFIG_DOMAIN_SPACES)
+
+static inline void brs_domain_set_pte_at(struct mm_struct *mm,
+					 unsigned long addr, pte_t *ptep,
+					 pte_t pte)
+{
+}
+static inline void brs_domain_set_pmd_at(struct mm_struct *mm,
+					 unsigned long addr, pmd_t *pmdp,
+					 pmd_t pmd)
+{
+}
+static inline void brs_domain_set_pud_at(struct mm_struct *mm,
+					 unsigned long addr, pud_t *pudp,
+					 pud_t pud)
+{
+}
+
+static inline void brs_domain_clear_pte(struct mm_struct *mm,
+					unsigned long addr, pte_t *ptep,
+					pte_t pte)
+{
+}
+static inline void brs_domain_clear_pmd(struct mm_struct *mm,
+					unsigned long addr, pmd_t *pmdp,
+					pmd_t pmd)
+{
+}
+static inline void brs_domain_clear_pud(struct mm_struct *mm,
+					unsigned long addr, pud_t *pudp,
+					pud_t pud)
+{
+}
+
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+#endif // __BRS_DOMAIN_PT_H__
diff --git include/bhv/drift_detection.h include/bhv/drift_detection.h
new file mode 100644
index 0000000000..180b718dc1
--- /dev/null
+++ include/bhv/drift_detection.h
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024-2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ */
+#ifndef __BRS_DRIFT_DETECTION_H__
+#define __BRS_DRIFT_DETECTION_H__
+
+#ifdef CONFIG_BRS
+#include <linux/fs.h>
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+#include <bhv/policy_sks_autogen.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool brs_drift_detection_is_enabled(void)
+{
+	return brs__is__DriftDetection__enabled();
+}
+
+int brs_drift_detection_file_permission(struct brs_policy *policy,
+					struct file *file, int mask);
+bool drift_detection_is_new_file_by_file(struct file *file);
+bool drift_detection_is_new_file_by_filename(const char *filename);
+
+#endif /* CONFIG_BRS */
+
+#endif /* __BRS_DRIFT_DETECTION_H__ */
\ No newline at end of file
diff --git include/bhv/event.h include/bhv/event.h
new file mode 100644
index 0000000000..dcace01c53
--- /dev/null
+++ include/bhv/event.h
@@ -0,0 +1,165 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - 2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 	    Sergej Proskurin <sergej@bedrocksystems.com>
+ * 	    Jonas Pfoh <jonas@bluerock.io>
+ */
+
+#ifndef __BHV_EVENT_H__
+#define __BHV_EVENT_H__
+
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/time_namespace.h>
+#include <linux/types.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+
+#include <bhv/bhv.h>
+#include <bhv/context.h>
+#include <bhv/events_autogen.h>
+#include <bhv/interface/common.h>
+#include <bhv/json_gen.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+static inline void _brs_event_copy_cap(uint64_t *effective_cred,
+				       uint64_t *permitted_cred)
+{
+	*effective_cred = current_real_cred()->cap_effective.val;
+	*permitted_cred = current_real_cred()->cap_permitted.val;
+}
+#else
+static inline void _brs_event_copy_cap(uint64_t *effective_cred,
+				       uint64_t *permitted_cred)
+{
+	memcpy(effective_cred, &current_real_cred()->cap_effective.cap[0],
+	       sizeof(*effective_cred));
+	memcpy(permitted_cred, &current_real_cred()->cap_permitted.cap[0],
+	       sizeof(*permitted_cred));
+}
+#endif
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+#define CLS NamespaceAttributes
+static void _brs_event_get_ns_inums(struct task_struct *tsk, BES(CLS) * bes)
+{
+	BUG_ON(tsk->nsproxy == NULL);
+
+	rcu_read_lock();
+	*bes = (BES(CLS)){
+		.cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns),
+		.ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns),
+		.mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum,
+		.net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns),
+		.pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk)),
+		.pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children),
+		.time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns),
+		.time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children),
+		.user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns),
+		.uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns),
+	};
+	rcu_read_unlock();
+}
+#undef CLS
+#undef NS_DEREF
+
+#define CLS CGroupAttributes
+static void _brs_event_get_cgroup_info(struct task_struct *tsk, BES(CLS) * bes)
+{
+	int rv;
+	struct cgroup *cgrp;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	bes->cgroup_id = cgroup_id(cgrp);
+	// The following will possibly cut off some of the cgroup's path.
+	// Should not be a problem in practice.
+	rv = cgroup_path(cgrp, bes->BES_FLD_MAX(CLS, cgroup_name));
+	rcu_read_unlock();
+	if (rv <= 0)
+		pr_err("BRU: cgroup path fetch failed! (sz=%d)\n", rv);
+	else if (rv >= BES_MAX(CLS, cgroup_name))
+		pr_err("BRU: cgroup path being truncated! (sz=%d)\n", rv);
+}
+#undef CLS
+
+#define CLS CECA_ContextProcessAttributes
+static inline int _brs_event_get_process_info(BES(CLS) * bes,
+					      bool get_full_path)
+{
+	uint64_t effective_cred;
+	uint64_t permitted_cred;
+
+	_brs_event_copy_cap(&effective_cred, &permitted_cred);
+
+	get_path_from_task(current, bes->BES_FLD_MAX(CLS, file_path),
+			   get_full_path);
+
+	get_comm_from_task(current, bes->BES_FLD_MAX(CLS, comm));
+
+	bes->uid = current_uid().val;
+	bes->euid = current_euid().val;
+	bes->gid = current_gid().val;
+	bes->egid = current_egid().val;
+	bes->pid = current->tgid;
+	bes->effective_capability = effective_cred;
+	bes->permitted_capability = permitted_cred;
+	bes->sys_daemon = is_system_daemon_in_task_hierarchy(current);
+
+	return 0;
+}
+#undef CLS
+
+#define CLS BasicProcessAttributes
+static inline int _brs_event_get_parent_process_info(bool *has_parent_process,
+						     BES(CLS) * bes)
+{
+	if (current->real_parent == NULL) {
+		*has_parent_process = false;
+	} else {
+		bes->pid = current->real_parent->tgid;
+		get_comm_from_task(current->real_parent,
+				   bes->BES_FLD_MAX(CLS, comm));
+		*has_parent_process = true;
+	}
+
+	return 0;
+}
+#undef CLS
+
+#define CLS CE_ContextAttributes
+static inline int populate_event_context(BES(CLS) * bes)
+{
+	int rv;
+	BUG_ON(bes == NULL);
+
+	rv = _brs_event_get_process_info(&bes->process, true);
+	if (rv)
+		return rv;
+
+	rv = _brs_event_get_parent_process_info(&bes->has_parent_process,
+						&bes->parent_process);
+	if (rv)
+		return rv;
+
+	_brs_event_get_cgroup_info(current, &bes->cgroup);
+
+	_brs_event_get_ns_inums(current, &bes->namespace);
+
+	return 0;
+}
+#undef CLS
+
+#endif /* __BHV_EVENT_H__ */
diff --git include/bhv/events_autogen.h include/bhv/events_autogen.h
new file mode 100644
index 0000000000..6cfca24999
--- /dev/null
+++ include/bhv/events_autogen.h
@@ -0,0 +1,4848 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by /builds/bedrocksystems/bru/models/octopipe/gen_events_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/kernel.h>
+#include <bhv/json_gen.h>
+#include <bhv/guestconn.h>
+#include <bhv/policy_scks_autogen.h>
+
+enum brs_evt_KAVE_AccessType {
+        BRS_EVT_ACCESSTYPE_READ = 0,
+        BRS_EVT_ACCESSTYPE_WRITE = 1,
+        BRS_EVT_ACCESSTYPE_EXECUTE = 2,
+};
+
+enum brs_evt_UFOE_UnsupportedFileOperationType {
+        BRS_EVT_UNSUPPORTEDFILEOPERATIONTYPE_UNKNOWN = 0,
+        BRS_EVT_UNSUPPORTEDFILEOPERATIONTYPE_FILE = 1,
+        BRS_EVT_UNSUPPORTEDFILEOPERATIONTYPE_DIRECTORY = 2,
+        BRS_EVT_UNSUPPORTEDFILEOPERATIONTYPE_SPECIAL = 3,
+};
+
+struct brs_evt_EventMeta {
+        // name = INVALID
+        // type = INVALID
+        // domain = sensor
+        uint64_t source_event_id;
+};
+
+struct brs_evt_NamespaceAttributes {
+        uint32_t cgroup_ns_inum;
+        uint32_t ipc_ns_inum;
+        uint32_t mnt_ns_inum;
+        uint32_t net_ns_inum;
+        uint32_t pid_ns_inum;
+        uint32_t pid_for_children_ns_inum;
+        uint32_t time_ns_inum;
+        uint32_t time_for_children_ns_inum;
+        uint32_t user_ns_inum;
+        uint32_t uts_ns_inum;
+};
+
+struct brs_evt_BasicProcessAttributes {
+        uint32_t pid;
+        uint16_t skipbytes_comm;
+        #define brs_evt_BasicProcessAttributes_comm_MAX ((size_t)15 + 1)
+        char comm[brs_evt_BasicProcessAttributes_comm_MAX];
+};
+
+struct brs_evt_CGroupAttributes {
+        uint64_t cgroup_id;
+        uint16_t skipbytes_cgroup_name;
+        #define brs_evt_CGroupAttributes_cgroup_name_MAX ((size_t)256 + 1)
+        char cgroup_name[brs_evt_CGroupAttributes_cgroup_name_MAX];
+};
+
+struct brs_evt_CECA_ContextProcessAttributes {
+        uint32_t pid;
+        uint16_t skipbytes_comm;
+        #define brs_evt_CECA_ContextProcessAttributes_comm_MAX ((size_t)15 + 1)
+        char comm[brs_evt_CECA_ContextProcessAttributes_comm_MAX];
+        uint32_t uid;
+        uint32_t euid;
+        uint32_t gid;
+        uint32_t egid;
+        uint64_t effective_capability;
+        uint64_t permitted_capability;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_CECA_ContextProcessAttributes_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_CECA_ContextProcessAttributes_file_path_MAX];
+        bool sys_daemon;
+};
+
+struct brs_evt_CE_ContextAttributes {
+        struct brs_evt_CECA_ContextProcessAttributes process;
+        bool has_parent_process;
+        union {
+                struct brs_evt_BasicProcessAttributes parent_process;
+        };
+        struct brs_evt_CGroupAttributes cgroup;
+        struct brs_evt_NamespaceAttributes namespace;
+};
+
+struct brs_evt_Log {
+        struct brs_evt_EventMeta meta;
+        uint16_t skipbytes_message;
+        #define brs_evt_Log_message_MAX ((size_t)256 + 1)
+        char message[brs_evt_Log_message_MAX];
+};
+
+// maximal event: {"meta":{"name":"brs_log","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"message":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_Log_MAX_SERIALIZED 429
+
+struct brs_evt_Error {
+        struct brs_evt_EventMeta meta;
+        bool global_event_timeout;
+        bool single_event_timeout;
+};
+
+// maximal event: {"meta":{"name":"error","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"global_event_timeout":false,"single_event_timeout":false}"
+#define BRS_EVT_Error_MAX_SERIALIZED 190
+
+struct brs_evt_ProcessFork {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        struct brs_evt_BasicProcessAttributes child;
+        struct brs_evt_BasicProcessAttributes parent;
+};
+
+// maximal event: {"meta":{"name":"process_fork","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"child":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"parent":{"pid":4294967294,"comm":"\\15 bytes \\of d"}}"
+#define BRS_EVT_ProcessFork_MAX_SERIALIZED 1492
+
+struct brs_evt_ProcessExec {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_ProcessExec_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_ProcessExec_file_path_MAX];
+        uint16_t skipbytes_args;
+        #define brs_evt_ProcessExec_args_MAX ((size_t)256 + 1)
+        char args[brs_evt_ProcessExec_args_MAX];
+        uint16_t skipbytes_env;
+        #define brs_evt_ProcessExec_env_MAX ((size_t)256 + 1)
+        char env[brs_evt_ProcessExec_env_MAX];
+        uint16_t skipbytes_fd0;
+        #define brs_evt_ProcessExec_fd0_MAX ((size_t)256 + 1)
+        char fd0[brs_evt_ProcessExec_fd0_MAX];
+        uint16_t skipbytes_fd1;
+        #define brs_evt_ProcessExec_fd1_MAX ((size_t)256 + 1)
+        char fd1[brs_evt_ProcessExec_fd1_MAX];
+        uint16_t skipbytes_fd2;
+        #define brs_evt_ProcessExec_fd2_MAX ((size_t)256 + 1)
+        char fd2[brs_evt_ProcessExec_fd2_MAX];
+        bool newfile;
+        bool newfile_arg;
+        bool has_newfile_arg_path;
+        uint16_t skipbytes_newfile_arg_path;
+        #define brs_evt_ProcessExec_newfile_arg_path_MAX ((size_t)256 + 1)
+        char newfile_arg_path[brs_evt_ProcessExec_newfile_arg_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"process_exec","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","args":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","env":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","fd0":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","fd1":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","fd2":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","newfile":false,"newfile_arg":false,"newfile_arg_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_ProcessExec_MAX_SERIALIZED 3476
+
+struct brs_evt_ProcessExit {
+        uint32_t pid;
+        uint16_t skipbytes_comm;
+        #define brs_evt_ProcessExit_comm_MAX ((size_t)15 + 1)
+        char comm[brs_evt_ProcessExit_comm_MAX];
+        struct brs_evt_EventMeta meta;
+        uint32_t exit_signal;
+        uint32_t exit_code;
+};
+
+// maximal event: {"pid":4294967294,"comm":"\\15 bytes \\of d","meta":{"name":"process_terminate","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"exit_signal":4294967294,"exit_code":4294967294}"
+#define BRS_EVT_ProcessExit_MAX_SERIALIZED 236
+
+struct brs_evt_ExectuableStack {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_ExectuableStack_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_ExectuableStack_file_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"executable_exec_stack","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_ExectuableStack_MAX_SERIALIZED 1689
+
+struct brs_evt_CgroupCreate {
+        uint64_t cgroup_id;
+        uint16_t skipbytes_cgroup_name;
+        #define brs_evt_CgroupCreate_cgroup_name_MAX ((size_t)256 + 1)
+        char cgroup_name[brs_evt_CgroupCreate_cgroup_name_MAX];
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+};
+
+// maximal event: {"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","meta":{"name":"cgroup_create","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}}}"
+#define BRS_EVT_CgroupCreate_MAX_SERIALIZED 1716
+
+struct brs_evt_CgroupDestroy {
+        uint64_t cgroup_id;
+        uint16_t skipbytes_cgroup_name;
+        #define brs_evt_CgroupDestroy_cgroup_name_MAX ((size_t)256 + 1)
+        char cgroup_name[brs_evt_CgroupDestroy_cgroup_name_MAX];
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+};
+
+// maximal event: {"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","meta":{"name":"cgroup_destroy","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}}}"
+#define BRS_EVT_CgroupDestroy_MAX_SERIALIZED 1717
+
+struct brs_evt_NamespaceChange {
+        uint32_t pid;
+        uint16_t skipbytes_comm;
+        #define brs_evt_NamespaceChange_comm_MAX ((size_t)15 + 1)
+        char comm[brs_evt_NamespaceChange_comm_MAX];
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        struct brs_evt_NamespaceAttributes namespace;
+};
+
+// maximal event: {"pid":4294967294,"comm":"\\15 bytes \\of d","meta":{"name":"namespace_change","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}}"
+#define BRS_EVT_NamespaceChange_MAX_SERIALIZED 1727
+
+struct brs_evt_Capable {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        int32_t cap_num;
+        uint16_t skipbytes_cap_name;
+        #define brs_evt_Capable_cap_name_MAX ((size_t)256 + 1)
+        char cap_name[brs_evt_Capable_cap_name_MAX];
+};
+
+// maximal event: {"meta":{"name":"capable","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"cap_num":2147483646,"cap_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_Capable_MAX_SERIALIZED 1695
+
+struct brs_evt_DriverLoad {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_DriverLoad_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_DriverLoad_file_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"driver_load","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_DriverLoad_MAX_SERIALIZED 1679
+
+struct brs_evt_KernelAccessViolation {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint64_t gva;
+        enum brs_evt_KAVE_AccessType type;
+};
+
+// maximal event: {"meta":{"name":"kernel_access_violation","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"gva":18446744073709551614,"type":2}"
+#define BRS_EVT_KernelAccessViolation_MAX_SERIALIZED 1430
+
+struct brs_evt_UnsupportedFileOperation {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint32_t magic;
+        uint64_t fops_gva;
+        enum brs_evt_UFOE_UnsupportedFileOperationType filesystem;
+        uint32_t major;
+        uint64_t minor;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_UnsupportedFileOperation_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_UnsupportedFileOperation_file_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"unsupported_file_operation","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"magic":4294967294,"fops_gva":18446744073709551614,"filesystem":3,"major":4294967294,"minor":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_UnsupportedFileOperation_MAX_SERIALIZED 1808
+
+struct brs_evt_KernelExec {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_KernelExec_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_KernelExec_file_path_MAX];
+        uint16_t skipbytes_args;
+        #define brs_evt_KernelExec_args_MAX ((size_t)256 + 1)
+        char args[brs_evt_KernelExec_args_MAX];
+        uint16_t skipbytes_env;
+        #define brs_evt_KernelExec_env_MAX ((size_t)256 + 1)
+        char env[brs_evt_KernelExec_env_MAX];
+};
+
+// maximal event: {"meta":{"name":"kernel_exec","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","args":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","env":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_KernelExec_MAX_SERIALIZED 2262
+
+struct brs_evt_IBE_InterpreterAttributes {
+        uint32_t pid;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_IBE_InterpreterAttributes_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_IBE_InterpreterAttributes_file_path_MAX];
+        uint32_t socket_fd;
+};
+
+struct brs_evt_InterpreterBound {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        struct brs_evt_IBE_InterpreterAttributes interpreter;
+        uint16_t skipbytes_source_addr;
+        #define brs_evt_InterpreterBound_source_addr_MAX ((size_t)256 + 1)
+        char source_addr[brs_evt_InterpreterBound_source_addr_MAX];
+        uint16_t skipbytes_remote_addr;
+        #define brs_evt_InterpreterBound_remote_addr_MAX ((size_t)256 + 1)
+        char remote_addr[brs_evt_InterpreterBound_remote_addr_MAX];
+};
+
+// maximal event: {"meta":{"name":"interpreter_bound","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"interpreter":{"pid":4294967294,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","socket_fd":4294967294},"source_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","remote_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_InterpreterBound_MAX_SERIALIZED 2339
+
+struct brs_evt_ITE_PiperAttributes {
+        uint32_t pid;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_ITE_PiperAttributes_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_ITE_PiperAttributes_file_path_MAX];
+        uint32_t pipe_fd;
+        uint32_t socket_fd;
+};
+
+struct brs_evt_ITE_InterpreterAttributes {
+        uint32_t pid;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_ITE_InterpreterAttributes_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_ITE_InterpreterAttributes_file_path_MAX];
+        uint32_t pipe_fd;
+};
+
+struct brs_evt_InterpreterTransitive {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        struct brs_evt_ITE_InterpreterAttributes interpreter;
+        struct brs_evt_ITE_PiperAttributes piper;
+        uint16_t skipbytes_source_addr;
+        #define brs_evt_InterpreterTransitive_source_addr_MAX ((size_t)256 + 1)
+        char source_addr[brs_evt_InterpreterTransitive_source_addr_MAX];
+        uint16_t skipbytes_remote_addr;
+        #define brs_evt_InterpreterTransitive_remote_addr_MAX ((size_t)256 + 1)
+        char remote_addr[brs_evt_InterpreterTransitive_remote_addr_MAX];
+};
+
+// maximal event: {"meta":{"name":"interpreter_bound_transitive","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"interpreter":{"pid":4294967294,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","pipe_fd":4294967294},"piper":{"pid":4294967294,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","pipe_fd":4294967294,"socket_fd":4294967294},"source_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","remote_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_InterpreterTransitive_MAX_SERIALIZED 2716
+
+struct brs_evt_SocketConnection {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        bool local;
+        bool domain;
+        bool protected;
+        uint16_t skipbytes_addr;
+        #define brs_evt_SocketConnection_addr_MAX ((size_t)256 + 1)
+        char addr[brs_evt_SocketConnection_addr_MAX];
+};
+
+// maximal event: {"meta":{"name":"socket_connection","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"local":false,"domain":false,"protected":false,"addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_SocketConnection_MAX_SERIALIZED 1727
+
+struct brs_evt_SocketAccept {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_source_addr;
+        #define brs_evt_SocketAccept_source_addr_MAX ((size_t)256 + 1)
+        char source_addr[brs_evt_SocketAccept_source_addr_MAX];
+        uint16_t skipbytes_remote_addr;
+        #define brs_evt_SocketAccept_remote_addr_MAX ((size_t)256 + 1)
+        char remote_addr[brs_evt_SocketAccept_remote_addr_MAX];
+};
+
+// maximal event: {"meta":{"name":"socket_accept","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"source_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","remote_addr":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_SocketAccept_MAX_SERIALIZED 1982
+
+struct brs_evt_FileOpen {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_FileOpen_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_FileOpen_file_path_MAX];
+        bool is_link;
+        bool is_dir;
+        bool is_fifo;
+        bool is_socket;
+        bool is_writeable;
+};
+
+// maximal event: {"meta":{"name":"file_open","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","is_link":false,"is_dir":false,"is_fifo":false,"is_socket":false,"is_writeable":false}"
+#define BRS_EVT_FileOpen_MAX_SERIALIZED 1763
+
+struct brs_evt_FileDrift {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_FileDrift_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_FileDrift_file_path_MAX];
+        bool is_link;
+        bool is_dir;
+        bool is_fifo;
+        bool is_socket;
+        bool is_writeable;
+};
+
+// maximal event: {"meta":{"name":"file_drift","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","is_link":false,"is_dir":false,"is_fifo":false,"is_socket":false,"is_writeable":false}"
+#define BRS_EVT_FileDrift_MAX_SERIALIZED 1764
+
+struct brs_evt_FileSetXattr {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_FileSetXattr_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_FileSetXattr_file_path_MAX];
+        uint16_t skipbytes_name;
+        #define brs_evt_FileSetXattr_name_MAX ((size_t)256 + 1)
+        char name[brs_evt_FileSetXattr_name_MAX];
+        uint16_t skipbytes_value;
+        #define brs_evt_FileSetXattr_value_MAX ((size_t)256 + 1)
+        char value[brs_evt_FileSetXattr_value_MAX];
+};
+
+// maximal event: {"meta":{"name":"file_set_xattr","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","value":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_FileSetXattr_MAX_SERIALIZED 2267
+
+struct brs_evt_StrongIsolation {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint64_t attacker_domain;
+        uint64_t target_domain;
+        uint64_t gva_start;
+        uint64_t gva_end;
+        bool is_write_access;
+        bool blocked;
+};
+
+// maximal event: {"meta":{"name":"strong_isolation","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"attacker_domain":18446744073709551614,"target_domain":18446744073709551614,"gva_start":18446744073709551614,"gva_end":18446744073709551614,"is_write_access":false,"blocked":false}"
+#define BRS_EVT_StrongIsolation_MAX_SERIALIZED 1567
+
+struct brs_evt_ForcedMemAccess {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint64_t gva_start;
+        uint64_t gva_end;
+        bool is_write_access;
+};
+
+// maximal event: {"meta":{"name":"forced_mem_access","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"gva_start":18446744073709551614,"gva_end":18446744073709551614,"is_write_access":false}"
+#define BRS_EVT_ForcedMemAccess_MAX_SERIALIZED 1476
+
+struct brs_evt_MmapExecFile {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_file_path;
+        #define brs_evt_MmapExecFile_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_MmapExecFile_file_path_MAX];
+        bool is_link;
+        bool is_newfile;
+        bool is_memfd;
+};
+
+// maximal event: {"meta":{"name":"mmap_exec_file","type":"event","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","is_link":false,"is_newfile":false,"is_memfd":false}"
+#define BRS_EVT_MmapExecFile_MAX_SERIALIZED 1734
+
+struct brs_evt_LibTrace {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_func_name;
+        #define brs_evt_LibTrace_func_name_MAX ((size_t)256 + 1)
+        char func_name[brs_evt_LibTrace_func_name_MAX];
+        uint16_t skipbytes_file_path;
+        #define brs_evt_LibTrace_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_LibTrace_file_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"lib_trace","type":"nonactionable","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"func_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_LibTrace_MAX_SERIALIZED 1982
+
+struct brs_evt_SkipSymHook {
+        struct brs_evt_EventMeta meta;
+        struct brs_evt_CE_ContextAttributes context;
+        uint16_t skipbytes_func_name;
+        #define brs_evt_SkipSymHook_func_name_MAX ((size_t)256 + 1)
+        char func_name[brs_evt_SkipSymHook_func_name_MAX];
+        uint16_t skipbytes_file_path;
+        #define brs_evt_SkipSymHook_file_path_MAX ((size_t)256 + 1)
+        char file_path[brs_evt_SkipSymHook_file_path_MAX];
+};
+
+// maximal event: {"meta":{"name":"skip_sym_hook","type":"nonactionable","domain":"acoustic","sensor_id":18446744073709551614,"source_event_id":18446744073709551614},"context":{"process":{"pid":4294967294,"comm":"\\15 bytes \\of d","uid":4294967294,"euid":4294967294,"gid":4294967294,"egid":4294967294,"effective_capability":18446744073709551614,"permitted_capability":18446744073709551614,"file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","sys_daemon":false},"parent_process":{"pid":4294967294,"comm":"\\15 bytes \\of d"},"cgroup":{"cgroup_id":18446744073709551614,"cgroup_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"},"namespace":{"cgroup_ns_inum":4294967294,"ipc_ns_inum":4294967294,"mnt_ns_inum":4294967294,"net_ns_inum":4294967294,"pid_ns_inum":4294967294,"pid_for_children_ns_inum":4294967294,"time_ns_inum":4294967294,"time_for_children_ns_inum":4294967294,"user_ns_inum":4294967294,"uts_ns_inum":4294967294}},"func_name":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##","file_path":"\\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ## 256\\ bytes of\\ data ## \\256 bytes\\ of data \\## 256 by\\tes of da\\ta ##"}"
+#define BRS_EVT_SkipSymHook_MAX_SERIALIZED 1986
+
+static inline int _brs_evt_write_EventMeta_SkipSymHook(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "skip_sym_hook", 13 + 1))) {
+                pr_err("%s: Failure writing name=skip_sym_hook (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "nonactionable", 13 + 1))) {
+                pr_err("%s: Failure writing type=nonactionable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_LibTrace(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "lib_trace", 9 + 1))) {
+                pr_err("%s: Failure writing name=lib_trace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "nonactionable", 13 + 1))) {
+                pr_err("%s: Failure writing type=nonactionable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_MmapExecFile(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "mmap_exec_file", 14 + 1))) {
+                pr_err("%s: Failure writing name=mmap_exec_file (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_ForcedMemAccess(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "forced_mem_access", 17 + 1))) {
+                pr_err("%s: Failure writing name=forced_mem_access (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_StrongIsolation(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "strong_isolation", 16 + 1))) {
+                pr_err("%s: Failure writing name=strong_isolation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_FileSetXattr(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "file_set_xattr", 14 + 1))) {
+                pr_err("%s: Failure writing name=file_set_xattr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_FileDrift(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "file_drift", 10 + 1))) {
+                pr_err("%s: Failure writing name=file_drift (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_FileOpen(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "file_open", 9 + 1))) {
+                pr_err("%s: Failure writing name=file_open (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_SocketAccept(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "socket_accept", 13 + 1))) {
+                pr_err("%s: Failure writing name=socket_accept (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_SocketConnection(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "socket_connection", 17 + 1))) {
+                pr_err("%s: Failure writing name=socket_connection (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_InterpreterTransitive(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "interpreter_bound_transitive", 28 + 1))) {
+                pr_err("%s: Failure writing name=interpreter_bound_transitive (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_InterpreterBound(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "interpreter_bound", 17 + 1))) {
+                pr_err("%s: Failure writing name=interpreter_bound (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_KernelExec(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "kernel_exec", 11 + 1))) {
+                pr_err("%s: Failure writing name=kernel_exec (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_UnsupportedFileOperation(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "unsupported_file_operation", 26 + 1))) {
+                pr_err("%s: Failure writing name=unsupported_file_operation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_KernelAccessViolation(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "kernel_access_violation", 23 + 1))) {
+                pr_err("%s: Failure writing name=kernel_access_violation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_DriverLoad(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "driver_load", 11 + 1))) {
+                pr_err("%s: Failure writing name=driver_load (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_Capable(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "capable", 7 + 1))) {
+                pr_err("%s: Failure writing name=capable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_NamespaceChange(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "namespace_change", 16 + 1))) {
+                pr_err("%s: Failure writing name=namespace_change (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_CgroupDestroy(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "cgroup_destroy", 14 + 1))) {
+                pr_err("%s: Failure writing name=cgroup_destroy (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_CgroupCreate(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "cgroup_create", 13 + 1))) {
+                pr_err("%s: Failure writing name=cgroup_create (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_ExectuableStack(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "executable_exec_stack", 21 + 1))) {
+                pr_err("%s: Failure writing name=executable_exec_stack (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_ProcessExit(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "process_terminate", 17 + 1))) {
+                pr_err("%s: Failure writing name=process_terminate (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_ProcessExec(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "process_exec", 12 + 1))) {
+                pr_err("%s: Failure writing name=process_exec (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_ProcessFork(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "process_fork", 12 + 1))) {
+                pr_err("%s: Failure writing name=process_fork (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_Error(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "error", 5 + 1))) {
+                pr_err("%s: Failure writing name=error (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta_Log(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "brs_log", 7 + 1))) {
+                pr_err("%s: Failure writing name=brs_log (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "event", 5 + 1))) {
+                pr_err("%s: Failure writing type=event (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_EventMeta(evt_writer_t *writer, const struct brs_evt_EventMeta *data) {
+        int rv;
+
+        // name: The name of the event.
+        if ((rv = brs_evt_write_string(writer, "name", 4, "INVALID", 7 + 1))) {
+                pr_err("%s: Failure writing name=INVALID (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of the event.
+        if ((rv = brs_evt_write_string(writer, "type", 4, "INVALID", 7 + 1))) {
+                pr_err("%s: Failure writing type=INVALID (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Origin of the event: sensor / acoustic / uc-gyro.
+        if ((rv = brs_evt_write_string(writer, "domain", 6, "sensor", 6 + 1))) {
+                pr_err("%s: Failure writing domain=sensor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // source_event_id: The id of the event. (uint64)
+        if ((rv = brs_evt_write_uint64(writer, "source_event_id", 15, data->source_event_id))) {
+                pr_err("%s: Failure writing source_event_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_NamespaceAttributes(evt_writer_t *writer, const struct brs_evt_NamespaceAttributes *data) {
+        int rv;
+
+        // cgroup_ns_inum: This is the cgroup namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "cgroup_ns_inum", 14, data->cgroup_ns_inum))) {
+                pr_err("%s: Failure writing cgroup_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // ipc_ns_inum: This is the ipc namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "ipc_ns_inum", 11, data->ipc_ns_inum))) {
+                pr_err("%s: Failure writing ipc_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // mnt_ns_inum: This is the mount namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "mnt_ns_inum", 11, data->mnt_ns_inum))) {
+                pr_err("%s: Failure writing mnt_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // net_ns_inum: This is the network namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "net_ns_inum", 11, data->net_ns_inum))) {
+                pr_err("%s: Failure writing net_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // pid_ns_inum: This is the pid namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "pid_ns_inum", 11, data->pid_ns_inum))) {
+                pr_err("%s: Failure writing pid_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // pid_for_children_ns_inum: This is the pid namespace identifier granted to child processes.
+        if ((rv = brs_evt_write_uint64(writer, "pid_for_children_ns_inum", 24, data->pid_for_children_ns_inum))) {
+                pr_err("%s: Failure writing pid_for_children_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // time_ns_inum: This is the time namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "time_ns_inum", 12, data->time_ns_inum))) {
+                pr_err("%s: Failure writing time_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // time_for_children_ns_inum: This is the time namespace identifier granted to child processes.
+        if ((rv = brs_evt_write_uint64(writer, "time_for_children_ns_inum", 25, data->time_for_children_ns_inum))) {
+                pr_err("%s: Failure writing time_for_children_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // user_ns_inum: This is the user namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "user_ns_inum", 12, data->user_ns_inum))) {
+                pr_err("%s: Failure writing user_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // uts_ns_inum: This is the uts namespace identifier.
+        if ((rv = brs_evt_write_uint64(writer, "uts_ns_inum", 11, data->uts_ns_inum))) {
+                pr_err("%s: Failure writing uts_ns_inum (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_BasicProcessAttributes(evt_writer_t *writer, const struct brs_evt_BasicProcessAttributes *data) {
+        int rv;
+
+        // pid: The pid of the process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // comm: The comm field of the process.
+        {
+                if (data->skipbytes_comm > brs_evt_BasicProcessAttributes_comm_MAX) {
+                        pr_err("%s: data->skipbytes_comm=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_comm, brs_evt_BasicProcessAttributes_comm_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "comm", 4, data->comm + data->skipbytes_comm, brs_evt_BasicProcessAttributes_comm_MAX - data->skipbytes_comm))) {
+                        pr_err("%s: Failure writing comm (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_CGroupAttributes(evt_writer_t *writer, const struct brs_evt_CGroupAttributes *data) {
+        int rv;
+
+        // cgroup_id: The cgroup identifier.
+        if ((rv = brs_evt_write_uint64(writer, "cgroup_id", 9, data->cgroup_id))) {
+                pr_err("%s: Failure writing cgroup_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // cgroup_name: The name of the cgroup.
+        {
+                if (data->skipbytes_cgroup_name > brs_evt_CGroupAttributes_cgroup_name_MAX) {
+                        pr_err("%s: data->skipbytes_cgroup_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_cgroup_name, brs_evt_CGroupAttributes_cgroup_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "cgroup_name", 11, data->cgroup_name + data->skipbytes_cgroup_name, brs_evt_CGroupAttributes_cgroup_name_MAX - data->skipbytes_cgroup_name))) {
+                        pr_err("%s: Failure writing cgroup_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_CECA_ContextProcessAttributes(evt_writer_t *writer, const struct brs_evt_CECA_ContextProcessAttributes *data) {
+        int rv;
+
+        // pid: The pid of the process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // comm: The comm field of the process.
+        {
+                if (data->skipbytes_comm > brs_evt_CECA_ContextProcessAttributes_comm_MAX) {
+                        pr_err("%s: data->skipbytes_comm=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_comm, brs_evt_CECA_ContextProcessAttributes_comm_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "comm", 4, data->comm + data->skipbytes_comm, brs_evt_CECA_ContextProcessAttributes_comm_MAX - data->skipbytes_comm))) {
+                        pr_err("%s: Failure writing comm (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // uid: The uid of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "uid", 3, data->uid))) {
+                pr_err("%s: Failure writing uid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // euid: The euid of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "euid", 4, data->euid))) {
+                pr_err("%s: Failure writing euid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // gid: The gid of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "gid", 3, data->gid))) {
+                pr_err("%s: Failure writing gid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // egid: The egid of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "egid", 4, data->egid))) {
+                pr_err("%s: Failure writing egid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // effective_capability: The effective capabilities of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "effective_capability", 20, data->effective_capability))) {
+                pr_err("%s: Failure writing effective_capability (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // permitted_capability: The permitted capabilities of the process in context.
+        if ((rv = brs_evt_write_uint64(writer, "permitted_capability", 20, data->permitted_capability))) {
+                pr_err("%s: Failure writing permitted_capability (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // file_path: The path of the binary of the process in context.
+        {
+                if (data->skipbytes_file_path > brs_evt_CECA_ContextProcessAttributes_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_CECA_ContextProcessAttributes_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_CECA_ContextProcessAttributes_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // sys_daemon: Whether the process in context is a system daemon.
+        if ((rv = brs_evt_write_bool(writer, "sys_daemon", 10, data->sys_daemon))) {
+                pr_err("%s: Failure writing sys_daemon (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_CE_ContextAttributes(evt_writer_t *writer, const struct brs_evt_CE_ContextAttributes *data) {
+        int rv;
+
+        // process: Information about the process in context.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "process", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject process (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CECA_ContextProcessAttributes(writer, &data->process))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject process (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // parent_process: Information about the parent of the process in context.
+        if (data->has_parent_process) {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "parent_process", 14, &doc))) {
+                        pr_err("%s: Failure starting subobject parent_process (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_BasicProcessAttributes(writer, &data->parent_process))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject parent_process (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // cgroup: Information about the cgroup of the process in context.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "cgroup", 6, &doc))) {
+                        pr_err("%s: Failure starting subobject cgroup (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CGroupAttributes(writer, &data->cgroup))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject cgroup (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // namespace: Information about the namespaces of the process in context.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "namespace", 9, &doc))) {
+                        pr_err("%s: Failure starting subobject namespace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_NamespaceAttributes(writer, &data->namespace))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject namespace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_Log(evt_writer_t *writer, const struct brs_evt_Log *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_Log(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // message: The log message
+        {
+                if (data->skipbytes_message > brs_evt_Log_message_MAX) {
+                        pr_err("%s: data->skipbytes_message=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_message, brs_evt_Log_message_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "message", 7, data->message + data->skipbytes_message, brs_evt_Log_message_MAX - data->skipbytes_message))) {
+                        pr_err("%s: Failure writing message (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_Log_ADD_CONTEXT(...)
+
+struct brs_evt_buf_Log {
+        uint8_t buf[BRS_EVT_Log_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_Log(struct brs_evt_Log **data, struct brs_evt_buf_Log **buf) {
+        *data = (struct brs_evt_Log *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_Log));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_Log *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_Log));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_Log));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_Log));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->skipbytes_message = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_Log(struct brs_evt_Log *data, struct brs_evt_buf_Log *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_Log_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event Log (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_Log(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_Error(evt_writer_t *writer, const struct brs_evt_Error *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_Error(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // global_event_timeout: A timeout occurred for the synchronous events.
+        if ((rv = brs_evt_write_bool(writer, "global_event_timeout", 20, data->global_event_timeout))) {
+                pr_err("%s: Failure writing global_event_timeout (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // single_event_timeout: A timeout occurred for an event or the event was interrupted.
+        if ((rv = brs_evt_write_bool(writer, "single_event_timeout", 20, data->single_event_timeout))) {
+                pr_err("%s: Failure writing single_event_timeout (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_Error_ADD_CONTEXT(...)
+
+struct brs_evt_buf_Error {
+        uint8_t buf[BRS_EVT_Error_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_Error(struct brs_evt_Error **data, struct brs_evt_buf_Error **buf) {
+        *data = (struct brs_evt_Error *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_Error));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_Error *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_Error));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_Error));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_Error));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        return 0;
+}
+
+static inline int brs_evt_send_Error(struct brs_evt_Error *data, struct brs_evt_buf_Error *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_Error_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event Error (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_Error(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_ProcessFork(evt_writer_t *writer, const struct brs_evt_ProcessFork *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_ProcessFork(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // child: Information about the new child process.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "child", 5, &doc))) {
+                        pr_err("%s: Failure starting subobject child (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_BasicProcessAttributes(writer, &data->child))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject child (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // parent: Information about the new parent process.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "parent", 6, &doc))) {
+                        pr_err("%s: Failure starting subobject parent (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_BasicProcessAttributes(writer, &data->parent))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject parent (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_ProcessFork_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_ProcessFork {
+        uint8_t buf[BRS_EVT_ProcessFork_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_ProcessFork(struct brs_evt_ProcessFork **data, struct brs_evt_buf_ProcessFork **buf) {
+        *data = (struct brs_evt_ProcessFork *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_ProcessFork));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_ProcessFork *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_ProcessFork));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_ProcessFork));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_ProcessFork));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->child.skipbytes_comm = 0;
+        (*data)->parent.skipbytes_comm = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_ProcessFork(struct brs_evt_ProcessFork *data, struct brs_evt_buf_ProcessFork *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_ProcessFork_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event ProcessFork (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_ProcessFork(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_ProcessExec(evt_writer_t *writer, const struct brs_evt_ProcessExec *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_ProcessExec(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the binary being executed.
+        {
+                if (data->skipbytes_file_path > brs_evt_ProcessExec_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_ProcessExec_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_ProcessExec_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // args: The arguments passed to the binary being executed.
+        {
+                if (data->skipbytes_args > brs_evt_ProcessExec_args_MAX) {
+                        pr_err("%s: data->skipbytes_args=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_args, brs_evt_ProcessExec_args_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "args", 4, data->args + data->skipbytes_args, brs_evt_ProcessExec_args_MAX - data->skipbytes_args))) {
+                        pr_err("%s: Failure writing args (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // env: The environment passed to the binary being executed.
+        {
+                if (data->skipbytes_env > brs_evt_ProcessExec_env_MAX) {
+                        pr_err("%s: data->skipbytes_env=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_env, brs_evt_ProcessExec_env_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "env", 3, data->env + data->skipbytes_env, brs_evt_ProcessExec_env_MAX - data->skipbytes_env))) {
+                        pr_err("%s: Failure writing env (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // fd0: The type of device attached to fd0 (stdin) and its path.
+        {
+                if (data->skipbytes_fd0 > brs_evt_ProcessExec_fd0_MAX) {
+                        pr_err("%s: data->skipbytes_fd0=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_fd0, brs_evt_ProcessExec_fd0_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "fd0", 3, data->fd0 + data->skipbytes_fd0, brs_evt_ProcessExec_fd0_MAX - data->skipbytes_fd0))) {
+                        pr_err("%s: Failure writing fd0 (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // fd1: The type of device attached to fd1 (stdout) and its path.
+        {
+                if (data->skipbytes_fd1 > brs_evt_ProcessExec_fd1_MAX) {
+                        pr_err("%s: data->skipbytes_fd1=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_fd1, brs_evt_ProcessExec_fd1_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "fd1", 3, data->fd1 + data->skipbytes_fd1, brs_evt_ProcessExec_fd1_MAX - data->skipbytes_fd1))) {
+                        pr_err("%s: Failure writing fd1 (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // fd2: The type of device attached to fd2 (stderr) and its path.
+        {
+                if (data->skipbytes_fd2 > brs_evt_ProcessExec_fd2_MAX) {
+                        pr_err("%s: data->skipbytes_fd2=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_fd2, brs_evt_ProcessExec_fd2_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "fd2", 3, data->fd2 + data->skipbytes_fd2, brs_evt_ProcessExec_fd2_MAX - data->skipbytes_fd2))) {
+                        pr_err("%s: Failure writing fd2 (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // newfile: Whether the binary being executed is a new file.
+        if ((rv = brs_evt_write_bool(writer, "newfile", 7, data->newfile))) {
+                pr_err("%s: Failure writing newfile (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // newfile_arg: Whether a new file is passed as an argument to the binary being executed.
+        if ((rv = brs_evt_write_bool(writer, "newfile_arg", 11, data->newfile_arg))) {
+                pr_err("%s: Failure writing newfile_arg (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // newfile_arg_path: The path of the new file passed as an argument (if newfile_arg=True).
+        if (data->has_newfile_arg_path) {
+                if (data->skipbytes_newfile_arg_path > brs_evt_ProcessExec_newfile_arg_path_MAX) {
+                        pr_err("%s: data->skipbytes_newfile_arg_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_newfile_arg_path, brs_evt_ProcessExec_newfile_arg_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "newfile_arg_path", 16, data->newfile_arg_path + data->skipbytes_newfile_arg_path, brs_evt_ProcessExec_newfile_arg_path_MAX - data->skipbytes_newfile_arg_path))) {
+                        pr_err("%s: Failure writing newfile_arg_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_ProcessExec_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_ProcessExec {
+        uint8_t buf[BRS_EVT_ProcessExec_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_ProcessExec(struct brs_evt_ProcessExec **data, struct brs_evt_buf_ProcessExec **buf) {
+        *data = (struct brs_evt_ProcessExec *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_ProcessExec));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_ProcessExec *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_ProcessExec));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_ProcessExec));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_ProcessExec));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+        (*data)->skipbytes_args = 0;
+        (*data)->skipbytes_env = 0;
+        (*data)->skipbytes_fd0 = 0;
+        (*data)->skipbytes_fd1 = 0;
+        (*data)->skipbytes_fd2 = 0;
+        (*data)->has_newfile_arg_path = false;
+        (*data)->skipbytes_newfile_arg_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_ProcessExec(struct brs_evt_ProcessExec *data, struct brs_evt_buf_ProcessExec *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_ProcessExec_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event ProcessExec (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_ProcessExec(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_PROCESS_EXEC(&writer);
+}
+
+static inline int _brs_evt_write_ProcessExit(evt_writer_t *writer, const struct brs_evt_ProcessExit *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_ProcessExit(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // pid: The pid of the process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // comm: The comm field of the process.
+        {
+                if (data->skipbytes_comm > brs_evt_ProcessExit_comm_MAX) {
+                        pr_err("%s: data->skipbytes_comm=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_comm, brs_evt_ProcessExit_comm_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "comm", 4, data->comm + data->skipbytes_comm, brs_evt_ProcessExit_comm_MAX - data->skipbytes_comm))) {
+                        pr_err("%s: Failure writing comm (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // exit_signal: The signal that the process received before exiting.
+        if ((rv = brs_evt_write_uint64(writer, "exit_signal", 11, data->exit_signal))) {
+                pr_err("%s: Failure writing exit_signal (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // exit_code: The exit code of the process.
+        if ((rv = brs_evt_write_uint64(writer, "exit_code", 9, data->exit_code))) {
+                pr_err("%s: Failure writing exit_code (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_ProcessExit_ADD_CONTEXT(...)
+
+struct brs_evt_buf_ProcessExit {
+        uint8_t buf[BRS_EVT_ProcessExit_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_ProcessExit(struct brs_evt_ProcessExit **data, struct brs_evt_buf_ProcessExit **buf) {
+        *data = (struct brs_evt_ProcessExit *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_ProcessExit));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_ProcessExit *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_ProcessExit));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_ProcessExit));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_ProcessExit));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->skipbytes_comm = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_ProcessExit(struct brs_evt_ProcessExit *data, struct brs_evt_buf_ProcessExit *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_ProcessExit_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event ProcessExit (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_ProcessExit(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_ExectuableStack(evt_writer_t *writer, const struct brs_evt_ExectuableStack *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_ExectuableStack(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path of the binary having an executable stack.
+        {
+                if (data->skipbytes_file_path > brs_evt_ExectuableStack_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_ExectuableStack_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_ExectuableStack_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_ExectuableStack_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_ExectuableStack {
+        uint8_t buf[BRS_EVT_ExectuableStack_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_ExectuableStack(struct brs_evt_ExectuableStack **data, struct brs_evt_buf_ExectuableStack **buf) {
+        *data = (struct brs_evt_ExectuableStack *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_ExectuableStack));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_ExectuableStack *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_ExectuableStack));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_ExectuableStack));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_ExectuableStack));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_ExectuableStack(struct brs_evt_ExectuableStack *data, struct brs_evt_buf_ExectuableStack *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_ExectuableStack_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event ExectuableStack (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_ExectuableStack(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_EXECUTABLE_EXEC_STACK(&writer);
+}
+
+static inline int _brs_evt_write_CgroupCreate(evt_writer_t *writer, const struct brs_evt_CgroupCreate *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_CgroupCreate(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // cgroup_id: The cgroup identifier.
+        if ((rv = brs_evt_write_uint64(writer, "cgroup_id", 9, data->cgroup_id))) {
+                pr_err("%s: Failure writing cgroup_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // cgroup_name: The name of the cgroup.
+        {
+                if (data->skipbytes_cgroup_name > brs_evt_CgroupCreate_cgroup_name_MAX) {
+                        pr_err("%s: data->skipbytes_cgroup_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_cgroup_name, brs_evt_CgroupCreate_cgroup_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "cgroup_name", 11, data->cgroup_name + data->skipbytes_cgroup_name, brs_evt_CgroupCreate_cgroup_name_MAX - data->skipbytes_cgroup_name))) {
+                        pr_err("%s: Failure writing cgroup_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_CgroupCreate_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_CgroupCreate {
+        uint8_t buf[BRS_EVT_CgroupCreate_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_CgroupCreate(struct brs_evt_CgroupCreate **data, struct brs_evt_buf_CgroupCreate **buf) {
+        *data = (struct brs_evt_CgroupCreate *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_CgroupCreate));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_CgroupCreate *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_CgroupCreate));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_CgroupCreate));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_CgroupCreate));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->skipbytes_cgroup_name = 0;
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_CgroupCreate(struct brs_evt_CgroupCreate *data, struct brs_evt_buf_CgroupCreate *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_CgroupCreate_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event CgroupCreate (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_CgroupCreate(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_CgroupDestroy(evt_writer_t *writer, const struct brs_evt_CgroupDestroy *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_CgroupDestroy(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // cgroup_id: The cgroup identifier.
+        if ((rv = brs_evt_write_uint64(writer, "cgroup_id", 9, data->cgroup_id))) {
+                pr_err("%s: Failure writing cgroup_id (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // cgroup_name: The name of the cgroup.
+        {
+                if (data->skipbytes_cgroup_name > brs_evt_CgroupDestroy_cgroup_name_MAX) {
+                        pr_err("%s: data->skipbytes_cgroup_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_cgroup_name, brs_evt_CgroupDestroy_cgroup_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "cgroup_name", 11, data->cgroup_name + data->skipbytes_cgroup_name, brs_evt_CgroupDestroy_cgroup_name_MAX - data->skipbytes_cgroup_name))) {
+                        pr_err("%s: Failure writing cgroup_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_CgroupDestroy_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_CgroupDestroy {
+        uint8_t buf[BRS_EVT_CgroupDestroy_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_CgroupDestroy(struct brs_evt_CgroupDestroy **data, struct brs_evt_buf_CgroupDestroy **buf) {
+        *data = (struct brs_evt_CgroupDestroy *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_CgroupDestroy));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_CgroupDestroy *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_CgroupDestroy));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_CgroupDestroy));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_CgroupDestroy));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->skipbytes_cgroup_name = 0;
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_CgroupDestroy(struct brs_evt_CgroupDestroy *data, struct brs_evt_buf_CgroupDestroy *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_CgroupDestroy_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event CgroupDestroy (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_CgroupDestroy(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_NamespaceChange(evt_writer_t *writer, const struct brs_evt_NamespaceChange *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_NamespaceChange(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // pid: The pid of the process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // comm: The comm field of the process.
+        {
+                if (data->skipbytes_comm > brs_evt_NamespaceChange_comm_MAX) {
+                        pr_err("%s: data->skipbytes_comm=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_comm, brs_evt_NamespaceChange_comm_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "comm", 4, data->comm + data->skipbytes_comm, brs_evt_NamespaceChange_comm_MAX - data->skipbytes_comm))) {
+                        pr_err("%s: Failure writing comm (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // namespace: Information about the new namespace.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "namespace", 9, &doc))) {
+                        pr_err("%s: Failure starting subobject namespace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_NamespaceAttributes(writer, &data->namespace))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject namespace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_NamespaceChange_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_NamespaceChange {
+        uint8_t buf[BRS_EVT_NamespaceChange_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_NamespaceChange(struct brs_evt_NamespaceChange **data, struct brs_evt_buf_NamespaceChange **buf) {
+        *data = (struct brs_evt_NamespaceChange *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_NamespaceChange));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_NamespaceChange *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_NamespaceChange));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_NamespaceChange));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_NamespaceChange));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_NamespaceChange(struct brs_evt_NamespaceChange *data, struct brs_evt_buf_NamespaceChange *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_NamespaceChange_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event NamespaceChange (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_NamespaceChange(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_NAMESPACE_CHANGE(&writer);
+}
+
+static inline int _brs_evt_write_Capable(evt_writer_t *writer, const struct brs_evt_Capable *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_Capable(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // cap_num: The Linux capability number of the capability being requested by the current task.
+        if ((rv = brs_evt_write_uint64(writer, "cap_num", 7, data->cap_num))) {
+                pr_err("%s: Failure writing cap_num (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // cap_name: The Linux capability name of the capability being requested by the current task.
+        {
+                if (data->skipbytes_cap_name > brs_evt_Capable_cap_name_MAX) {
+                        pr_err("%s: data->skipbytes_cap_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_cap_name, brs_evt_Capable_cap_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "cap_name", 8, data->cap_name + data->skipbytes_cap_name, brs_evt_Capable_cap_name_MAX - data->skipbytes_cap_name))) {
+                        pr_err("%s: Failure writing cap_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_Capable_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_Capable {
+        uint8_t buf[BRS_EVT_Capable_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_Capable(struct brs_evt_Capable **data, struct brs_evt_buf_Capable **buf) {
+        *data = (struct brs_evt_Capable *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_Capable));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_Capable *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_Capable));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_Capable));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_Capable));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_cap_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_Capable(struct brs_evt_Capable *data, struct brs_evt_buf_Capable *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_Capable_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event Capable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_Capable(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_DriverLoad(evt_writer_t *writer, const struct brs_evt_DriverLoad *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_DriverLoad(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: A NUL-terminated ASCII string that contains the path of the binary that was loaded as a driver.
+        {
+                if (data->skipbytes_file_path > brs_evt_DriverLoad_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_DriverLoad_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_DriverLoad_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_DriverLoad_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_DriverLoad {
+        uint8_t buf[BRS_EVT_DriverLoad_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_DriverLoad(struct brs_evt_DriverLoad **data, struct brs_evt_buf_DriverLoad **buf) {
+        *data = (struct brs_evt_DriverLoad *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_DriverLoad));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_DriverLoad *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_DriverLoad));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_DriverLoad));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_DriverLoad));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_DriverLoad(struct brs_evt_DriverLoad *data, struct brs_evt_buf_DriverLoad *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_DriverLoad_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event DriverLoad (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_DriverLoad(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_DRIVER_LOAD(&writer);
+}
+
+static inline int _brs_evt_write_KernelAccessViolation(evt_writer_t *writer, const struct brs_evt_KernelAccessViolation *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_KernelAccessViolation(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // gva: The address that caused the access violation.
+        if ((rv = brs_evt_write_uint64(writer, "gva", 3, data->gva))) {
+                pr_err("%s: Failure writing gva (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // type: The type of access.
+        BUG_ON((uint64_t)data->type > 2);
+        if ((rv = brs_evt_write_uint64(writer, "type", 4, (uint64_t)data->type))) {
+                pr_err("%s: Failure writing type (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_KernelAccessViolation_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_KernelAccessViolation {
+        uint8_t buf[BRS_EVT_KernelAccessViolation_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_KernelAccessViolation(struct brs_evt_KernelAccessViolation **data, struct brs_evt_buf_KernelAccessViolation **buf) {
+        *data = (struct brs_evt_KernelAccessViolation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_KernelAccessViolation));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_KernelAccessViolation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_KernelAccessViolation));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_KernelAccessViolation));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_KernelAccessViolation));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_KernelAccessViolation(struct brs_evt_KernelAccessViolation *data, struct brs_evt_buf_KernelAccessViolation *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_KernelAccessViolation_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event KernelAccessViolation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_KernelAccessViolation(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_UnsupportedFileOperation(evt_writer_t *writer, const struct brs_evt_UnsupportedFileOperation *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_UnsupportedFileOperation(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // magic: The file-system magic number as defined in [Linux](https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/magic.h).
+        if ((rv = brs_evt_write_uint64(writer, "magic", 5, data->magic))) {
+                pr_err("%s: Failure writing magic (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // fops_gva: The address of the file operations pointer
+        if ((rv = brs_evt_write_uint64(writer, "fops_gva", 8, data->fops_gva))) {
+                pr_err("%s: Failure writing fops_gva (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // filesystem: The type of the file struct using the unknown file operation instance.
+        BUG_ON((uint64_t)data->filesystem > 3);
+        if ((rv = brs_evt_write_uint64(writer, "filesystem", 10, (uint64_t)data->filesystem))) {
+                pr_err("%s: Failure writing filesystem (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // major: Major device number for the unknown file operation instance (only relevant for special files).
+        if ((rv = brs_evt_write_uint64(writer, "major", 5, data->major))) {
+                pr_err("%s: Failure writing major (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // minor: Minor device number for the unknown file operation instance (only relevant for special files).
+        if ((rv = brs_evt_write_uint64(writer, "minor", 5, data->minor))) {
+                pr_err("%s: Failure writing minor (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // file_path: The name of the accessed file or directory which used the unknown file operations instance.
+        {
+                if (data->skipbytes_file_path > brs_evt_UnsupportedFileOperation_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_UnsupportedFileOperation_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_UnsupportedFileOperation_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_UnsupportedFileOperation_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_UnsupportedFileOperation {
+        uint8_t buf[BRS_EVT_UnsupportedFileOperation_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_UnsupportedFileOperation(struct brs_evt_UnsupportedFileOperation **data, struct brs_evt_buf_UnsupportedFileOperation **buf) {
+        *data = (struct brs_evt_UnsupportedFileOperation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_UnsupportedFileOperation));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_UnsupportedFileOperation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_UnsupportedFileOperation));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_UnsupportedFileOperation));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_UnsupportedFileOperation));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_UnsupportedFileOperation(struct brs_evt_UnsupportedFileOperation *data, struct brs_evt_buf_UnsupportedFileOperation *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_UnsupportedFileOperation_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event UnsupportedFileOperation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_UnsupportedFileOperation(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_UNSUPPORTED_FILE_OPERATION(&writer);
+}
+
+static inline int _brs_evt_write_KernelExec(evt_writer_t *writer, const struct brs_evt_KernelExec *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_KernelExec(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the program that is being executed.
+        {
+                if (data->skipbytes_file_path > brs_evt_KernelExec_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_KernelExec_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_KernelExec_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // args: The arguments passed to the program that is being executed.
+        {
+                if (data->skipbytes_args > brs_evt_KernelExec_args_MAX) {
+                        pr_err("%s: data->skipbytes_args=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_args, brs_evt_KernelExec_args_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "args", 4, data->args + data->skipbytes_args, brs_evt_KernelExec_args_MAX - data->skipbytes_args))) {
+                        pr_err("%s: Failure writing args (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // env: The environment passed to the program that is being executed.
+        {
+                if (data->skipbytes_env > brs_evt_KernelExec_env_MAX) {
+                        pr_err("%s: data->skipbytes_env=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_env, brs_evt_KernelExec_env_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "env", 3, data->env + data->skipbytes_env, brs_evt_KernelExec_env_MAX - data->skipbytes_env))) {
+                        pr_err("%s: Failure writing env (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_KernelExec_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_KernelExec {
+        uint8_t buf[BRS_EVT_KernelExec_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_KernelExec(struct brs_evt_KernelExec **data, struct brs_evt_buf_KernelExec **buf) {
+        *data = (struct brs_evt_KernelExec *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_KernelExec));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_KernelExec *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_KernelExec));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_KernelExec));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_KernelExec));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+        (*data)->skipbytes_args = 0;
+        (*data)->skipbytes_env = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_KernelExec(struct brs_evt_KernelExec *data, struct brs_evt_buf_KernelExec *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_KernelExec_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event KernelExec (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_KernelExec(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_KERNEL_EXEC(&writer);
+}
+
+static inline int _brs_evt_write_IBE_InterpreterAttributes(evt_writer_t *writer, const struct brs_evt_IBE_InterpreterAttributes *data) {
+        int rv;
+
+        // pid: The pid of the interpreter process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // file_path: The path of the interpreter.
+        {
+                if (data->skipbytes_file_path > brs_evt_IBE_InterpreterAttributes_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_IBE_InterpreterAttributes_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_IBE_InterpreterAttributes_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // socket_fd: The file descriptor number of the socket.
+        if ((rv = brs_evt_write_uint64(writer, "socket_fd", 9, data->socket_fd))) {
+                pr_err("%s: Failure writing socket_fd (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_InterpreterBound(evt_writer_t *writer, const struct brs_evt_InterpreterBound *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_InterpreterBound(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // interpreter: Information about the interpreter that was bound.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "interpreter", 11, &doc))) {
+                        pr_err("%s: Failure starting subobject interpreter (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_IBE_InterpreterAttributes(writer, &data->interpreter))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject interpreter (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // source_addr: The source address of the connection.
+        {
+                if (data->skipbytes_source_addr > brs_evt_InterpreterBound_source_addr_MAX) {
+                        pr_err("%s: data->skipbytes_source_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_source_addr, brs_evt_InterpreterBound_source_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "source_addr", 11, data->source_addr + data->skipbytes_source_addr, brs_evt_InterpreterBound_source_addr_MAX - data->skipbytes_source_addr))) {
+                        pr_err("%s: Failure writing source_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // remote_addr: The destination address of the connection.
+        {
+                if (data->skipbytes_remote_addr > brs_evt_InterpreterBound_remote_addr_MAX) {
+                        pr_err("%s: data->skipbytes_remote_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_remote_addr, brs_evt_InterpreterBound_remote_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "remote_addr", 11, data->remote_addr + data->skipbytes_remote_addr, brs_evt_InterpreterBound_remote_addr_MAX - data->skipbytes_remote_addr))) {
+                        pr_err("%s: Failure writing remote_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_InterpreterBound_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_InterpreterBound {
+        uint8_t buf[BRS_EVT_InterpreterBound_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_InterpreterBound(struct brs_evt_InterpreterBound **data, struct brs_evt_buf_InterpreterBound **buf) {
+        *data = (struct brs_evt_InterpreterBound *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_InterpreterBound));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_InterpreterBound *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_InterpreterBound));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_InterpreterBound));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_InterpreterBound));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->interpreter.skipbytes_file_path = 0;
+        (*data)->skipbytes_source_addr = 0;
+        (*data)->skipbytes_remote_addr = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_InterpreterBound(struct brs_evt_InterpreterBound *data, struct brs_evt_buf_InterpreterBound *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_InterpreterBound_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event InterpreterBound (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_InterpreterBound(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_INTERPRETER_BOUND(&writer);
+}
+
+static inline int _brs_evt_write_ITE_PiperAttributes(evt_writer_t *writer, const struct brs_evt_ITE_PiperAttributes *data) {
+        int rv;
+
+        // pid: The pid of the transitive process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // file_path: The path of the process which is piping into the interpreter and has a socket connection.
+        {
+                if (data->skipbytes_file_path > brs_evt_ITE_PiperAttributes_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_ITE_PiperAttributes_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_ITE_PiperAttributes_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // pipe_fd: The file descriptor number used for the pipe to the interpreter process.
+        if ((rv = brs_evt_write_uint64(writer, "pipe_fd", 7, data->pipe_fd))) {
+                pr_err("%s: Failure writing pipe_fd (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // socket_fd: The file descriptor number used for the network connection.
+        if ((rv = brs_evt_write_uint64(writer, "socket_fd", 9, data->socket_fd))) {
+                pr_err("%s: Failure writing socket_fd (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_ITE_InterpreterAttributes(evt_writer_t *writer, const struct brs_evt_ITE_InterpreterAttributes *data) {
+        int rv;
+
+        // pid: The pid of the interpreter process.
+        if ((rv = brs_evt_write_uint64(writer, "pid", 3, data->pid))) {
+                pr_err("%s: Failure writing pid (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // file_path: The path of the interpreter.
+        {
+                if (data->skipbytes_file_path > brs_evt_ITE_InterpreterAttributes_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_ITE_InterpreterAttributes_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_ITE_InterpreterAttributes_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // pipe_fd: The file descriptor number used for the pipe.
+        if ((rv = brs_evt_write_uint64(writer, "pipe_fd", 7, data->pipe_fd))) {
+                pr_err("%s: Failure writing pipe_fd (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+static inline int _brs_evt_write_InterpreterTransitive(evt_writer_t *writer, const struct brs_evt_InterpreterTransitive *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_InterpreterTransitive(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // interpreter: Information about the interpreter that was bound.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "interpreter", 11, &doc))) {
+                        pr_err("%s: Failure starting subobject interpreter (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_ITE_InterpreterAttributes(writer, &data->interpreter))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject interpreter (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // piper: Information about the process with a socket connection and a pipe to the interpreter.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "piper", 5, &doc))) {
+                        pr_err("%s: Failure starting subobject piper (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_ITE_PiperAttributes(writer, &data->piper))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject piper (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // source_addr: The source address of the connection.
+        {
+                if (data->skipbytes_source_addr > brs_evt_InterpreterTransitive_source_addr_MAX) {
+                        pr_err("%s: data->skipbytes_source_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_source_addr, brs_evt_InterpreterTransitive_source_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "source_addr", 11, data->source_addr + data->skipbytes_source_addr, brs_evt_InterpreterTransitive_source_addr_MAX - data->skipbytes_source_addr))) {
+                        pr_err("%s: Failure writing source_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // remote_addr: The destination address of the connection.
+        {
+                if (data->skipbytes_remote_addr > brs_evt_InterpreterTransitive_remote_addr_MAX) {
+                        pr_err("%s: data->skipbytes_remote_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_remote_addr, brs_evt_InterpreterTransitive_remote_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "remote_addr", 11, data->remote_addr + data->skipbytes_remote_addr, brs_evt_InterpreterTransitive_remote_addr_MAX - data->skipbytes_remote_addr))) {
+                        pr_err("%s: Failure writing remote_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_InterpreterTransitive_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_InterpreterTransitive {
+        uint8_t buf[BRS_EVT_InterpreterTransitive_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_InterpreterTransitive(struct brs_evt_InterpreterTransitive **data, struct brs_evt_buf_InterpreterTransitive **buf) {
+        *data = (struct brs_evt_InterpreterTransitive *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_InterpreterTransitive));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_InterpreterTransitive *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_InterpreterTransitive));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_InterpreterTransitive));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_InterpreterTransitive));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->interpreter.skipbytes_file_path = 0;
+        (*data)->piper.skipbytes_file_path = 0;
+        (*data)->skipbytes_source_addr = 0;
+        (*data)->skipbytes_remote_addr = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_InterpreterTransitive(struct brs_evt_InterpreterTransitive *data, struct brs_evt_buf_InterpreterTransitive *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_InterpreterTransitive_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event InterpreterTransitive (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_InterpreterTransitive(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_INTERPRETER_BOUND_TRANSITIVE(&writer);
+}
+
+static inline int _brs_evt_write_SocketConnection(evt_writer_t *writer, const struct brs_evt_SocketConnection *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_SocketConnection(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // local: Whether this event is about a socket bind (true) or a remote connection (false)
+        if ((rv = brs_evt_write_bool(writer, "local", 5, data->local))) {
+                pr_err("%s: Failure writing local (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // domain: Whether this is a domain or network socket.
+        if ((rv = brs_evt_write_bool(writer, "domain", 6, data->domain))) {
+                pr_err("%s: Failure writing domain (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // protected: Whether this is a protected domain socket.
+        if ((rv = brs_evt_write_bool(writer, "protected", 9, data->protected))) {
+                pr_err("%s: Failure writing protected (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // addr: The address the socket was bound to/connected to.
+        {
+                if (data->skipbytes_addr > brs_evt_SocketConnection_addr_MAX) {
+                        pr_err("%s: data->skipbytes_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_addr, brs_evt_SocketConnection_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "addr", 4, data->addr + data->skipbytes_addr, brs_evt_SocketConnection_addr_MAX - data->skipbytes_addr))) {
+                        pr_err("%s: Failure writing addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_SocketConnection_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_SocketConnection {
+        uint8_t buf[BRS_EVT_SocketConnection_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_SocketConnection(struct brs_evt_SocketConnection **data, struct brs_evt_buf_SocketConnection **buf) {
+        *data = (struct brs_evt_SocketConnection *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_SocketConnection));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_SocketConnection *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_SocketConnection));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_SocketConnection));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_SocketConnection));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_addr = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_SocketConnection(struct brs_evt_SocketConnection *data, struct brs_evt_buf_SocketConnection *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_SocketConnection_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event SocketConnection (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_SocketConnection(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_SOCKET_CONNECTION(&writer);
+}
+
+static inline int _brs_evt_write_SocketAccept(evt_writer_t *writer, const struct brs_evt_SocketAccept *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_SocketAccept(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // source_addr: The local address of the connection.
+        {
+                if (data->skipbytes_source_addr > brs_evt_SocketAccept_source_addr_MAX) {
+                        pr_err("%s: data->skipbytes_source_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_source_addr, brs_evt_SocketAccept_source_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "source_addr", 11, data->source_addr + data->skipbytes_source_addr, brs_evt_SocketAccept_source_addr_MAX - data->skipbytes_source_addr))) {
+                        pr_err("%s: Failure writing source_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // remote_addr: The remote address of the connection.
+        {
+                if (data->skipbytes_remote_addr > brs_evt_SocketAccept_remote_addr_MAX) {
+                        pr_err("%s: data->skipbytes_remote_addr=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_remote_addr, brs_evt_SocketAccept_remote_addr_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "remote_addr", 11, data->remote_addr + data->skipbytes_remote_addr, brs_evt_SocketAccept_remote_addr_MAX - data->skipbytes_remote_addr))) {
+                        pr_err("%s: Failure writing remote_addr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_SocketAccept_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_SocketAccept {
+        uint8_t buf[BRS_EVT_SocketAccept_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_SocketAccept(struct brs_evt_SocketAccept **data, struct brs_evt_buf_SocketAccept **buf) {
+        *data = (struct brs_evt_SocketAccept *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_SocketAccept));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_SocketAccept *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_SocketAccept));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_SocketAccept));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_SocketAccept));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_source_addr = 0;
+        (*data)->skipbytes_remote_addr = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_SocketAccept(struct brs_evt_SocketAccept *data, struct brs_evt_buf_SocketAccept *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_SocketAccept_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event SocketAccept (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_SocketAccept(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_SOCKET_ACCEPT(&writer);
+}
+
+static inline int _brs_evt_write_FileOpen(evt_writer_t *writer, const struct brs_evt_FileOpen *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_FileOpen(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the file being opened
+        {
+                if (data->skipbytes_file_path > brs_evt_FileOpen_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_FileOpen_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_FileOpen_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // is_link: Whether the file opened is a link.
+        if ((rv = brs_evt_write_bool(writer, "is_link", 7, data->is_link))) {
+                pr_err("%s: Failure writing is_link (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_dir: Whether the file opened is a directory.
+        if ((rv = brs_evt_write_bool(writer, "is_dir", 6, data->is_dir))) {
+                pr_err("%s: Failure writing is_dir (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_fifo: Whether the file opened is a pipe.
+        if ((rv = brs_evt_write_bool(writer, "is_fifo", 7, data->is_fifo))) {
+                pr_err("%s: Failure writing is_fifo (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_socket: Whether the file opened is a socket.
+        if ((rv = brs_evt_write_bool(writer, "is_socket", 9, data->is_socket))) {
+                pr_err("%s: Failure writing is_socket (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_writeable: Whether the file is opened in a write-able manner.
+        if ((rv = brs_evt_write_bool(writer, "is_writeable", 12, data->is_writeable))) {
+                pr_err("%s: Failure writing is_writeable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_FileOpen_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_FileOpen {
+        uint8_t buf[BRS_EVT_FileOpen_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_FileOpen(struct brs_evt_FileOpen **data, struct brs_evt_buf_FileOpen **buf) {
+        *data = (struct brs_evt_FileOpen *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_FileOpen));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_FileOpen *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_FileOpen));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_FileOpen));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_FileOpen));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_FileOpen(struct brs_evt_FileOpen *data, struct brs_evt_buf_FileOpen *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_FileOpen_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event FileOpen (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_FileOpen(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_FILE_OPEN(&writer);
+}
+
+static inline int _brs_evt_write_FileDrift(evt_writer_t *writer, const struct brs_evt_FileDrift *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_FileDrift(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the file being modified.
+        {
+                if (data->skipbytes_file_path > brs_evt_FileDrift_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_FileDrift_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_FileDrift_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // is_link: Whether the file is a link.
+        if ((rv = brs_evt_write_bool(writer, "is_link", 7, data->is_link))) {
+                pr_err("%s: Failure writing is_link (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_dir: Whether the file is a directory.
+        if ((rv = brs_evt_write_bool(writer, "is_dir", 6, data->is_dir))) {
+                pr_err("%s: Failure writing is_dir (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_fifo: Whether the file is a pipe.
+        if ((rv = brs_evt_write_bool(writer, "is_fifo", 7, data->is_fifo))) {
+                pr_err("%s: Failure writing is_fifo (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_socket: Whether the file is a socket.
+        if ((rv = brs_evt_write_bool(writer, "is_socket", 9, data->is_socket))) {
+                pr_err("%s: Failure writing is_socket (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_writeable: Whether the file can be written to.
+        if ((rv = brs_evt_write_bool(writer, "is_writeable", 12, data->is_writeable))) {
+                pr_err("%s: Failure writing is_writeable (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_FileDrift_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_FileDrift {
+        uint8_t buf[BRS_EVT_FileDrift_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_FileDrift(struct brs_evt_FileDrift **data, struct brs_evt_buf_FileDrift **buf) {
+        *data = (struct brs_evt_FileDrift *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_FileDrift));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_FileDrift *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_FileDrift));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_FileDrift));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_FileDrift));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_FileDrift(struct brs_evt_FileDrift *data, struct brs_evt_buf_FileDrift *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_FileDrift_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event FileDrift (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_FileDrift(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_FILE_DRIFT(&writer);
+}
+
+static inline int _brs_evt_write_FileSetXattr(evt_writer_t *writer, const struct brs_evt_FileSetXattr *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_FileSetXattr(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the file that has its xattr modified.
+        {
+                if (data->skipbytes_file_path > brs_evt_FileSetXattr_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_FileSetXattr_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_FileSetXattr_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // name: The name of the xattr being set.
+        {
+                if (data->skipbytes_name > brs_evt_FileSetXattr_name_MAX) {
+                        pr_err("%s: data->skipbytes_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_name, brs_evt_FileSetXattr_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "name", 4, data->name + data->skipbytes_name, brs_evt_FileSetXattr_name_MAX - data->skipbytes_name))) {
+                        pr_err("%s: Failure writing name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // value: The value for the new xattr entry.
+        {
+                if (data->skipbytes_value > brs_evt_FileSetXattr_value_MAX) {
+                        pr_err("%s: data->skipbytes_value=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_value, brs_evt_FileSetXattr_value_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "value", 5, data->value + data->skipbytes_value, brs_evt_FileSetXattr_value_MAX - data->skipbytes_value))) {
+                        pr_err("%s: Failure writing value (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_FileSetXattr_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_FileSetXattr {
+        uint8_t buf[BRS_EVT_FileSetXattr_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_FileSetXattr(struct brs_evt_FileSetXattr **data, struct brs_evt_buf_FileSetXattr **buf) {
+        *data = (struct brs_evt_FileSetXattr *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_FileSetXattr));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_FileSetXattr *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_FileSetXattr));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_FileSetXattr));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_FileSetXattr));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+        (*data)->skipbytes_name = 0;
+        (*data)->skipbytes_value = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_FileSetXattr(struct brs_evt_FileSetXattr *data, struct brs_evt_buf_FileSetXattr *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_FileSetXattr_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event FileSetXattr (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_FileSetXattr(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_FILE_SET_XATTR(&writer);
+}
+
+static inline int _brs_evt_write_StrongIsolation(evt_writer_t *writer, const struct brs_evt_StrongIsolation *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_StrongIsolation(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // attacker_domain: The domain that intends to request access permissions.
+        if ((rv = brs_evt_write_uint64(writer, "attacker_domain", 15, data->attacker_domain))) {
+                pr_err("%s: Failure writing attacker_domain (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // target_domain: The domain that was requested to be accessed.
+        if ((rv = brs_evt_write_uint64(writer, "target_domain", 13, data->target_domain))) {
+                pr_err("%s: Failure writing target_domain (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // gva_start: The start of the GVA range that was requested to be accessed.
+        if ((rv = brs_evt_write_uint64(writer, "gva_start", 9, data->gva_start))) {
+                pr_err("%s: Failure writing gva_start (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // gva_end: The end of the GVA range that was requested to be accessed.
+        if ((rv = brs_evt_write_uint64(writer, "gva_end", 7, data->gva_end))) {
+                pr_err("%s: Failure writing gva_end (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_write_access: Differentiates the requested access permission between read and read/write.
+        if ((rv = brs_evt_write_bool(writer, "is_write_access", 15, data->is_write_access))) {
+                pr_err("%s: Failure writing is_write_access (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // blocked: Whether the access has been blocked.
+        if ((rv = brs_evt_write_bool(writer, "blocked", 7, data->blocked))) {
+                pr_err("%s: Failure writing blocked (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_StrongIsolation_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_StrongIsolation {
+        uint8_t buf[BRS_EVT_StrongIsolation_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_StrongIsolation(struct brs_evt_StrongIsolation **data, struct brs_evt_buf_StrongIsolation **buf) {
+        *data = (struct brs_evt_StrongIsolation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_StrongIsolation));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_StrongIsolation *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_StrongIsolation));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_StrongIsolation));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_StrongIsolation));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_StrongIsolation(struct brs_evt_StrongIsolation *data, struct brs_evt_buf_StrongIsolation *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_StrongIsolation_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event StrongIsolation (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_StrongIsolation(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_ForcedMemAccess(evt_writer_t *writer, const struct brs_evt_ForcedMemAccess *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_ForcedMemAccess(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // gva_start: The start of the GVA range that was requested to be accessed.
+        if ((rv = brs_evt_write_uint64(writer, "gva_start", 9, data->gva_start))) {
+                pr_err("%s: Failure writing gva_start (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // gva_end: The end of the GVA range that was requested to be accessed.
+        if ((rv = brs_evt_write_uint64(writer, "gva_end", 7, data->gva_end))) {
+                pr_err("%s: Failure writing gva_end (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_write_access: Differentiates the requested access permission between read and read/write.
+        if ((rv = brs_evt_write_bool(writer, "is_write_access", 15, data->is_write_access))) {
+                pr_err("%s: Failure writing is_write_access (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_ForcedMemAccess_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_ForcedMemAccess {
+        uint8_t buf[BRS_EVT_ForcedMemAccess_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_ForcedMemAccess(struct brs_evt_ForcedMemAccess **data, struct brs_evt_buf_ForcedMemAccess **buf) {
+        *data = (struct brs_evt_ForcedMemAccess *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_ForcedMemAccess));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_ForcedMemAccess *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_ForcedMemAccess));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_ForcedMemAccess));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_ForcedMemAccess));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_ForcedMemAccess(struct brs_evt_ForcedMemAccess *data, struct brs_evt_buf_ForcedMemAccess *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_ForcedMemAccess_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event ForcedMemAccess (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_ForcedMemAccess(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_FORCED_MEM_ACCESS(&writer);
+}
+
+static inline int _brs_evt_write_MmapExecFile(evt_writer_t *writer, const struct brs_evt_MmapExecFile *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_MmapExecFile(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the executable file being mmapped.
+        {
+                if (data->skipbytes_file_path > brs_evt_MmapExecFile_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_MmapExecFile_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_MmapExecFile_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // is_link: Whether the executable file to be mmapped is a link.
+        if ((rv = brs_evt_write_bool(writer, "is_link", 7, data->is_link))) {
+                pr_err("%s: Failure writing is_link (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_newfile: Whether the library is a new file/has been modified.
+        if ((rv = brs_evt_write_bool(writer, "is_newfile", 10, data->is_newfile))) {
+                pr_err("%s: Failure writing is_newfile (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        // is_memfd: Whether the executable to be mmapped is fileless.
+        if ((rv = brs_evt_write_bool(writer, "is_memfd", 8, data->is_memfd))) {
+                pr_err("%s: Failure writing is_memfd (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                return rv;
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_MmapExecFile_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_MmapExecFile {
+        uint8_t buf[BRS_EVT_MmapExecFile_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_MmapExecFile(struct brs_evt_MmapExecFile **data, struct brs_evt_buf_MmapExecFile **buf) {
+        *data = (struct brs_evt_MmapExecFile *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_MmapExecFile));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_MmapExecFile *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_MmapExecFile));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_MmapExecFile));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_MmapExecFile));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_MmapExecFile(struct brs_evt_MmapExecFile *data, struct brs_evt_buf_MmapExecFile *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_MmapExecFile_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event MmapExecFile (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_MmapExecFile(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return BRS_SEND_EVT_MSG_MMAP_EXEC_FILE(&writer);
+}
+
+static inline int _brs_evt_write_LibTrace(evt_writer_t *writer, const struct brs_evt_LibTrace *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_LibTrace(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // func_name: The name of the exported function that is being executed.
+        {
+                if (data->skipbytes_func_name > brs_evt_LibTrace_func_name_MAX) {
+                        pr_err("%s: data->skipbytes_func_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_func_name, brs_evt_LibTrace_func_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "func_name", 9, data->func_name + data->skipbytes_func_name, brs_evt_LibTrace_func_name_MAX - data->skipbytes_func_name))) {
+                        pr_err("%s: Failure writing func_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the executable exporting the called function.
+        {
+                if (data->skipbytes_file_path > brs_evt_LibTrace_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_LibTrace_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_LibTrace_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_LibTrace_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_LibTrace {
+        uint8_t buf[BRS_EVT_LibTrace_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_LibTrace(struct brs_evt_LibTrace **data, struct brs_evt_buf_LibTrace **buf) {
+        *data = (struct brs_evt_LibTrace *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_LibTrace));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_LibTrace *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_LibTrace));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_LibTrace));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_LibTrace));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_func_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_LibTrace(struct brs_evt_LibTrace *data, struct brs_evt_buf_LibTrace *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_LibTrace_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event LibTrace (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_LibTrace(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+static inline int _brs_evt_write_SkipSymHook(evt_writer_t *writer, const struct brs_evt_SkipSymHook *data) {
+        int rv;
+
+        // meta: Common event information fields.
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "meta", 4, &doc))) {
+                        pr_err("%s: Failure starting subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_EventMeta_SkipSymHook(writer, &data->meta))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject meta (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // context: All the information stored in the context
+        {
+                evt_writer_doc_t doc;
+                if ((rv = brs_evt_start_doc(writer, "context", 7, &doc))) {
+                        pr_err("%s: Failure starting subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+                if ((rv = _brs_evt_write_CE_ContextAttributes(writer, &data->context))) {
+                        return rv;
+                }
+                if ((rv = brs_evt_close_doc(writer, &doc))) {
+                        pr_err("%s: Failure closing subobject context (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // func_name: The name of the exported function that cannot be hooked.
+        {
+                if (data->skipbytes_func_name > brs_evt_SkipSymHook_func_name_MAX) {
+                        pr_err("%s: data->skipbytes_func_name=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_func_name, brs_evt_SkipSymHook_func_name_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "func_name", 9, data->func_name + data->skipbytes_func_name, brs_evt_SkipSymHook_func_name_MAX - data->skipbytes_func_name))) {
+                        pr_err("%s: Failure writing func_name (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        // file_path: The path to the executable exporting the called function.
+        {
+                if (data->skipbytes_file_path > brs_evt_SkipSymHook_file_path_MAX) {
+                        pr_err("%s: data->skipbytes_file_path=%d > %zu (%s:%d)!", __FUNCTION__, data->skipbytes_file_path, brs_evt_SkipSymHook_file_path_MAX, __FILE__, __LINE__);
+                        BUG();
+                }
+                if ((rv = brs_evt_write_string(writer, "file_path", 9, data->file_path + data->skipbytes_file_path, brs_evt_SkipSymHook_file_path_MAX - data->skipbytes_file_path))) {
+                        pr_err("%s: Failure writing file_path (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                        return rv;
+                }
+        }
+
+        return 0;
+}
+
+#define BRS_EVT_SkipSymHook_ADD_CONTEXT(...) populate_event_context(__VA_ARGS__)
+
+struct brs_evt_buf_SkipSymHook {
+        uint8_t buf[BRS_EVT_SkipSymHook_MAX_SERIALIZED];
+};
+
+static inline int brs_evt_alloc_SkipSymHook(struct brs_evt_SkipSymHook **data, struct brs_evt_buf_SkipSymHook **buf) {
+        *data = (struct brs_evt_SkipSymHook *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_SkipSymHook));
+        if (!*data) {
+                return -ENOMEM;
+        }
+
+        *buf = (struct brs_evt_buf_SkipSymHook *) brs_guestconn_alloc_msg(sizeof(struct brs_evt_buf_SkipSymHook));
+        if (!*buf) {
+                brs_guestconn_free_msg(*data);
+                return -ENOMEM;
+        }
+
+        #ifdef CONFIG_BHV_VAS_DEBUG
+                memset(*data, 'A', sizeof(struct brs_evt_SkipSymHook));
+                memset(*buf, 'A', sizeof(struct brs_evt_buf_SkipSymHook));
+        #endif //CONFIG_BHV_VAS_DEBUG
+
+        (*data)->context.process.skipbytes_comm = 0;
+        (*data)->context.process.skipbytes_file_path = 0;
+        (*data)->context.has_parent_process = false;
+        (*data)->context.parent_process.skipbytes_comm = 0;
+        (*data)->context.cgroup.skipbytes_cgroup_name = 0;
+        (*data)->skipbytes_func_name = 0;
+        (*data)->skipbytes_file_path = 0;
+
+        return 0;
+}
+
+static inline int brs_evt_send_SkipSymHook(struct brs_evt_SkipSymHook *data, struct brs_evt_buf_SkipSymHook *buf) {
+        int rv;
+        evt_writer_t writer;
+
+        if ((rv = brs_evt_write_start(&writer, buf->buf, BRS_EVT_SkipSymHook_MAX_SERIALIZED))) {
+                pr_err("%s: Failure starting event SkipSymHook (%s:%d)!", __FUNCTION__, __FILE__, __LINE__);
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        writer.data->header.id = data->meta.source_event_id;
+
+        if ((rv = _brs_evt_write_SkipSymHook(&writer, data))) {
+                brs_guestconn_free_msg(data);
+                brs_guestconn_free_msg(buf);
+                return rv;
+        }
+
+        brs_guestconn_free_msg(data);
+        return send_evt_msg(&writer);
+}
+
+#undef brs_evt_write_start
+#pragma GCC poison brs_evt_write_start
+#undef brs_evt_write_string
+#pragma GCC poison brs_evt_write_string
+#undef brs_evt_write_bool
+#pragma GCC poison brs_evt_write_bool
+#undef brs_evt_write_int64
+#pragma GCC poison brs_evt_write_int64
+#undef brs_evt_write_uint64
+#pragma GCC poison brs_evt_write_uint64
+#undef brs_evt_start_doc
+#pragma GCC poison brs_evt_start_doc
+#undef brs_evt_close_doc
+#pragma GCC poison brs_evt_close_doc
+
+#define BES(EVT) struct CONCATENATE(brs_evt_, EVT)
+#define BES_MAX(EVT, FN) CONCATENATE(CONCATENATE(brs_evt_, EVT), _##FN##_MAX)
+#define BES_FLD_MAX(EVT, FN) FN, BES_MAX(EVT, FN)
+#define BES_STRSCPY(EVT, DATA, FN, SRC) strscpy(DATA FN, SRC, BES_MAX(EVT, FN))
+
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 0000000000..60582c64ad
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void);
+/***********************************************************************/
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!brs__is__FileProtection__read_only__enabled())
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_fileops_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!brs__is__FileProtection__file_ops__enabled())
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_dirtycred_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!brs__is__FileProtection__dirtycred__enabled())
+		return false;
+
+	return true;
+}
+
+bool bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+	const char *target);
+bool bhv_block_read_only_file_write_ViolationDirtyCredWrite(const char *target);
+
+bool bhv_block_read_only_file_write(const char *target, bool dirtycred);
+void bhv_check_file_dirty_cred(struct file *file, int mask);
+
+#else /* !CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+static inline void bhv_check_file_dirty_cred(struct file *, int)
+{
+	return;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_internal.h include/bhv/fileops_internal.h
new file mode 100644
index 0000000000..7885114579
--- /dev/null
+++ include/bhv/fileops_internal.h
@@ -0,0 +1,39 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS__
+#define __BHV_FILEOPS__
+
+// used by security/bhv.c
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h> // simple_dir_operations
+#include <linux/ramfs.h> // ramfs_file_operations
+#include <linux/printk.h> // kmsg_fops
+#include <linux/mnt_namespace.h> // proc_mount{s,stats,info}_operations
+
+#define FOPS(sym) extern const struct file_operations sym;
+#include <bhv/fileops_internal_symlist.h>
+
+typedef const struct file_operations *fops_t[2];
+
+// basic regular file + directory file ops
+extern const fops_t fileops_map[];
+
+// additional /proc/ file operations
+extern struct file_operations const *proc_fops[] __ro_after_init;
+
+/******************************************************************
+ * init
+ ******************************************************************/
+void __init bhv_init_fileops(void);
+/******************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **);
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr);
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS__ */
diff --git include/bhv/fileops_internal_fopsmap.h include/bhv/fileops_internal_fopsmap.h
new file mode 100644
index 0000000000..56d27e0f67
--- /dev/null
+++ include/bhv/fileops_internal_fopsmap.h
@@ -0,0 +1,49 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+
+#if defined CONFIG_EXT4_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // ext4 files and dirs
+FOPS_MAP(ext4, FT(EXT4), ext4_file_operations, ext4_dir_operations)
+#endif
+        // tmpfs files and dirs
+FOPS_MAP(tmpfs, FT(TMPFS), shmem_file_operations, simple_dir_operations)
+        // sockets
+FOPS_MAP_DIRNULL(sockfs, FT(SOCKFS), socket_file_ops)
+        // pipes
+FOPS_MAP_DIRNULL(pipefs, FT(PIPEFS), pipefifo_fops)
+        // special chardevs
+#if defined CONFIG_DEVMEM || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+FOPS_MAP_DIRNULL(mem, FT(DEV_MEM), mem_fops)
+#endif
+FOPS_MAP_DIRNULL(null, FT(DEV_NULL), null_fops)
+FOPS_MAP_DIRNULL(port, FT(DEV_PORT), port_fops)
+FOPS_MAP_DIRNULL(zero, FT(DEV_ZERO), zero_fops)
+FOPS_MAP_DIRNULL(full, FT(DEV_FULL), full_fops)
+FOPS_MAP_DIRNULL(random, FT(DEV_RANDOM), random_fops)
+FOPS_MAP_DIRNULL(urandom, FT(DEV_URANDOM), urandom_fops)
+FOPS_MAP_DIRNULL(kmsg, FT(DEV_KMSG), kmsg_fops)
+FOPS_MAP_DIRNULL(tty, FT(DEV_TTY), tty_fops)
+FOPS_MAP_DIRNULL(console, FT(DEV_CONSOLE), console_fops)
+        // proc basic
+FOPS_MAP(proc, FT(PROC), proc_reg_file_ops, proc_root_operations)
+#if defined CONFIG_XFS_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // xfs files and dirs
+FOPS_MAP(xfs, FT(XFS), xfs_file_operations, xfs_dir_file_operations)
+#endif
+        // sys fs
+FOPS_MAP(sysfs, FT(SYSFS), kernfs_file_fops, kernfs_dir_fops)
+
+#undef FT
+#undef FOPS_MAP
+#undef FOPS_MAP_DIRNULL
+#ifdef FILEOPS_INTERNAL_FOPSMAP_ALL
+#undef FILEOPS_INTERNAL_FOPSMAP_ALL
+#endif
\ No newline at end of file
diff --git include/bhv/fileops_internal_symlist.h include/bhv/fileops_internal_symlist.h
new file mode 100644
index 0000000000..a297917a4a
--- /dev/null
+++ include/bhv/fileops_internal_symlist.h
@@ -0,0 +1,184 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#ifndef FOPS_PROC
+#define FOPS_PROC FOPS
+#endif
+
+// fs/char_dev.c
+FOPS(def_chr_fops)
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+FOPS(ext4_dir_operations)
+FOPS(ext4_file_operations)
+#endif
+
+// ramfs/file-[no]mmu.c
+FOPS(ramfs_file_operations)
+// mm/shmem.c
+#ifdef CONFIG_SHMEM
+FOPS(shmem_file_operations)
+#else
+#define shmem_file_operations ramfs_file_operations
+#endif
+// fs/libfs.c
+FOPS(simple_dir_operations)
+// drivers/char/mem.c
+#ifdef CONFIG_DEVMEM
+FOPS(mem_fops)
+#endif
+FOPS(null_fops)
+FOPS(port_fops)
+FOPS(zero_fops)
+FOPS(full_fops)
+// drivers/char/random.c
+FOPS(random_fops)
+FOPS(urandom_fops)
+// kernel/printk/printk.c
+FOPS(kmsg_fops)
+// drivers/tty/tty_io.c
+FOPS(tty_fops)
+FOPS(console_fops)
+FOPS(hung_up_tty_fops)
+
+#ifdef CONFIG_XFS_FS
+// fs/xfs/xfs_iops.h
+FOPS(xfs_dir_file_operations)
+FOPS(xfs_file_operations)
+#endif
+
+// net/sockets.c
+FOPS(socket_file_ops)
+// fs/pipe.c
+FOPS(pipefifo_fops)
+
+// fs/kernfs/file.c
+FOPS(kernfs_file_fops)
+// fs/kernfs/dir.c
+FOPS(kernfs_dir_fops)
+
+// DEBUGFS
+// sys/kernel/debug
+FOPS(debugfs_noop_file_operations)
+FOPS(debugfs_open_proxy_file_operations)
+FOPS(debugfs_full_proxy_file_operations)
+
+// used by SOCKFS
+// fs/inode.c
+FOPS(no_open_fops)
+
+#ifdef CONFIG_INOTIFY_USER
+// fs/notify/inotify/inotify_user.c
+FOPS(inotify_fops)
+#endif
+
+// block/fops.c
+FOPS(def_blk_fops)
+
+// PROC:
+// fs/proc/inode.c
+FOPS_PROC(proc_reg_file_ops)
+FOPS_PROC(proc_iter_file_ops)
+#ifdef CONFIG_COMPAT
+FOPS_PROC(proc_reg_file_ops_compat)
+FOPS_PROC(proc_iter_file_ops_compat)
+#endif
+// fs/proc/root.c
+FOPS_PROC(proc_root_operations)
+// fs/proc/proc_sysctl.c
+FOPS_PROC(proc_sys_file_operations)
+FOPS_PROC(proc_sys_dir_file_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fd_operations)
+// fs/proc/base.c
+FOPS_PROC(proc_oom_score_adj_operations)
+FOPS_PROC(proc_pid_cmdline_ops)
+#ifdef CONFIG_LATENCYTOP
+FOPS_PROC(proc_lstats_operations)
+#endif
+FOPS_PROC(proc_mem_operations)
+FOPS_PROC(proc_environ_operations)
+FOPS_PROC(proc_auxv_operations)
+FOPS_PROC(proc_oom_adj_operations)
+FOPS_PROC(proc_loginuid_operations)
+#ifdef CONFIG_AUDIT
+FOPS_PROC(proc_sessionid_operations)
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+FOPS_PROC(proc_fault_inject_operations)
+FOPS_PROC(proc_fail_nth_operations)
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+FOPS_PROC(proc_pid_sched_operations)
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+FOPS_PROC(proc_pid_sched_autogroup_operations)
+#endif
+#ifdef CONFIG_TIME_NS
+FOPS_PROC(proc_timens_offsets_operations)
+#endif
+FOPS_PROC(proc_pid_set_comm_operations)
+FOPS_PROC(proc_map_files_operations)
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+FOPS_PROC(proc_timers_operations)
+#endif
+FOPS_PROC(proc_pid_set_timerslack_ns_operations)
+#ifdef CONFIG_SECURITY
+FOPS_PROC(proc_pid_attr_operations)
+FOPS_PROC(proc_attr_dir_operations)
+#endif
+#ifdef CONFIG_ELF_CORE
+FOPS_PROC(proc_coredump_filter_operations)
+#endif
+#ifdef CONFIG_USER_NS
+FOPS_PROC(proc_uid_map_operations)
+FOPS_PROC(proc_gid_map_operations)
+FOPS_PROC(proc_projid_map_operations)
+FOPS_PROC(proc_setgroups_operations)
+#endif
+FOPS_PROC(proc_tgid_base_operations)
+FOPS_PROC(proc_tid_base_operations)
+FOPS_PROC(proc_task_operations)
+FOPS_PROC(proc_single_file_operations)
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+FOPS_PROC(proc_zram_block_state_op)
+#endif
+#ifdef CONFIG_PAGE_OWNER
+// mm/page_owner.c
+FOPS_PROC(proc_page_owner_operations)
+#endif
+// fs/proc/internal.h
+FOPS_PROC(proc_ns_dir_operations)
+FOPS_PROC(proc_net_operations)
+FOPS_PROC(proc_pid_maps_operations)
+#ifdef CONFIG_NUMA
+FOPS_PROC(proc_pid_numa_maps_operations)
+#endif
+FOPS_PROC(proc_pid_smaps_operations)
+FOPS_PROC(proc_pid_smaps_rollup_operations)
+FOPS_PROC(proc_clear_refs_operations)
+FOPS_PROC(proc_pagemap_operations)
+#ifdef CONFIG_PROC_CHILDREN
+FOPS_PROC(proc_tid_children_operations)
+#endif
+// fs/proc/generic.c
+FOPS_PROC(proc_dir_operations)
+// fs/proc/fd.h
+FOPS_PROC(proc_fdinfo_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fdinfo_file_operations)
+// fs/proc_namespace.c
+FOPS_PROC(proc_mounts_operations)
+FOPS_PROC(proc_mountinfo_operations)
+FOPS_PROC(proc_mountstats_operations)
+
+FOPS_PROC(empty_dir_operations)
+
+#undef FOPS
+#undef FOPS_PROC
\ No newline at end of file
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 0000000000..e659ac5f58
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <linux/init.h>
+#include <bhv/interface/common.h>
+
+bool bhv_strict_fileops_enforced(void);
+bool bhv_block_fileops(const char *, u8, bool, const void *);
+u8 bhv_fileops_type(u32 fs_magic);
+bool bhv_fileops_is_ro(u64 f_op);
+
+#endif // CONFIG_BHV_VAS
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/flast.h include/bhv/flast.h
new file mode 100644
index 0000000000..53fcb459f2
--- /dev/null
+++ include/bhv/flast.h
@@ -0,0 +1,884 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ *          Tommaso Frassetto <tommaso@bluerock.io>
+ */
+
+/**
+ * Good day your flastulency. What do you want to flast today?
+ *
+ * IMPORTANT
+ *  - Flast is not thread-safe
+ *  - We cannot add fields to dicts once they have been created
+ */
+
+#ifndef __FLAST_H__
+#define __FLAST_H__
+
+#ifdef __cplusplus
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wold-style-cast"
+#pragma GCC diagnostic ignored "-Wcast-align"
+#pragma GCC diagnostic ignored "-Wcast-qual"
+#if defined(__GNUC__) && !defined(__clang__) // GCC, not clang
+#pragma GCC diagnostic ignored "-Wpacked"
+#pragma GCC diagnostic ignored "-Wattributes"
+#endif // GCC
+
+extern "C" {
+
+// Compatability for BHV
+#ifndef BRASS_TYPE_VAS
+#warning "Not in BRASS?!"
+#endif
+
+#include <errno.hpp>
+#include <string.hpp>
+#include <types.hpp>
+
+#define uint64_t uint64
+#define uint32_t uint32
+#define uint16_t uint16
+#define uint8_t uint8
+
+#ifndef NULL
+#define NULL nullptr
+#endif // NULL
+#ifndef ENOMEM
+#define ENOMEM (int)Errno::NOMEM
+#endif // ENOMEM
+#ifndef EINVAL
+#define EINVAL (int)Errno::INVAL
+#endif // EINVAL
+#ifndef EFAULT
+#define EFAULT (int)Errno::NOENT
+#endif // EFAULT
+
+#else // C
+#ifdef __KERNEL__
+#include <linux/string.h>
+#include <linux/types.h>
+
+#include <asm-generic/errno.h>
+
+#else // userspace C
+#include <errno.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <string.h>
+
+#endif // __KERNEL__
+
+#endif // __cplusplus
+
+#ifdef STANDALONE
+#define IFACE_FN
+#else
+#define IFACE_FN static inline
+#endif
+
+#ifdef INIT_STATEMENTS
+INIT_STATEMENTS
+#endif
+
+// ----------------------------------------------------------
+// + Config
+// ----------------------------------------------------------
+#define FLAST_TYPE_CHECK 1
+#define FLAST_BOUNDS_CHECK 1
+// ----------------------------------------------------------
+
+// ----------------------------------------------------------
+// + Types
+// ----------------------------------------------------------
+typedef uint32_t flast_index;
+typedef uint32_t flast_offset;
+
+typedef struct __attribute__((packed)) {
+    uint8_t *buf;
+    size_t size;
+} flast_buf;
+
+typedef struct __attribute__((packed)) {
+    flast_buf *buf;       // Pointer to the flast_buf to use for this lookup.
+    flast_index *indices; // Pointer to an array of flast_indices that will be used
+                          // for the lookup.
+    uint32_t nr_indices;  // The nr of indices in the indices array.
+    flast_offset offset;  // The offset of the flast_dict to start the lookup form.
+                          // Usually, lookups start from the root dict. But we can
+                          // use this offset to specify a different dict in the
+                          // flast buffer.
+} flast_lookup;
+typedef struct __attribute__((packed)) {
+    uint8_t magic[8];
+    uint64_t version;
+    uint32_t size;
+} flast_envelope;
+
+typedef struct __attribute__((packed)) {
+    uint32_t entries; // Number of entries in the dict
+} flast_dict;
+
+typedef struct __attribute__((packed)) {
+    uint32_t elements;      // Number of elements in the array
+    uint32_t size_per_elem; // The size of an entry and type
+} flast_array;
+
+#define FLAST_TYPE_BITS 4UL
+#define FLAST_TYPE_MASK 0x0fffffffUL
+
+typedef enum {
+    FT_EMPTY = 0,
+    FT_BOOL,
+    FT_UINT8,
+    FT_UINT16,
+    FT_UINT32,
+    FT_UINT64,
+    FT_ARRAY,
+    FT_DICT,
+    FT_INVALID
+} flast_type;
+
+// TODO: Add check for flast type bits.
+// ----------------------------------------------------------
+
+// ----------------------------------------------------------
+// + Defines
+// ----------------------------------------------------------
+#define likely(x) __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+
+// Helper macros to create flast lookup structs.
+#define FLAST_CREATE_CONST_LOOKUP(name, _buf, _indices, _nr_indices)                               \
+    const flast_lookup name = {.buf = (_buf),                                                      \
+                               .indices = (_indices),                                              \
+                               .nr_indices = (_nr_indices),                                        \
+                               .offset = sizeof(flast_envelope)}
+
+#define FLAST_GET_LOOKUP_MACRO(A1, A2, A3, A4, A5, MACRO_NAME, ...) MACRO_NAME
+
+#define FLAST_CREATE_LOOKUP5(name, buf, indices, nr_indices, ptr)                                  \
+    flast_lookup name;                                                                             \
+    flast_init_lookup_offset(&name, buf, indices, nr_indices, ptr)
+
+#define FLAST_CREATE_LOOKUP4(name, buf, indices, nr_indices)                                       \
+    flast_lookup name;                                                                             \
+    flast_init_lookup(&name, buf, indices, nr_indices)
+
+#define FLAST_CREATE_LOOKUP1(name) flast_lookup name
+
+#define FLAST_CREATE_LOOKUP(...)                                                                   \
+    FLAST_GET_LOOKUP_MACRO(__VA_ARGS__, FLAST_CREATE_LOOKUP5, FLAST_CREATE_LOOKUP4, NONE3, NONE2,  \
+                           FLAST_CREATE_LOOKUP1)                                                   \
+    (__VA_ARGS__)
+
+#define FLAST_INIT_LOOKUP5(name, buf, indices, nr_indices, offset)                                 \
+    flast_init_lookup_offset(name, buf, indices, nr_indices, offset)
+#define FLAST_INIT_LOOKUP4(name, buf, indices, nr_indices)                                         \
+    flast_init_lookup(name, buf, indices, nr_indices)
+
+#define FLAST_INIT_LOOKUP(...)                                                                     \
+    FLAST_GET_LOOKUP_MACRO(__VA_ARGS__, FLAST_INIT_LOOKUP5, FLAST_INIT_LOOKUP4)(__VA_ARGS__)
+
+// ----------------------------------------------------------
+
+// ----------------------------------------------------------
+// + Functions
+// ----------------------------------------------------------
+/**
+ * Init a flast lookup struct.
+ */
+IFACE_FN int flast_init_lookup(flast_lookup *lookup, flast_buf *buf, flast_index *indices,
+                               uint32_t nr_indices) {
+    lookup->buf = buf;
+    lookup->indices = indices;
+    lookup->nr_indices = nr_indices;
+    // Root node.
+    lookup->offset = sizeof(flast_envelope);
+    return 0;
+}
+
+/**
+ * Init a flast lookup struct with offset.
+ */
+IFACE_FN int flast_init_lookup_offset(flast_lookup *lookup, flast_buf *buf, flast_index *indices,
+                                      uint32_t nr_indices, flast_dict *dict) {
+    lookup->buf = buf;
+    lookup->indices = indices;
+    lookup->nr_indices = nr_indices;
+
+#if FLAST_BOUNDS_CHECK
+    if (unlikely((uint8_t *)dict >= lookup->buf->buf + lookup->buf->size)
+        || unlikely((uint8_t *)dict < lookup->buf->buf)) {
+        return EINVAL;
+    }
+#endif
+
+    lookup->offset = (flast_offset)((uint8_t *)dict - lookup->buf->buf);
+    return 0;
+}
+
+/**
+ * Get the size of a flast buffer.
+ */
+static inline uint32_t _flast_get_size(const flast_lookup *lookup) {
+    flast_envelope *env = (flast_envelope *)lookup->buf->buf;
+    return env->size;
+}
+
+/**
+ * Increase the size of a flast buffer by size.
+ */
+static inline void _flast_add_to_size(const flast_lookup *lookup, size_t size) {
+    flast_envelope *env = (flast_envelope *)lookup->buf->buf;
+    env->size += (uint32_t)size;
+}
+
+/**
+ * Retrieve the type from a flast offset.
+ */
+static inline flast_type _flast_get_type(flast_offset offset) {
+    return (flast_type)((offset & ~FLAST_TYPE_MASK) >> (32 - FLAST_TYPE_BITS));
+}
+
+/**
+ * Add the type to a flast_offset.
+ */
+static inline flast_offset _flast_set_type(flast_offset offset, flast_type type) {
+    return (offset | (((uint32_t)type) << (32 - FLAST_TYPE_BITS)));
+}
+
+/**
+ * Determine whether a type is an inline type.
+ */
+static inline bool _flast_is_inline_type(flast_type type) {
+    return type == FT_BOOL || type == FT_UINT8 || type == FT_UINT16;
+}
+
+/**
+ * Get the pointer to an index within a flast dictionary.
+ */
+static inline flast_offset *_flast_get_ptr_dict_index(const flast_dict *cur, flast_index index) {
+    return (flast_offset *)(((uint8_t *)cur) + sizeof(flast_dict) + index * sizeof(flast_index));
+}
+
+/**
+ * Get a pointer to an index in a flast dictionary as well as its type and offset.
+ *
+ * \warning The calling function must guarantee that cur and index are valid!
+ */
+static inline uint8_t *_flast_get_dict_index(const flast_dict *cur, flast_index index,
+                                             flast_offset *offset, flast_type *type) {
+    uint8_t *rv;
+    flast_offset _offset;
+
+    rv = (uint8_t *)_flast_get_ptr_dict_index(cur, index);
+    _offset = *(flast_offset *)rv;
+
+    // Get type and offset
+    *type = _flast_get_type(_offset);
+    *offset = _offset & FLAST_TYPE_MASK;
+
+    return rv;
+}
+
+/**
+ * Lookup a value/dict in a flast buffer.
+ */
+static inline void *_flast_lookup_generic(const flast_lookup *lookup, flast_type type) {
+    uint32_t i;
+    uint8_t *ptr;
+    flast_type cur_type = FT_DICT;
+    flast_offset offset;
+#if FLAST_BOUNDS_CHECK
+    size_t flast_size;
+#endif
+
+#if FLAST_BOUNDS_CHECK
+    if (unlikely(lookup == NULL)) {
+        return NULL;
+    }
+
+    if (unlikely(lookup->buf == NULL) || unlikely(lookup->buf->buf == NULL)) {
+        return NULL;
+    }
+
+    if (unlikely(lookup->indices == NULL)) {
+        return NULL;
+    }
+
+    flast_size = _flast_get_size(lookup);
+    if (unlikely(flast_size < sizeof(flast_envelope))) {
+        return NULL;
+    }
+
+    if (unlikely(lookup->offset >= flast_size)) {
+        return NULL;
+    }
+#endif
+
+    ptr = lookup->buf->buf + lookup->offset;
+    for (i = 0; i < lookup->nr_indices; i++) {
+#if FLAST_BOUNDS_CHECK
+        // Check entry bounds
+        if (unlikely((((flast_dict *)ptr)->entries <= lookup->indices[i]))) {
+            return NULL;
+        }
+#endif
+
+        // Get next offset.
+        ptr = _flast_get_dict_index((flast_dict *)ptr, lookup->indices[i], &offset, &cur_type);
+
+#if FLAST_BOUNDS_CHECK
+        if (unlikely(ptr == NULL)) {
+            return NULL;
+        }
+
+        if (unlikely(offset == 0 && cur_type != FT_EMPTY && !_flast_is_inline_type(cur_type))) {
+            return NULL;
+        }
+#endif
+
+        // Is this the last lookup index?
+        if (i == lookup->nr_indices - 1) {
+#if FLAST_TYPE_CHECK
+            if (unlikely(cur_type != type)) {
+                return NULL;
+            }
+#endif
+            // When the type is smaller than uint16_t the offset is the value.
+            if (_flast_is_inline_type(cur_type)) {
+                return ptr;
+            }
+
+            return (ptr + offset);
+        }
+
+        ptr += offset;
+
+#if FLAST_TYPE_CHECK
+        // Not the last index. This must be a dict!
+        if (unlikely(cur_type != FT_DICT)) {
+            return NULL;
+        }
+#endif
+
+#if FLAST_BOUNDS_CHECK
+        if (unlikely(ptr >= (lookup->buf->buf + flast_size) || ptr < lookup->buf->buf)) {
+            return NULL;
+        }
+
+        if (unlikely(ptr >= (lookup->buf->buf + lookup->buf->size))) {
+            return NULL;
+        }
+#endif
+    }
+
+    // Root dict
+#if FLAST_TYPE_CHECK
+    if (unlikely(type != FT_DICT)) {
+        return NULL;
+    }
+#endif
+
+    return ptr;
+}
+
+/**
+ * Lookup a dictionary in a flast buffer.
+ */
+static inline flast_dict *_flast_lookup_dict(const flast_lookup *lookup) {
+    return (flast_dict *)_flast_lookup_generic(lookup, FT_DICT);
+}
+
+/**
+ * Get a string from a flast array.
+ */
+static inline const char *_flast_array_to_str(flast_array *arr, int *error, size_t *size) {
+    flast_type type;
+    size_t sz;
+
+    if (unlikely(arr == NULL)) {
+        *error = -EINVAL;
+        return NULL;
+    }
+
+    sz = arr->size_per_elem;
+
+#if FLAST_TYPE_CHECK
+    type = _flast_get_type(arr->size_per_elem);
+    if (unlikely(type != FT_UINT8)) {
+        *error = -EINVAL;
+        return NULL;
+    }
+
+    sz &= FLAST_TYPE_MASK;
+#endif
+
+    *error = 0;
+    *size = arr->elements * sz;
+    return (const char *)arr + sizeof(flast_array);
+}
+
+// Helper for the generation of lookup functions
+#define FLAST_GEN_LOOKUP_FOR_TYPE(type, flast_type)                                                \
+    type *v = (type *)_flast_lookup_generic(lookup, flast_type);                                   \
+    if (unlikely(v == NULL)) {                                                                     \
+        *error = -EINVAL;                                                                          \
+        return 0;                                                                                  \
+    }                                                                                              \
+    *error = 0;                                                                                    \
+    return *v;
+
+/**
+ *  Lookup a flast dictionary in the flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return A pointer to the flast dict unless error is not 0.
+ */
+IFACE_FN flast_dict *flast_lookup_dict(const flast_lookup *lookup, int *error) {
+    flast_dict *rv = _flast_lookup_dict(lookup);
+    if (unlikely(rv == NULL)) {
+        *error = -EINVAL;
+        return NULL;
+    }
+    *error = 0;
+    return rv;
+}
+
+/**
+ * Lookup a bool in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return The value of the bool unless error is not 0.
+ */
+IFACE_FN bool flast_lookup_bool(const flast_lookup *lookup, int *error) {
+    FLAST_GEN_LOOKUP_FOR_TYPE(bool, FT_BOOL);
+}
+
+/**
+ * Lookup a uin8_t in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return The value of the uin8_t unless error is not 0.
+ */
+IFACE_FN uint8_t flast_lookup_uint8(const flast_lookup *lookup, int *error) {
+    FLAST_GEN_LOOKUP_FOR_TYPE(uint8_t, FT_UINT8);
+}
+
+/**
+ * Lookup a uin16_t in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return The value of the uin16_t unless error is not 0.
+ */
+IFACE_FN uint16_t flast_lookup_uint16(const flast_lookup *lookup, int *error) {
+    FLAST_GEN_LOOKUP_FOR_TYPE(uint16_t, FT_UINT16);
+}
+
+/**
+ * Lookup a uin32_t in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return The value of the uin32_t unless error is not 0.
+ */
+IFACE_FN uint32_t flast_lookup_uint32(const flast_lookup *lookup, int *error) {
+    FLAST_GEN_LOOKUP_FOR_TYPE(uint32_t, FT_UINT32);
+}
+
+/**
+ * Lookup a uin64_t in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return The value of the uin64_t unless error is not 0.
+ */
+IFACE_FN uint64_t flast_lookup_uint64(const flast_lookup *lookup, int *error) {
+    FLAST_GEN_LOOKUP_FOR_TYPE(uint64_t, FT_UINT64);
+}
+
+/**
+ * Lookup a cstring (\0-terminated) in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \return A pointer to the cstring if error is 0, NULL otherwise.
+ */
+IFACE_FN const char *flast_lookup_cstring(const flast_lookup *lookup, int *error) {
+    size_t size;
+    return _flast_array_to_str((flast_array *)_flast_lookup_generic(lookup, FT_ARRAY), error,
+                               &size);
+}
+
+/**
+ * Lookup a string (not \0-terminated) in a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \param[out] size The size of the string in bytes.
+ * \return A pointer to the string if error is 0, NULL otherwise.
+ */
+IFACE_FN const char *flast_lookup_string(const flast_lookup *lookup, int *error, size_t *size) {
+    return _flast_array_to_str((flast_array *)_flast_lookup_generic(lookup, FT_ARRAY), error, size);
+}
+
+/**
+ * Iterate through leaf nodes of a flast dict containing strings.
+ *
+ * \param lookup The lookup structure to use.
+ * \param[out] error Whether an error occurred during the lookup.
+ * \param[out] result The string selected by the cb, if any.
+ * \param[out] result_size The size of the string selected by the cb, if any.
+ * \param cb The callback
+ * \param arg The argument to cb
+ * \return what the cb returned
+ */
+IFACE_FN bool flast_iterate_string_dict(const flast_lookup *lookup, int *error, const char **result,
+                                        size_t *result_size,
+                                        bool (*cb)(flast_index, const char *, size_t, void *),
+                                        void *arg) {
+    flast_index i;
+    flast_dict *dict = _flast_lookup_dict(lookup);
+
+    const char *strp;
+    size_t str_sz;
+
+    if (unlikely(dict == NULL)) {
+        *error = -EINVAL;
+        return false;
+    }
+
+    for (i = 0; i < dict->entries; ++i) {
+        FLAST_CREATE_LOOKUP(str_lookup, lookup->buf, &i, 1, dict);
+        strp = _flast_array_to_str((flast_array *)_flast_lookup_generic(&str_lookup, FT_ARRAY),
+                                   error, &str_sz);
+
+        if (unlikely(strp == NULL)) {
+            return false;
+        }
+
+        if (cb(i, strp, str_sz, arg)) {
+            *error = 0;
+            if (result != NULL) {
+                *result = strp;
+            }
+            if (result_size != NULL) {
+                *result_size = str_sz;
+            }
+            return true;
+        }
+    }
+
+    *error = 0;
+    return false;
+}
+
+/**
+ * Get the size of a flast dictionary.
+ */
+static inline size_t _flast_get_dict_size(flast_index nr_entries) {
+    return sizeof(flast_dict) + nr_entries * sizeof(flast_index);
+}
+
+/**
+ * Update an entry in a flast dictionary.
+ */
+static inline bool _flast_set_dict_entry(flast_dict *dict, flast_index index, flast_offset offset,
+                                         flast_type type) {
+    flast_offset *entry = _flast_get_ptr_dict_index(dict, index);
+
+#if FLAST_TYPE_CHECK
+    if (unlikely(type != FT_EMPTY && _flast_get_type(*entry) != FT_EMPTY)) {
+        return false;
+    }
+#endif
+
+    offset = _flast_set_type(offset, type);
+    *entry = offset;
+
+    return true;
+}
+
+/**
+ * Initialize a flast dictionary.
+ */
+static inline bool _flast_init_dict(flast_dict *dict, flast_index nr_entries) {
+    uint32_t i;
+    dict->entries = nr_entries;
+
+    for (i = 0; i < nr_entries; i++) {
+        if (!_flast_set_dict_entry(dict, i, 0, FT_EMPTY)) {
+            return false;
+        }
+    }
+
+    return true;
+}
+
+/**
+ * Initialize a flast buffer.
+ *
+ * This is the first function that needs to be called to create a new flast buffer.
+ *
+ * \param buf The memory buffer to use.
+ * \param version The version to set.
+ * \param nr_root_entries The number of entries in the root dictionary.
+ *
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_create_new(flast_buf *buf, uint64_t version, flast_index nr_root_entries) {
+    size_t root_size;
+    flast_envelope *env;
+    flast_dict *dict;
+
+    if (buf == NULL || buf->buf == NULL) {
+        return -ENOMEM;
+    }
+
+    root_size = _flast_get_dict_size(nr_root_entries);
+    if (buf->size < sizeof(flast_envelope) + root_size) {
+        return -ENOMEM;
+    }
+
+    env = (flast_envelope *)buf->buf;
+    memcpy(env->magic, "FLAST\n\n\n", sizeof(env->magic));
+    env->version = version;
+    env->size = (uint32_t)(sizeof(flast_envelope) + root_size);
+
+    dict = (flast_dict *)(buf->buf + sizeof(flast_envelope));
+    _flast_init_dict(dict, nr_root_entries);
+
+    return 0;
+}
+
+// Helper for generating a creator function for offset types.
+#define FLAST_DCL_ADDER_FOR_OFFSET_TYPE                                                            \
+    flast_dict *dict;                                                                              \
+    flast_offset offset;                                                                           \
+    flast_lookup _tmp;                                                                             \
+    flast_index dest;                                                                              \
+    size_t flast_size
+
+// Helper for generating a creator function for offset types.
+#define FLAST_GEN_ADDER_FOR_OFFSET_TYPE(SIZE, flast_type)                                          \
+    dest = lookup->indices[lookup->nr_indices - 1];                                                \
+    flast_init_lookup(&_tmp, lookup->buf, lookup->indices, lookup->nr_indices - 1);                \
+                                                                                                   \
+    if (!(dict = _flast_lookup_dict(&_tmp)))                                                       \
+        return -EINVAL;                                                                            \
+                                                                                                   \
+    if (unlikely(dict->entries <= dest))                                                           \
+        return -EINVAL;                                                                            \
+                                                                                                   \
+    flast_size = _flast_get_size(lookup);                                                          \
+    if (unlikely(flast_size + (SIZE) > lookup->buf->size))                                         \
+        return -ENOMEM;                                                                            \
+                                                                                                   \
+    if (unlikely(flast_size + (SIZE) > FLAST_TYPE_MASK))                                           \
+        return -EFAULT;                                                                            \
+                                                                                                   \
+    offset = (uint32_t)(lookup->buf->buf + flast_size                                              \
+                        - ((uint8_t *)_flast_get_ptr_dict_index(dict, dest)));                     \
+    if (!_flast_set_dict_entry(dict, dest, offset, flast_type))                                    \
+        return -EINVAL;                                                                            \
+    _flast_add_to_size(lookup, SIZE);
+
+// Helper for generating a creator function for inline types.
+#define FLAST_DCL_ADDER_FOR_INLINE_TYPE                                                            \
+    flast_dict *dict;                                                                              \
+    flast_offset offset;                                                                           \
+    flast_lookup _tmp;                                                                             \
+    flast_index dest
+
+// Helper for generating a creator function for inline types.
+#define FLAST_GEN_ADDER_FOR_INLINE_TYPE(flast_type, value)                                         \
+    dest = lookup->indices[lookup->nr_indices - 1];                                                \
+    flast_init_lookup(&_tmp, lookup->buf, lookup->indices, lookup->nr_indices - 1);                \
+                                                                                                   \
+    if (unlikely(lookup->nr_indices <= 0))                                                         \
+        return -EINVAL;                                                                            \
+                                                                                                   \
+    if (unlikely(!(dict = _flast_lookup_dict(&_tmp))))                                             \
+        return -EINVAL;                                                                            \
+                                                                                                   \
+    if (unlikely(dict->entries <= dest))                                                           \
+        return -EINVAL;                                                                            \
+                                                                                                   \
+    offset = value;                                                                                \
+    if (!_flast_set_dict_entry(dict, dest, offset, flast_type))                                    \
+        return -EINVAL;
+
+/**
+ * Add a dictionary to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param nr_entries The number of entries in the dictionary.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_dict(const flast_lookup *lookup, flast_index nr_entries) {
+    FLAST_DCL_ADDER_FOR_OFFSET_TYPE;
+    size_t size = _flast_get_dict_size(nr_entries);
+    FLAST_GEN_ADDER_FOR_OFFSET_TYPE(size, FT_DICT);
+    // Setup dict.
+    dict = (flast_dict *)(lookup->buf->buf + flast_size);
+    _flast_init_dict(dict, nr_entries);
+
+    return 0;
+}
+
+/**
+ * Add an array to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param data A pointer to the data to store.
+ * \param size_element The size of an element in the array.
+ * \param nr_elements The number of elements in data.
+ * \param type The flast type of the data.
+ * \return 0 on success. An error code otherwise.
+ */
+static inline int _flast_add_array(const flast_lookup *lookup, const void *data,
+                                   size_t size_element, size_t nr_elements, flast_type type) {
+    FLAST_DCL_ADDER_FOR_OFFSET_TYPE;
+    size_t arr_size = sizeof(flast_array) + size_element * nr_elements;
+    size_t size = (arr_size & 0x3) == 0 ? arr_size : ((arr_size >> 2) + 1) << 2; // padding.
+    flast_array *arr;
+    FLAST_GEN_ADDER_FOR_OFFSET_TYPE(size, FT_ARRAY);
+    // Setup array.
+    arr = (flast_array *)(lookup->buf->buf + flast_size);
+    arr->elements = (uint32_t)nr_elements;
+    // Type included in size per element.
+    arr->size_per_elem = (uint32_t)size_element | (((uint32_t)type) << (32 - FLAST_TYPE_BITS));
+    memcpy((uint8_t *)arr + sizeof(flast_array), data, size_element * nr_elements);
+    memset((uint8_t *)arr + sizeof(flast_array) + size_element * nr_elements, 0, size - arr_size);
+
+    return 0;
+}
+
+/**
+ * Add a cstring (\0-terminated) to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param data The pointer to the cstring.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_cstring(const flast_lookup *lookup, const char *data) {
+    return _flast_add_array(lookup, data, 1, strlen(data) + 1, FT_UINT8);
+}
+
+/**
+ * Add a string (NOT \0-terminated) to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param data The pointer to the string.
+ * \param len The length of the string.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_string(const flast_lookup *lookup, const char *data, size_t len) {
+    return _flast_add_array(lookup, data, 1, len, FT_UINT8);
+}
+
+/**
+ * Add a bool to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param value The value to add.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_bool(const flast_lookup *lookup, bool value) {
+    FLAST_DCL_ADDER_FOR_INLINE_TYPE;
+    FLAST_GEN_ADDER_FOR_INLINE_TYPE(FT_BOOL, value);
+    return 0;
+}
+
+/**
+ * Add a uint8_t to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param value The value to add.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_uint8(const flast_lookup *lookup, uint8_t value) {
+    FLAST_DCL_ADDER_FOR_INLINE_TYPE;
+    FLAST_GEN_ADDER_FOR_INLINE_TYPE(FT_UINT8, value);
+    return 0;
+}
+
+/**
+ * Add a uint16_t to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param value The value to add.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_uint16(const flast_lookup *lookup, uint16_t value) {
+    FLAST_DCL_ADDER_FOR_INLINE_TYPE;
+    FLAST_GEN_ADDER_FOR_INLINE_TYPE(FT_UINT16, value);
+    return 0;
+}
+
+/**
+ * Add a uint32_t to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param value The value to add.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_uint32(const flast_lookup *lookup, uint32_t value) {
+    FLAST_DCL_ADDER_FOR_OFFSET_TYPE;
+    size_t size = sizeof(uint32_t);
+    FLAST_GEN_ADDER_FOR_OFFSET_TYPE(size, FT_UINT32);
+    // Set value
+    *(uint32_t *)(lookup->buf->buf + flast_size) = value;
+
+    return 0;
+}
+
+/**
+ * Add a uint64_t to a flast buffer.
+ *
+ * \param lookup The lookup structure to use.
+ * \param value The value to add.
+ * \return 0 on success. An error code otherwise.
+ */
+IFACE_FN int flast_add_uint64(const flast_lookup *lookup, uint64_t value) {
+    FLAST_DCL_ADDER_FOR_OFFSET_TYPE;
+    size_t size = sizeof(uint64_t);
+    { // Pad to a multile of 8
+        size_t off
+            = (sizeof(uint64_t) - (_flast_get_size(lookup) % sizeof(uint64_t))) % sizeof(uint64_t);
+        if (unlikely(_flast_get_size(lookup) + off > lookup->buf->size)) {
+            return -ENOMEM;
+        }
+        _flast_add_to_size(lookup, off);
+    }
+    FLAST_GEN_ADDER_FOR_OFFSET_TYPE(size, FT_UINT64);
+    // Set value
+    *(uint64_t *)(lookup->buf->buf + flast_size) = value;
+
+    return 0;
+}
+// ----------------------------------------------------------
+
+#ifdef __cplusplus
+}
+
+#undef uint64_t
+#undef uint32_t
+#undef uint16_t
+#undef uint8_t
+
+#endif // __cplusplus
+
+#ifdef FINL_STATEMENTS
+FINL_STATEMENTS
+#endif
+
+#endif /* __FLAST_H__ */
diff --git include/bhv/flast_ids_autogen.h include/bhv/flast_ids_autogen.h
new file mode 100644
index 0000000000..1bb243900e
--- /dev/null
+++ include/bhv/flast_ids_autogen.h
@@ -0,0 +1,767 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_policy_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <bhv/flast.h>
+
+extern const flast_index FLAST__Version__ARR[1];
+extern const flast_index FLAST__BrassPolicyOptionsHash__ARR[1];
+extern const flast_index FLAST__DriverACL__enabled__ARR[2];
+extern const flast_index FLAST__DriverACL__remediate__ARR[2];
+extern const flast_index FLAST__DriverACL__type__ARR[2];
+extern const flast_index FLAST__DriverACL__acl__ARR[2];
+extern const flast_index FLAST__ProcessACL__enabled__ARR[2];
+extern const flast_index FLAST__ProcessACL__remediate__ARR[2];
+extern const flast_index FLAST__ProcessACL__type__ARR[2];
+extern const flast_index FLAST__ProcessACL__acl__ARR[2];
+extern const flast_index FLAST__FileProtection__read_only__enabled__ARR[3];
+extern const flast_index FLAST__FileProtection__read_only__remediate__ARR[3];
+extern const flast_index FLAST__FileProtection__file_ops__enabled__ARR[3];
+extern const flast_index FLAST__FileProtection__file_ops__remediate__ARR[3];
+extern const flast_index FLAST__FileProtection__file_ops__strict__ARR[3];
+extern const flast_index FLAST__FileProtection__dirtycred__enabled__ARR[3];
+extern const flast_index FLAST__FileProtection__dirtycred__remediate__ARR[3];
+extern const flast_index FLAST__Logging__driver__driver_load__enabled__ARR[4];
+static inline bool FLAST__Logging__driver__driver_load__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__driver__driver_load__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__driver__driver_load__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__driver__driver_load__synchronous__ARR[4];
+static inline bool FLAST__Logging__driver__driver_load__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__driver__driver_load__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__driver__driver_load__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__process_exec__enabled__ARR[4];
+static inline bool FLAST__Logging__process__process_exec__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__process_exec__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__process_exec__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__process_exec__synchronous__ARR[4];
+static inline bool FLAST__Logging__process__process_exec__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__process_exec__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__process_exec__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__process_fork__enabled__ARR[4];
+static inline bool FLAST__Logging__process__process_fork__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__process_fork__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__process_fork__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__process_terminate__enabled__ARR[4];
+static inline bool FLAST__Logging__process__process_terminate__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__process_terminate__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__process_terminate__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__executable_exec_stack__enabled__ARR[4];
+static inline bool FLAST__Logging__process__executable_exec_stack__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__executable_exec_stack__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__executable_exec_stack__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__process__executable_exec_stack__synchronous__ARR[4];
+static inline bool FLAST__Logging__process__executable_exec_stack__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__process__executable_exec_stack__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__process__executable_exec_stack__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__kernel_access_violation__enabled__ARR[4];
+static inline bool FLAST__Logging__kernel__kernel_access_violation__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__kernel_access_violation__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__kernel_access_violation__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__kernel_exec__enabled__ARR[4];
+static inline bool FLAST__Logging__kernel__kernel_exec__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__kernel_exec__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__kernel_exec__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__kernel_exec__synchronous__ARR[4];
+static inline bool FLAST__Logging__kernel__kernel_exec__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__kernel_exec__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__kernel_exec__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__forced_mem_access__enabled__ARR[4];
+static inline bool FLAST__Logging__kernel__forced_mem_access__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__forced_mem_access__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__forced_mem_access__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__forced_mem_access__synchronous__ARR[4];
+static inline bool FLAST__Logging__kernel__forced_mem_access__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__forced_mem_access__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__forced_mem_access__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__unsupported_file_operation__enabled__ARR[4];
+static inline bool FLAST__Logging__kernel__unsupported_file_operation__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__unsupported_file_operation__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__unsupported_file_operation__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__kernel__unsupported_file_operation__synchronous__ARR[4];
+static inline bool FLAST__Logging__kernel__unsupported_file_operation__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__kernel__unsupported_file_operation__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__kernel__unsupported_file_operation__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__container__capable__enabled__ARR[4];
+static inline bool FLAST__Logging__container__capable__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__container__capable__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__container__capable__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__container__cgroup_create__enabled__ARR[4];
+static inline bool FLAST__Logging__container__cgroup_create__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__container__cgroup_create__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__container__cgroup_create__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__container__cgroup_destroy__enabled__ARR[4];
+static inline bool FLAST__Logging__container__cgroup_destroy__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__container__cgroup_destroy__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__container__cgroup_destroy__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__container__namespace_change__enabled__ARR[4];
+static inline bool FLAST__Logging__container__namespace_change__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__container__namespace_change__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__container__namespace_change__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__container__namespace_change__synchronous__ARR[4];
+static inline bool FLAST__Logging__container__namespace_change__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__container__namespace_change__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__container__namespace_change__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__interpreter_bound__enabled__ARR[4];
+static inline bool FLAST__Logging__socket__interpreter_bound__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__interpreter_bound__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__interpreter_bound__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__interpreter_bound__synchronous__ARR[4];
+static inline bool FLAST__Logging__socket__interpreter_bound__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__interpreter_bound__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__interpreter_bound__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__interpreter_bound_transitive__enabled__ARR[4];
+static inline bool FLAST__Logging__socket__interpreter_bound_transitive__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__interpreter_bound_transitive__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__interpreter_bound_transitive__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__interpreter_bound_transitive__synchronous__ARR[4];
+static inline bool FLAST__Logging__socket__interpreter_bound_transitive__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__interpreter_bound_transitive__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__interpreter_bound_transitive__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__socket_accept__enabled__ARR[4];
+static inline bool FLAST__Logging__socket__socket_accept__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__socket_accept__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__socket_accept__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__socket_accept__synchronous__ARR[4];
+static inline bool FLAST__Logging__socket__socket_accept__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__socket_accept__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__socket_accept__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__socket_connection__enabled__ARR[4];
+static inline bool FLAST__Logging__socket__socket_connection__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__socket_connection__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__socket_connection__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__socket__socket_connection__synchronous__ARR[4];
+static inline bool FLAST__Logging__socket__socket_connection__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__socket__socket_connection__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__socket__socket_connection__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_open__enabled__ARR[4];
+static inline bool FLAST__Logging__file__file_open__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_open__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_open__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_open__synchronous__ARR[4];
+static inline bool FLAST__Logging__file__file_open__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_open__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_open__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__mmap_exec_file__enabled__ARR[4];
+static inline bool FLAST__Logging__file__mmap_exec_file__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__mmap_exec_file__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__mmap_exec_file__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__mmap_exec_file__synchronous__ARR[4];
+static inline bool FLAST__Logging__file__mmap_exec_file__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__mmap_exec_file__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__mmap_exec_file__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_drift__enabled__ARR[4];
+static inline bool FLAST__Logging__file__file_drift__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_drift__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_drift__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_drift__synchronous__ARR[4];
+static inline bool FLAST__Logging__file__file_drift__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_drift__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_drift__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_set_xattr__enabled__ARR[4];
+static inline bool FLAST__Logging__file__file_set_xattr__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_set_xattr__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_set_xattr__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__file_set_xattr__synchronous__ARR[4];
+static inline bool FLAST__Logging__file__file_set_xattr__synchronous(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__file_set_xattr__synchronous__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__file_set_xattr__synchronous\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__lib_trace__enabled__ARR[4];
+static inline bool FLAST__Logging__file__lib_trace__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__lib_trace__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__lib_trace__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__file__skip_sym_hook__enabled__ARR[4];
+static inline bool FLAST__Logging__file__skip_sym_hook__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__Logging__file__skip_sym_hook__enabled__ARR,
+                                4);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__Logging__file__skip_sym_hook__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__Logging__filter_options__protected_unix_sockets__ARR[3];
+extern const flast_index FLAST__Logging__filter_options__exempt_cgroups__ARR[3];
+extern const flast_index FLAST__Logging__filter_options__trace_exclusion_list__ARR[3];
+extern const flast_index FLAST__Hypercall__Timeout__enabled__ARR[2];
+extern const flast_index FLAST__Hypercall__Timeout__remediate__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__enabled__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__remediate__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__kernel_integrity_mutable__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__kernel_integrity_dynamic__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__kernel_integrity_freeze_after_start__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__kernel_integrity_freeze_after_boot__ARR[2];
+extern const flast_index FLAST__IntegrityProtection__pagetable_protection__enabled__ARR[3];
+extern const flast_index FLAST__IntegrityProtection__pagetable_protection__remediate__ARR[3];
+extern const flast_index FLAST__StrongIsolation__enabled__ARR[2];
+static inline bool FLAST__StrongIsolation__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__StrongIsolation__enabled__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__StrongIsolation__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__StrongIsolation__remediate__ARR[2];
+static inline bool FLAST__StrongIsolation__remediate(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__StrongIsolation__remediate__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__StrongIsolation__remediate\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__StrongIsolation__isolate__ARR[2];
+extern const flast_index FLAST__CredProtection__enabled__ARR[2];
+extern const flast_index FLAST__CredProtection__remediate__ARR[2];
+extern const flast_index FLAST__GuestKernelConfig__userspace_force_nx_stack__ARR[2];
+static inline bool FLAST__GuestKernelConfig__userspace_force_nx_stack(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__GuestKernelConfig__userspace_force_nx_stack__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__GuestKernelConfig__userspace_force_nx_stack\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__GuestKernelConfig__modprobe_path__ARR[2];
+extern const flast_index FLAST__GuestKernelConfig__poweroff_cmd__ARR[2];
+extern const flast_index FLAST__GuestKernelConfig__core_pattern__ARR[2];
+extern const flast_index FLAST__RegisterProtect__filter__ARR[2];
+#define FLAST__RegisterProtect__filter__remediate__ID 0U
+#define FLAST__RegisterProtect__filter__reg__ID 1U
+#define FLAST__RegisterProtect__filter__bitmask__ID 2U
+#define FLAST__RegisterProtect__filter__values__ID 3U
+extern const flast_index FLAST__RegisterProtect__counter__ARR[2];
+#define FLAST__RegisterProtect__counter__remediate__ID 0U
+#define FLAST__RegisterProtect__counter__reg__ID 1U
+#define FLAST__RegisterProtect__counter__bitmask__ID 2U
+#define FLAST__RegisterProtect__counter__updates__ID 3U
+extern const flast_index FLAST__RegisterProtectVAS__enabled__ARR[2];
+extern const flast_index FLAST__RegisterProtectVAS__remediate__ARR[2];
+extern const flast_index FLAST__GuestPolicy__enabled__ARR[2];
+extern const flast_index FLAST__HeartBeat__enabled__ARR[2];
+extern const flast_index FLAST__HeartBeat__interval_sec__ARR[2];
+extern const flast_index FLAST__DriftDetection__enabled__ARR[2];
+static inline bool FLAST__DriftDetection__enabled(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__DriftDetection__enabled__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__DriftDetection__enabled\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__DriftDetection__container_host__ARR[2];
+static inline bool FLAST__DriftDetection__container_host(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__DriftDetection__container_host__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__DriftDetection__container_host\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__ReverseShellDetection__container_host__ARR[2];
+static inline bool FLAST__ReverseShellDetection__container_host(flast_buf *buf) {
+    int err;
+    bool rv;
+    FLAST_CREATE_CONST_LOOKUP(lookup,
+                                buf,
+                                (flast_index *) FLAST__ReverseShellDetection__container_host__ARR,
+                                2);
+    rv = flast_lookup_bool(&lookup, &err);
+    if (err != 0) {
+            pr_err_once("Could not read config: FLAST__ReverseShellDetection__container_host\n");
+            return false;
+    }
+    return rv;
+}
+
+extern const flast_index FLAST__ReverseShellDetection__interpreter__ARR[2];
diff --git include/bhv/flast_lists.h include/bhv/flast_lists.h
new file mode 100644
index 0000000000..3bff88f393
--- /dev/null
+++ include/bhv/flast_lists.h
@@ -0,0 +1,76 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ *          Tommaso Frassetto <tommaso@bluerock.io>
+ */
+
+#ifndef __FLAST_LISTS_H__
+#define __FLAST_LISTS_H__
+
+#include <bhv/flast.h>
+
+typedef struct {
+	const char *target;
+	size_t sz;
+} FLAST_FILTER_FIND_STR;
+
+static inline bool __is_prefix(const char *s1, size_t s1l, const char *s2,
+			       size_t s2l)
+{
+	if (s1l > s2l)
+		return false;
+
+	// s*z includes the \0 byte. We want to check without it
+	// Such that we are able to match /python against /python3,
+	// python3.12, etc.
+	return 0 == strncmp(s1, s2, s1l - 1);
+}
+
+static inline bool __flast_filter_find_str(flast_index i, const char *s,
+					   size_t sz, void *arg)
+{
+	FLAST_FILTER_FIND_STR *a = (FLAST_FILTER_FIND_STR *)arg;
+
+	return __is_prefix(s, sz, a->target, a->sz);
+}
+
+static inline bool flast_cstringarray_contains_str(const flast_lookup *lookup,
+						   const char *name)
+{
+	FLAST_FILTER_FIND_STR arg = { .target = name, .sz = strlen(name) + 1 };
+	int error;
+
+	bool rv = flast_iterate_string_dict(lookup, &error, NULL, NULL,
+					    __flast_filter_find_str, &arg);
+	if (error != 0) {
+		pr_err("Unexpected failure %d at %s:%d\n", error, __FILE__,
+		       __LINE__);
+		return false;
+	}
+
+	return rv;
+}
+
+static inline bool __flast_filter_is_not_empty(flast_index i, const char *s,
+					       size_t sz, void *arg)
+{
+	return true;
+}
+
+static inline bool flast_cstringarray_is_not_empty(const flast_lookup *lookup)
+{
+	int error;
+
+	bool rv = flast_iterate_string_dict(lookup, &error, NULL, NULL,
+					    __flast_filter_is_not_empty, NULL);
+	if (error != 0) {
+		pr_err("Unexpected failure %d at %s:%d\n", error, __FILE__,
+		       __LINE__);
+		return false;
+	}
+
+	return rv;
+}
+
+#endif /* __FLAST_LISTS_H__ */
diff --git include/bhv/guestcmd.h include/bhv/guestcmd.h
new file mode 100644
index 0000000000..f070b7b53f
--- /dev/null
+++ include/bhv/guestcmd.h
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+#ifndef __BHV_GUESTCMD_H__
+#define __BHV_GUESTCMD_H__
+
+#include <linux/init.h>
+#include <linux/types.h>
+
+/*********************************************************
+ * init
+ *********************************************************/
+void __init bhv_init_guestcmd(void);
+/*********************************************************/
+
+#endif /* __BHV_GUESTCMD_H__ */
\ No newline at end of file
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 0000000000..b4c81ca25b
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BRS_GUESTCONN_H__
+#define __BRS_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init brs_init_guestconn(uint32_t cid, uint32_t port);
+/*********************************************************/
+
+void __init brs_guestconn_init(void);
+
+void *brs_guestconn_alloc_msg(size_t sz);
+void brs_guestconn_free_msg(void *msg);
+
+int brs_guestconn_send(uint8_t *data, size_t size);
+int brs_guestconn_send_sync(uint8_t *data, size_t size, uint64_t id);
+
+typedef void (*brs_guestconn_backend_handler_t)(void *, size_t);
+int __init brs_guestconn_register_backend(
+	brs_guestconn_backend_handler_t);
+
+#endif /* __BRS_GUESTCONN_H__ */
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 0000000000..7a4245a79c
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,105 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#ifdef CONFIG_BRS
+
+#include <linux/cgroup-defs.h>
+#include <linux/types.h>
+#include <linux/binfmts.h>
+#include <linux/nsproxy.h>
+#include <linux/socket.h>
+#include <linux/un.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+#include <bhv/policy_sks_autogen.h>
+
+int brs_guestlog_error(bool global_timeout, bool event_timeout);
+int brs_guestlog_log_str(char *fmt, ...);
+int brs_guestlog_log_process_fork(struct brs_policy *policy, uint32_t child_pid,
+				  const char *child_comm, uint32_t parent_pid,
+				  const char *parent_comm);
+int brs_guestlog_log_process_exec(struct brs_policy *policy,
+				  struct linux_binprm *bprm,
+				  struct task_struct *task, const char *path);
+int brs_guestlog_log_process_exit(struct brs_policy *policy,
+				  struct task_struct *target);
+int brs_guestlog_log_elf_load_exec_stack(struct brs_policy *policy,
+					 struct linux_binprm *bprm);
+int brs_guestlog_log_driver_load(struct brs_policy *policy, const char *name);
+int brs_guestlog_log_kaccess(struct brs_policy *policy, uint64_t addr,
+			     uint8_t event_id);
+int brs_guestlog_log_fops_unknown(struct brs_policy *policy, uint32_t magic,
+				  const char *pathname, uint8_t type,
+				  uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr);
+int brs_guestlog_log_kernel_exec(struct brs_policy *policy, const char *path,
+				 char **argv, char **envp);
+int brs_guestlog_log_cgroup_create(struct brs_policy *policy,
+				   struct cgroup *cgrp);
+int brs_guestlog_log_cgroup_destroy(struct brs_policy *policy,
+				    struct cgroup *cgrp);
+int brs_guestlog_log_namespace_change(struct brs_policy *policy,
+				      struct task_struct *tsk,
+				      struct nsset *nsset);
+int brs_guestlog_log_reverse_shell_detection_interpreter_bound(
+	struct brs_policy *policy, pid_t interpreter_pid,
+	const char *interpreter_path, uint32_t socket_fd, struct file *socket);
+int brs_guestlog_log_reverse_shell_detection_interpreter_transitive(
+	struct brs_policy *policy, struct task_struct *interpreter,
+	uint32_t interpreter_fd, const char *interpreter_path,
+	struct task_struct *transitive, uint32_t transitive_pipe_fd,
+	uint32_t transitive_socket_fd, struct sockaddr *dest_address);
+int brs_guestlog_log_capable(struct brs_policy *policy, int cap);
+int brs_guestlog_log_net_socket_connection(struct brs_policy *policy,
+					   bool local,
+					   struct sockaddr *address);
+int brs_guestlog_log_unix_socket_connection(struct brs_policy *policy,
+					    struct socket *sock,
+					    struct sockaddr_un *address,
+					    int addrlen);
+int brs_guestlog_log_socket_accept(struct brs_policy *policy,
+				   struct socket *sock);
+int brs_guestlog_log_file_open(struct brs_policy *policy, struct file *file);
+int brs_guestlog_log_file_drift(struct brs_policy *policy, struct file *file,
+				bool writeable);
+int brs_guestlog_log_file_set_xattr(struct brs_policy *policy,
+				    struct dentry *dentry, const char *name,
+				    const void *value, size_t value_sz);
+
+int brs_guestlog_log_strong_isolation_report(
+	struct brs_policy *policy, uint64_t domain_src, uint64_t domain_target,
+	uint64_t gva_start, uint64_t gva_end, bool write, bool blocked);
+int brs_guestlog_log_forced_mem_access(struct brs_policy *policy,
+				       uint64_t gva_start, uint64_t gva_end,
+				       bool write);
+
+int brs_guestlog_log_mmap_exec_file(struct brs_policy *policy,
+				    struct file *file, unsigned long prot,
+				    unsigned long flags);
+
+int brs_guestlog_log_lib_trace(struct brs_policy *policy,
+			       const char *lib_name, const char *func_name);
+
+int brs_guestlog_log_skip_sym_hook(struct brs_policy *policy,
+				   const char *lib_name,
+				   const char *func_name);
+
+#else // defined CONFIG_BRS
+#include <linux/types.h>
+static inline int brs_guestlog_log_elf_load_exec_stack(struct brs_policy *,
+						       struct linux_binprm *)
+{
+	return 0;
+}
+#endif // defined CONFIG_BRS
+#endif /* __BHV_GUESTLOG_H__ */
diff --git include/bhv/guestlog_internal.h include/bhv/guestlog_internal.h
new file mode 100644
index 0000000000..d9661f7f78
--- /dev/null
+++ include/bhv/guestlog_internal.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ */
+#ifndef __BHV_GUESTLOG_INTERNAL_H__
+#define __BHV_GUESTLOG_INTERNAL_H__
+
+#ifdef CONFIG_BRS
+
+struct evt_writer;
+
+int send_evt_msg_sync(struct evt_writer *writer);
+int send_evt_msg(struct evt_writer *writer);
+
+#endif // defined CONFIG_BRS
+#endif /* __BHV_GUESTLOG_INTERNAL_H__ */
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 0000000000..cdbc69dca0
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__GuestPolicy__enabled();
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init/init.h include/bhv/init/init.h
new file mode 100644
index 0000000000..d9467d29a7
--- /dev/null
+++ include/bhv/init/init.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_INIT_H__
+#define __BHV_INIT_INIT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/integrity.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+#define BHV_INIT_MAX_REGIONS 7
+
+void __init bhv_init_platform(void);
+bool __init bhv_init_arch(void);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+#else /* !CONFIG_BHV_VAS */
+static inline void bhv_init_platform(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_INIT_H__ */
diff --git include/bhv/init/late_start.h include/bhv/init/late_start.h
new file mode 100644
index 0000000000..c21f96459c
--- /dev/null
+++ include/bhv/init/late_start.h
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_LATE_START_H__
+#define __BHV_INIT_LATE_START_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_late_start(void);
+int bhv_late_start_arch(void);
+#else /* !CONFIG_BHV_VAS */
+static inline void bhv_late_start(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_LATE_START_H__ */
diff --git include/bhv/init/mm_init.h include/bhv/init/mm_init.h
new file mode 100644
index 0000000000..f567b9ff43
--- /dev/null
+++ include/bhv/init/mm_init.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_MM_INIT_H__
+#define __BHV_INIT_MM_INIT_H__
+#ifdef CONFIG_BHV_VAS
+void __init bhv_mm_init(void);
+#else /* !CONFIG_BHV_VAS */
+static inline void bhv_mm_init(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_MM_INIT_H__ */
diff --git include/bhv/init/start.h include/bhv/init/start.h
new file mode 100644
index 0000000000..f2d0fff0c7
--- /dev/null
+++ include/bhv/init/start.h
@@ -0,0 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_START_H__
+#define __BHV_INIT_START_H__
+
+#include <linux/types.h>
+
+#ifdef CONFIG_BHV_VAS
+bool bhv_start(void);
+int bhv_start_arch(void);
+#else /* !CONFIG_BHV_VAS */
+static inline bool bhv_start(void)
+{
+	return true;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_START_H__ */
diff --git include/bhv/inode.h include/bhv/inode.h
new file mode 100644
index 0000000000..2946f05001
--- /dev/null
+++ include/bhv/inode.h
@@ -0,0 +1,59 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INODE_H__
+#define __BHV_INODE_H__
+
+#include <linux/binfmts.h>
+#include <linux/version.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_inode_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__CredProtection__enabled();
+}
+
+int __init bhv_inode_init(void);
+
+/* LSM Hooks */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   const struct file *file);
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   struct file *file);
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old,
+			      int flags);
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old,
+			      int flags);
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode);
+
+void bhv_inode_iput_final(struct inode *inode);
+
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid, umode_t mode);
+
+#else /* !CONFIG_BHV_VAS */
+
+#define bhv_inode_iput_final(i)                                                \
+	do {                                                                   \
+	} while (0)
+#define bhv_inode_post_setattr(d, v, m)                                        \
+	do {                                                                   \
+	} while (0)
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INODE_H__ */
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 0000000000..378f2c16b0
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,271 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity_base.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+typedef union {
+	HypABI__Integrity__Create__Mem_Region__T create;
+	HypABI__Integrity__Update__Mem_Region__T update;
+	HypABI__Integrity__Remove__Mem_Region__T remove;
+} bhv_mem_region_t;
+
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * start
+ ************************************************************/
+int bhv_start_integrity_arch(void);
+void bhv_start_ptpg(void);
+void bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+				       uint64_t *pgd_value);
+/************************************************************/
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void);
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+int bhv_late_start_integrity_arch(void);
+int bhv_late_start_init_ptpg(void);
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg);
+/************************************************************/
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+#if defined(CONFIG_DOMAIN_SPACES)
+	if (!is_bhv_initialized())
+		return false;
+
+	if (!bhv_integrity_is_enabled())
+		return false;
+#endif
+
+	return brs__is__IntegrityProtection__pagetable_protection__enabled();
+}
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
+extern bool bhv_integrity_freeze_create_currently_frozen;
+extern bool bhv_integrity_freeze_update_currently_frozen;
+extern bool bhv_integrity_freeze_remove_currently_frozen;
+extern bool bhv_integrity_freeze_patch_currently_frozen;
+int bhv_integrity_freeze_events(uint64_t flags);
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks);
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head);
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(
+	HypABI__MemoryRegionOwner owner);
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm);
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline void bhv_mem_region_create_ctor(bhv_mem_region_t *curr_item,
+					      bhv_mem_region_t *prev_item,
+					      uint64_t addr, uint64_t size,
+					      uint32_t type, uint64_t flags,
+					      const char *label)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_mem_region_t){
+		.create =
+			(HypABI__Integrity__Create__Mem_Region__T){
+				.start_addr = addr,
+				.size = size,
+				.type = type,
+				.flags = flags,
+				.next = BHV_INVALID_PHYS_ADDR,
+			}
+	};
+	strncpy(curr_item->create.label, label,
+		HypABI__Integrity__MAX_LABEL_SIZE);
+	curr_item->create.label[HypABI__Integrity__MAX_LABEL_SIZE - 1] = '\0';
+
+	if (prev_item)
+		prev_item->create.next = BHV_VIRT_TO_PHYS_SIZEOF(curr_item);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags, const char *label)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	bhv_mem_region_create_ctor(&n->region, NULL, addr, size, type, flags,
+				   label);
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.create.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.update.start_addr = addr;
+	n->region.update.type = type;
+	n->region.update.flags = flags;
+	n->region.update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.update.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.remove.start_addr = addr;
+	n->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.remove.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* !CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_integrity_freeze_events(uint64_t)
+{
+	return 0;
+}
+
+static inline int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return 0;
+}
+
+static inline void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+}
+
+#define bhv_allow_kmod_loads true
+#define bhv_allow_patch true
+#define bhv_integrity_freeze_create_currently_frozen false
+#define bhv_integrity_freeze_update_currently_frozen false
+#define bhv_integrity_freeze_remove_currently_frozen false
+#define bhv_integrity_freeze_patch_currently_frozen false
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/integrity_base.h include/bhv/integrity_base.h
new file mode 100644
index 0000000000..c80ddda958
--- /dev/null
+++ include/bhv/integrity_base.h
@@ -0,0 +1,36 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_BASE_H__
+#define __BHV_INTEGRITY_BASE_H__
+
+#include <linux/bitops.h>
+#include <bhv/base.h>
+#include <bhv/config.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	// Integrity is enabled if the platform is there.
+	return true;
+}
+
+#else /* !CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_BASE_H__ */
diff --git include/bhv/interface/abi_base_autogen.h include/bhv/interface/abi_base_autogen.h
new file mode 100644
index 0000000000..795ae045c2
--- /dev/null
+++ include/bhv/interface/abi_base_autogen.h
@@ -0,0 +1,1461 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/types.h>
+
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#define BHV__HypABI__VAS__TARGET_ID 1
+
+_Static_assert(sizeof(char) == sizeof(uint8_t), "Unexpected char size");
+
+typedef uint64_t HypABI__MemoryRegionOwner;
+
+#define HypABI__Context__MAX_PATH_SZ 256ULL
+
+
+// start of HypABI__Context__Inums
+
+struct __attribute__((packed)) HypABI__Context__Inums {
+        /** This is the cgroup namespace identifier. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier. */
+        uint32_t uts_ns_inum;
+};
+typedef struct HypABI__Context__Inums HypABI__Context__Inums__T;
+#define HypABI__Context__Inums__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Context__Inums) == HypABI__Context__Inums__SZ, "Unexpected size for HypABI__Context__Inums");
+
+#define HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ 256ULL
+
+
+// start of HypABI__Context__CGroupInfo
+
+struct __attribute__((packed)) HypABI__Context__CGroupInfo {
+        /** This is the cgroup identifier. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct HypABI__Context__CGroupInfo HypABI__Context__CGroupInfo__T;
+#define HypABI__Context__CGroupInfo__SZ 264ULL
+_Static_assert(sizeof(struct HypABI__Context__CGroupInfo) == HypABI__Context__CGroupInfo__SZ, "Unexpected size for HypABI__Context__CGroupInfo");
+
+
+// start of HypABI__Context
+
+struct __attribute__((packed)) HypABI__Context {
+        /** This field indicates to the host whether the event context is valid. */
+        uint8_t valid;
+        /** This field indicates whether the current process belongs to the hierarchy of a system daemon. */
+        uint8_t sys_daemon;
+        uint8_t padding[2];
+        /** The ID of the vCPU that the event occurred on. */
+        uint32_t vcpu_id;
+        /** The user ID that was in use when the event occurred. This helps us to tie user information to an event. For example, it may allow us to answer the question which user account caused a certain security violation. Note that not this information is not always reliable. Just because an attack happened in a certain context does not necessarily mean that it was caused by the given user. */
+        uint32_t uid;
+        /** The effective user ID that was in use when the event occurred. */
+        uint32_t euid;
+        /** The group ID that was in use when the event occurred. */
+        uint32_t gid;
+        /** The and effective group ID that was in use when the event occurred. */
+        uint32_t egid;
+        /** The effective capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_effective;
+        /** The permitted capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_permitted;
+        /** The PID of the process that was in context when the event occurred. */
+        uint32_t pid;
+        /** The parent PID of the process that was in context when the event occurred. */
+        uint32_t parent_pid;
+        /** The COMM field of the task struct for the process that was in context when the event occurred. */
+        char comm[HypABI__Context__MAX_PATH_SZ];
+        /** The COMM field of the task struct for the parent binary of the process that was in context when the event occurred. */
+        char parent_comm[HypABI__Context__MAX_PATH_SZ];
+        /** The namespace identifiers for the current process. */
+        struct HypABI__Context__Inums inums;
+        /** The information about the cgroup for the current process. */
+        struct HypABI__Context__CGroupInfo cgroup;
+        /** The PATH of the process that was in context when the event occurred. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Context HypABI__Context__T;
+#define HypABI__Context__SZ 1120ULL
+_Static_assert(sizeof(struct HypABI__Context) == HypABI__Context__SZ, "Unexpected size for HypABI__Context");
+
+#define HypABI__Init__BACKEND_ID 1
+
+#define HypABI__Init__Init__OP_ID 0
+
+enum HypABI__Init__Init__BHVSectionRun__BHVSectionRunType {
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT = 42,
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA = 43,
+};
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__COUNT 2
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__LABELS \
+        OP(RICHARD_VAULT) OP(DATA)
+
+// start of HypABI__Init__Init__BHVSectionRun
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVSectionRun {
+        /** Guest-physical start address of this run. Must be page-aligned. */
+        uint64_t gpa_start;
+        /** Size in bytes. Must be a non-zero multiple of the page size. */
+        uint64_t size;
+        /** The guest-physical address of the next item in the linked list, or `INVALID_GPA` if this item is the last one. */
+        uint64_t next;
+        /** Type of the run */
+        uint8_t type;
+};
+typedef struct HypABI__Init__Init__BHVSectionRun HypABI__Init__Init__BHVSectionRun__T;
+#define HypABI__Init__Init__BHVSectionRun__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVSectionRun) == HypABI__Init__Init__BHVSectionRun__SZ, "Unexpected size for HypABI__Init__Init__BHVSectionRun");
+
+
+// start of HypABI__Init__Init__BHVData
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVData {
+        /** This field contains the cid of the vsocket endpoint the guest should can connect to. If this value is `0`, this indicates the host is not listening for a vsocket connection. */
+        uint32_t vsocket_cid;
+        /** This field contains the port of the vsocket endpoint the guest should can connect to. If the `vsocket_cid` value is `0`, this value is to be ignored. */
+        uint32_t vsocket_port;
+};
+typedef struct HypABI__Init__Init__BHVData HypABI__Init__Init__BHVData__T;
+#define HypABI__Init__Init__BHVData__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVData) == HypABI__Init__Init__BHVData__SZ, "Unexpected size for HypABI__Init__Init__BHVData");
+
+
+// start of HypABI__Init__Init__arg
+
+struct __attribute__((packed)) HypABI__Init__Init__arg {
+        /** The size of the buffer passed in `modprobe_path`. This field is ignored if `modprobe_path == INVALID_GPA`. */
+        uint64_t modprobe_path_sz;
+        /** The physical address of the guest's modprobe path. In certain cases, this can be set from the host. This field can be set to `INVALID_GPA` and it will be ignored by the host. */
+        uint64_t modprobe_path;
+        /** This establishes the owner id of the guest kernel. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0`. */
+        uint64_t owner;
+        /** The physical address of the head of a list of BHV-specific memory regions to be protected. */
+        uint64_t bhv_region_head;
+        /** The physical address of the head of a list of memory regions to be protected. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Init__Init__arg HypABI__Init__Init__arg__T;
+#define HypABI__Init__Init__arg__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__arg) == HypABI__Init__Init__arg__SZ, "Unexpected size for HypABI__Init__Init__arg");
+
+#define HypABI__Init__Start__OP_ID 1
+
+
+// start of HypABI__Init__Start__arg
+
+struct __attribute__((packed)) HypABI__Init__Start__arg {
+        /** This field is used to indicate whether the `data_sz` and `data` fields are valid. When this field is set to `false`, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        uint8_t padding[1];
+        /** The guest is expected to provide the size of the buffer (in 4k pages) that the `bhv_init_start_config_t` struct resides in. The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire guest policy into the `data` field of the `bhv_init_start_config_t` struct. */
+        uint16_t num_pages;
+        /** If the `valid` field is true, this field contains the size of buffer in bytes stored in the `data` field. */
+        uint32_t data_sz;
+        /** If the `valid` field is true and the `data_sz` field is greater than 0, this field contains the guest policy buffer. This can be used to load an arbitrary guest policy into the guest from the host. For example, this is used by the BHV Linux kernel to load SELinux policies into the guest. */
+        uint8_t data[];
+};
+typedef struct HypABI__Init__Start__arg HypABI__Init__Start__arg__T;
+#define HypABI__Init__Start__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Init__Start__arg) == HypABI__Init__Start__arg__SZ, "Unexpected size for HypABI__Init__Start__arg");
+
+#define HypABI__Integrity__BACKEND_ID 2
+
+enum HypABI__Integrity__MemType {
+        HypABI__Integrity__MemType__UNKNOWN = 0,
+        HypABI__Integrity__MemType__CODE = 1,
+        HypABI__Integrity__MemType__CODE_WRITABLE = 2,
+        HypABI__Integrity__MemType__CODE_PATCHABLE = 3,
+        HypABI__Integrity__MemType__DATA = 4,
+        HypABI__Integrity__MemType__DATA_READ_ONLY = 5,
+        HypABI__Integrity__MemType__VDSO = 6,
+        HypABI__Integrity__MemType__VVAR = 7,
+};
+#define HypABI__Integrity__MemType__COUNT 8
+#define HypABI__Integrity__MemType__LABELS \
+        OP(UNKNOWN) OP(CODE) OP(CODE_WRITABLE) OP(CODE_PATCHABLE) OP(DATA) OP(DATA_READ_ONLY) OP(VDSO) OP(VVAR)
+
+// start of HypABI__Integrity__HypABI__Integrity__MemFlags
+
+typedef unsigned long HypABI__Integrity__MemFlags__T;
+#define HypABI__Integrity__MemFlags__NONE 0UL
+#define HypABI__Integrity__MemFlags__TRANSIENT__BIT 0
+#define HypABI__Integrity__MemFlags__TRANSIENT (1UL<<0)
+#define HypABI__Integrity__MemFlags__MUTABLE__BIT 1
+#define HypABI__Integrity__MemFlags__MUTABLE (1UL<<1)
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr);
+
+#define HypABI__Integrity__MAX_LABEL_SIZE 32ULL
+
+#define HypABI__Integrity__Create__OP_ID 0
+
+
+// start of HypABI__Integrity__Create__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Create__Mem_Region {
+        /** The start address of the memory range to create. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The size of the memory region to create. This must be greater than 0 and must be a multiple of 4K. */
+        uint64_t size;
+        /** The type of the memory region to create. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The flags associated with the memory region to create. */
+        uint64_t flags;
+        /** A custom label for this section. The purpose of this label is to easier identify a memory range if an event is observed. */
+        char label[HypABI__Integrity__MAX_LABEL_SIZE];
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Create__Mem_Region HypABI__Integrity__Create__Mem_Region__T;
+#define HypABI__Integrity__Create__Mem_Region__SZ 72ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__Mem_Region) == HypABI__Integrity__Create__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Create__Mem_Region");
+
+
+// start of HypABI__Integrity__Create__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Create__arg {
+        /** This establishes the owner id of this memory region. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0` and kernel modules/drivers use unique non-zero ids. This allows the user to remove regions by owner later. */
+        uint64_t owner;
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Create__arg HypABI__Integrity__Create__arg__T;
+#define HypABI__Integrity__Create__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__arg) == HypABI__Integrity__Create__arg__SZ, "Unexpected size for HypABI__Integrity__Create__arg");
+
+#define HypABI__Integrity__Update__OP_ID 1
+
+
+// start of HypABI__Integrity__Update__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Update__Mem_Region {
+        /** The start address of the memory range to update. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The type of the memory region to update to. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The new flags to associate with the region. */
+        uint64_t flags;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Update__Mem_Region HypABI__Integrity__Update__Mem_Region__T;
+#define HypABI__Integrity__Update__Mem_Region__SZ 32ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__Mem_Region) == HypABI__Integrity__Update__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Update__Mem_Region");
+
+
+// start of HypABI__Integrity__Update__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Update__arg {
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Update__arg HypABI__Integrity__Update__arg__T;
+#define HypABI__Integrity__Update__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__arg) == HypABI__Integrity__Update__arg__SZ, "Unexpected size for HypABI__Integrity__Update__arg");
+
+#define HypABI__Integrity__Remove__OP_ID 2
+
+
+// start of HypABI__Integrity__Remove__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__Mem_Region {
+        /** The start address of the memory range to remove. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Remove__Mem_Region HypABI__Integrity__Remove__Mem_Region__T;
+#define HypABI__Integrity__Remove__Mem_Region__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__Mem_Region) == HypABI__Integrity__Remove__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Remove__Mem_Region");
+
+
+// start of HypABI__Integrity__Remove__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__arg {
+        /** This parameter determines whether the call is used remove multiple sections by an owner id or whether to explicitly remove defined sections. */
+        uint8_t rm_by_owner;
+        uint8_t padding[7];
+        union {
+                /** This field is only considered if `rm_by_owner` is true. When considered, this field contains the owner id of one or more regions to be removed. */
+                uint64_t owner;
+                /** This field is only considered if `rm_by_owner` is false. When considered, this field is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+                uint64_t region_head;
+        };
+};
+typedef struct HypABI__Integrity__Remove__arg HypABI__Integrity__Remove__arg__T;
+#define HypABI__Integrity__Remove__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__arg) == HypABI__Integrity__Remove__arg__SZ, "Unexpected size for HypABI__Integrity__Remove__arg");
+
+#define HypABI__Integrity__Freeze__OP_ID 3
+
+
+// start of HypABI__Integrity__Freeze__HypABI__Integrity__Freeze__FreezeFlags
+
+typedef unsigned long HypABI__Integrity__Freeze__FreezeFlags__T;
+#define HypABI__Integrity__Freeze__FreezeFlags__NONE 0UL
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT 0
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE (1UL<<0)
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT 1
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE (1UL<<1)
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT 2
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE (1UL<<2)
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT 3
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH (1UL<<3)
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr);
+
+
+// start of HypABI__Integrity__Freeze__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Freeze__arg {
+        /** This field contains a bitfield that specifies which operations should be frozen. */
+        uint64_t flags;
+};
+typedef struct HypABI__Integrity__Freeze__arg HypABI__Integrity__Freeze__arg__T;
+#define HypABI__Integrity__Freeze__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Freeze__arg) == HypABI__Integrity__Freeze__arg__SZ, "Unexpected size for HypABI__Integrity__Freeze__arg");
+
+#define HypABI__Integrity__PtpgInit__OP_ID 4
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES 8ULL
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO 16ULL
+
+
+// start of HypABI__Integrity__PtpgInit__arg
+
+struct __attribute__((packed)) HypABI__Integrity__PtpgInit__arg {
+        /** This must contain the physical address of the kernel's page table. In Linux this is the `pgd` stored in `init_mm`. */
+        uint64_t init_pgd;
+        /** This must contain the number of paging levels the kernel is using.  Currently only 4 and 5 level paging is supported. */
+        uint8_t pt_levels;
+        uint8_t padding[3];
+        /** This must contain the number of ranges in the succeeding `ranges` array. This must be a number >= 0 and <= `MAX_RANGES`. */
+        uint32_t num_ranges;
+        /** This is an array of pairs of `uint64_t` values in which the first value represents the start GVA of a range and the second value of the pair represents the end GVA of a range. */
+        uint64_t ranges[HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO];
+};
+typedef struct HypABI__Integrity__PtpgInit__arg HypABI__Integrity__PtpgInit__arg__T;
+#define HypABI__Integrity__PtpgInit__arg__SZ 144ULL
+_Static_assert(sizeof(struct HypABI__Integrity__PtpgInit__arg) == HypABI__Integrity__PtpgInit__arg__SZ, "Unexpected size for HypABI__Integrity__PtpgInit__arg");
+
+#define HypABI__Integrity__PtpgReport__OP_ID 5
+
+#define HypABI__Patch__BACKEND_ID 3
+
+#define HypABI__Patch__MAX_PATCH_SZ 32ULL
+
+#define HypABI__Patch__Patch__OP_ID 0
+
+
+// start of HypABI__Patch__Patch__arg
+
+struct __attribute__((packed)) HypABI__Patch__Patch__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__Patch__arg HypABI__Patch__Patch__arg__T;
+#define HypABI__Patch__Patch__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__Patch__arg) == HypABI__Patch__Patch__arg__SZ, "Unexpected size for HypABI__Patch__Patch__arg");
+
+#define HypABI__Patch__PatchNoClose__OP_ID 1
+
+
+// start of HypABI__Patch__PatchNoClose__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchNoClose__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__PatchNoClose__arg HypABI__Patch__PatchNoClose__arg__T;
+#define HypABI__Patch__PatchNoClose__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchNoClose__arg) == HypABI__Patch__PatchNoClose__arg__SZ, "Unexpected size for HypABI__Patch__PatchNoClose__arg");
+
+#define HypABI__Patch__PatchViolation__OP_ID 2
+
+#define HypABI__Patch__PatchViolation__MAX_MSG_SZ 256ULL
+
+
+// start of HypABI__Patch__PatchViolation__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchViolation__arg {
+        /** The virtual address that the guest attempted to patch. */
+        uint64_t dest_virt_addr;
+        /** The physical address that the guest attempted to patch. This is the translated `dest_virt_addr`. */
+        uint64_t dest_phys_addr;
+        /** A custom message that provides additional information about the violation. This allows each subsystem (e.g. jump labels) to provide detailed information about the violation. */
+        char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+        /** This field is set by the host and will be processed by the guest after the hypercall. It tells the guest whether to block the violation or whether to allow the patch. */
+        uint8_t block;
+};
+typedef struct HypABI__Patch__PatchViolation__arg HypABI__Patch__PatchViolation__arg__T;
+#define HypABI__Patch__PatchViolation__arg__SZ 273ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchViolation__arg) == HypABI__Patch__PatchViolation__arg__SZ, "Unexpected size for HypABI__Patch__PatchViolation__arg");
+
+#define HypABI__Richard__BACKEND_ID 4
+
+#define HypABI__Richard__Open__OP_ID 0
+
+#define HypABI__Richard__Close__OP_ID 1
+
+#define HypABI__Acl__BACKEND_ID 5
+
+#define HypABI__Acl__ProcessInit__OP_ID 0
+
+
+// start of HypABI__Acl__ProcessInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__ProcessInit__arg HypABI__Acl__ProcessInit__arg__T;
+#define HypABI__Acl__ProcessInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessInit__arg) == HypABI__Acl__ProcessInit__arg__SZ, "Unexpected size for HypABI__Acl__ProcessInit__arg");
+
+#define HypABI__Acl__DriverInit__OP_ID 1
+
+
+// start of HypABI__Acl__DriverInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__DriverInit__arg HypABI__Acl__DriverInit__arg__T;
+#define HypABI__Acl__DriverInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverInit__arg) == HypABI__Acl__DriverInit__arg__SZ, "Unexpected size for HypABI__Acl__DriverInit__arg");
+
+#define HypABI__Acl__ProcessViolation__OP_ID 2
+
+
+// start of HypABI__Acl__ProcessViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__ProcessViolation__arg HypABI__Acl__ProcessViolation__arg__T;
+#define HypABI__Acl__ProcessViolation__arg__SZ 1131ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessViolation__arg) == HypABI__Acl__ProcessViolation__arg__SZ, "Unexpected size for HypABI__Acl__ProcessViolation__arg");
+
+#define HypABI__Acl__DriverViolation__OP_ID 3
+
+
+// start of HypABI__Acl__DriverViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__DriverViolation__arg HypABI__Acl__DriverViolation__arg__T;
+#define HypABI__Acl__DriverViolation__arg__SZ 1131ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverViolation__arg) == HypABI__Acl__DriverViolation__arg__SZ, "Unexpected size for HypABI__Acl__DriverViolation__arg");
+
+#define HypABI__Guestlog__BACKEND_ID 6
+
+#define HypABI__Guestlog__Init__OP_ID 0
+
+
+// start of HypABI__Guestlog__Init__HypABI__Guestlog__Init__GuestlogFlags
+
+typedef unsigned long HypABI__Guestlog__Init__GuestlogFlags__T;
+#define HypABI__Guestlog__Init__GuestlogFlags__NONE 0UL
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT 0
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS (1UL<<0)
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT 1
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS (1UL<<1)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT 2
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS (1UL<<2)
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT 3
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS (1UL<<3)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT 4
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS (1UL<<4)
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT 5
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS (1UL<<5)
+#define HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS__BIT 6
+#define HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS (1UL<<6)
+#define HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS__BIT 7
+#define HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS (1UL<<7)
+#define HypABI__Guestlog__Init__GuestlogFlags__MMAP_EXEC_FILE_EVENTS__BIT 8
+#define HypABI__Guestlog__Init__GuestlogFlags__MMAP_EXEC_FILE_EVENTS (1UL<<8)
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr);
+
+
+// start of HypABI__Guestlog__Init__arg
+
+struct __attribute__((packed)) HypABI__Guestlog__Init__arg {
+        /** This contains a bit map that is valid if the `valid` field is true and is written by the **host**. This bit map is used to indicate to the guest which logging information is to be sent to the host. */
+        uint64_t log_bitmap;
+        /** This field is written by the **host**. If this is false, no logging events are expected by the host. If this field is true, the `log_bitmap` field is valid and should be respected by the guest. */
+        uint8_t valid;
+};
+typedef struct HypABI__Guestlog__Init__arg HypABI__Guestlog__Init__arg__T;
+#define HypABI__Guestlog__Init__arg__SZ 9ULL
+_Static_assert(sizeof(struct HypABI__Guestlog__Init__arg) == HypABI__Guestlog__Init__arg__SZ, "Unexpected size for HypABI__Guestlog__Init__arg");
+
+#define HypABI__Creds__BACKEND_ID 7
+
+enum HypABI__Creds__EventType {
+        HypABI__Creds__EventType__EVENT_NONE = 0,
+        HypABI__Creds__EventType__CORRUPTION = 1,
+        HypABI__Creds__EventType__INVALID_ASSIGNMENT = 2,
+        HypABI__Creds__EventType__DOUBLE_ASSIGNMENT = 3,
+        HypABI__Creds__EventType__INVALID_COMMIT = 4,
+        HypABI__Creds__EventType__DOUBLE_COMMIT = 5,
+};
+#define HypABI__Creds__EventType__COUNT 6
+#define HypABI__Creds__EventType__LABELS \
+        OP(EVENT_NONE) OP(CORRUPTION) OP(INVALID_ASSIGNMENT) OP(DOUBLE_ASSIGNMENT) OP(INVALID_COMMIT) OP(DOUBLE_COMMIT)
+
+// start of HypABI__Creds__TaskCred
+
+struct __attribute__((packed)) HypABI__Creds__TaskCred {
+        /** This is the guest virtual address of the `task_struct` associated with this cThis is the guest virtual address of the `task_struct` associated with this call. */
+        uint64_t addr;
+        /** This is the guest virtual address of the `cred` struct associated with task whose `task_struct` address is passed in the `addr` parameter. */
+        uint64_t cred;
+        /** This is the computed HMAC for the `task_struct` and `cred` structures passed in the previous two values. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Creds__TaskCred HypABI__Creds__TaskCred__T;
+#define HypABI__Creds__TaskCred__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__TaskCred) == HypABI__Creds__TaskCred__SZ, "Unexpected size for HypABI__Creds__TaskCred");
+
+#define HypABI__Creds__Configure__OP_ID 0
+
+
+// start of HypABI__Creds__Configure__arg
+
+struct __attribute__((packed)) HypABI__Creds__Configure__arg {
+        /** The SipHash key. */
+        uint64_t key[2];
+};
+typedef struct HypABI__Creds__Configure__arg HypABI__Creds__Configure__arg__T;
+#define HypABI__Creds__Configure__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Creds__Configure__arg) == HypABI__Creds__Configure__arg__SZ, "Unexpected size for HypABI__Creds__Configure__arg");
+
+#define HypABI__Creds__RegisterInitTask__OP_ID 1
+
+
+// start of HypABI__Creds__RegisterInitTask__arg
+
+struct __attribute__((packed)) HypABI__Creds__RegisterInitTask__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the first kernel thread (`swapper`). */
+        struct HypABI__Creds__TaskCred init_task;
+};
+typedef struct HypABI__Creds__RegisterInitTask__arg HypABI__Creds__RegisterInitTask__arg__T;
+#define HypABI__Creds__RegisterInitTask__arg__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__RegisterInitTask__arg) == HypABI__Creds__RegisterInitTask__arg__SZ, "Unexpected size for HypABI__Creds__RegisterInitTask__arg");
+
+#define HypABI__Creds__Assign__OP_ID 2
+
+
+// start of HypABI__Creds__Assign__arg
+
+struct __attribute__((packed)) HypABI__Creds__Assign__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the new task for which credentials are being allocated. */
+        struct HypABI__Creds__TaskCred new_task;
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the parent task of the task specified in the `new_task` parameter. The parent task is the task from which the privileges will be copied. */
+        struct HypABI__Creds__TaskCred parent;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Assign__arg HypABI__Creds__Assign__arg__T;
+#define HypABI__Creds__Assign__arg__SZ 49ULL
+_Static_assert(sizeof(struct HypABI__Creds__Assign__arg) == HypABI__Creds__Assign__arg__SZ, "Unexpected size for HypABI__Creds__Assign__arg");
+
+#define HypABI__Creds__AssignPriv__OP_ID 3
+
+
+// start of HypABI__Creds__AssignPriv__arg
+
+struct __attribute__((packed)) HypABI__Creds__AssignPriv__arg {
+        /** This is the guest virtual address of the `cred` struct that is being copied into. */
+        uint64_t cred;
+        /** This parameter expects the guest virtual address of the `task_struct` for the process from which credentials are being copied. If this parameter is `NULL`, it is assumed that the `swapper` process is the process from which credentials are being copied. */
+        uint64_t daemon;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__AssignPriv__arg HypABI__Creds__AssignPriv__arg__T;
+#define HypABI__Creds__AssignPriv__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Creds__AssignPriv__arg) == HypABI__Creds__AssignPriv__arg__SZ, "Unexpected size for HypABI__Creds__AssignPriv__arg");
+
+#define HypABI__Creds__Commit__OP_ID 4
+
+
+// start of HypABI__Creds__Commit__arg
+
+struct __attribute__((packed)) HypABI__Creds__Commit__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the current task that is committing its credentials. */
+        struct HypABI__Creds__TaskCred currnt;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Commit__arg HypABI__Creds__Commit__arg__T;
+#define HypABI__Creds__Commit__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Commit__arg) == HypABI__Creds__Commit__arg__SZ, "Unexpected size for HypABI__Creds__Commit__arg");
+
+#define HypABI__Creds__Release__OP_ID 5
+
+
+// start of HypABI__Creds__Release__arg
+
+struct __attribute__((packed)) HypABI__Creds__Release__arg {
+        /** This is the guest virtual address of the `cred` struct that is being released. */
+        uint64_t cred;
+};
+typedef struct HypABI__Creds__Release__arg HypABI__Creds__Release__arg__T;
+#define HypABI__Creds__Release__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Creds__Release__arg) == HypABI__Creds__Release__arg__SZ, "Unexpected size for HypABI__Creds__Release__arg");
+
+#define HypABI__Creds__Verification__OP_ID 6
+
+
+// start of HypABI__Creds__Verification__arg
+
+struct __attribute__((packed)) HypABI__Creds__Verification__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the task whose credentials must be verified. */
+        struct HypABI__Creds__TaskCred task;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Verification__arg HypABI__Creds__Verification__arg__T;
+#define HypABI__Creds__Verification__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Verification__arg) == HypABI__Creds__Verification__arg__SZ, "Unexpected size for HypABI__Creds__Verification__arg");
+
+#define HypABI__Creds__Log__OP_ID 7
+
+#define HypABI__Creds__Log__MAX_TASK_NAME_LEN 16ULL
+
+
+// start of HypABI__Creds__Log__arg
+
+struct __attribute__((packed)) HypABI__Creds__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter interpreted as a boolean. It is written by the **host** and indicates whether the guest to block the event or not. */
+        uint8_t block;
+        uint8_t padding[2];
+        /** This parameter contains the process ID of the process for which the event is being sent. */
+        uint32_t task_pid;
+        /** This parameter contains the guest virtual address of the `task_struct` of the process for which the event is being sent. */
+        uint64_t task_addr;
+        /** This parameter contains the guest virtual address of the `cred` struct of the process for which the event is being sent. */
+        uint64_t task_cred;
+        /** This parameter contains a NUL- terminated ASCII string that contains the name of the process for which the event is being sent. */
+        char task_name[HypABI__Creds__Log__MAX_TASK_NAME_LEN];
+};
+typedef struct HypABI__Creds__Log__arg HypABI__Creds__Log__arg__T;
+#define HypABI__Creds__Log__arg__SZ 1160ULL
+_Static_assert(sizeof(struct HypABI__Creds__Log__arg) == HypABI__Creds__Log__arg__SZ, "Unexpected size for HypABI__Creds__Log__arg");
+
+#define HypABI__FileProtection__BACKEND_ID 8
+
+#define HypABI__FileProtection__Init__OP_ID 0
+
+
+// start of HypABI__FileProtection__Init__HypABI__FileProtection__Init__Config
+
+typedef unsigned long HypABI__FileProtection__Init__Config__T;
+#define HypABI__FileProtection__Init__Config__NONE 0UL
+#define HypABI__FileProtection__Init__Config__READ_ONLY__BIT 0
+#define HypABI__FileProtection__Init__Config__READ_ONLY (1UL<<0)
+#define HypABI__FileProtection__Init__Config__FILE_OPS__BIT 1
+#define HypABI__FileProtection__Init__Config__FILE_OPS (1UL<<1)
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT 2
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED (1UL<<2)
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr);
+
+
+// start of HypABI__FileProtection__Init__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__Init__arg {
+        /** This bitmap is written by the **host** and is updated to indicate which file protection violations the host is expecting from the guest. */
+        uint64_t feature_bitmap;
+};
+typedef struct HypABI__FileProtection__Init__arg HypABI__FileProtection__Init__arg__T;
+#define HypABI__FileProtection__Init__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__Init__arg) == HypABI__FileProtection__Init__arg__SZ, "Unexpected size for HypABI__FileProtection__Init__arg");
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID 1
+
+
+// start of HypABI__FileProtection__ViolationWriteReadOnlyFile__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationWriteReadOnlyFile__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ 1131ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg) == HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationWriteReadOnlyFile__arg");
+
+#define HypABI__FileProtection__ViolationFileOps__OP_ID 2
+
+enum HypABI__FileProtection__ViolationFileOps__FopsType {
+        HypABI__FileProtection__ViolationFileOps__FopsType__EXT4 = 0,
+        HypABI__FileProtection__ViolationFileOps__FopsType__TMPFS = 1,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_MEM = 2,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_NULL = 3,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_PORT = 4,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_ZERO = 5,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_FULL = 6,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_RANDOM = 7,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_URANDOM = 8,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_KMSG = 9,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_TTY = 10,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_CONSOLE = 11,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PROC = 12,
+        HypABI__FileProtection__ViolationFileOps__FopsType__XFS = 13,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SOCKFS = 14,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PIPEFS = 15,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SYSFS = 16,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEBUGFS = 17,
+        HypABI__FileProtection__ViolationFileOps__FopsType__RAMFS = 18,
+        HypABI__FileProtection__ViolationFileOps__FopsType__UNSUPPORTED = 255,
+};
+#define HypABI__FileProtection__ViolationFileOps__FopsType__COUNT 20
+#define HypABI__FileProtection__ViolationFileOps__FopsType__LABELS \
+        OP(EXT4) OP(TMPFS) OP(DEV_MEM) OP(DEV_NULL) OP(DEV_PORT) OP(DEV_ZERO) OP(DEV_FULL) OP(DEV_RANDOM) OP(DEV_URANDOM) OP(DEV_KMSG) OP(DEV_TTY) OP(DEV_CONSOLE) OP(PROC) OP(XFS) OP(SOCKFS) OP(PIPEFS) OP(SYSFS) OP(DEBUGFS) OP(RAMFS) OP(UNSUPPORTED)
+#define HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ 1024ULL
+
+
+// start of HypABI__FileProtection__ViolationFileOps__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationFileOps__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a byte value set by the guest to indicate the file system type on which a violation occurred. */
+        uint8_t fops_type;
+        /** This parameter denotes whether the violating access was on a file (`false`) or directory (`true`). */
+        uint8_t is_dir;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+        uint8_t padding[5];
+        /** This parameter contains the address of the `file_ops` struct that was corrupted. */
+        uint64_t fops_ptr;
+        /** The path whose access generated the violation. This char array is at most 1024 bytes long. */
+        char path_name[HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ];
+};
+typedef struct HypABI__FileProtection__ViolationFileOps__arg HypABI__FileProtection__ViolationFileOps__arg__T;
+#define HypABI__FileProtection__ViolationFileOps__arg__SZ 2160ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationFileOps__arg) == HypABI__FileProtection__ViolationFileOps__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationFileOps__arg");
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID 3
+
+
+// start of HypABI__FileProtection__ViolationDirtyCredWrite__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationDirtyCredWrite__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationDirtyCredWrite__arg HypABI__FileProtection__ViolationDirtyCredWrite__arg__T;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ 1131ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationDirtyCredWrite__arg) == HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationDirtyCredWrite__arg");
+
+#define HypABI__RegisterProtection__BACKEND_ID 9
+
+#define HypABI__RegisterProtection__Freeze__OP_ID 0
+
+#ifdef CONFIG_X86_64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR3 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR4 = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__EFER = 8,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 4
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(CR0) OP(CR3) OP(CR4) OP(EFER)
+
+#elif defined CONFIG_ARM64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR1 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TCR = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__SCTLR = 8,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__VBAR = 16,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 5
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(TTBR0) OP(TTBR1) OP(TCR) OP(SCTLR) OP(VBAR)
+
+#else
+#error Unsupported architecture
+#endif
+
+// start of HypABI__RegisterProtection__Freeze__arg
+
+struct __attribute__((packed)) HypABI__RegisterProtection__Freeze__arg {
+        /** This parameter selects which register this freeze request applies to. */
+        uint64_t register_selector;
+        /** Any bit which is set in this bitfield will be frozen. As an example, a value of `ffffffffffffffff` will completely freeze a register. */
+        uint64_t freeze_bitfield;
+};
+typedef struct HypABI__RegisterProtection__Freeze__arg HypABI__RegisterProtection__Freeze__arg__T;
+#define HypABI__RegisterProtection__Freeze__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__RegisterProtection__Freeze__arg) == HypABI__RegisterProtection__Freeze__arg__SZ, "Unexpected size for HypABI__RegisterProtection__Freeze__arg");
+
+#define HypABI__Domain__BACKEND_ID 10
+
+
+// start of HypABI__Domain__Domain
+
+struct __attribute__((packed)) HypABI__Domain__Domain {
+        /** The ID of the domain that the operation refers to. */
+        uint64_t id;
+        /** The page global directory (process) that the operation refers to. */
+        uint64_t pgd;
+};
+typedef struct HypABI__Domain__Domain HypABI__Domain__Domain__T;
+#define HypABI__Domain__Domain__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Domain__Domain) == HypABI__Domain__Domain__SZ, "Unexpected size for HypABI__Domain__Domain");
+
+#define HypABI__Domain__Configure__OP_ID 0
+
+
+// start of HypABI__Domain__Configure__arg
+
+struct __attribute__((packed)) HypABI__Domain__Configure__arg {
+        uint8_t padding0[16];
+        /** Whether or not the guest is using page table isolation. */
+        uint8_t pti;
+        uint8_t padding1[7];
+        /** The GPA of the physical region that is used for the indirect communication channel. */
+        uint64_t batched_region;
+        /** Differentiates between the heavy- and light-weight strong isolation version. If `true`, the heavy-weight strong isolation version is in use. */
+        uint8_t isolate;
+};
+typedef struct HypABI__Domain__Configure__arg HypABI__Domain__Configure__arg__T;
+#define HypABI__Domain__Configure__arg__SZ 33ULL
+_Static_assert(sizeof(struct HypABI__Domain__Configure__arg) == HypABI__Domain__Configure__arg__SZ, "Unexpected size for HypABI__Domain__Configure__arg");
+
+#define HypABI__Domain__Report__OP_ID 10
+
+
+// start of HypABI__Domain__Report__arg
+
+struct __attribute__((packed)) HypABI__Domain__Report__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__Report__arg HypABI__Domain__Report__arg__T;
+#define HypABI__Domain__Report__arg__SZ 1186ULL
+_Static_assert(sizeof(struct HypABI__Domain__Report__arg) == HypABI__Domain__Report__arg__SZ, "Unexpected size for HypABI__Domain__Report__arg");
+
+#define HypABI__Domain__ReportForcedMemAccess__OP_ID 11
+
+
+// start of HypABI__Domain__ReportForcedMemAccess__arg
+
+struct __attribute__((packed)) HypABI__Domain__ReportForcedMemAccess__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__ReportForcedMemAccess__arg HypABI__Domain__ReportForcedMemAccess__arg__T;
+#define HypABI__Domain__ReportForcedMemAccess__arg__SZ 1186ULL
+_Static_assert(sizeof(struct HypABI__Domain__ReportForcedMemAccess__arg) == HypABI__Domain__ReportForcedMemAccess__arg__SZ, "Unexpected size for HypABI__Domain__ReportForcedMemAccess__arg");
+
+#define HypABI__Inode__BACKEND_ID 11
+
+enum HypABI__Inode__EventType {
+        HypABI__Inode__EventType__EVENT_NONE = 0,
+        HypABI__Inode__EventType__NO_INODE = 1,
+        HypABI__Inode__EventType__CORRUPTION = 2,
+};
+#define HypABI__Inode__EventType__COUNT 3
+#define HypABI__Inode__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_INODE) OP(CORRUPTION)
+
+// start of HypABI__Inode__InodeInfo
+
+struct __attribute__((packed)) HypABI__Inode__InodeInfo {
+        /** This is the guest virtual address of the `struct inode` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct inode` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Inode__InodeInfo HypABI__Inode__InodeInfo__T;
+#define HypABI__Inode__InodeInfo__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__InodeInfo) == HypABI__Inode__InodeInfo__SZ, "Unexpected size for HypABI__Inode__InodeInfo");
+
+#define HypABI__Inode__Register__OP_ID 0
+
+
+// start of HypABI__Inode__Register__arg
+
+struct __attribute__((packed)) HypABI__Inode__Register__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Register__arg HypABI__Inode__Register__arg__T;
+#define HypABI__Inode__Register__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Register__arg) == HypABI__Inode__Register__arg__SZ, "Unexpected size for HypABI__Inode__Register__arg");
+
+#define HypABI__Inode__Update__OP_ID 1
+
+
+// start of HypABI__Inode__Update__arg
+
+struct __attribute__((packed)) HypABI__Inode__Update__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be updated by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Update__arg HypABI__Inode__Update__arg__T;
+#define HypABI__Inode__Update__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Update__arg) == HypABI__Inode__Update__arg__SZ, "Unexpected size for HypABI__Inode__Update__arg");
+
+#define HypABI__Inode__Release__OP_ID 2
+
+
+// start of HypABI__Inode__Release__arg
+
+struct __attribute__((packed)) HypABI__Inode__Release__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` that is to be released by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Release__arg HypABI__Inode__Release__arg__T;
+#define HypABI__Inode__Release__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Release__arg) == HypABI__Inode__Release__arg__SZ, "Unexpected size for HypABI__Inode__Release__arg");
+
+#define HypABI__Inode__Verify__OP_ID 3
+
+
+// start of HypABI__Inode__Verify__arg
+
+struct __attribute__((packed)) HypABI__Inode__Verify__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be verified by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Inode__Verify__arg HypABI__Inode__Verify__arg__T;
+#define HypABI__Inode__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Inode__Verify__arg) == HypABI__Inode__Verify__arg__SZ, "Unexpected size for HypABI__Inode__Verify__arg");
+
+#define HypABI__Inode__Log__OP_ID 4
+
+
+// start of HypABI__Inode__Log__arg
+
+struct __attribute__((packed)) HypABI__Inode__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the inode in question. */
+        uint64_t inode_addr;
+        /** This parameter contains the uid field of the inode in question. */
+        uint32_t inode_uid;
+        /** This parameter contains the gid field of the inode in question. */
+        uint32_t inode_gid;
+        /** This parameter contains the mode field of the inode in question. */
+        uint16_t inode_mode;
+        uint8_t padding[4];
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /**  */
+        uint8_t block;
+        /** This parameter contains the file path to the binary that is represented by the inode in question. */
+        char file_path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Inode__Log__arg HypABI__Inode__Log__arg__T;
+#define HypABI__Inode__Log__arg__SZ 1400ULL
+_Static_assert(sizeof(struct HypABI__Inode__Log__arg) == HypABI__Inode__Log__arg__SZ, "Unexpected size for HypABI__Inode__Log__arg");
+
+#define HypABI__Keyring__BACKEND_ID 12
+
+enum HypABI__Keyring__EventType {
+        HypABI__Keyring__EventType__EVENT_NONE = 0,
+        HypABI__Keyring__EventType__NO_KEYRING = 1,
+        HypABI__Keyring__EventType__CORRUPTION = 2,
+};
+#define HypABI__Keyring__EventType__COUNT 3
+#define HypABI__Keyring__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_KEYRING) OP(CORRUPTION)
+
+// start of HypABI__Keyring__Keyring
+
+struct __attribute__((packed)) HypABI__Keyring__Keyring {
+        /** This is the guest virtual address of the `struct key` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct key` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Keyring__Keyring HypABI__Keyring__Keyring__T;
+#define HypABI__Keyring__Keyring__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Keyring) == HypABI__Keyring__Keyring__SZ, "Unexpected size for HypABI__Keyring__Keyring");
+
+#define HypABI__Keyring__Register__OP_ID 0
+
+
+// start of HypABI__Keyring__Register__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Register__arg {
+        /** This is the keyring structure containing the GVA of the `struct key` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        uint8_t block;
+};
+typedef struct HypABI__Keyring__Register__arg HypABI__Keyring__Register__arg__T;
+#define HypABI__Keyring__Register__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Register__arg) == HypABI__Keyring__Register__arg__SZ, "Unexpected size for HypABI__Keyring__Register__arg");
+
+#define HypABI__Keyring__Verify__OP_ID 2
+
+
+// start of HypABI__Keyring__Verify__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Verify__arg {
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Keyring__Verify__arg HypABI__Keyring__Verify__arg__T;
+#define HypABI__Keyring__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Verify__arg) == HypABI__Keyring__Verify__arg__SZ, "Unexpected size for HypABI__Keyring__Verify__arg");
+
+#define HypABI__Keyring__Log__OP_ID 3
+
+
+// start of HypABI__Keyring__Log__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the keyring in question. */
+        uint64_t keyring_addr;
+        /** This parameter contains the uid field of the keyring in question. */
+        uint32_t keyring_uid;
+        /** This parameter contains the gid field of the keyring in question. */
+        uint32_t keyring_gid;
+        /** This parameter contains the permissions of the keyring in question. */
+        uint32_t keyring_perm;
+        /** This parameter contains the ID of the keyring in question. */
+        uint32_t keyring_serial;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter tells the guest whether to block the event. */
+        uint8_t block;
+        /** This parameter contains the keyrings name/description. */
+        char keyring_desc[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Keyring__Log__arg HypABI__Keyring__Log__arg__T;
+#define HypABI__Keyring__Log__arg__SZ 1402ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Log__arg) == HypABI__Keyring__Log__arg__SZ, "Unexpected size for HypABI__Keyring__Log__arg");
+
+#define HypABI__Confserver__BACKEND_ID 13
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID 0
+
+
+// start of HypABI__Confserver__FreezeMemoryAfterBoot__arg
+
+struct __attribute__((packed)) HypABI__Confserver__FreezeMemoryAfterBoot__arg {
+        /** This field will be set to `true` if the host wants the guest to freeze its memory protections after boot. */
+        uint8_t freeze_memory_after_boot;
+};
+typedef struct HypABI__Confserver__FreezeMemoryAfterBoot__arg HypABI__Confserver__FreezeMemoryAfterBoot__arg__T;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__FreezeMemoryAfterBoot__arg) == HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ, "Unexpected size for HypABI__Confserver__FreezeMemoryAfterBoot__arg");
+
+#define HypABI__Confserver__StrictFileops__OP_ID 1
+
+
+// start of HypABI__Confserver__StrictFileops__arg
+
+struct __attribute__((packed)) HypABI__Confserver__StrictFileops__arg {
+        /** This field will be set to `true` if the host wants the guest to enforce strict file operation checks. */
+        uint8_t strict_fileops;
+};
+typedef struct HypABI__Confserver__StrictFileops__arg HypABI__Confserver__StrictFileops__arg__T;
+#define HypABI__Confserver__StrictFileops__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__StrictFileops__arg) == HypABI__Confserver__StrictFileops__arg__SZ, "Unexpected size for HypABI__Confserver__StrictFileops__arg");
+
+#define HypABI__GuestPolicy__BACKEND_ID 14
+
+#define HypABI__GuestPolicy__GetPolicy__OP_ID 0
+
+enum HypABI__GuestPolicy__GetPolicy__PolicyType {
+        HypABI__GuestPolicy__GetPolicy__PolicyType__BOOT = 0,
+        HypABI__GuestPolicy__GetPolicy__PolicyType__RUNTIME = 1,
+};
+#define HypABI__GuestPolicy__GetPolicy__PolicyType__COUNT 2
+#define HypABI__GuestPolicy__GetPolicy__PolicyType__LABELS \
+        OP(BOOT) OP(RUNTIME)
+
+// start of HypABI__GuestPolicy__GetPolicy__arg
+
+struct __attribute__((packed)) HypABI__GuestPolicy__GetPolicy__arg {
+        /** The guest physical address that the policy should be written to. The area needs to be consecutive in physical memory. */
+        uint64_t dest;
+        /** The guest is expected to provide the size of `dest` buffer. The host updates this value to the size of the written data. If the `dest buffer` should be too small, the host will set the field to the required size and will set `valid` to false. */
+        uint32_t dest_sz;
+        /** - */
+        uint8_t padding;
+        /** This parameter is written by the **host** and determines whether the policy could be written. */
+        uint8_t valid;
+        uint8_t padding2[2];
+        /** The guest virtual address of the dest buffer within the guest. */
+        uint64_t dest_gva;
+};
+typedef struct HypABI__GuestPolicy__GetPolicy__arg HypABI__GuestPolicy__GetPolicy__arg__T;
+#define HypABI__GuestPolicy__GetPolicy__arg__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__GuestPolicy__GetPolicy__arg) == HypABI__GuestPolicy__GetPolicy__arg__SZ, "Unexpected size for HypABI__GuestPolicy__GetPolicy__arg");
+
+#define HypABI__GuestPolicy__Init__OP_ID 1
+
+
+// start of HypABI__GuestPolicy__Init__arg
+
+struct __attribute__((packed)) HypABI__GuestPolicy__Init__arg {
+        /** The guest physical address of the boot policy struct. */
+        uint64_t boot_policy_struct_addr;
+        /** The guest physical address of the runtime policy struct. */
+        uint64_t runtime_policy_struct_addr;
+};
+typedef struct HypABI__GuestPolicy__Init__arg HypABI__GuestPolicy__Init__arg__T;
+#define HypABI__GuestPolicy__Init__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__GuestPolicy__Init__arg) == HypABI__GuestPolicy__Init__arg__SZ, "Unexpected size for HypABI__GuestPolicy__Init__arg");
+
+#define HypABI__Wagner__BACKEND_ID 15
+
+
+// start of HypABI__Wagner__MemRegion
+
+struct __attribute__((packed)) HypABI__Wagner__MemRegion {
+        /** This is the guest-physical start address of the memory region. */
+        uint64_t gpa;
+        /** This is the size of the memory region (in Bytes). */
+        uint64_t size;
+};
+typedef struct HypABI__Wagner__MemRegion HypABI__Wagner__MemRegion__T;
+#define HypABI__Wagner__MemRegion__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemRegion) == HypABI__Wagner__MemRegion__SZ, "Unexpected size for HypABI__Wagner__MemRegion");
+
+
+// start of HypABI__Wagner__MemSegment
+
+struct __attribute__((packed)) HypABI__Wagner__MemSegment {
+        /** - */
+        uint64_t start;
+        /** - */
+        uint64_t end;
+};
+typedef struct HypABI__Wagner__MemSegment HypABI__Wagner__MemSegment__T;
+#define HypABI__Wagner__MemSegment__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemSegment) == HypABI__Wagner__MemSegment__SZ, "Unexpected size for HypABI__Wagner__MemSegment");
+
+
+// start of HypABI__Wagner__TransitPoint
+
+struct __attribute__((packed)) HypABI__Wagner__TransitPoint {
+        /** This is the guest-physical address of a transit point, that can be an entry point or a return point, to the vault. An entry point is a guest-physical address, the execution of which allows to enter the vault; a return point is a guest-physical address, to which the vault can safely return, e.g., after a function call. */
+        uint64_t gpa;
+};
+typedef struct HypABI__Wagner__TransitPoint HypABI__Wagner__TransitPoint__T;
+#define HypABI__Wagner__TransitPoint__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Wagner__TransitPoint) == HypABI__Wagner__TransitPoint__SZ, "Unexpected size for HypABI__Wagner__TransitPoint");
+
+#define HypABI__Wagner__Create__OP_ID 0
+
+
+// start of HypABI__Wagner__Create__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Create__arg {
+        /** The vault code region. */
+        struct HypABI__Wagner__MemRegion code;
+        /** The vault ref code region. */
+        struct HypABI__Wagner__MemRegion ref_code;
+        /** The shared code region. */
+        struct HypABI__Wagner__MemRegion shared_code;
+        /** The vault alternative instruction region. */
+        struct HypABI__Wagner__MemRegion altinstr_aux;
+        /** The vault noinstr region. */
+        struct HypABI__Wagner__MemRegion noinstr_text;
+        /** The vault thunk region. */
+        struct HypABI__Wagner__MemRegion thunks;
+        /** The vault data region. */
+        struct HypABI__Wagner__MemRegion data;
+        /** The vault read-only data region. */
+        struct HypABI__Wagner__MemRegion ro_data;
+        /** The vault entry text region. */
+        struct HypABI__Wagner__MemSegment entry_text;
+        /** This is the total number of entry points into the vault. */
+        uint32_t nr_entry_points;
+        /** This is the total number of return points into the vault. */
+        uint32_t nr_return_points;
+        /** This buffer contains `nr_entry_points` + `nr_return_points` entries with transit points. */
+        uint64_t transit_points;
+};
+typedef struct HypABI__Wagner__Create__arg HypABI__Wagner__Create__arg__T;
+#define HypABI__Wagner__Create__arg__SZ 160ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Create__arg) == HypABI__Wagner__Create__arg__SZ, "Unexpected size for HypABI__Wagner__Create__arg");
+
+#define HypABI__Wagner__Extend__OP_ID 1
+
+
+// start of HypABI__Wagner__Extend__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Extend__arg {
+        /** This entry contains the memory holding data that should be added to the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Extend__arg HypABI__Wagner__Extend__arg__T;
+#define HypABI__Wagner__Extend__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Extend__arg) == HypABI__Wagner__Extend__arg__SZ, "Unexpected size for HypABI__Wagner__Extend__arg");
+
+#define HypABI__Wagner__Delete__OP_ID 2
+
+
+// start of HypABI__Wagner__Delete__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Delete__arg {
+        /** This entry contains the memory holding data that should be removed from the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Delete__arg HypABI__Wagner__Delete__arg__T;
+#define HypABI__Wagner__Delete__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Delete__arg) == HypABI__Wagner__Delete__arg__SZ, "Unexpected size for HypABI__Wagner__Delete__arg");
+
+
+
+// GuestConnABI
+
+#define GuestConnABI__MAX_MSG_SZ 4096ULL
+
+
+// start of GuestConnABI__Header
+
+struct __attribute__((packed)) GuestConnABI__Header {
+        uint16_t backend;
+        uint16_t sz;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__Header GuestConnABI__Header__T;
+#define GuestConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__Header) == GuestConnABI__Header__SZ, "Unexpected size for GuestConnABI__Header");
+
+#define GuestConnABI__GuestLog__BACKEND_ID 0
+
+#define GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ 32ULL
+
+#define GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ 1024ULL
+
+#define GuestConnABI__GuestLog__PROC_COMM_SZ 16ULL
+
+#define GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ 256ULL
+
+
+// start of GuestConnABI__GuestLog__Header
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Header {
+        uint16_t evt;
+        uint16_t sz;
+};
+typedef struct GuestConnABI__GuestLog__Header GuestConnABI__GuestLog__Header__T;
+#define GuestConnABI__GuestLog__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Header) == GuestConnABI__GuestLog__Header__SZ, "Unexpected size for GuestConnABI__GuestLog__Header");
+
+
+// start of GuestConnABI__GuestLog__Message
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Message {
+        struct GuestConnABI__GuestLog__Header header;
+        struct HypABI__Context context;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__GuestLog__Message GuestConnABI__GuestLog__Message__T;
+#define GuestConnABI__GuestLog__Message__SZ 1124ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Message) == GuestConnABI__GuestLog__Message__SZ, "Unexpected size for GuestConnABI__GuestLog__Message");
+
+#define GuestConnABI__GuestLog__KernelAccess__EVT_ID 5
+
+enum GuestConnABI__GuestLog__KernelAccess__AccessType {
+        GuestConnABI__GuestLog__KernelAccess__AccessType__READ = 0,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE = 1,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE = 2,
+};
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__COUNT 3
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__LABELS \
+        OP(READ) OP(WRITE) OP(EXECUTE)
+#define GuestConnABI__GuestLog__FopsUnknown__EVT_ID 6
+
+enum GuestConnABI__GuestLog__FopsUnknown__FileStructType {
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__UNKNOWN = 0,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE = 1,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY = 2,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL = 3,
+};
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__COUNT 4
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__LABELS \
+        OP(UNKNOWN) OP(FILE) OP(DIRECTORY) OP(SPECIAL)
+#define SendConnABI__CMD_BACKEND 0ULL
+
+#define SendConnABI__NUM_BACKENDS 1ULL
+
+#define SendConnABI__CMD_KILL_PROC 0ULL
+
+#define SendConnABI__CMD_KILL_CONTAINER 1ULL
+
+#define SendConnABI__CMD_POLICY_UPDATE 2ULL
+
+#define SendConnABI__NUM_CMDS 3ULL
+
+
+// start of SendConnABI__Header
+
+struct __attribute__((packed)) SendConnABI__Header {
+        /** The id of the backend that this message is destined for. */
+        uint16_t backend;
+        /** The size of the message in bytes including the header. */
+        uint16_t sz;
+};
+typedef struct SendConnABI__Header SendConnABI__Header__T;
+#define SendConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct SendConnABI__Header) == SendConnABI__Header__SZ, "Unexpected size for SendConnABI__Header");
+
+
+// start of SendConnABI__CmdKillProc
+
+struct __attribute__((packed)) SendConnABI__CmdKillProc {
+        /** The id of the kill proc cmd. */
+        uint64_t cmd_id;
+        /** The process id of the process to kill */
+        uint64_t pid;
+};
+typedef struct SendConnABI__CmdKillProc SendConnABI__CmdKillProc__T;
+#define SendConnABI__CmdKillProc__SZ 16ULL
+_Static_assert(sizeof(struct SendConnABI__CmdKillProc) == SendConnABI__CmdKillProc__SZ, "Unexpected size for SendConnABI__CmdKillProc");
+
+
+// start of SendConnABI__CmdKillContainer
+
+struct __attribute__((packed)) SendConnABI__CmdKillContainer {
+        /** The id of the kill container cmd. */
+        uint64_t cmd_id;
+        /** The ID of the container to kill. */
+        char container_id[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct SendConnABI__CmdKillContainer SendConnABI__CmdKillContainer__T;
+#define SendConnABI__CmdKillContainer__SZ 264ULL
+_Static_assert(sizeof(struct SendConnABI__CmdKillContainer) == SendConnABI__CmdKillContainer__SZ, "Unexpected size for SendConnABI__CmdKillContainer");
+
+
+// start of SendConnABI__CmdPolicyUpdate
+
+struct __attribute__((packed)) SendConnABI__CmdPolicyUpdate {
+        /** The id of the policy update cmd. */
+        uint64_t cmd_id;
+};
+typedef struct SendConnABI__CmdPolicyUpdate SendConnABI__CmdPolicyUpdate__T;
+#define SendConnABI__CmdPolicyUpdate__SZ 8ULL
+_Static_assert(sizeof(struct SendConnABI__CmdPolicyUpdate) == SendConnABI__CmdPolicyUpdate__SZ, "Unexpected size for SendConnABI__CmdPolicyUpdate");
+
diff --git include/bhv/interface/abi_hl_autogen.h include/bhv/interface/abi_hl_autogen.h
new file mode 100644
index 0000000000..69f00d58b5
--- /dev/null
+++ include/bhv/interface/abi_hl_autogen.h
@@ -0,0 +1,1601 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+void HypABI__init_slabs(void);
+
+extern struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab;
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__Mem_Region__T *)kmem_cache_alloc(HypABI__Integrity__Create__Mem_Region__slab, gfp)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__Mem_Region__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__Mem_Region__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = &static; \
+        if (HypABI__Integrity__Create__Mem_Region__slab) \
+                __arg = HypABI__Integrity__Create__Mem_Region__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__Mem_Region__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Integrity__Create__arg__slab;
+#define HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__arg__T *)kmem_cache_alloc(HypABI__Integrity__Create__arg__slab, gfp)
+#define HypABI__Integrity__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__ALLOC() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Create__arg__slab) \
+                __arg = HypABI__Integrity__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall(const HypABI__Integrity__Create__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Create__arg__T *bhv_arg = HypABI__Integrity__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Create__arg__T));
+        rc = HypABI__Integrity__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Create__HYPERCALL(...) HypABI__Integrity__Create__hypercall((HypABI__Integrity__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Update__arg__slab;
+#define HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Update__arg__T *)kmem_cache_alloc(HypABI__Integrity__Update__arg__slab, gfp)
+#define HypABI__Integrity__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__ALLOC() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Update__arg__slab) \
+                __arg = HypABI__Integrity__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall(const HypABI__Integrity__Update__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Update__arg__T *bhv_arg = HypABI__Integrity__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Update__arg__T));
+        rc = HypABI__Integrity__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Update__HYPERCALL(...) HypABI__Integrity__Update__hypercall((HypABI__Integrity__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Remove__arg__slab;
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Remove__arg__T *)kmem_cache_alloc(HypABI__Integrity__Remove__arg__slab, gfp)
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Remove__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__ALLOC() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Remove__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Remove__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Remove__arg__slab) \
+                __arg = HypABI__Integrity__Remove__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Remove__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall(const HypABI__Integrity__Remove__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Remove__arg__T *bhv_arg = HypABI__Integrity__Remove__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Remove__arg__T));
+        rc = HypABI__Integrity__Remove__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Remove__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Remove__HYPERCALL(...) HypABI__Integrity__Remove__hypercall((HypABI__Integrity__Remove__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Freeze__arg__slab;
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Freeze__arg__T *)kmem_cache_alloc(HypABI__Integrity__Freeze__arg__slab, gfp)
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__ALLOC() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Freeze__arg__slab) \
+                __arg = HypABI__Integrity__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall(const HypABI__Integrity__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Freeze__arg__T *bhv_arg = HypABI__Integrity__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Freeze__arg__T));
+        rc = HypABI__Integrity__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Freeze__HYPERCALL(...) HypABI__Integrity__Freeze__hypercall((HypABI__Integrity__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab;
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__PtpgInit__arg__T *)kmem_cache_alloc(HypABI__Integrity__PtpgInit__arg__slab, gfp)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__PtpgInit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__PtpgInit__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = &static; \
+        if (HypABI__Integrity__PtpgInit__arg__slab) \
+                __arg = HypABI__Integrity__PtpgInit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__PtpgInit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall(const HypABI__Integrity__PtpgInit__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__PtpgInit__arg__T *bhv_arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__PtpgInit__arg__T));
+        rc = HypABI__Integrity__PtpgInit__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__PtpgInit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__PtpgInit__HYPERCALL(...) HypABI__Integrity__PtpgInit__hypercall((HypABI__Integrity__PtpgInit__arg__T){__VA_ARGS__})
+
+#define HypABI__Integrity__PtpgReport__hypercall() \
+        HypABI__Integrity__PtpgReport__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Patch__Patch__arg__slab;
+#define HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__Patch__arg__T *)kmem_cache_alloc(HypABI__Patch__Patch__arg__slab, gfp)
+#define HypABI__Patch__Patch__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__Patch__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__ALLOC() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__Patch__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__Patch__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = &static; \
+        if (HypABI__Patch__Patch__arg__slab) \
+                __arg = HypABI__Patch__Patch__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__Patch__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall(const HypABI__Patch__Patch__arg__T s)
+{
+        int rc;
+        HypABI__Patch__Patch__arg__T *bhv_arg = HypABI__Patch__Patch__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__Patch__arg__T));
+        rc = HypABI__Patch__Patch__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__Patch__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__Patch__HYPERCALL(...) HypABI__Patch__Patch__hypercall((HypABI__Patch__Patch__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab;
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchNoClose__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchNoClose__arg__slab, gfp)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchNoClose__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchNoClose__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchNoClose__arg__slab) \
+                __arg = HypABI__Patch__PatchNoClose__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchNoClose__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall(const HypABI__Patch__PatchNoClose__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchNoClose__arg__T *bhv_arg = HypABI__Patch__PatchNoClose__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchNoClose__arg__T));
+        rc = HypABI__Patch__PatchNoClose__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__PatchNoClose__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchNoClose__HYPERCALL(...) HypABI__Patch__PatchNoClose__hypercall((HypABI__Patch__PatchNoClose__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab;
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchViolation__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchViolation__arg__slab, gfp)
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__ALLOC() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchViolation__arg__slab) \
+                __arg = HypABI__Patch__PatchViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchViolation__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall(uint8_t *block_out, const HypABI__Patch__PatchViolation__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchViolation__arg__T *bhv_arg = HypABI__Patch__PatchViolation__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchViolation__arg__T));
+        rc = HypABI__Patch__PatchViolation__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Patch__PatchViolation__arg__T*)bhv_arg)->block;
+        HypABI__Patch__PatchViolation__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchViolation__HYPERCALL(BLOCK_OUT, ...) HypABI__Patch__PatchViolation__hypercall(BLOCK_OUT, (HypABI__Patch__PatchViolation__arg__T){__VA_ARGS__})
+
+#define HypABI__Richard__Open__hypercall() \
+        HypABI__Richard__Open__hypercall_noalloc()
+
+#define HypABI__Richard__Close__hypercall() \
+        HypABI__Richard__Close__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab;
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__ProcessViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__ProcessViolation__arg__slab, gfp)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__ProcessViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__ProcessViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__ProcessViolation__arg__slab) \
+                __arg = HypABI__Acl__ProcessViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__ProcessViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab;
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__DriverViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__DriverViolation__arg__slab, gfp)
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__DriverViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__ALLOC() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__DriverViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__DriverViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__DriverViolation__arg__slab) \
+                __arg = HypABI__Acl__DriverViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__DriverViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Guestlog__Init__arg__slab;
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Guestlog__Init__arg__T *)kmem_cache_alloc(HypABI__Guestlog__Init__arg__slab, gfp)
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Guestlog__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__ALLOC_NOCHECK() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__ALLOC() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__Guestlog__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__Guestlog__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = &static; \
+        if (HypABI__Guestlog__Init__arg__slab) \
+                __arg = HypABI__Guestlog__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Guestlog__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall(uint64_t *log_bitmap_out, uint8_t *valid_out, const HypABI__Guestlog__Init__arg__T s)
+{
+        int rc;
+        HypABI__Guestlog__Init__arg__T *bhv_arg = HypABI__Guestlog__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Guestlog__Init__arg__T));
+        rc = HypABI__Guestlog__Init__hypercall_noalloc(bhv_arg);
+        *log_bitmap_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->log_bitmap;
+        *valid_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->valid;
+        HypABI__Guestlog__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Guestlog__Init__HYPERCALL(LOG_BITMAP_OUT, VALID_OUT, ...) HypABI__Guestlog__Init__hypercall(LOG_BITMAP_OUT, VALID_OUT, (HypABI__Guestlog__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Configure__arg__slab;
+#define HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Configure__arg__T *)kmem_cache_alloc(HypABI__Creds__Configure__arg__slab, gfp)
+#define HypABI__Creds__Configure__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Configure__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__ALLOC() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Configure__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = &static; \
+        if (HypABI__Creds__Configure__arg__slab) \
+                __arg = HypABI__Creds__Configure__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Configure__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab;
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__RegisterInitTask__arg__T *)kmem_cache_alloc(HypABI__Creds__RegisterInitTask__arg__slab, gfp)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__RegisterInitTask__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__RegisterInitTask__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = &static; \
+        if (HypABI__Creds__RegisterInitTask__arg__slab) \
+                __arg = HypABI__Creds__RegisterInitTask__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__RegisterInitTask__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall(const HypABI__Creds__RegisterInitTask__arg__T s)
+{
+        int rc;
+        HypABI__Creds__RegisterInitTask__arg__T *bhv_arg = HypABI__Creds__RegisterInitTask__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__RegisterInitTask__arg__T));
+        rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__RegisterInitTask__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__RegisterInitTask__HYPERCALL(...) HypABI__Creds__RegisterInitTask__hypercall((HypABI__Creds__RegisterInitTask__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Assign__arg__slab;
+#define HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Assign__arg__T *)kmem_cache_alloc(HypABI__Creds__Assign__arg__slab, gfp)
+#define HypABI__Creds__Assign__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Assign__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__ALLOC() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Assign__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Assign__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = &static; \
+        if (HypABI__Creds__Assign__arg__slab) \
+                __arg = HypABI__Creds__Assign__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Assign__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall(uint8_t *ret_out, const HypABI__Creds__Assign__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Assign__arg__T *bhv_arg = HypABI__Creds__Assign__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Assign__arg__T));
+        rc = HypABI__Creds__Assign__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Assign__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Assign__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Assign__HYPERCALL(RET_OUT, ...) HypABI__Creds__Assign__hypercall(RET_OUT, (HypABI__Creds__Assign__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab;
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__AssignPriv__arg__T *)kmem_cache_alloc(HypABI__Creds__AssignPriv__arg__slab, gfp)
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__AssignPriv__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__ALLOC() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__AssignPriv__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__AssignPriv__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = &static; \
+        if (HypABI__Creds__AssignPriv__arg__slab) \
+                __arg = HypABI__Creds__AssignPriv__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__AssignPriv__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall(uint8_t *ret_out, const HypABI__Creds__AssignPriv__arg__T s)
+{
+        int rc;
+        HypABI__Creds__AssignPriv__arg__T *bhv_arg = HypABI__Creds__AssignPriv__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__AssignPriv__arg__T));
+        rc = HypABI__Creds__AssignPriv__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__AssignPriv__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__AssignPriv__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__AssignPriv__HYPERCALL(RET_OUT, ...) HypABI__Creds__AssignPriv__hypercall(RET_OUT, (HypABI__Creds__AssignPriv__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Commit__arg__slab;
+#define HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Commit__arg__T *)kmem_cache_alloc(HypABI__Creds__Commit__arg__slab, gfp)
+#define HypABI__Creds__Commit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Commit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__ALLOC() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Commit__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Commit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = &static; \
+        if (HypABI__Creds__Commit__arg__slab) \
+                __arg = HypABI__Creds__Commit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Commit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall(uint8_t *ret_out, const HypABI__Creds__Commit__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Commit__arg__T *bhv_arg = HypABI__Creds__Commit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Commit__arg__T));
+        rc = HypABI__Creds__Commit__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Commit__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Commit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Commit__HYPERCALL(RET_OUT, ...) HypABI__Creds__Commit__hypercall(RET_OUT, (HypABI__Creds__Commit__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Release__arg__slab;
+#define HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Release__arg__T *)kmem_cache_alloc(HypABI__Creds__Release__arg__slab, gfp)
+#define HypABI__Creds__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Release__arg__T *__arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__ALLOC() \
+        HypABI__Creds__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Release__arg__T *__arg = &static; \
+        if (HypABI__Creds__Release__arg__slab) \
+                __arg = HypABI__Creds__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall(const HypABI__Creds__Release__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Release__arg__T *bhv_arg = HypABI__Creds__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Release__arg__T));
+        rc = HypABI__Creds__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Release__HYPERCALL(...) HypABI__Creds__Release__hypercall((HypABI__Creds__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Verification__arg__slab;
+#define HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Verification__arg__T *)kmem_cache_alloc(HypABI__Creds__Verification__arg__slab, gfp)
+#define HypABI__Creds__Verification__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Verification__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__ALLOC() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Verification__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Verification__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = &static; \
+        if (HypABI__Creds__Verification__arg__slab) \
+                __arg = HypABI__Creds__Verification__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Verification__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall(uint8_t *ret_out, const HypABI__Creds__Verification__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Verification__arg__T *bhv_arg = HypABI__Creds__Verification__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Verification__arg__T));
+        rc = HypABI__Creds__Verification__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Verification__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Verification__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Verification__HYPERCALL(RET_OUT, ...) HypABI__Creds__Verification__hypercall(RET_OUT, (HypABI__Creds__Verification__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Log__arg__slab;
+#define HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Log__arg__T *)kmem_cache_alloc(HypABI__Creds__Log__arg__slab, gfp)
+#define HypABI__Creds__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Log__arg__T *__arg = HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__ALLOC() \
+        HypABI__Creds__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Log__arg__T *__arg = &static; \
+        if (HypABI__Creds__Log__arg__slab) \
+                __arg = HypABI__Creds__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall(uint8_t *block_out, const HypABI__Creds__Log__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Log__arg__T *bhv_arg = HypABI__Creds__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Log__arg__T));
+        rc = HypABI__Creds__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Creds__Log__arg__T*)bhv_arg)->block;
+        HypABI__Creds__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Creds__Log__hypercall(BLOCK_OUT, (HypABI__Creds__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__Init__arg__slab;
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__Init__arg__T *)kmem_cache_alloc(HypABI__FileProtection__Init__arg__slab, gfp)
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__ALLOC() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__Init__arg__slab) \
+                __arg = HypABI__FileProtection__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall(uint64_t *feature_bitmap_out, const HypABI__FileProtection__Init__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__Init__arg__T *bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__Init__arg__T));
+        rc = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+        *feature_bitmap_out = ((volatile HypABI__FileProtection__Init__arg__T*)bhv_arg)->feature_bitmap;
+        HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__Init__HYPERCALL(FEATURE_BITMAP_OUT, ...) HypABI__FileProtection__Init__hypercall(FEATURE_BITMAP_OUT, (HypABI__FileProtection__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *bhv_arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T));
+        rc = HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab;
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationFileOps__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationFileOps__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationFileOps__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationFileOps__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationFileOps__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationFileOps__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationFileOps__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationFileOps__arg__T *bhv_arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationFileOps__arg__T));
+        rc = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationFileOps__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationFileOps__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationFileOps__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationFileOps__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationFileOps__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationDirtyCredWrite__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationDirtyCredWrite__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *bhv_arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T));
+        rc = HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationDirtyCredWrite__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab;
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__RegisterProtection__Freeze__arg__T *)kmem_cache_alloc(HypABI__RegisterProtection__Freeze__arg__slab, gfp)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__RegisterProtection__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__RegisterProtection__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = &static; \
+        if (HypABI__RegisterProtection__Freeze__arg__slab) \
+                __arg = HypABI__RegisterProtection__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__RegisterProtection__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall(const HypABI__RegisterProtection__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__RegisterProtection__Freeze__arg__T *bhv_arg = HypABI__RegisterProtection__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__RegisterProtection__Freeze__arg__T));
+        rc = HypABI__RegisterProtection__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__RegisterProtection__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__RegisterProtection__Freeze__HYPERCALL(...) HypABI__RegisterProtection__Freeze__hypercall((HypABI__RegisterProtection__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Register__arg__slab;
+#define HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Register__arg__T *)kmem_cache_alloc(HypABI__Inode__Register__arg__slab, gfp)
+#define HypABI__Inode__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Register__arg__T *__arg = HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__ALLOC() \
+        HypABI__Inode__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Register__arg__T *__arg = &static; \
+        if (HypABI__Inode__Register__arg__slab) \
+                __arg = HypABI__Inode__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall(const HypABI__Inode__Register__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Register__arg__T *bhv_arg = HypABI__Inode__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Register__arg__T));
+        rc = HypABI__Inode__Register__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Register__HYPERCALL(...) HypABI__Inode__Register__hypercall((HypABI__Inode__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Update__arg__slab;
+#define HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Update__arg__T *)kmem_cache_alloc(HypABI__Inode__Update__arg__slab, gfp)
+#define HypABI__Inode__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Update__arg__T *__arg = HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__ALLOC() \
+        HypABI__Inode__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Update__arg__T *__arg = &static; \
+        if (HypABI__Inode__Update__arg__slab) \
+                __arg = HypABI__Inode__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall(const HypABI__Inode__Update__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Update__arg__T *bhv_arg = HypABI__Inode__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Update__arg__T));
+        rc = HypABI__Inode__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Update__HYPERCALL(...) HypABI__Inode__Update__hypercall((HypABI__Inode__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Release__arg__slab;
+#define HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Release__arg__T *)kmem_cache_alloc(HypABI__Inode__Release__arg__slab, gfp)
+#define HypABI__Inode__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Release__arg__T *__arg = HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__ALLOC() \
+        HypABI__Inode__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Release__arg__T *__arg = &static; \
+        if (HypABI__Inode__Release__arg__slab) \
+                __arg = HypABI__Inode__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall(const HypABI__Inode__Release__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Release__arg__T *bhv_arg = HypABI__Inode__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Release__arg__T));
+        rc = HypABI__Inode__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Release__HYPERCALL(...) HypABI__Inode__Release__hypercall((HypABI__Inode__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Verify__arg__slab;
+#define HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Verify__arg__T *)kmem_cache_alloc(HypABI__Inode__Verify__arg__slab, gfp)
+#define HypABI__Inode__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__ALLOC() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = &static; \
+        if (HypABI__Inode__Verify__arg__slab) \
+                __arg = HypABI__Inode__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall(uint8_t *ret_out, const HypABI__Inode__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Verify__arg__T *bhv_arg = HypABI__Inode__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Verify__arg__T));
+        rc = HypABI__Inode__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Inode__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Inode__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Verify__HYPERCALL(RET_OUT, ...) HypABI__Inode__Verify__hypercall(RET_OUT, (HypABI__Inode__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Log__arg__slab;
+#define HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Log__arg__T *)kmem_cache_alloc(HypABI__Inode__Log__arg__slab, gfp)
+#define HypABI__Inode__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Log__arg__T *__arg = HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__ALLOC() \
+        HypABI__Inode__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Log__arg__T *__arg = &static; \
+        if (HypABI__Inode__Log__arg__slab) \
+                __arg = HypABI__Inode__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall(uint8_t *block_out, const HypABI__Inode__Log__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Log__arg__T *bhv_arg = HypABI__Inode__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Log__arg__T));
+        rc = HypABI__Inode__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Inode__Log__arg__T*)bhv_arg)->block;
+        HypABI__Inode__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Inode__Log__hypercall(BLOCK_OUT, (HypABI__Inode__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Register__arg__slab;
+#define HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Register__arg__T *)kmem_cache_alloc(HypABI__Keyring__Register__arg__slab, gfp)
+#define HypABI__Keyring__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__ALLOC() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Register__arg__slab) \
+                __arg = HypABI__Keyring__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall(uint8_t *block_out, const HypABI__Keyring__Register__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Register__arg__T *bhv_arg = HypABI__Keyring__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Register__arg__T));
+        rc = HypABI__Keyring__Register__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Register__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Register__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Register__hypercall(BLOCK_OUT, (HypABI__Keyring__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Verify__arg__slab;
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Verify__arg__T *)kmem_cache_alloc(HypABI__Keyring__Verify__arg__slab, gfp)
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__ALLOC() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Verify__arg__slab) \
+                __arg = HypABI__Keyring__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall(uint8_t *ret_out, const HypABI__Keyring__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Verify__arg__T *bhv_arg = HypABI__Keyring__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Verify__arg__T));
+        rc = HypABI__Keyring__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Keyring__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Keyring__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Verify__HYPERCALL(RET_OUT, ...) HypABI__Keyring__Verify__hypercall(RET_OUT, (HypABI__Keyring__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Log__arg__slab;
+#define HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Log__arg__T *)kmem_cache_alloc(HypABI__Keyring__Log__arg__slab, gfp)
+#define HypABI__Keyring__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__ALLOC() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Log__arg__slab) \
+                __arg = HypABI__Keyring__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall(uint8_t *block_out, const HypABI__Keyring__Log__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Log__arg__T *bhv_arg = HypABI__Keyring__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Log__arg__T));
+        rc = HypABI__Keyring__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Log__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Log__hypercall(BLOCK_OUT, (HypABI__Keyring__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *)kmem_cache_alloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, gfp)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__FreezeMemoryAfterBoot__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = &static; \
+        if (HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab) \
+                __arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(uint8_t *freeze_memory_after_boot_out, const HypABI__Confserver__FreezeMemoryAfterBoot__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *bhv_arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T));
+        rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(bhv_arg);
+        *freeze_memory_after_boot_out = ((volatile HypABI__Confserver__FreezeMemoryAfterBoot__arg__T*)bhv_arg)->freeze_memory_after_boot;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__HYPERCALL(FREEZE_MEMORY_AFTER_BOOT_OUT, ...) HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(FREEZE_MEMORY_AFTER_BOOT_OUT, (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab;
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__StrictFileops__arg__T *)kmem_cache_alloc(HypABI__Confserver__StrictFileops__arg__slab, gfp)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__StrictFileops__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__StrictFileops__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = &static; \
+        if (HypABI__Confserver__StrictFileops__arg__slab) \
+                __arg = HypABI__Confserver__StrictFileops__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__StrictFileops__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall(uint8_t *strict_fileops_out, const HypABI__Confserver__StrictFileops__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__StrictFileops__arg__T *bhv_arg = HypABI__Confserver__StrictFileops__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__StrictFileops__arg__T));
+        rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+        *strict_fileops_out = ((volatile HypABI__Confserver__StrictFileops__arg__T*)bhv_arg)->strict_fileops;
+        HypABI__Confserver__StrictFileops__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__StrictFileops__HYPERCALL(STRICT_FILEOPS_OUT, ...) HypABI__Confserver__StrictFileops__hypercall(STRICT_FILEOPS_OUT, (HypABI__Confserver__StrictFileops__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__GuestPolicy__GetPolicy__arg__slab;
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__GuestPolicy__GetPolicy__arg__T *)kmem_cache_alloc(HypABI__GuestPolicy__GetPolicy__arg__slab, gfp)
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__GuestPolicy__GetPolicy__arg__T *__arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__GuestPolicy__GetPolicy__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_NOCHECK() \
+        HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC() \
+        HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__GuestPolicy__GetPolicy__arg__FREE(name) \
+        kmem_cache_free(HypABI__GuestPolicy__GetPolicy__arg__slab, name); \
+        name = NULL
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__GuestPolicy__GetPolicy__arg__T *__arg = &static; \
+        if (HypABI__GuestPolicy__GetPolicy__arg__slab) \
+                __arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__GuestPolicy__GetPolicy__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__GuestPolicy__GetPolicy__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__GuestPolicy__GetPolicy__hypercall(uint32_t *dest_sz_out, uint8_t *valid_out, const HypABI__GuestPolicy__GetPolicy__arg__T s)
+{
+        int rc;
+        HypABI__GuestPolicy__GetPolicy__arg__T *bhv_arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__GuestPolicy__GetPolicy__arg__T));
+        rc = HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(bhv_arg);
+        *dest_sz_out = ((volatile HypABI__GuestPolicy__GetPolicy__arg__T*)bhv_arg)->dest_sz;
+        *valid_out = ((volatile HypABI__GuestPolicy__GetPolicy__arg__T*)bhv_arg)->valid;
+        HypABI__GuestPolicy__GetPolicy__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__GuestPolicy__GetPolicy__HYPERCALL(DEST_SZ_OUT, VALID_OUT, ...) HypABI__GuestPolicy__GetPolicy__hypercall(DEST_SZ_OUT, VALID_OUT, (HypABI__GuestPolicy__GetPolicy__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__GuestPolicy__Init__arg__slab;
+#define HypABI__GuestPolicy__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__GuestPolicy__Init__arg__T *)kmem_cache_alloc(HypABI__GuestPolicy__Init__arg__slab, gfp)
+#define HypABI__GuestPolicy__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__GuestPolicy__Init__arg__T *__arg = HypABI__GuestPolicy__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__GuestPolicy__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__GuestPolicy__Init__arg__ALLOC_NOCHECK() \
+        HypABI__GuestPolicy__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__GuestPolicy__Init__arg__ALLOC() \
+        HypABI__GuestPolicy__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__GuestPolicy__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__GuestPolicy__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__GuestPolicy__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__GuestPolicy__Init__arg__T *__arg = &static; \
+        if (HypABI__GuestPolicy__Init__arg__slab) \
+                __arg = HypABI__GuestPolicy__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__GuestPolicy__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__GuestPolicy__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__GuestPolicy__Init__hypercall(const HypABI__GuestPolicy__Init__arg__T s)
+{
+        int rc;
+        HypABI__GuestPolicy__Init__arg__T *bhv_arg = HypABI__GuestPolicy__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__GuestPolicy__Init__arg__T));
+        rc = HypABI__GuestPolicy__Init__hypercall_noalloc(bhv_arg);
+        HypABI__GuestPolicy__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__GuestPolicy__Init__HYPERCALL(...) HypABI__GuestPolicy__Init__hypercall((HypABI__GuestPolicy__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Create__arg__slab;
+#define HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Create__arg__T *)kmem_cache_alloc(HypABI__Wagner__Create__arg__slab, gfp)
+#define HypABI__Wagner__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__ALLOC() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Create__arg__slab) \
+                __arg = HypABI__Wagner__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall(const HypABI__Wagner__Create__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Create__arg__T *bhv_arg = HypABI__Wagner__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Create__arg__T));
+        rc = HypABI__Wagner__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Create__HYPERCALL(...) HypABI__Wagner__Create__hypercall((HypABI__Wagner__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Extend__arg__slab;
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Extend__arg__T *)kmem_cache_alloc(HypABI__Wagner__Extend__arg__slab, gfp)
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Extend__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__ALLOC() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Extend__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Extend__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Extend__arg__slab) \
+                __arg = HypABI__Wagner__Extend__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Extend__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall(const HypABI__Wagner__Extend__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Extend__arg__T *bhv_arg = HypABI__Wagner__Extend__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Extend__arg__T));
+        rc = HypABI__Wagner__Extend__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Extend__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Extend__HYPERCALL(...) HypABI__Wagner__Extend__hypercall((HypABI__Wagner__Extend__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Delete__arg__slab;
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Delete__arg__T *)kmem_cache_alloc(HypABI__Wagner__Delete__arg__slab, gfp)
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Delete__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__ALLOC() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Delete__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Delete__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Delete__arg__slab) \
+                __arg = HypABI__Wagner__Delete__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Delete__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall(const HypABI__Wagner__Delete__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Delete__arg__T *bhv_arg = HypABI__Wagner__Delete__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Delete__arg__T));
+        rc = HypABI__Wagner__Delete__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Delete__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Delete__HYPERCALL(...) HypABI__Wagner__Delete__hypercall((HypABI__Wagner__Delete__arg__T){__VA_ARGS__})
+
diff --git include/bhv/interface/abi_ml_autogen.h include/bhv/interface/abi_ml_autogen.h
new file mode 100644
index 0000000000..9bbe0bd6f1
--- /dev/null
+++ include/bhv/interface/abi_ml_autogen.h
@@ -0,0 +1,332 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/bitops.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+static __always_inline __must_check int HypABI__Init__Init__hypercall_noalloc(HypABI__Init__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Init__OP_ID, arg, HypABI__Init__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Init__Start__hypercall_noalloc(HypABI__Init__Start__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Start__OP_ID, arg, HypABI__Init__Start__arg__SZ + extra_size);
+}
+
+static inline bool HypABI__Integrity__MemFlags__has_TRANSIENT(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__TRANSIENT__BIT, addr);
+}
+static inline bool HypABI__Integrity__MemFlags__has_MUTABLE(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__MUTABLE__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall_noalloc(HypABI__Integrity__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Create__OP_ID, arg, HypABI__Integrity__Create__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall_noalloc(HypABI__Integrity__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Update__OP_ID, arg, HypABI__Integrity__Update__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall_noalloc(HypABI__Integrity__Remove__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Remove__OP_ID, arg, HypABI__Integrity__Remove__arg__SZ);
+}
+
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall_noalloc(HypABI__Integrity__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Freeze__OP_ID, arg, HypABI__Integrity__Freeze__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall_noalloc(HypABI__Integrity__PtpgInit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgInit__OP_ID, arg, HypABI__Integrity__PtpgInit__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgReport__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgReport__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall_noalloc(HypABI__Patch__Patch__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__Patch__OP_ID, arg, HypABI__Patch__Patch__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall_noalloc(HypABI__Patch__PatchNoClose__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchNoClose__OP_ID, arg, HypABI__Patch__PatchNoClose__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall_noalloc(HypABI__Patch__PatchViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchViolation__OP_ID, arg, HypABI__Patch__PatchViolation__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Richard__Open__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Open__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Richard__Close__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Close__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessInit__hypercall_noalloc(HypABI__Acl__ProcessInit__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessInit__OP_ID, arg, HypABI__Acl__ProcessInit__arg__SZ + extra_size);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverInit__hypercall_noalloc(HypABI__Acl__DriverInit__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverInit__OP_ID, arg, HypABI__Acl__DriverInit__arg__SZ + extra_size);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessViolation__hypercall_noalloc(HypABI__Acl__ProcessViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessViolation__OP_ID, arg, HypABI__Acl__ProcessViolation__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverViolation__hypercall_noalloc(HypABI__Acl__DriverViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverViolation__OP_ID, arg, HypABI__Acl__DriverViolation__arg__SZ);
+}
+
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_SOCKET_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_FILE_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_MMAP_EXEC_FILE_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__MMAP_EXEC_FILE_EVENTS__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall_noalloc(HypABI__Guestlog__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Guestlog__BACKEND_ID, HypABI__Guestlog__Init__OP_ID, arg, HypABI__Guestlog__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Configure__hypercall_noalloc(HypABI__Creds__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Configure__OP_ID, arg, HypABI__Creds__Configure__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall_noalloc(HypABI__Creds__RegisterInitTask__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__RegisterInitTask__OP_ID, arg, HypABI__Creds__RegisterInitTask__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall_noalloc(HypABI__Creds__Assign__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Assign__OP_ID, arg, HypABI__Creds__Assign__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall_noalloc(HypABI__Creds__AssignPriv__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__AssignPriv__OP_ID, arg, HypABI__Creds__AssignPriv__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall_noalloc(HypABI__Creds__Commit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Commit__OP_ID, arg, HypABI__Creds__Commit__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall_noalloc(HypABI__Creds__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Release__OP_ID, arg, HypABI__Creds__Release__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall_noalloc(HypABI__Creds__Verification__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Verification__OP_ID, arg, HypABI__Creds__Verification__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall_noalloc(HypABI__Creds__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Log__OP_ID, arg, HypABI__Creds__Log__arg__SZ);
+}
+
+static inline bool HypABI__FileProtection__Init__Config__has_READ_ONLY(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__READ_ONLY__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_FILE_OPS(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__FILE_OPS__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_DIRTY_CRED(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT, addr);
+}
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall_noalloc(HypABI__FileProtection__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__Init__OP_ID, arg, HypABI__FileProtection__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID, arg, HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(HypABI__FileProtection__ViolationFileOps__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationFileOps__OP_ID, arg, HypABI__FileProtection__ViolationFileOps__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID, arg, HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall_noalloc(HypABI__RegisterProtection__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__RegisterProtection__BACKEND_ID, HypABI__RegisterProtection__Freeze__OP_ID, arg, HypABI__RegisterProtection__Freeze__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__Configure__hypercall_noalloc(HypABI__Domain__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Configure__OP_ID, arg, HypABI__Domain__Configure__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__Report__hypercall_noalloc(HypABI__Domain__Report__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Report__OP_ID, arg, HypABI__Domain__Report__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__ReportForcedMemAccess__hypercall_noalloc(HypABI__Domain__ReportForcedMemAccess__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__ReportForcedMemAccess__OP_ID, arg, HypABI__Domain__ReportForcedMemAccess__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall_noalloc(HypABI__Inode__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Register__OP_ID, arg, HypABI__Inode__Register__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall_noalloc(HypABI__Inode__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Update__OP_ID, arg, HypABI__Inode__Update__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall_noalloc(HypABI__Inode__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Release__OP_ID, arg, HypABI__Inode__Release__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall_noalloc(HypABI__Inode__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Verify__OP_ID, arg, HypABI__Inode__Verify__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall_noalloc(HypABI__Inode__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Log__OP_ID, arg, HypABI__Inode__Log__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall_noalloc(HypABI__Keyring__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Register__OP_ID, arg, HypABI__Keyring__Register__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall_noalloc(HypABI__Keyring__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Verify__OP_ID, arg, HypABI__Keyring__Verify__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall_noalloc(HypABI__Keyring__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Log__OP_ID, arg, HypABI__Keyring__Log__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID, arg, HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall_noalloc(HypABI__Confserver__StrictFileops__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__StrictFileops__OP_ID, arg, HypABI__Confserver__StrictFileops__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(HypABI__GuestPolicy__GetPolicy__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__GuestPolicy__BACKEND_ID, HypABI__GuestPolicy__GetPolicy__OP_ID, arg, HypABI__GuestPolicy__GetPolicy__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__GuestPolicy__Init__hypercall_noalloc(HypABI__GuestPolicy__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__GuestPolicy__BACKEND_ID, HypABI__GuestPolicy__Init__OP_ID, arg, HypABI__GuestPolicy__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall_noalloc(HypABI__Wagner__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Create__OP_ID, arg, HypABI__Wagner__Create__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall_noalloc(HypABI__Wagner__Extend__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Extend__OP_ID, arg, HypABI__Wagner__Extend__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall_noalloc(HypABI__Wagner__Delete__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Delete__OP_ID, arg, HypABI__Wagner__Delete__arg__SZ);
+}
+
diff --git include/bhv/interface/abi_version_autogen.h include/bhv/interface/abi_version_autogen.h
new file mode 100644
index 0000000000..685071ab7f
--- /dev/null
+++ include/bhv/interface/abi_version_autogen.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#define HypABI__ABI_VERSION __BHV_VAS_ABI_VERSION(25, 17, 0, 1093)
+
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 0000000000..67c6ebbd80
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,51 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define __BHV_VAS_ABI_VERSION(rel_year, rel_week, rel_extra, internal_info)    \
+	({                                                                     \
+		static_assert((unsigned long)(rel_year) <= 99);                \
+		static_assert((unsigned long)(rel_year) >= 23);                \
+		static_assert((unsigned long)(rel_week) < 53);                 \
+		static_assert((unsigned long)(rel_extra) < 0xff);              \
+		static_assert((unsigned long)(internal_info) > 0x00000);       \
+		static_assert((unsigned long)(internal_info) < 0xfffff);       \
+		(unsigned long)0xbedUL << (13 * 4) |                           \
+			(unsigned long)(rel_year) << (11 * 4) |                \
+			(unsigned long)(rel_week) << (9 * 4) |                 \
+			(unsigned long)(rel_extra) << (7 * 4) |                \
+			(unsigned long)(0x00UL) << (5 * 4) |                   \
+			(unsigned long)(internal_info) << (0 * 4);             \
+	})
+
+#include <bhv/version.h>
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_REGISTER_PROTECTION 9
+#define BHV_VAS_BACKEND_DOMAIN 10
+#define BHV_VAS_BACKEND_INODE 11
+#define BHV_VAS_BACKEND_KEYRING 12
+#define BHV_VAS_BACKEND_CONFSERVER 13
+#define BHV_VAS_BACKEND_CONTAINER_INTEGRITY 14
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 0000000000..862c0a1de4
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static __always_inline int bhv_hypercall_vas(uint32_t backend, uint32_t op,
+					     void *arg, size_t arg_len)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	BUG_ON(!!arg != !!arg_len);
+
+	if (arg != NULL)
+		phys_addr = bhv_virt_to_phys(arg, arg_len);
+
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_start(TARGET_BHV_VAS, backend, op);
+#endif
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			   phys_addr);
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_end(TARGET_BHV_VAS, backend, op);
+#endif
+
+	if (rv) {
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION, arg,
+		      phys_addr);
+#else
+		pr_warn("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+			rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			arg, phys_addr);
+		dump_stack();
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 0000000000..66baa03cfd
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,205 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_PATCH_H
+#define _ASM_INTERFACE_BHV_PATCH_H
+
+#include <linux/kernel.h>
+
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+static __always_inline int __bhv_patch_hypercall_single(void *dest_virt_addr,
+							const uint8_t *src,
+							uint64_t size)
+{
+	int rc;
+	HypABI__Patch__Patch__arg__T bhv_arg;
+
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);
+
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys(dest_virt_addr, size);
+	memcpy(bhv_arg.src_value, src, size);
+	bhv_arg.size = size;
+
+	rc = HypABI__Patch__Patch__hypercall_noalloc(&bhv_arg);
+
+	return rc;
+}
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define PATCH_BODY(TYP)                                                       \
+	int rc;                                                               \
+	static HypABI__Patch__##TYP##__arg__T early_arg;                      \
+	HypABI__Patch__##TYP##__arg__T *bhv_arg =                             \
+		HypABI__Patch__##TYP##__arg__ALLOC_STATICFALLBACK(early_arg); \
+                                                                              \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                           \
+                                                                              \
+	bhv_arg->dest_phys_addr = bhv_virt_to_phys(dest_virt_addr, size);     \
+	memcpy(bhv_arg->src_value, src, size);                                \
+	bhv_arg->size = size;                                                 \
+                                                                              \
+	rc = HypABI__Patch__##TYP##__hypercall_noalloc(bhv_arg);              \
+                                                                              \
+	HypABI__Patch__##TYP##__arg__FREE_STATICFALLBACK(bhv_arg, early_arg); \
+                                                                              \
+	return rc;
+
+static __always_inline int
+__bhv_patchnoclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				    uint64_t size)
+{
+	PATCH_BODY(PatchNoClose);
+}
+
+static __always_inline int
+__bhv_patchclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+                                 uint64_t size)
+{
+       PATCH_BODY(Patch);
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#undef PATCH_BODY
+
+static __always_inline uint64_t __bhv_patch_round(void **const dest_virt_addr,
+						  const uint8_t *const src,
+						  uint64_t *const size,
+#ifndef CONFIG_BHV_VAULT_SPACES
+						  const bool close_vault,
+#endif
+						  unsigned long *const rc)
+{
+	unsigned long r;
+	uint64_t bytes_until_page_boundary =
+		PAGE_SIZE - ((uint64_t)*dest_virt_addr % PAGE_SIZE);
+	uint64_t this_patch_size = min3(*size, bytes_until_page_boundary,
+					(uint64_t)HypABI__Patch__MAX_PATCH_SZ);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	r = __bhv_patch_hypercall_single(*dest_virt_addr, src, this_patch_size);
+#else
+	if (close_vault && *size == this_patch_size) {
+		r = __bhv_patchclose_hypercall_single(*dest_virt_addr, src,
+						      this_patch_size);
+	} else {
+		r = __bhv_patchnoclose_hypercall_single(*dest_virt_addr, src,
+							this_patch_size);
+	}
+#endif
+	if (r)
+		*rc = r;
+
+	*dest_virt_addr += this_patch_size;
+	*size -= this_patch_size;
+	return this_patch_size;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ */
+static __always_inline int bhv_patch_hypercall(void *dest_virt_addr,
+					       const uint8_t *src,
+					       uint64_t size
+#ifndef CONFIG_BHV_VAULT_SPACES
+					       , const bool close_vault
+#endif
+					       )
+{
+	unsigned long rc = 0;
+
+	BUG_ON(!src);
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, &rc);
+#else
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ * Sets memory using a one-byte pattern rather than just copying (like memset)
+ */
+static __always_inline int bhv_patch_hypercall_memset(void *dest_virt_addr,
+						      uint64_t size,
+                             uint8_t pattern
+#ifndef CONFIG_BHV_VAULT_SPACES
+                             , bool close_vault
+#endif
+					       	      )
+{
+	unsigned long rc = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(__arch64__)
+	size_t i = 0;
+
+	for (i = 0; i < HypABI__Patch__MAX_PATCH_SZ; i++)
+		buf[i] = pattern;
+#else
+	memset(buf, pattern, HypABI__Patch__MAX_PATCH_SZ);
+#endif
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		__bhv_patch_round(&dest_virt_addr, buf, &size, &rc);
+#else
+		__bhv_patch_round(&dest_virt_addr, buf, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+* Handle patch violation hypercalls
+*
+* This function sends a patch violation hypercall and determines whether the
+* patch should be blocked.
+*
+* \returns True if the patch should be blocked, false otherwise.
+*/
+static __always_inline bool bhv_patch_violation_hypercall(void *dest_virt_addr,
+							  const char *message)
+{
+	unsigned long r;
+	bool rc;
+
+	HypABI__Patch__PatchViolation__arg__T bhv_arg;
+
+	// Setup arguments. We block by default
+	bhv_arg.dest_virt_addr = (uint64_t)dest_virt_addr;
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys_single(dest_virt_addr);
+	bhv_arg.block = true;
+
+	if (message != NULL)
+		strncpy(bhv_arg.message, message,
+			HypABI__Patch__PatchViolation__MAX_MSG_SZ);
+	bhv_arg.message[HypABI__Patch__PatchViolation__MAX_MSG_SZ - 1] = '\0';
+
+	r = HypABI__Patch__PatchViolation__hypercall_noalloc(&bhv_arg);
+	rc = r || bhv_arg.block;
+
+	// Block in case of error or if block is set
+	return rc;
+}
+
+#endif /* _ASM_INTERFACE_BHV_PATCH_H */
diff --git include/bhv/json_gen.h include/bhv/json_gen.h
new file mode 100644
index 0000000000..2169dbbd05
--- /dev/null
+++ include/bhv/json_gen.h
@@ -0,0 +1,311 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso@bluerock.io>
+ *          Jonas Pfoh <jonas@bluerock.io>
+ *          Sergej Proskurin <sergej@bluerock.io>
+ */
+#ifndef __BHV_JSON_GEN_H__
+#define __BHV_JSON_GEN_H__
+
+#define BRS_EVENT_TYPE_SYNC 0U
+#define BRS_EVENT_TYPE_ASYNC 1U
+
+struct json_event_header {
+	uint32_t magic;
+	uint32_t type;
+	uint64_t id;
+	int32_t size;
+} __attribute__((packed));
+
+struct json_writer {
+	union {
+		char *buf;
+		struct json_writer_payload {
+			struct json_event_header header;
+			char json[];
+		} __attribute__((packed)) * data;
+	};
+	size_t buf_sz;
+	int32_t offset;
+};
+typedef struct json_writer json_writer_t;
+
+typedef void *json_writer_doc_t;
+
+#define BRS_JSON_CHECK_WR_SZ(writer, write_sz)                                 \
+	{                                                                      \
+		if ((writer->offset + write_sz) > writer->buf_sz)              \
+			return -EOVERFLOW;                                     \
+	}
+
+static inline int brs_json_start_doc(json_writer_t *writer, const char *key,
+				     size_t key_len, json_writer_doc_t *out);
+
+static inline int brs_json_write_start(json_writer_t *writer, char *buf,
+				       size_t buf_sz)
+{
+	static const int32_t MAGIC_BEGIN_MSG = 0xCAFED00D;
+
+	BUG_ON(buf_sz > INT_MAX); // for consistency with BSON
+
+	// check whether the buf can handle at least the header + "{}"
+	if (buf_sz < (sizeof(struct json_event_header) + 2))
+		return -EINVAL;
+
+	writer->buf = buf;
+	writer->buf_sz = buf_sz;
+
+	// make room for the header
+	writer->offset = sizeof(struct json_event_header);
+	writer->data->header.magic = cpu_to_le32(MAGIC_BEGIN_MSG);
+	writer->data->header.id = cpu_to_le32(0);
+
+	writer->buf[writer->offset++] = '{';
+
+	return 0;
+}
+
+static inline size_t brs_escape_json_string(char *const out_start,
+					    size_t out_buf_sz,
+					    const char *const src_start,
+					    const size_t src_buf_sz)
+{
+	// This function will just skip any characters that don't fit in
+	// the output buffer/budget.
+
+	// This is my attempt at a performant JSON escaping function.
+	//   -- Premature optimization is the root of all evil. --
+
+	const char *src = src_start;
+	char *out = out_start;
+	const char *out_end;
+	const char *src_end;
+
+	BUG_ON(out_buf_sz < 2 /* "" */);
+
+	// We have a maximum budget of 10% for extra escaping characters
+	out_buf_sz = min(out_buf_sz,
+			 2 /* "" */ + src_buf_sz + ((src_buf_sz - 1) / 10 + 1));
+	out_end = out + out_buf_sz - 1 /* " */;
+
+	// For now we optimistically assume that no chars need escaping
+	src_end = src + min(src_buf_sz, out_buf_sz - 2 /* "" */);
+
+	*(out++) = '"';
+
+	for (; src < src_end; ++src) {
+		switch (*src) {
+		case '\0':
+			goto done;
+
+		// Control characters other than \0 and \n
+		case '\1' ... '\n' - 1:
+		case '\n' + 1 ... '\x1F':
+			if (out + 6 /* \\uXXXX */ > out_end)
+				goto done;
+			BUG_ON(6 !=
+			       scnprintf(out, out_end - out, "\\u%04x", *src));
+			out += 6;
+			src_end = src + min(src_end - src, out_end - out);
+			break;
+
+		case '\n':
+			if (out + 2 /* \\n */ > out_end)
+				goto done;
+			*(out++) = '\\';
+			*(out++) = 'n';
+			src_end = src + min(src_end - src, out_end - out);
+			break;
+
+		case '"':
+			if (out + 2 /* \\n */ > out_end)
+				goto done;
+			*(out++) = '\\';
+			*(out++) = '"';
+			src_end = src + min(src_end - src, out_end - out);
+			break;
+
+		case '\\':
+			if (out + 2 /* \\n */ > out_end)
+				goto done;
+			*(out++) = '\\';
+			*(out++) = '\\';
+			src_end = src + min(src_end - src, out_end - out);
+			break;
+
+		default:
+			*(out++) = *src;
+			break;
+		}
+	}
+
+done:
+	*(out++) = '"';
+	return out - out_start;
+}
+
+static inline bool brs_needs_pre_comma(json_writer_t *writer)
+{
+	if (unlikely(writer->offset == 0)) {
+		BUG();
+	} else if (writer->buf[writer->offset - 1] == '{') {
+		return false;
+	} else {
+		return true;
+	}
+}
+
+static inline int brs_json_write_label(json_writer_t *writer, const char *key,
+				       size_t key_len)
+{
+	bool needs_pre_comma;
+	BUG_ON(key[key_len] != '\0');
+
+	needs_pre_comma = brs_needs_pre_comma(writer);
+
+	BRS_JSON_CHECK_WR_SZ(writer, 3 /* "": */ + needs_pre_comma + key_len);
+	if (needs_pre_comma)
+		writer->buf[writer->offset++] = ',';
+	writer->buf[writer->offset++] = '"';
+	strncpy(writer->buf + writer->offset, key, key_len);
+	writer->offset += key_len;
+	writer->buf[writer->offset++] = '"';
+	writer->buf[writer->offset++] = ':';
+
+	return 0;
+}
+
+static inline int brs_json_write_string(json_writer_t *writer, const char *key,
+					size_t key_len, const char *val,
+					size_t val_buf_size)
+{
+	int rc;
+
+	if ((rc = brs_json_write_label(writer, key, key_len)))
+		return rc;
+
+	writer->offset +=
+		brs_escape_json_string(writer->buf + writer->offset,
+				       writer->buf_sz - writer->offset, val,
+				       val_buf_size);
+
+	return 0;
+}
+
+static inline int brs_json_write_bool(json_writer_t *writer, const char *key,
+				      size_t key_len, bool val)
+{
+#define TRUE_STR "true"
+#define TRUE_LEN (sizeof(TRUE_STR) - 1 /* \0 */)
+#define FALSE_STR "false"
+#define FALSE_LEN (sizeof(FALSE_STR) - 1 /* \0 */)
+	_Static_assert(TRUE_LEN == 4);
+	_Static_assert(FALSE_LEN == 5);
+
+	int rc;
+
+	if ((rc = brs_json_write_label(writer, key, key_len)))
+		return rc;
+
+	BRS_JSON_CHECK_WR_SZ(writer, FALSE_LEN);
+
+	if (val) {
+		memcpy(writer->buf + writer->offset, TRUE_STR, TRUE_LEN);
+		writer->offset += TRUE_LEN;
+	} else {
+		memcpy(writer->buf + writer->offset, FALSE_STR, FALSE_LEN);
+		writer->offset += FALSE_LEN;
+	}
+
+	return 0;
+}
+
+#define BRS_JSON_MAKE_WRITE_INT(NAME, FMT, TYPE)                               \
+	static inline int brs_json_write_##NAME(json_writer_t *writer,         \
+						const char *key,               \
+						size_t key_len, TYPE val)      \
+	{                                                                      \
+		int rc;                                                        \
+                                                                               \
+		if ((rc = brs_json_write_label(writer, key, key_len)))         \
+			return rc;                                             \
+                                                                               \
+		/* 20 chars: string length of either */                        \
+		/* -9223372036854775807 (INT64_MIN) or */                      \
+		/* 18446744073709551615 (UINT64_MAX) */                        \
+		BRS_JSON_CHECK_WR_SZ(writer, 20);                              \
+                                                                               \
+		writer->offset +=                                              \
+			scnprintf(writer->buf + writer->offset,                \
+				  writer->buf_sz - writer->offset, FMT, val);  \
+                                                                               \
+		return 0;                                                      \
+	}
+
+BRS_JSON_MAKE_WRITE_INT(int64, "%lld", int64_t)
+BRS_JSON_MAKE_WRITE_INT(uint64, "%llu", uint64_t)
+
+static inline int brs_json_start_doc(json_writer_t *writer, const char *key,
+				     size_t key_len, json_writer_doc_t *out)
+{
+	int rc;
+
+	if ((rc = brs_json_write_label(writer, key, key_len)))
+		return rc;
+
+	BRS_JSON_CHECK_WR_SZ(writer, 2 /* {} */);
+
+	writer->buf[writer->offset++] = '{';
+
+	return 0;
+}
+
+static inline int brs_json_close_doc(json_writer_t *writer,
+				     json_writer_doc_t *doc)
+{
+	BRS_JSON_CHECK_WR_SZ(writer, 1 /* } */);
+
+	writer->buf[writer->offset++] = '}';
+
+	return 0;
+}
+
+static inline int brs_json_finalize(json_writer_t *writer, uint32_t event_type)
+{
+	int rc;
+
+	if ((rc = brs_json_close_doc(writer, NULL)))
+		return rc;
+
+	writer->data->header.type = event_type;
+	writer->data->header.size = writer->offset;
+
+	return writer->offset;
+}
+
+#define brs_evt_write_start brs_json_write_start
+#define brs_evt_write_string brs_json_write_string
+#define brs_evt_write_bool brs_json_write_bool
+#define brs_evt_write_int64 brs_json_write_int64
+#define brs_evt_write_uint64 brs_json_write_uint64
+#define brs_evt_start_doc brs_json_start_doc
+#define brs_evt_close_doc brs_json_close_doc
+#define brs_evt_finalize brs_json_finalize
+#define evt_writer json_writer
+#define evt_writer_t json_writer_t
+#define evt_writer_doc_t json_writer_doc_t
+
+#pragma GCC poison brs_json_write_start
+#pragma GCC poison brs_json_write_string
+#pragma GCC poison brs_json_write_bool
+#pragma GCC poison brs_json_write_int64
+#pragma GCC poison brs_json_write_uint64
+#pragma GCC poison brs_json_start_doc
+#pragma GCC poison brs_json_close_doc
+#pragma GCC poison brs_json_finalize
+#pragma GCC poison json_writer
+#pragma GCC poison json_writer_t
+#pragma GCC poison json_writer_doc_t
+
+#endif /* __BHV_JSON_GEN_H__ */
\ No newline at end of file
diff --git include/bhv/keyring.h include/bhv/keyring.h
new file mode 100644
index 0000000000..2274bc3f84
--- /dev/null
+++ include/bhv/keyring.h
@@ -0,0 +1,40 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_KEYRING_H__
+#define __BHV_KEYRING_H__
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_KEYS
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_keyring_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__CredProtection__enabled();
+}
+
+int __init bhv_init_keyring(void);
+
+int bhv_keyring_register_system_trusted(struct key **k);
+int bhv_keyring_verify(struct key *keyring, void *anchor);
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor);
+
+#else /* !CONFIG_BHV_VAS */
+
+#define bhv_keyring_register_system_trusted(k) 0
+#define bhv_keyring_verify(k, a) 0
+#define bhv_keyring_verify_locked(k, a) 0
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* CONFIG_KEYS */
+
+#endif /* __BHV_KEYRING_H__ */
diff --git include/bhv/kversion.h include/bhv/kversion.h
new file mode 100644
index 0000000000..0a8bec6ba1
--- /dev/null
+++ include/bhv/kversion.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 31) && LINUX_VERSION_CODE < KERNEL_VERSION(6, 2, 0)
+#define BHV_KVERS_6_1
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(6, 12, 1)
+#define BHV_KVERS_ATLEAST_6_12
+#else
+#error Unsupported linux version
+#endif
+
+#undef LINUX_VERSION_CODE
+#undef KERNEL_VERSION
+#undef LINUX_VERSION_MAJOR
+#undef LINUX_VERSION_PATCHLEVEL
+#undef LINUX_VERSION_SUBLEVEL
diff --git include/bhv/libinsight.h include/bhv/libinsight.h
new file mode 100644
index 0000000000..feaf440acf
--- /dev/null
+++ include/bhv/libinsight.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 BlueRock Security, Inc.
+ * All rights reserved.
+ */
+
+#ifndef __BRS_LIBINSIGHT_H__
+#define __BRS_LIBINSIGHT_H__
+
+#include <linux/fs.h>
+
+int brs_trace_lib(struct file *lib);
+void brs_trace_disable(void);
+void brs_trace_inode_free(struct inode *inode);
+
+#endif /* __BRS_LIBINSIGHT_H__ */
diff --git include/bhv/lsm/hook_defs.inc.h include/bhv/lsm/hook_defs.inc.h
new file mode 100644
index 0000000000..2933c2ef48
--- /dev/null
+++ include/bhv/lsm/hook_defs.inc.h
@@ -0,0 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - 2025 BlueRock Security Inc.
+ */
+
+LSM_HOOK(int, 0, module_loaded, struct module *mod)
+LSM_HOOK(int, 0, unshare, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, setns, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, cgroup_mkdir, struct cgroup *cgrp)
+LSM_HOOK(void, LSM_RET_VOID, cgroup_rmdir, struct cgroup *cgrp)
+LSM_HOOK(int, 0, fd_dup, unsigned int fd, struct file *file)
+LSM_HOOK(int, 0, socket_accepted, struct socket *sock, struct socket *newsock)
+LSM_HOOK(void, LSM_RET_VOID, kaccess, uint64_t addr, uint8_t event_id)
+LSM_HOOK(int, 0, elf_load_exec_stack, struct linux_binprm *bprm)
+LSM_HOOK(int, 0, kernel_exec, const char *path, char **argv, char **envp)
+LSM_HOOK(int, 1, forced_mem_access_permitted, struct vm_area_struct *vma,
+	 bool write, bool foreign)
+#ifdef CONFIG_MEM_NS
+LSM_HOOK(int, 0, domain_report, const struct task_struct *t,
+	 const struct mm_struct *mm_target, const struct vm_area_struct *vma,
+	 unsigned int gup_flags)
+#endif // CONFIG_MEM_NS
diff --git include/bhv/lsm/security.c.inc.h include/bhv/lsm/security.c.inc.h
new file mode 100644
index 0000000000..b6567bf8d0
--- /dev/null
+++ include/bhv/lsm/security.c.inc.h
@@ -0,0 +1,80 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 BlueRock Security Inc.
+ */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
+#define CALL_INT_HOOK(FN, DEF, ...) call_int_hook(FN, __VA_ARGS__)
+#else // LINUX_VERSION_CODE < 6.9
+#define CALL_INT_HOOK(FN, DEF, ...) call_int_hook(FN, DEF, __VA_ARGS__)
+#endif // LINUX_VERSION_CODE <> 6.9
+
+
+int security_module_loaded(struct module *mod)
+{
+	return CALL_INT_HOOK(module_loaded, 0, mod);
+}
+
+int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return CALL_INT_HOOK(unshare, 0, tsk, nsset);
+}
+
+int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return CALL_INT_HOOK(setns, 0, tsk, nsset);
+}
+
+int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return CALL_INT_HOOK(cgroup_mkdir, 0, cgrp);
+}
+
+void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+	call_void_hook(cgroup_rmdir, cgrp);
+}
+
+int security_fd_dup(unsigned int fd, struct file *file)
+{
+	return CALL_INT_HOOK(fd_dup, 0, fd, file);
+}
+
+int security_socket_accepted(struct socket *sock, struct socket *newsock)
+{
+	return CALL_INT_HOOK(socket_accepted, 0, sock, newsock);
+}
+
+void security_kaccess(uint64_t addr, uint8_t event_id)
+{
+	call_void_hook(kaccess, addr, event_id);
+}
+
+int security_elf_load_exec_stack(struct linux_binprm *bprm)
+{
+	return CALL_INT_HOOK(elf_load_exec_stack, 0, bprm);
+}
+
+int security_kernel_exec(const char *path, char **argv, char **envp)
+{
+	return CALL_INT_HOOK(kernel_exec, 0, path, argv, envp);
+}
+
+int security_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+					 bool foreign)
+{
+	return CALL_INT_HOOK(forced_mem_access_permitted, 1, vma, write,
+			     foreign);
+}
+
+#ifdef CONFIG_MEM_NS
+int security_domain_report(const struct task_struct *t,
+			   const struct mm_struct *mm_target,
+			   const struct vm_area_struct *vma,
+			   unsigned int gup_flags)
+{
+	return CALL_INT_HOOK(domain_report, 0, t, mm_target, vma, gup_flags);
+}
+#endif // CONFIG_MEM_NS
+
+#undef CALL_INT_HOOK
diff --git include/bhv/lsm/security.h.inc.h include/bhv/lsm/security.h.inc.h
new file mode 100644
index 0000000000..03c1e8877b
--- /dev/null
+++ include/bhv/lsm/security.h.inc.h
@@ -0,0 +1,81 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - 2025 BlueRock Security Inc.
+ */
+
+#ifdef CONFIG_SECURITY
+int security_module_loaded(struct module *mod);
+int security_unshare(struct task_struct *tsk, struct nsset *nsset);
+int security_setns(struct task_struct *tsk, struct nsset *nsset);
+int security_cgroup_mkdir(struct cgroup *cgrp);
+void security_cgroup_rmdir(struct cgroup *cgrp);
+int security_fd_dup(unsigned int fd, struct file *file);
+int security_socket_accepted(struct socket *sock, struct socket *newsock);
+void security_kaccess(uint64_t addr, uint8_t event_id);
+int security_elf_load_exec_stack(struct linux_binprm *bprm);
+int security_kernel_exec(const char *path, char **argv, char **envp);
+int security_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+					 bool foreign);
+#ifdef CONFIG_MEM_NS
+int security_domain_report(const struct task_struct *t,
+			   const struct mm_struct *mm_target,
+			   const struct vm_area_struct *vma,
+			   unsigned int gup_flags);
+#endif // CONFIG_MEM_NS
+
+#else /* CONFIG_SECURITY */
+static inline int security_module_loaded(struct module *)
+{
+	return 0;
+}
+static inline int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return 0;
+}
+static inline void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+}
+static inline int security_fd_dup(unsigned int fd, struct file *file)
+{
+	return 0;
+}
+static inline int security_socket_accepted(struct socket *sock,
+					   struct socket *newsock)
+{
+	return 0;
+}
+static inline void security_kaccess(uint64_t addr, uint8_t event_id)
+{
+}
+static inline int security_elf_load_exec_stack(struct linux_binprm *bprm)
+{
+	return 0;
+}
+static inline int security_kernel_exec(const char *path, char **argv,
+				       char **envp)
+{
+	return 0;
+}
+int security_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+					 bool foreign)
+{
+	return 0;
+}
+#ifdef CONFIG_MEM_NS
+int security_domain_report(const struct task_struct *t,
+			   const struct mm_struct *mm_target,
+			   const struct vm_area_struct *vma,
+			   unsigned int gup_flags)
+{
+	return 0;
+}
+#endif // CONFIG_MEM_NS
+#endif /* CONFIG_SECURITY */
diff --git include/bhv/memory_freeze.h include/bhv/memory_freeze.h
new file mode 100644
index 0000000000..fbea9c9e0c
--- /dev/null
+++ include/bhv/memory_freeze.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#ifdef CONFIG_BHV_VAS
+
+void bhv_memory_freeze_init(void);
+
+#else // !CONFIG_BHV_VAS
+
+static inline void bhv_memory_freeze_init(void)
+{
+}
+
+#endif // CONFIG_BHV_VAS
\ No newline at end of file
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 0000000000..e74d500b3a
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* !CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 0000000000..7900e45a14
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,150 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+
+#include <bhv/kversion.h>
+
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+extern struct mutex bhv_alternatives_mutex;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_init_alternatives(void);
+#ifdef CONFIG_JUMP_LABEL
+void bhv_init_jump_label(void);
+#else // CONFIG_JUMP_LABEL
+static inline void bhv_init_jump_label(void) {}
+#endif // CONFIG_JUMP_LABEL
+#ifdef CONFIG_HAVE_STATIC_CALL
+void bhv_init_static_call(void);
+#else // CONFIG_BHV_VAULT_SPACES
+static inline void bhv_init_static_call(void) {}
+#endif // CONFIG_BHV_VAULT_SPACES
+
+#endif // CONFIG_BHV_VAS
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void);
+/**************************************************/
+
+int bhv_bpf_write(void *dst, void *src, size_t sz);
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz);
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages);
+void bhv_rm_bpf_code_range(uint64_t pfn);
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+#ifndef CONFIG_BHV_VAULT_SPACES
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_JUMP_LABEL */
+
+static __always_inline void bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static __always_inline void bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len);
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+// CONFIG_MITIGATION_RETPOLINE is used in 6.12 and CONFIG_RETPOLINE in prev
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) &&     \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __init_or_module bhv_apply_retpolines(s32 *s);
+// CONFIG_MITIGATION_RETHUNK is used in 6.12 and CONFIG_RETHUNK in prev
+#if (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK))
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK)) */
+#endif /* (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) */
+
+#if defined CONFIG_PARAVIRT && defined CONFIG_X86 && \
+	LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0)
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* defined CONFIG_PARAVIRT && defined CONFIG_X86 && LINUX_VERSION_CODE < 6.8 */
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __init_or_module bhv_apply_ibt_endbr(s32 *s);
+#endif
+
+#else // CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif // CONFIG_BHV_VAS
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/patch_base.h include/bhv/patch_base.h
new file mode 100644
index 0000000000..6853b3e304
--- /dev/null
+++ include/bhv/patch_base.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_BASE_H__
+#define __BHV_PATCH_BASE_H__
+
+#include <asm-generic/rwonce.h>
+#include <linux/cache.h>
+#include <linux/bug.h>
+#include <asm/string.h>
+
+#include <bhv/integrity_base.h>
+
+#ifdef CONFIG_BHV_VAS
+void __bhv_patch(void *addr, const void *data, size_t len);
+#endif // CONFIG_BHV_VAS
+
+static __always_inline void bhv_patch(void *addr, const void *data, size_t len)
+{
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		return __bhv_patch(addr, data, len);
+	}
+#endif // CONFIG_BHV_VAS
+	switch (len) {
+	case sizeof(uint64_t):
+		*(volatile uint64_t *)addr = *(volatile uint64_t *)data;
+		break;
+	case sizeof(uint32_t):
+		*(volatile uint32_t *)addr = *(volatile uint32_t *)data;
+		break;
+	case sizeof(uint16_t):
+		*(volatile uint16_t *)addr = *(volatile uint16_t *)data;
+		break;
+	case sizeof(uint8_t):
+		*(volatile uint8_t *)addr = *(volatile uint8_t *)data;
+		break;
+	default:
+		memcpy(addr, data, len);
+	}
+}
+
+#endif /* __BHV_PATCH_BASE_H__ */
diff --git include/bhv/policy_scks_autogen.h include/bhv/policy_scks_autogen.h
new file mode 100644
index 0000000000..24c9f23722
--- /dev/null
+++ include/bhv/policy_scks_autogen.h
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_policy_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/static_call.h>
+#include <bhv/guestlog_internal.h>
+
+DECLARE_STATIC_CALL(__brs__Logging__driver__driver_load__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__driver__driver_load__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_DRIVER_LOAD(...) static_call(__brs__Logging__driver__driver_load__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__process__process_exec__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__process__process_exec__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_PROCESS_EXEC(...) static_call(__brs__Logging__process__process_exec__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__process__executable_exec_stack__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__process__executable_exec_stack__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_EXECUTABLE_EXEC_STACK(...) static_call(__brs__Logging__process__executable_exec_stack__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__kernel__kernel_exec__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__kernel__kernel_exec__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_KERNEL_EXEC(...) static_call(__brs__Logging__kernel__kernel_exec__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__kernel__forced_mem_access__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__kernel__forced_mem_access__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_FORCED_MEM_ACCESS(...) static_call(__brs__Logging__kernel__forced_mem_access__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_UNSUPPORTED_FILE_OPERATION(...) static_call(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__container__namespace_change__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__container__namespace_change__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_NAMESPACE_CHANGE(...) static_call(__brs__Logging__container__namespace_change__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__socket__interpreter_bound__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__socket__interpreter_bound__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_INTERPRETER_BOUND(...) static_call(__brs__Logging__socket__interpreter_bound__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_INTERPRETER_BOUND_TRANSITIVE(...) static_call(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__socket__socket_accept__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__socket__socket_accept__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_SOCKET_ACCEPT(...) static_call(__brs__Logging__socket__socket_accept__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__socket__socket_connection__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__socket__socket_connection__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_SOCKET_CONNECTION(...) static_call(__brs__Logging__socket__socket_connection__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__file__file_open__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__file__file_open__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_FILE_OPEN(...) static_call(__brs__Logging__file__file_open__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__file__mmap_exec_file__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__file__mmap_exec_file__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_MMAP_EXEC_FILE(...) static_call(__brs__Logging__file__mmap_exec_file__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__file__file_drift__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__file__file_drift__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_FILE_DRIFT(...) static_call(__brs__Logging__file__file_drift__synchronous__sck)(__VA_ARGS__)
+
+DECLARE_STATIC_CALL(__brs__Logging__file__file_set_xattr__synchronous__sck, send_evt_msg);
+extern void *____brs__Logging__file__file_set_xattr__synchronous__sck_state;
+#define BRS_SEND_EVT_MSG_FILE_SET_XATTR(...) static_call(__brs__Logging__file__file_set_xattr__synchronous__sck)(__VA_ARGS__)
+
diff --git include/bhv/policy_sks_autogen.h include/bhv/policy_sks_autogen.h
new file mode 100644
index 0000000000..db11ff9c61
--- /dev/null
+++ include/bhv/policy_sks_autogen.h
@@ -0,0 +1,275 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_policy_c.py (2025-10-29T15:55:25).
+ */
+
+#pragma once
+
+#include <linux/jump_label.h>
+
+#include <bhv/flast.h>
+#include <bhv/flast_ids_autogen.h>
+
+extern struct static_key_false __brs__DriverACL__enabled__sk;
+static inline bool brs__is__DriverACL__enabled(void) {
+    return static_branch_unlikely(&__brs__DriverACL__enabled__sk);
+};
+
+extern struct static_key_false __brs__ProcessACL__enabled__sk;
+static inline bool brs__is__ProcessACL__enabled(void) {
+    return static_branch_unlikely(&__brs__ProcessACL__enabled__sk);
+};
+
+extern struct static_key_false __brs__FileProtection__read_only__enabled__sk;
+static inline bool brs__is__FileProtection__read_only__enabled(void) {
+    return static_branch_unlikely(&__brs__FileProtection__read_only__enabled__sk);
+};
+
+extern struct static_key_false __brs__FileProtection__file_ops__enabled__sk;
+static inline bool brs__is__FileProtection__file_ops__enabled(void) {
+    return static_branch_unlikely(&__brs__FileProtection__file_ops__enabled__sk);
+};
+
+extern struct static_key_false __brs__FileProtection__dirtycred__enabled__sk;
+static inline bool brs__is__FileProtection__dirtycred__enabled(void) {
+    return static_branch_unlikely(&__brs__FileProtection__dirtycred__enabled__sk);
+};
+
+extern struct static_key_false __brs__Logging__driver__driver_load__enabled__sk;
+static inline bool brs__is__Logging__driver__driver_load__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__driver__driver_load__enabled__sk);
+};
+#define __BRS_EVT_driver_load_ENABLED() brs__is__Logging__driver__driver_load__enabled()
+#define __BRS_EVT_driver_load_policy_ENABLED(policy) FLAST__Logging__driver__driver_load__enabled(policy)
+
+extern struct static_key_false __brs__Logging__process__process_exec__enabled__sk;
+static inline bool brs__is__Logging__process__process_exec__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__process__process_exec__enabled__sk);
+};
+#define __BRS_EVT_process_exec_ENABLED() brs__is__Logging__process__process_exec__enabled()
+#define __BRS_EVT_process_exec_policy_ENABLED(policy) FLAST__Logging__process__process_exec__enabled(policy)
+
+extern struct static_key_false __brs__Logging__process__process_fork__enabled__sk;
+static inline bool brs__is__Logging__process__process_fork__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__process__process_fork__enabled__sk);
+};
+#define __BRS_EVT_process_fork_ENABLED() brs__is__Logging__process__process_fork__enabled()
+#define __BRS_EVT_process_fork_policy_ENABLED(policy) FLAST__Logging__process__process_fork__enabled(policy)
+
+extern struct static_key_false __brs__Logging__process__process_terminate__enabled__sk;
+static inline bool brs__is__Logging__process__process_terminate__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__process__process_terminate__enabled__sk);
+};
+#define __BRS_EVT_process_terminate_ENABLED() brs__is__Logging__process__process_terminate__enabled()
+#define __BRS_EVT_process_terminate_policy_ENABLED(policy) FLAST__Logging__process__process_terminate__enabled(policy)
+
+extern struct static_key_false __brs__Logging__process__executable_exec_stack__enabled__sk;
+static inline bool brs__is__Logging__process__executable_exec_stack__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__process__executable_exec_stack__enabled__sk);
+};
+#define __BRS_EVT_executable_exec_stack_ENABLED() brs__is__Logging__process__executable_exec_stack__enabled()
+#define __BRS_EVT_executable_exec_stack_policy_ENABLED(policy) FLAST__Logging__process__executable_exec_stack__enabled(policy)
+
+extern struct static_key_false __brs__Logging__kernel__kernel_access_violation__enabled__sk;
+static inline bool brs__is__Logging__kernel__kernel_access_violation__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__kernel__kernel_access_violation__enabled__sk);
+};
+#define __BRS_EVT_kernel_access_violation_ENABLED() brs__is__Logging__kernel__kernel_access_violation__enabled()
+#define __BRS_EVT_kernel_access_violation_policy_ENABLED(policy) FLAST__Logging__kernel__kernel_access_violation__enabled(policy)
+
+extern struct static_key_false __brs__Logging__kernel__kernel_exec__enabled__sk;
+static inline bool brs__is__Logging__kernel__kernel_exec__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__kernel__kernel_exec__enabled__sk);
+};
+#define __BRS_EVT_kernel_exec_ENABLED() brs__is__Logging__kernel__kernel_exec__enabled()
+#define __BRS_EVT_kernel_exec_policy_ENABLED(policy) FLAST__Logging__kernel__kernel_exec__enabled(policy)
+
+extern struct static_key_false __brs__Logging__kernel__forced_mem_access__enabled__sk;
+static inline bool brs__is__Logging__kernel__forced_mem_access__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__kernel__forced_mem_access__enabled__sk);
+};
+#define __BRS_EVT_forced_mem_access_ENABLED() brs__is__Logging__kernel__forced_mem_access__enabled()
+#define __BRS_EVT_forced_mem_access_policy_ENABLED(policy) FLAST__Logging__kernel__forced_mem_access__enabled(policy)
+
+extern struct static_key_false __brs__Logging__kernel__unsupported_file_operation__enabled__sk;
+static inline bool brs__is__Logging__kernel__unsupported_file_operation__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__kernel__unsupported_file_operation__enabled__sk);
+};
+#define __BRS_EVT_unsupported_file_operation_ENABLED() brs__is__Logging__kernel__unsupported_file_operation__enabled()
+#define __BRS_EVT_unsupported_file_operation_policy_ENABLED(policy) FLAST__Logging__kernel__unsupported_file_operation__enabled(policy)
+
+extern struct static_key_false __brs__Logging__container__capable__enabled__sk;
+static inline bool brs__is__Logging__container__capable__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__container__capable__enabled__sk);
+};
+#define __BRS_EVT_capable_ENABLED() brs__is__Logging__container__capable__enabled()
+#define __BRS_EVT_capable_policy_ENABLED(policy) FLAST__Logging__container__capable__enabled(policy)
+
+extern struct static_key_false __brs__Logging__container__cgroup_create__enabled__sk;
+static inline bool brs__is__Logging__container__cgroup_create__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__container__cgroup_create__enabled__sk);
+};
+#define __BRS_EVT_cgroup_create_ENABLED() brs__is__Logging__container__cgroup_create__enabled()
+#define __BRS_EVT_cgroup_create_policy_ENABLED(policy) FLAST__Logging__container__cgroup_create__enabled(policy)
+
+extern struct static_key_false __brs__Logging__container__cgroup_destroy__enabled__sk;
+static inline bool brs__is__Logging__container__cgroup_destroy__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__container__cgroup_destroy__enabled__sk);
+};
+#define __BRS_EVT_cgroup_destroy_ENABLED() brs__is__Logging__container__cgroup_destroy__enabled()
+#define __BRS_EVT_cgroup_destroy_policy_ENABLED(policy) FLAST__Logging__container__cgroup_destroy__enabled(policy)
+
+extern struct static_key_false __brs__Logging__container__namespace_change__enabled__sk;
+static inline bool brs__is__Logging__container__namespace_change__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__container__namespace_change__enabled__sk);
+};
+#define __BRS_EVT_namespace_change_ENABLED() brs__is__Logging__container__namespace_change__enabled()
+#define __BRS_EVT_namespace_change_policy_ENABLED(policy) FLAST__Logging__container__namespace_change__enabled(policy)
+
+extern struct static_key_false __brs__Logging__socket__interpreter_bound__enabled__sk;
+static inline bool brs__is__Logging__socket__interpreter_bound__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__socket__interpreter_bound__enabled__sk);
+};
+#define __BRS_EVT_interpreter_bound_ENABLED() brs__is__Logging__socket__interpreter_bound__enabled()
+#define __BRS_EVT_interpreter_bound_policy_ENABLED(policy) FLAST__Logging__socket__interpreter_bound__enabled(policy)
+
+extern struct static_key_false __brs__Logging__socket__interpreter_bound_transitive__enabled__sk;
+static inline bool brs__is__Logging__socket__interpreter_bound_transitive__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__socket__interpreter_bound_transitive__enabled__sk);
+};
+#define __BRS_EVT_interpreter_bound_transitive_ENABLED() brs__is__Logging__socket__interpreter_bound_transitive__enabled()
+#define __BRS_EVT_interpreter_bound_transitive_policy_ENABLED(policy) FLAST__Logging__socket__interpreter_bound_transitive__enabled(policy)
+
+extern struct static_key_false __brs__Logging__socket__socket_accept__enabled__sk;
+static inline bool brs__is__Logging__socket__socket_accept__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__socket__socket_accept__enabled__sk);
+};
+#define __BRS_EVT_socket_accept_ENABLED() brs__is__Logging__socket__socket_accept__enabled()
+#define __BRS_EVT_socket_accept_policy_ENABLED(policy) FLAST__Logging__socket__socket_accept__enabled(policy)
+
+extern struct static_key_false __brs__Logging__socket__socket_connection__enabled__sk;
+static inline bool brs__is__Logging__socket__socket_connection__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__socket__socket_connection__enabled__sk);
+};
+#define __BRS_EVT_socket_connection_ENABLED() brs__is__Logging__socket__socket_connection__enabled()
+#define __BRS_EVT_socket_connection_policy_ENABLED(policy) FLAST__Logging__socket__socket_connection__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__file_open__enabled__sk;
+static inline bool brs__is__Logging__file__file_open__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__file_open__enabled__sk);
+};
+#define __BRS_EVT_file_open_ENABLED() brs__is__Logging__file__file_open__enabled()
+#define __BRS_EVT_file_open_policy_ENABLED(policy) FLAST__Logging__file__file_open__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__mmap_exec_file__enabled__sk;
+static inline bool brs__is__Logging__file__mmap_exec_file__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__mmap_exec_file__enabled__sk);
+};
+#define __BRS_EVT_mmap_exec_file_ENABLED() brs__is__Logging__file__mmap_exec_file__enabled()
+#define __BRS_EVT_mmap_exec_file_policy_ENABLED(policy) FLAST__Logging__file__mmap_exec_file__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__file_drift__enabled__sk;
+static inline bool brs__is__Logging__file__file_drift__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__file_drift__enabled__sk);
+};
+#define __BRS_EVT_file_drift_ENABLED() brs__is__Logging__file__file_drift__enabled()
+#define __BRS_EVT_file_drift_policy_ENABLED(policy) FLAST__Logging__file__file_drift__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__file_set_xattr__enabled__sk;
+static inline bool brs__is__Logging__file__file_set_xattr__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__file_set_xattr__enabled__sk);
+};
+#define __BRS_EVT_file_set_xattr_ENABLED() brs__is__Logging__file__file_set_xattr__enabled()
+#define __BRS_EVT_file_set_xattr_policy_ENABLED(policy) FLAST__Logging__file__file_set_xattr__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__lib_trace__enabled__sk;
+static inline bool brs__is__Logging__file__lib_trace__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__lib_trace__enabled__sk);
+};
+#define __BRS_EVT_lib_trace_ENABLED() brs__is__Logging__file__lib_trace__enabled()
+#define __BRS_EVT_lib_trace_policy_ENABLED(policy) FLAST__Logging__file__lib_trace__enabled(policy)
+
+extern struct static_key_false __brs__Logging__file__skip_sym_hook__enabled__sk;
+static inline bool brs__is__Logging__file__skip_sym_hook__enabled(void) {
+    return static_branch_unlikely(&__brs__Logging__file__skip_sym_hook__enabled__sk);
+};
+#define __BRS_EVT_skip_sym_hook_ENABLED() brs__is__Logging__file__skip_sym_hook__enabled()
+#define __BRS_EVT_skip_sym_hook_policy_ENABLED(policy) FLAST__Logging__file__skip_sym_hook__enabled(policy)
+
+extern struct static_key_false __brs__IntegrityProtection__enabled__sk;
+static inline bool brs__is__IntegrityProtection__enabled(void) {
+    return static_branch_unlikely(&__brs__IntegrityProtection__enabled__sk);
+};
+
+extern struct static_key_false __brs__IntegrityProtection__pagetable_protection__enabled__sk;
+static inline bool brs__is__IntegrityProtection__pagetable_protection__enabled(void) {
+    return static_branch_unlikely(&__brs__IntegrityProtection__pagetable_protection__enabled__sk);
+};
+
+extern struct static_key_false __brs__StrongIsolation__enabled__sk;
+static inline bool brs__is__StrongIsolation__enabled(void) {
+    return static_branch_unlikely(&__brs__StrongIsolation__enabled__sk);
+};
+
+extern struct static_key_false __brs__StrongIsolation__remediate__sk;
+static inline bool brs__is__StrongIsolation__remediate(void) {
+    return static_branch_unlikely(&__brs__StrongIsolation__remediate__sk);
+};
+
+extern struct static_key_false __brs__StrongIsolation__isolate__sk;
+static inline bool brs__is__StrongIsolation__isolate(void) {
+    return static_branch_unlikely(&__brs__StrongIsolation__isolate__sk);
+};
+
+extern struct static_key_false __brs__CredProtection__enabled__sk;
+static inline bool brs__is__CredProtection__enabled(void) {
+    return static_branch_unlikely(&__brs__CredProtection__enabled__sk);
+};
+
+extern struct static_key_false __brs__GuestKernelConfig__userspace_force_nx_stack__sk;
+static inline bool brs__is__GuestKernelConfig__userspace_force_nx_stack(void) {
+    return static_branch_unlikely(&__brs__GuestKernelConfig__userspace_force_nx_stack__sk);
+};
+
+extern struct static_key_false __brs__RegisterProtectVAS__enabled__sk;
+static inline bool brs__is__RegisterProtectVAS__enabled(void) {
+    return static_branch_unlikely(&__brs__RegisterProtectVAS__enabled__sk);
+};
+
+extern struct static_key_false __brs__GuestPolicy__enabled__sk;
+static inline bool brs__is__GuestPolicy__enabled(void) {
+    return static_branch_unlikely(&__brs__GuestPolicy__enabled__sk);
+};
+
+extern struct static_key_false __brs__DriftDetection__enabled__sk;
+static inline bool brs__is__DriftDetection__enabled(void) {
+    return static_branch_unlikely(&__brs__DriftDetection__enabled__sk);
+};
+
+extern struct static_key_false __brs__DriftDetection__container_host__sk;
+static inline bool brs__is__DriftDetection__container_host(void) {
+    return static_branch_unlikely(&__brs__DriftDetection__container_host__sk);
+};
+
+extern struct static_key_false __brs__ReverseShellDetection__container_host__sk;
+static inline bool brs__is__ReverseShellDetection__container_host(void) {
+    return static_branch_unlikely(&__brs__ReverseShellDetection__container_host__sk);
+};
+
+extern struct static_key_false brs_retrieve_from_policy_sk;
+static inline bool brs_retrieve_from_policy(void) {
+    return static_branch_unlikely(&brs_retrieve_from_policy_sk);
+};
+
+void brs_set_sks(flast_buf *buf);
+void brs_dump_sks(void);
+bool brs_policy_check(const char *_flast_data, size_t _size);
+void brs_guestlog_disable_all(void);
+void brs_disable_sync(void);
+
+#define BRS_EVT_ENABLED(policy, evt)                                               \
+        ((brs_retrieve_from_policy() && __BRS_EVT_##evt##_policy_ENABLED(policy)) || \
+        __BRS_EVT_##evt##_ENABLED())
+
diff --git include/bhv/reg_protect.h include/bhv/reg_protect.h
new file mode 100644
index 0000000000..faae5348fd
--- /dev/null
+++ include/bhv/reg_protect.h
@@ -0,0 +1,42 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void);
+void bhv_start_reg_protect_arch(void);
+/***************************************************/
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield);
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return brs__is__RegisterProtectVAS__enabled();
+}
+
+#else /* !CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/reverse_shell_detection.h include/bhv/reverse_shell_detection.h
new file mode 100644
index 0000000000..278934f168
--- /dev/null
+++ include/bhv/reverse_shell_detection.h
@@ -0,0 +1,37 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024-2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ */
+#ifndef __BRS_REVERSE_SHELL_DETECTION_H__
+#define __BRS_REVERSE_SHELL_DETECTION_H__
+
+#ifdef CONFIG_BRS
+#include <linux/fs.h>
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+#include <bhv/policy_sks_autogen.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool
+brs_reverse_shell_detetection_is_enabled(struct brs_policy *policy)
+{
+	return BRS_EVT_ENABLED(&policy->flast, interpreter_bound) ||
+	       BRS_EVT_ENABLED(&policy->flast, interpreter_bound_transitive);
+}
+
+int brs_reverse_shell_detection_socket_connect(struct brs_policy *policy,
+					       struct socket *sock,
+					       struct sockaddr *address,
+					       int addrlen);
+int brs_reverse_shell_fd_dup(struct brs_policy *policy, unsigned int fd,
+			     struct file *file);
+int brs_reverse_shell_exec(struct brs_policy *policy, struct linux_binprm *bprm,
+			   const char *path);
+#endif /* CONFIG_BRS */
+
+#endif /* __BRS_REVERSE_SHELL_DETECTION_H__ */
\ No newline at end of file
diff --git include/bhv/sysfs.h include/bhv/sysfs.h
new file mode 100644
index 0000000000..8231447a78
--- /dev/null
+++ include/bhv/sysfs.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_sysfs(void);
+/**********************************************************/
diff --git include/bhv/sysfs_fops.h include/bhv/sysfs_fops.h
new file mode 100644
index 0000000000..be85ee86b0
--- /dev/null
+++ include/bhv/sysfs_fops.h
@@ -0,0 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_fileops_protection(struct kobject *fops,
+					struct kobject *status);
+/***************************************************/
+
+#endif /* CONFIG_BHV_VAS */
+
+
+
diff --git include/bhv/sysfs_integrity_freeze.h include/bhv/sysfs_integrity_freeze.h
new file mode 100644
index 0000000000..62ab213f17
--- /dev/null
+++ include/bhv/sysfs_integrity_freeze.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_integrity_freeze(struct kobject *kobj);
+/***************************************************/
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
\ No newline at end of file
diff --git include/bhv/sysfs_reg_protect.h include/bhv/sysfs_reg_protect.h
new file mode 100644
index 0000000000..fdd6c5fe7c
--- /dev/null
+++ include/bhv/sysfs_reg_protect.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <linux/types.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+struct kobject;
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_reg_protect(struct kobject *kobj);
+/***************************************************/
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs_version.h include/bhv/sysfs_version.h
new file mode 100644
index 0000000000..4b2915c3fc
--- /dev/null
+++ include/bhv/sysfs_version.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_version(struct kobject *kobj);
+/***************************************************/
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/util.h include/bhv/util.h
new file mode 100644
index 0000000000..94376108cb
--- /dev/null
+++ include/bhv/util.h
@@ -0,0 +1,92 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024-2025 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ */
+
+#include <linux/cgroup.h>
+#include <linux/version.h>
+#include <linux/namei.h>
+#include <linux/binfmts.h>
+#include <linux/file.h>
+
+// Various utility functions
+static inline const char *brs_get_file_path(struct file *file, char *buf,
+					    size_t buf_sz)
+{
+	const char *rv;
+
+	if (buf == NULL)
+		return "UNKNOWN (NO BUF)";
+
+	rv = d_path(&file->f_path, buf, buf_sz);
+	if (IS_ERR(rv))
+		return "UNKNOWN (ERROR)";
+
+	return rv;
+}
+
+static inline const char *brs_get_dentry_path(struct dentry *dentry, char *buf,
+					      size_t buf_sz)
+{
+	const char *rv;
+
+	if (buf == NULL)
+		return "UNKNOWN (NO BUF)";
+
+	rv = dentry_path(dentry, buf, buf_sz);
+	if (IS_ERR(rv)) {
+		return "UNKNOWN (ERROR)";
+	}
+
+	return rv;
+}
+
+static inline bool brs_task_in_container(struct task_struct *tsk)
+{
+#define CGRP_NAME_SZ 128
+#define CONTAINER_ID_LEN 64
+	static const char *CONTAINER_CGRP_PREFIX[] = { "libpod-conmon-",
+						       "docker-",
+						       "cri-containerd-" };
+	static const size_t CONTAINER_CGRP_PREFIX_SZ =
+		(ARRAY_SIZE(CONTAINER_CGRP_PREFIX));
+
+	int r = 0;
+	struct cgroup *cgrp;
+	char cgrp_name[CGRP_NAME_SZ];
+	int i;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	while (cgrp != NULL) {
+		r = cgroup_name(cgrp, cgrp_name, CGRP_NAME_SZ);
+		if (r < 0) {
+			goto out;
+		}
+
+		for (i = 0; i < CONTAINER_CGRP_PREFIX_SZ; i++) {
+			size_t prefix_len = strlen(CONTAINER_CGRP_PREFIX[i]);
+			if (strncmp(cgrp_name, CONTAINER_CGRP_PREFIX[i],
+				    prefix_len) == 0) {
+				rcu_read_unlock();
+				return true;
+			}
+		}
+
+		// Try next
+		cgrp = cgroup_parent(cgrp);
+	}
+
+out:
+	rcu_read_unlock();
+	return false;
+}
+
+static inline bool brs_is_pipe(struct file *f)
+{
+	if (f == NULL)
+		return false;
+
+	return S_ISFIFO(f->f_path.dentry->d_inode->i_mode);
+}
\ No newline at end of file
diff --git include/bhv/vault.h include/bhv/vault.h
new file mode 100644
index 0000000000..e326ff3642
--- /dev/null
+++ include/bhv/vault.h
@@ -0,0 +1,269 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VAULT_H__
+#define __BHV_VAULT_H__
+
+#ifdef __ASSEMBLY__
+
+#ifdef CONFIG_X86_64
+.macro BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE vault
+#ifdef CONFIG_BHV_VAULT_SPACES
+	.pushsection .bhv.vault.shared.text.\vault,"ax"
+#endif
+.endm
+
+.macro BHV_ASM_POP_SECTION
+#ifdef CONFIG_BHV_VAULT_SPACES
+	.popsection
+#endif
+.endm
+#endif /* !CONFIG_X86_64 */
+
+#else /* !__ASSEMBLY__ */
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool bhv_vault_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return true;
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+typedef struct {
+	int type;
+	unsigned long start;
+	unsigned long end;
+} bhv_vault_memory_region_helper_t;
+
+typedef struct {
+	void *ep;
+} bhv_vault_entry_point_helper_t;
+
+typedef struct {
+	void *rp;
+} bhv_vault_return_point_helper_t;
+
+#define STRINGIFY(x) #x
+#define ARG_DECL(t, a) t a
+#define RET(t) STRINGIFY(t)
+
+#define void_fn(fn, ...) fn(__VA_ARGS__)
+#define nonvoid_fn(fn, ...) return fn(__VA_ARGS__)
+
+/* Check out system call macros. */
+
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)                               \
+	static rettype noinline notrace __attribute__((                        \
+		section(".bhv.vault.shared.text.jump_label"),                  \
+		__optimize__("no-optimize-sibling-calls",                      \
+			     "no-omit-frame-pointer"))) bhv_wrapper_##fn(void) \
+	{                                                                      \
+		void_fn(fn);                                                   \
+	}
+
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)                                     \
+	static rettype noinline notrace __attribute__((                        \
+		section(".bhv.vault.shared.text.jump_label"),                  \
+		__optimize__("no-optimize-sibling-calls",                      \
+			     "no-omit-frame-pointer"))) bhv_wrapper_##fn(void) \
+	{                                                                      \
+		nonvoid_fn(fn);                                                \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)                       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		void_fn(fn, a1);                                               \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)                             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1);                                            \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)                           \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn##_##a1(void)                                  \
+	{                                                                      \
+		return fn(a1);                                                 \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)               \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		void_fn(fn, a1, a2);                                           \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)                     \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2);                                        \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		void_fn(fn, a1, a2, a3);                                       \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls",       \
+					    "no-omit-frame-pointer")))         \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2, a3);                                    \
+	}
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.data." #vault)))
+
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)                                 \
+	__attribute__((section(".bhv.vault.rodata." #vault)))
+
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.text." #vault),                     \
+		       __optimize__("no-optimize-sibling-calls",               \
+				    "no-omit-frame-pointer"),                  \
+		       no_instrument_function))
+
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault)                                \
+	__attribute__((section(".ref.text.bhv.vault.text." #vault),            \
+		       __optimize__("no-optimize-sibling-calls",               \
+				    "no-omit-frame-pointer"),                  \
+		       no_instrument_function))
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)                             \
+	__attribute__((section(".bhv.vault.shared.text." #vault),              \
+		       __optimize__("no-optimize-sibling-calls",               \
+				    "no-omit-frame-pointer"),                  \
+		       no_instrument_function))
+#else
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)
+#endif
+
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)                                 \
+	static bhv_vault_entry_point_helper_t bhv_vault_##vault##_ep_##func    \
+		__attribute__((used,                                           \
+			       section(".bhv.vault." #vault ".eps"))) = {      \
+			.ep = func,                                            \
+		}
+
+#define BHV_VAULT_FOR_EACH(section_name, type_t, elem)                         \
+		/*type_t *elem;	*/					       \
+		for (elem = ({                                         	       \
+		     	extern type_t section_name##_start;                    \
+		     	&section_name##_start;                                 \
+	     	});                                                            \
+	     	elem != ({                                                     \
+		     	extern type_t section_name##_end;                      \
+		     	&section_name##_end;                                   \
+	     	});                                                            \
+	     	++elem)
+
+#define BHV_VAULT_FOR_EACH_SECTION(vault, elem_name)                           \
+	BHV_VAULT_FOR_EACH(bhv_vault_##vault##_regions,                        \
+			   bhv_vault_memory_region_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, elem_name)                       \
+	BHV_VAULT_FOR_EACH(__bhv_vault_##vault##_eps,                          \
+			   bhv_vault_entry_point_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_RETURN_POINT(vault, elem)                           \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_return_sites;                       \
+		     &__start_vault_return_sites;                              \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_return_sites;                        \
+		     &__stop_vault_return_sites;                               \
+	     });                                                               \
+	     ++elem)
+
+#define BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, elem)                          \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_rethunk_sites;                      \
+		     &__start_vault_rethunk_sites;                             \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_rethunk_sites;                       \
+		     &__stop_vault_rethunk_sites;                              \
+	     });                                                               \
+	     ++elem)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#else /* !CONFIG_BHV_VAS */
+
+#define bhv_vault_is_enabled() 0
+
+#endif /* CONFIG_BHV_VAS */
+
+#if !defined(CONFIG_BHV_VAS) || !defined(CONFIG_BHV_VAULT_SPACES)
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault) __ref
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)
+#endif /* !CONFIG_BHV_VAS || !CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* __BHV_VAULT_H__ */
diff --git include/bhv/version.h include/bhv/version.h
new file mode 100644
index 0000000000..3f4f0499b9
--- /dev/null
+++ include/bhv/version.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VERSION_H__
+#define __BHV_VERSION_H__
+
+#define BHV_VERSION __BHV_VERSION(25, 8, 0)
+
+#include <bhv/interface/abi_version_autogen.h>
+
+#endif //__BHV_VERSION_H__
diff --git include/keys/system_keyring.h include/keys/system_keyring.h
index 8365adf842..5cea37ef75 100644
--- include/keys/system_keyring.h
+++ include/keys/system_keyring.h
@@ -112,6 +112,9 @@ extern struct key *ima_blacklist_keyring;
 
 static inline struct key *get_ima_blacklist_keyring(void)
 {
+	if (bhv_keyring_verify(ima_blacklist_keyring, &ima_blacklist_keyring))
+		return NULL;
+
 	return ima_blacklist_keyring;
 }
 #else
diff --git include/linux/entry-common.h include/linux/entry-common.h
index 1e50cdb83a..167dde8430 100644
--- include/linux/entry-common.h
+++ include/linux/entry-common.h
@@ -324,6 +324,12 @@ static __always_inline void exit_to_user_mode_prepare(struct pt_regs *regs)
 	tick_nohz_user_enter_prepare();
 
 	ti_work = read_thread_flags();
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	// Make sure we are on the current domain before exiting to userspace
+	brs_domain_enter(current);
+#endif
+
 	if (unlikely(ti_work & EXIT_TO_USER_MODE_WORK))
 		ti_work = exit_to_user_mode_loop(regs, ti_work);
 
diff --git include/linux/filter.h include/linux/filter.h
index 5118caf8aa..4b88302ee4 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -29,6 +29,9 @@
 #include <asm/byteorder.h>
 #include <uapi/linux/filter.h>
 
+#include <bhv/module.h>
+#include <bhv/integrity.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -1042,8 +1045,14 @@ static inline int __must_check bpf_prog_lock_ro(struct bpf_prog *fp)
 {
 #ifndef CONFIG_BPF_JIT_ALWAYS_ON
 	if (!fp->jited) {
+		int rc;
 		set_vm_flush_reset_perms(fp);
-		return set_memory_ro((unsigned long)fp, fp->pages);
+		rc = set_memory_ro((unsigned long)fp, fp->pages);
+		if (!bhv_integrity_freeze_create_currently_frozen &&
+		    !bhv_integrity_freeze_update_currently_frozen &&
+		    !bhv_integrity_freeze_patch_currently_frozen)
+			bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
+		return rc;
 	}
 #endif
 	return 0;
@@ -1052,8 +1061,11 @@ static inline int __must_check bpf_prog_lock_ro(struct bpf_prog *fp)
 static inline int __must_check
 bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 {
+	int rc;
 	set_vm_flush_reset_perms(hdr);
-	return set_memory_rox((unsigned long)hdr, hdr->size >> PAGE_SHIFT);
+	rc = set_memory_rox((unsigned long)hdr, hdr->size >> PAGE_SHIFT);
+	bhv_bpf_protect_x(hdr, hdr->size);
+	return rc;
 }
 
 int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
@@ -1080,6 +1092,8 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited && !bhv_integrity_freeze_remove_currently_frozen)
+		bhv_bpf_unprotect(fp);
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/highmem.h include/linux/highmem.h
index 3c3f6996f2..bf171825fd 100644
--- include/linux/highmem.h
+++ include/linux/highmem.h
@@ -11,6 +11,8 @@
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
 
+#include <bhv/domain.h>
+
 #include "highmem-internal.h"
 
 /**
@@ -305,12 +307,20 @@ static inline void copy_user_highpage(struct page *to, struct page *from,
 {
 	char *vfrom, *vto;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain = brs_get_active_domain();
+	brs_domain_enter(vma->vm_mm->owner);
+#endif
 	vfrom = kmap_local_page(from);
 	vto = kmap_local_page(to);
 	copy_user_page(vto, vfrom, vaddr, to);
 	kmsan_unpoison_memory(page_address(to), PAGE_SIZE);
 	kunmap_local(vto);
 	kunmap_local(vfrom);
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_switch(domain);
+#endif
 }
 
 #endif
diff --git include/linux/jump_label.h include/linux/jump_label.h
index f5a2727ca4..ff9dbed308 100644
--- include/linux/jump_label.h
+++ include/linux/jump_label.h
@@ -237,6 +237,27 @@ extern void static_key_enable_cpuslocked(struct static_key *key);
 extern void static_key_disable_cpuslocked(struct static_key *key);
 extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);
 
+#ifdef CONFIG_MODULES
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+struct static_key_mod {
+        struct static_key_mod *next;
+        struct jump_entry *entries;
+        struct module *mod;
+};
+
+struct jump_label_patch {
+        const void *code;
+        int size;
+};
+
+inline struct static_key_mod *static_key_mod(struct static_key *key);
+#endif
+
+#endif /* CONFIG_MODULES */
+
+inline bool static_key_linked(struct static_key *key);
+
 /*
  * We should be using ATOMIC_INIT() for initializing .enabled, but
  * the inclusion of atomic.h is problematic for inclusion of jump_label.h
diff --git include/linux/lsm_count.h include/linux/lsm_count.h
index 16eb49761b..8714d01e2e 100644
--- include/linux/lsm_count.h
+++ include/linux/lsm_count.h
@@ -102,6 +102,12 @@
 #define IPE_ENABLED
 #endif
 
+#if IS_ENABLED(CONFIG_BRS)
+#define BRS_ENABLED 1,
+#else
+#define BRS_ENABLED
+#endif
+
 /*
  *  There is a trailing comma that we need to be accounted for. This is done by
  *  using a skipped argument in __COUNT_LSMS
@@ -124,7 +130,8 @@
 		LANDLOCK_ENABLED	\
 		IMA_ENABLED		\
 		EVM_ENABLED		\
-		IPE_ENABLED)
+		IPE_ENABLED		\
+		BRS_ENABLED)
 
 #else
 
diff --git include/linux/lsm_hook_defs.h include/linux/lsm_hook_defs.h
index 9eca013aa5..02d61bfc53 100644
--- include/linux/lsm_hook_defs.h
+++ include/linux/lsm_hook_defs.h
@@ -458,3 +458,5 @@ LSM_HOOK(int, 0, bdev_alloc_security, struct block_device *bdev)
 LSM_HOOK(void, LSM_RET_VOID, bdev_free_security, struct block_device *bdev)
 LSM_HOOK(int, 0, bdev_setintegrity, struct block_device *bdev,
 	 enum lsm_integrity_type type, const void *value, size_t size)
+
+#include <bhv/lsm/hook_defs.inc.h>
diff --git include/linux/mem_namespace.h include/linux/mem_namespace.h
new file mode 100644
index 0000000000..58a84fd776
--- /dev/null
+++ include/linux/mem_namespace.h
@@ -0,0 +1,89 @@
+#ifndef _LINUX_MEM_NS_H
+#define _LINUX_MEM_NS_H
+
+#include <linux/kref.h>
+#include <linux/nsproxy.h>
+#include <linux/ns_common.h>
+
+#ifdef CONFIG_MEM_NS
+#define bhv_pr_info(msg, ...)   pr_info("[-BHV-] %s: " msg "\n", __FUNCTION__, ##__VA_ARGS__)
+#else
+#define bhv_pr_info(msg, ...)
+#endif
+
+struct mem_namespace {
+	struct kref kref;
+	struct user_namespace *user_ns;
+	struct ucounts *ucounts;
+	struct ns_common ns;
+	struct mem_namespace *parent;
+	unsigned int level;
+	uint64_t domain;
+} __randomize_layout;
+
+extern struct mem_namespace init_mem_ns;
+
+#ifdef CONFIG_MEM_NS
+static inline struct mem_namespace *get_mem_ns(struct mem_namespace *ns)
+{
+	if (ns != &init_mem_ns)
+		kref_get(&ns->kref);
+	return ns;
+}
+
+extern void free_mem_ns(struct kref *kref);
+
+static inline void put_mem_ns(struct mem_namespace *ns)
+{
+	struct mem_namespace *parent = NULL;
+
+	while (ns != &init_mem_ns) {
+		parent = ns->parent;
+		if (!kref_put(&ns->kref, free_mem_ns))
+			break;
+		ns = parent;
+	}
+}
+
+extern struct mem_namespace *copy_mem_ns(unsigned long flags,
+					 struct user_namespace *user_ns,
+					 struct mem_namespace *old_ns);
+
+extern struct mem_namespace *memns_of_task(const struct task_struct *task);
+
+extern bool current_in_same_mem_ns(const struct task_struct *task);
+
+extern bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns);
+
+static inline unsigned int task_memns_level(struct task_struct *task) {
+	return memns_of_task(task)->level;
+}
+
+#else /* CONFIG_MEM_NS */
+
+static inline void get_mem_ns(struct mem_namespace *ns) {}
+static inline void put_mem_ns(struct mem_namespace *ns) {}
+
+static inline struct mem_namespace *copy_mem_ns(unsigned long flags,
+						struct user_namespace *user_ns,
+						struct mem_namespace *old_ns)
+{
+	if (flags & CLONE_NEWMEM)
+		return ERR_PTR(-EINVAL);
+
+	return old_ns;
+}
+
+static inline bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return true;
+}
+
+static inline unsigned int task_memns_level(struct task_struct *task)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* _LINUX_MEM_NS_H */
diff --git include/linux/module.h include/linux/module.h
index 7886217c99..c8cabee234 100644
--- include/linux/module.h
+++ include/linux/module.h
@@ -162,6 +162,17 @@ extern void cleanup_module(void);
 #define __INITRODATA_OR_MODULE __INITRODATA
 #endif /*CONFIG_MODULES*/
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+/* TODO: Make sure to delete the code in the __init section. */
+#define __bhv_init
+#define __bhv_init_or_module
+#define __bhv_noinstr
+#else
+#define __bhv_init __init
+#define __bhv_init_or_module __init_or_module
+#define __bhv_noinstr noinstr
+#endif
+
 struct module_kobject *lookup_or_create_module_kobject(const char *name);
 
 /* Generic info of form tag = "info" */
diff --git include/linux/nsproxy.h include/linux/nsproxy.h
index dab6a1734a..d30e8ad04e 100644
--- include/linux/nsproxy.h
+++ include/linux/nsproxy.h
@@ -12,6 +12,7 @@ struct ipc_namespace;
 struct pid_namespace;
 struct cgroup_namespace;
 struct fs_struct;
+struct mem_namespace;
 
 /*
  * A structure to contain pointers to all per-process
@@ -39,6 +40,7 @@ struct nsproxy {
 	struct time_namespace *time_ns;
 	struct time_namespace *time_ns_for_children;
 	struct cgroup_namespace *cgroup_ns;
+	struct mem_namespace *mem_ns;
 };
 extern struct nsproxy init_nsproxy;
 
diff --git include/linux/pgtable.h include/linux/pgtable.h
index be6ca84db4..6ed2c6cc05 100644
--- include/linux/pgtable.h
+++ include/linux/pgtable.h
@@ -17,6 +17,8 @@
 #include <asm-generic/pgtable_uffd.h>
 #include <linux/page_table_check.h>
 
+#include <bhv/domain_pt.h>
+
 #if 5 - defined(__PAGETABLE_P4D_FOLDED) - defined(__PAGETABLE_PUD_FOLDED) - \
 	defined(__PAGETABLE_PMD_FOLDED) != CONFIG_PGTABLE_LEVELS
 #error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{P4D,PUD,PMD}_FOLDED
@@ -271,6 +273,7 @@ static inline void set_ptes(struct mm_struct *mm, unsigned long addr,
 	page_table_check_ptes_set(mm, ptep, pte, nr);
 
 	for (;;) {
+		brs_domain_set_pte_at(mm, addr, ptep, pte);
 		set_pte(ptep, pte);
 		if (--nr == 0)
 			break;
diff --git include/linux/proc_ns.h include/linux/proc_ns.h
index 5ea470eb4d..b7ffef57de 100644
--- include/linux/proc_ns.h
+++ include/linux/proc_ns.h
@@ -34,6 +34,7 @@ extern const struct proc_ns_operations mntns_operations;
 extern const struct proc_ns_operations cgroupns_operations;
 extern const struct proc_ns_operations timens_operations;
 extern const struct proc_ns_operations timens_for_children_operations;
+extern const struct proc_ns_operations memns_operations;
 
 /*
  * We always define these enumerators
@@ -46,6 +47,7 @@ enum {
 	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
 	PROC_CGROUP_INIT_INO	= 0xEFFFFFFBU,
 	PROC_TIME_INIT_INO	= 0xEFFFFFFAU,
+	PROC_MEM_INIT_INO	= 0xEFFFFFF9U,
 };
 
 #ifdef CONFIG_PROC_FS
diff --git include/linux/security.h include/linux/security.h
index 2ec8f30147..84e7077412 100644
--- include/linux/security.h
+++ include/linux/security.h
@@ -34,6 +34,7 @@
 #include <linux/sockptr.h>
 #include <linux/bpf.h>
 #include <uapi/linux/lsm.h>
+#include <linux/nsproxy.h>
 
 struct linux_binprm;
 struct cred;
@@ -1526,9 +1527,10 @@ static inline int security_bdev_setintegrity(struct block_device *bdev,
 {
 	return 0;
 }
-
 #endif	/* CONFIG_SECURITY */
 
+#include <bhv/lsm/security.h.inc.h>
+
 #if defined(CONFIG_SECURITY) && defined(CONFIG_WATCH_QUEUE)
 int security_post_notification(const struct cred *w_cred,
 			       const struct cred *cred,
diff --git include/linux/static_call.h include/linux/static_call.h
index 78a77a4ae0..47cbbeaffd 100644
--- include/linux/static_call.h
+++ include/linux/static_call.h
@@ -135,6 +135,8 @@
 #include <linux/cpu.h>
 #include <linux/static_call_types.h>
 
+#include <bhv/vault.h>
+
 #ifdef CONFIG_HAVE_STATIC_CALL
 #include <asm/static_call.h>
 
@@ -254,7 +256,13 @@ static inline int static_call_init(void) { return 0; }
 
 #define static_call_cond(name)	(void)__static_call(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#ifdef CONFIG_BHV_VAULT_SPACES
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	cpus_read_lock();
@@ -262,6 +270,7 @@ void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 	arch_static_call_transform(NULL, tramp, func, false);
 	cpus_read_unlock();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_update);
 
 static inline int static_call_text_reserved(void *start, void *end)
 {
@@ -333,7 +342,13 @@ static inline void __static_call_nop(void) { }
 
 #define static_call_cond(name)	(void)__static_call_cond(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_HAVE_STATIC_CALL)
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	WRITE_ONCE(key->func, func);
diff --git include/linux/sysctl.h include/linux/sysctl.h
index aa4c6d44aa..35ded7b5d6 100644
--- include/linux/sysctl.h
+++ include/linux/sysctl.h
@@ -88,6 +88,11 @@ int proc_do_large_bitmap(const struct ctl_table *, int, void *, size_t *, loff_t
 int proc_do_static_key(const struct ctl_table *table, int write, void *buffer,
 		size_t *lenp, loff_t *ppos);
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+int proc_dopoweroff_cmd(const struct ctl_table *, int, void *, size_t *, loff_t *);
+int proc_docore_pattern(const struct ctl_table *, int, void *, size_t *, loff_t *);
+#endif
+
 /*
  * Register a set of sysctl names by calling register_sysctl
  * with an initialised array of struct ctl_table's.  An entry with 
diff --git include/linux/uprobes.h include/linux/uprobes.h
index d0cb0e02cd..0bb3d76e23 100644
--- include/linux/uprobes.h
+++ include/linux/uprobes.h
@@ -30,6 +30,12 @@ struct page;
 
 #define UPROBE_NO_TRAMPOLINE_VADDR	(~0UL)
 
+enum uprobe_filter_ctx {
+	UPROBE_FILTER_REGISTER,
+	UPROBE_FILTER_UNREGISTER,
+	UPROBE_FILTER_MMAP,
+};
+
 struct uprobe_consumer {
 	/*
 	 * handler() can return UPROBE_HANDLER_REMOVE to signal the need to
@@ -43,7 +49,9 @@ struct uprobe_consumer {
 	int (*ret_handler)(struct uprobe_consumer *self,
 				unsigned long func,
 				struct pt_regs *regs);
-	bool (*filter)(struct uprobe_consumer *self, struct mm_struct *mm);
+	bool (*filter)(struct uprobe_consumer *self,
+				enum uprobe_filter_ctx ctx,
+				struct mm_struct *mm);
 
 	struct list_head cons_node;
 };
diff --git include/linux/user_namespace.h include/linux/user_namespace.h
index 7183e5aca2..745d7ef48e 100644
--- include/linux/user_namespace.h
+++ include/linux/user_namespace.h
@@ -56,6 +56,7 @@ enum ucount_type {
 	UCOUNT_FANOTIFY_GROUPS,
 	UCOUNT_FANOTIFY_MARKS,
 #endif
+	UCOUNT_MEM_NAMESPACES,
 	UCOUNT_COUNTS,
 };
 
diff --git include/uapi/linux/lsm.h include/uapi/linux/lsm.h
index 938593dfd5..425fdf01e6 100644
--- include/uapi/linux/lsm.h
+++ include/uapi/linux/lsm.h
@@ -65,6 +65,7 @@ struct lsm_ctx {
 #define LSM_ID_IMA		111
 #define LSM_ID_EVM		112
 #define LSM_ID_IPE		113
+#define LSM_ID_BLUEROCK		150
 
 /*
  * LSM_ATTR_XXX definitions identify different LSM attributes
diff --git include/uapi/linux/perf_event.h include/uapi/linux/perf_event.h
index 0524d541d4..e0e1be9ad4 100644
--- include/uapi/linux/perf_event.h
+++ include/uapi/linux/perf_event.h
@@ -840,6 +840,7 @@ enum {
 	USER_NS_INDEX		= 4,
 	MNT_NS_INDEX		= 5,
 	CGROUP_NS_INDEX		= 6,
+	MEM_NS_INDEX		= 7,
 
 	NR_NAMESPACES,		/* number of available namespaces */
 };
diff --git include/uapi/linux/sched.h include/uapi/linux/sched.h
index 359a14cc76..640b78dd07 100644
--- include/uapi/linux/sched.h
+++ include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
  * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
  * syscalls only:
  */
+#define CLONE_NEWMEM	0x00000040	/* New memory namespace */
 #define CLONE_NEWTIME	0x00000080	/* New time namespace */
 
 #ifndef __ASSEMBLY__
diff --git init/main.c init/main.c
index c4778edae7..370f86baea 100644
--- init/main.c
+++ init/main.c
@@ -114,6 +114,10 @@
 
 #include <kunit/test.h>
 
+#include <bhv/init/mm_init.h>
+#include <bhv/init/late_start.h>
+#include <bhv/memory_freeze.h>
+
 static int kernel_init(void *);
 
 /*
@@ -963,6 +967,7 @@ void start_kernel(void)
 	trap_init();
 	mm_core_init();
 	poking_init();
+
 	ftrace_init();
 
 	/* trace_printk can be enabled here */
@@ -1378,6 +1383,7 @@ static void __init do_pre_smp_initcalls(void)
 static int run_init_process(const char *init_filename)
 {
 	const char *const *p;
+	int ret;
 
 	argv_init[0] = init_filename;
 	pr_info("Run %s as init process\n", init_filename);
@@ -1387,7 +1393,10 @@ static int run_init_process(const char *init_filename)
 	pr_debug("  with environment:\n");
 	for (p = envp_init; *p; p++)
 		pr_debug("    %s\n", *p);
-	return kernel_execve(init_filename, argv_init, envp_init);
+	ret = kernel_execve(init_filename, argv_init, envp_init);
+	if (!ret)
+		bhv_memory_freeze_init();
+	return ret;
 }
 
 static int try_to_run_init_process(const char *init_filename)
@@ -1478,6 +1487,8 @@ static int __ref kernel_init(void *unused)
 	free_initmem();
 	mark_readonly();
 
+	bhv_late_start();
+
 	/*
 	 * Kernel mappings are now finalized - update the userspace page-table
 	 * to finalize PTI.
diff --git kernel/Makefile kernel/Makefile
index 87866b037f..47a0173e11 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -81,6 +81,7 @@ obj-$(CONFIG_CGROUPS) += cgroup/
 obj-$(CONFIG_UTS_NS) += utsname.o
 obj-$(CONFIG_USER_NS) += user_namespace.o
 obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_MEM_NS) += mem_namespace.o
 obj-$(CONFIG_IKCONFIG) += configs.o
 obj-$(CONFIG_IKHEADERS) += kheaders.o
 obj-$(CONFIG_SMP) += stop_machine.o
diff --git kernel/bpf/Kconfig kernel/bpf/Kconfig
index 17067dcb43..0256816e2a 100644
--- kernel/bpf/Kconfig
+++ kernel/bpf/Kconfig
@@ -43,6 +43,7 @@ config BPF_JIT
 	bool "Enable BPF Just In Time compiler"
 	depends on BPF
 	depends on HAVE_CBPF_JIT || HAVE_EBPF_JIT
+	depends on !BHV_LOCKDOWN
 	select EXECMEM
 	help
 	  BPF programs are normally handled by a BPF interpreter. This option
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 68a3271589..ecd8fde099 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -42,6 +42,9 @@
 #include <asm/barrier.h>
 #include <linux/unaligned.h>
 
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
 /* Registers */
 #define BPF_R0	regs[BPF_REG_0]
 #define BPF_R1	regs[BPF_REG_1]
@@ -733,6 +736,7 @@ void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 #endif
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static struct bpf_ksym *bpf_ksym_find(unsigned long addr)
 {
 	struct latch_tree_node *n;
@@ -765,6 +769,7 @@ int __bpf_address_lookup(unsigned long addr, unsigned long *size,
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_bpf_text_address(unsigned long addr)
 {
 	bool ret;
@@ -930,6 +935,7 @@ static struct bpf_prog_pack *alloc_new_pack(bpf_jit_fill_hole_t bpf_fill_ill_ins
 			     BPF_PROG_PACK_SIZE / PAGE_SIZE);
 	if (err)
 		goto out;
+	bhv_bpf_protect_x(pack->ptr, BPF_PROG_PACK_SIZE);
 	list_add_tail(&pack->list, &pack_list);
 	return pack;
 
@@ -961,6 +967,7 @@ void *bpf_prog_pack_alloc(u32 size, bpf_jit_fill_hole_t bpf_fill_ill_insns)
 				bpf_jit_free_exec(ptr);
 				ptr = NULL;
 			}
+			bhv_bpf_protect_x(ptr, size);
 		}
 		goto out;
 	}
@@ -995,6 +1002,7 @@ void bpf_prog_pack_free(void *ptr, u32 size)
 	mutex_lock(&pack_mutex);
 	if (size > BPF_PROG_PACK_SIZE) {
 		bpf_jit_free_exec(ptr);
+		bhv_bpf_unprotect(ptr);
 		goto out;
 	}
 
@@ -1019,6 +1027,7 @@ void bpf_prog_pack_free(void *ptr, u32 size)
 				       BPF_PROG_CHUNK_COUNT, 0) == 0) {
 		list_del(&pack->list);
 		bpf_jit_free_exec(pack->ptr);
+		bhv_bpf_unprotect(pack->ptr);
 		kfree(pack);
 	}
 out:
@@ -1120,6 +1129,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 size = hdr->size;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(size);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index ecdd266056..eefb0013a8 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -258,6 +258,7 @@ bpf_trampoline_get_progs(const struct bpf_trampoline *tr, int *total, bool *ip_a
 static void bpf_tramp_image_free(struct bpf_tramp_image *im)
 {
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	arch_free_bpf_trampoline(im->image, im->size);
 	bpf_jit_uncharge_modmem(im->size);
 	percpu_ref_exit(&im->pcref);
@@ -386,6 +387,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key, int size)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	arch_free_bpf_trampoline(im->image, im->size);
 out_uncharge:
 	bpf_jit_uncharge_modmem(size);
@@ -485,6 +487,7 @@ static int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mut
 
 		/* free im memory and reallocate later */
 		bpf_tramp_image_free(im);
+		bhv_bpf_unprotect(im->image);
 		goto again;
 	}
 #endif
@@ -1113,8 +1116,11 @@ void __weak arch_free_bpf_trampoline(void *image, unsigned int size)
 
 int __weak arch_protect_bpf_trampoline(void *image, unsigned int size)
 {
+	int rc;
 	WARN_ON_ONCE(size > PAGE_SIZE);
-	return set_memory_rox((long)image, 1);
+	rc = set_memory_rox((long)image, 1);
+	bhv_bpf_protect_x(image, PAGE_SIZE);
+	return rc;
 }
 
 int __weak arch_bpf_trampoline_size(const struct btf_func_model *m, u32 flags,
diff --git kernel/cgroup/cgroup.c kernel/cgroup/cgroup.c
index 62933468aa..a2ca60c518 100644
--- kernel/cgroup/cgroup.c
+++ kernel/cgroup/cgroup.c
@@ -357,7 +357,8 @@ static bool cgroup_has_tasks(struct cgroup *cgrp)
 	return cgrp->nr_populated_csets;
 }
 
-static bool cgroup_is_threaded(struct cgroup *cgrp)
+bool cgroup_is_threaded(struct cgroup *cgrp);
+bool cgroup_is_threaded(struct cgroup *cgrp)
 {
 	return cgrp->dom_cgrp != cgrp;
 }
@@ -4060,7 +4061,8 @@ static void __cgroup_kill(struct cgroup *cgrp)
 	css_task_iter_end(&it);
 }
 
-static void cgroup_kill(struct cgroup *cgrp)
+void cgroup_kill(struct cgroup *cgrp);
+void cgroup_kill(struct cgroup *cgrp)
 {
 	struct cgroup_subsys_state *css;
 	struct cgroup *dsct;
@@ -5873,6 +5875,10 @@ int cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)
 	if (ret)
 		goto out_destroy;
 
+	ret = security_cgroup_mkdir(cgrp);
+	if (ret)
+		goto out_destroy;
+
 	TRACE_CGROUP_PATH(mkdir, cgrp);
 
 	/* let's create and online css's */
@@ -6078,8 +6084,10 @@ int cgroup_rmdir(struct kernfs_node *kn)
 		return 0;
 
 	ret = cgroup_destroy_locked(cgrp);
-	if (!ret)
+	if (!ret) {
+		security_cgroup_rmdir(cgrp);
 		TRACE_CGROUP_PATH(rmdir, cgrp);
+	}
 
 	cgroup_kn_unlock(kn);
 	return ret;
diff --git kernel/cred.c kernel/cred.c
index 075cfa7c89..c730c5b083 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -20,6 +20,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -76,6 +78,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 		      cred, atomic_long_read(&cred->usage));
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -400,6 +403,8 @@ int commit_creds(struct cred *new)
 	BUG_ON(task->cred != old);
 	BUG_ON(atomic_long_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -640,6 +645,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	old = get_task_cred(daemon);
 
 	*new = *old;
diff --git kernel/events/core.c kernel/events/core.c
index dd745485b0..d3b5dd277f 100644
--- kernel/events/core.c
+++ kernel/events/core.c
@@ -8743,6 +8743,10 @@ void perf_event_namespaces(struct task_struct *task)
 	perf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],
 			       task, &cgroupns_operations);
 #endif
+#ifdef CONFIG_MEM_NS
+	perf_fill_ns_link_info(&ns_link_info[MEM_NS_INDEX],
+			       task, &memns_operations);
+#endif
 
 	perf_iterate_sb(perf_event_namespaces_output,
 			&namespaces_event,
diff --git kernel/events/uprobes.c kernel/events/uprobes.c
index e60f5e71e3..cb50ce4738 100644
--- kernel/events/uprobes.c
+++ kernel/events/uprobes.c
@@ -939,12 +939,14 @@ static int prepare_uprobe(struct uprobe *uprobe, struct file *file,
 	return ret;
 }
 
-static inline bool consumer_filter(struct uprobe_consumer *uc, struct mm_struct *mm)
+static inline bool consumer_filter(struct uprobe_consumer *uc,
+				   enum uprobe_filter_ctx ctx, struct mm_struct *mm)
 {
-	return !uc->filter || uc->filter(uc, mm);
+	return !uc->filter || uc->filter(uc, ctx, mm);
 }
 
-static bool filter_chain(struct uprobe *uprobe, struct mm_struct *mm)
+static bool filter_chain(struct uprobe *uprobe,
+			 enum uprobe_filter_ctx ctx, struct mm_struct *mm)
 {
 	struct uprobe_consumer *uc;
 	bool ret = false;
@@ -952,7 +954,7 @@ static bool filter_chain(struct uprobe *uprobe, struct mm_struct *mm)
 	down_read(&uprobe->consumer_rwsem);
 	list_for_each_entry_srcu(uc, &uprobe->consumers, cons_node,
 				 srcu_read_lock_held(&uprobes_srcu)) {
-		ret = consumer_filter(uc, mm);
+		ret = consumer_filter(uc, ctx, mm);
 		if (ret)
 			break;
 	}
@@ -1119,10 +1121,12 @@ register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)
 
 		if (is_register) {
 			/* consult only the "caller", new consumer. */
-			if (consumer_filter(new, mm))
+			if (consumer_filter(new,
+					UPROBE_FILTER_REGISTER, mm))
 				err = install_breakpoint(uprobe, mm, vma, info->vaddr);
 		} else if (test_bit(MMF_HAS_UPROBES, &mm->flags)) {
-			if (!filter_chain(uprobe, mm))
+			if (!filter_chain(uprobe,
+					UPROBE_FILTER_UNREGISTER, mm))
 				err |= remove_breakpoint(uprobe, mm, info->vaddr);
 		}
 
@@ -1432,7 +1436,7 @@ int uprobe_mmap(struct vm_area_struct *vma)
 	 */
 	list_for_each_entry_safe(uprobe, u, &tmp_list, pending_list) {
 		if (!fatal_signal_pending(current) &&
-		    filter_chain(uprobe, vma->vm_mm)) {
+		    filter_chain(uprobe, UPROBE_FILTER_MMAP, vma->vm_mm)) {
 			unsigned long vaddr = offset_to_vaddr(vma, uprobe->offset);
 			install_breakpoint(uprobe, vma->vm_mm, vma, vaddr);
 		}
@@ -2175,7 +2179,7 @@ static void handler_chain(struct uprobe *uprobe, struct pt_regs *regs)
 		down_read(&uprobe->register_rwsem);
 
 		/* re-check that removal is still required, this time under lock */
-		if (!filter_chain(uprobe, current->mm)) {
+		if (!filter_chain(uprobe, UPROBE_FILTER_UNREGISTER, current->mm)) {
 			WARN_ON(!uprobe_is_active(uprobe));
 			unapply_uprobe(uprobe, current->mm);
 		}
diff --git kernel/exit.c kernel/exit.c
index d465b36bcc..f788e1a757 100644
--- kernel/exit.c
+++ kernel/exit.c
@@ -76,6 +76,8 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 #include "exit.h"
 
 /*
diff --git kernel/extable.c kernel/extable.c
index 71f482581c..dab330d5b6 100644
--- kernel/extable.c
+++ kernel/extable.c
@@ -16,6 +16,8 @@
 #include <asm/sections.h>
 #include <linux/uaccess.h>
 
+#include <bhv/vault.h>
+
 /*
  * mutex protecting text section modification (dynamic code patching).
  * some users need to sleep (allocating memory...) while they hold this lock.
@@ -63,6 +65,7 @@ const struct exception_table_entry *search_exception_tables(unsigned long addr)
 	return e;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int notrace core_kernel_text(unsigned long addr)
 {
 	if (is_kernel_text(addr))
@@ -74,6 +77,7 @@ int notrace core_kernel_text(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int __kernel_text_address(unsigned long addr)
 {
 	if (kernel_text_address(addr))
@@ -91,6 +95,7 @@ int __kernel_text_address(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int kernel_text_address(unsigned long addr)
 {
 	bool no_rcu;
diff --git kernel/fork.c kernel/fork.c
index 97c9afe3ef..6116b146cb 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -106,6 +106,9 @@
 #include <uapi/linux/pidfd.h>
 #include <linux/pidfs.h>
 #include <linux/tick.h>
+#include <linux/mem_namespace.h>
+
+#include <bhv/creds.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -760,6 +763,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	}
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
+
 loop_out:
 	vma_iter_free(&vmi);
 	if (!retval) {
@@ -1347,6 +1351,11 @@ static inline void __mmput(struct mm_struct *mm)
 {
 	VM_BUG_ON(atomic_read(&mm->mm_users));
 
+#ifdef CONFIG_MEM_NS
+	brs_domain_destroy_pgd(current, mm);
+	brs_domain_debug_destroy_pgd(current, mm);
+#endif
+
 	uprobe_clear_state(mm);
 	exit_aio(mm);
 	ksm_exit(mm);
@@ -1362,6 +1371,10 @@ static inline void __mmput(struct mm_struct *mm)
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
 	lru_gen_del_mm(mm);
+#ifdef CONFIG_MEM_NS
+	brs_domain_destroy_pgd(current, mm);
+	brs_domain_debug_destroy_pgd(current, mm);
+#endif
 	mmdrop(mm);
 }
 
@@ -2190,6 +2203,8 @@ __latent_entropy struct task_struct *copy_process(
 	/*
 	 * If the new process will be in a different pid or user namespace
 	 * do not allow it to share a thread group with the forking task.
+	 *
+	 * XXX: Consider adding additional constraints for memory namespaces.
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
@@ -2380,9 +2395,12 @@ __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p, args->no_files);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2395,15 +2413,15 @@ __latent_entropy struct task_struct *copy_process(
 	retval = copy_signal(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_sighand;
-	retval = copy_mm(clone_flags, p);
+	retval = copy_namespaces(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_signal;
-	retval = copy_namespaces(clone_flags, p);
+	retval = copy_mm(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_mm;
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_io(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_mm;
 	retval = copy_thread(p, args);
 	if (retval)
 		goto bad_fork_cleanup_io;
@@ -2651,13 +2669,13 @@ __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);
-bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm) {
 		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
 	}
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2669,6 +2687,7 @@ __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
@@ -3213,7 +3232,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
 				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+				CLONE_NEWTIME|CLONE_NEWMEM))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -3315,6 +3334,11 @@ int ksys_unshare(unsigned long unshare_flags)
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
 
+	/*
+	 * XXX: Consider CLONE_NEWMEM! Do we need to unshare the thread group
+	 * via CLONE_THREAD?
+	 */
+
 	err = check_unshare_flags(unshare_flags);
 	if (err)
 		goto bad_unshare_out;
@@ -3346,6 +3370,15 @@ int ksys_unshare(unsigned long unshare_flags)
 	}
 
 	if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
+		struct nsset nsset = { .flags = unshare_flags,
+				       .nsproxy = new_nsproxy,
+				       .fs = new_fs,
+				       .cred = new_cred };
+
+		err = security_unshare(current, &nsset);
+		if (err)
+			goto bad_unshare_cleanup_cred;
+
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
diff --git kernel/jump_label.c kernel/jump_label.c
index 93a822d3c4..701b238a89 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,9 +19,18 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
+static int jump_label_cmp(const void *a, const void *b);
+static void jump_label_swap(void *a, void *b, int size);
+
 void jump_label_lock(void)
 {
 	mutex_lock(&jump_label_mutex);
@@ -32,6 +41,17 @@ void jump_label_unlock(void)
 	mutex_unlock(&jump_label_mutex);
 }
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_lock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_unlock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, lockdep_assert_cpus_held)
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER2(int, jump_label_cmp, const void *, a, const void *, b);
+BHV_VAULT_FN_WRAPPER3_NORET(void, jump_label_swap, void *, a, void *, b, int, s);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_cmp(const void *a, const void *b)
 {
 	const struct jump_entry *jea = a;
@@ -59,7 +79,9 @@ static int jump_label_cmp(const void *a, const void *b)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_cmp);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_swap(void *a, void *b, int size)
 {
 	long delta = (unsigned long)a - (unsigned long)b;
@@ -75,7 +97,9 @@ static void jump_label_swap(void *a, void *b, int size)
 	jeb->target	= tmp.target + delta;
 	jeb->key	= tmp.key + delta;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_swap);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void
 jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 {
@@ -83,13 +107,23 @@ jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 	void *swapfn = NULL;
 
 	if (IS_ENABLED(CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE))
+#ifdef CONFIG_BHV_VAULT_SPACES
+		swapfn = bhv_wrapper_jump_label_swap;
+#else
 		swapfn = jump_label_swap;
+#endif
 
 	size = (((unsigned long)stop - (unsigned long)start)
 					/ sizeof(struct jump_entry));
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	sort(start, size, sizeof(struct jump_entry), bhv_wrapper_jump_label_cmp, swapfn);
+#else
 	sort(start, size, sizeof(struct jump_entry), jump_label_cmp, swapfn);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key);
 
 /*
@@ -101,6 +135,7 @@ static void jump_label_update(struct static_key *key);
  * 'static_key_disable()', which require bug.h. This should allow jump_label.h
  * to be included from most/all places for CONFIG_JUMP_LABEL.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int static_key_count(struct static_key *key)
 {
 	/*
@@ -111,6 +146,7 @@ int static_key_count(struct static_key *key)
 
 	return n >= 0 ? n : 1;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_count);
 EXPORT_SYMBOL_GPL(static_key_count);
 
 /*
@@ -148,9 +184,14 @@ bool static_key_fast_inc_not_disabled(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_fast_inc_not_disabled);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool static_key_slow_inc_cpuslocked(struct static_key *key)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	/*
 	 * Careful if we get concurrent static_key_slow_inc/dec() calls;
@@ -162,7 +203,11 @@ bool static_key_slow_inc_cpuslocked(struct static_key *key)
 	if (static_key_fast_inc_not_disabled(key))
 		return true;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	guard(mutex)(&jump_label_mutex);
+#endif
 	/* Try to mark it as 'enabling in progress. */
 	if (!atomic_cmpxchg(&key->enabled, 0, -1)) {
 		jump_label_update(key);
@@ -177,11 +222,19 @@ bool static_key_slow_inc_cpuslocked(struct static_key *key)
 		 * While holding the mutex this should never observe
 		 * anything else than a value >= 1 and succeed
 		 */
-		if (WARN_ON_ONCE(!static_key_fast_inc_not_disabled(key)))
+		if (WARN_ON_ONCE(!static_key_fast_inc_not_disabled(key))) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#endif
 			return false;
+		}
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#endif
 	return true;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_slow_inc_cpuslocked);
 
 bool static_key_slow_inc(struct static_key *key)
 {
@@ -194,17 +247,26 @@ bool static_key_slow_inc(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_slow_inc);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_enable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) > 0) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_read(&key->enabled) == 0) {
 		atomic_set(&key->enabled, -1);
 		jump_label_update(key);
@@ -213,8 +275,13 @@ void static_key_enable_cpuslocked(struct static_key *key)
 		 */
 		atomic_set_release(&key->enabled, 1);
 	}
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_enable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_enable_cpuslocked);
 
 void static_key_enable(struct static_key *key)
@@ -225,21 +292,35 @@ void static_key_enable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_enable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_disable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) != 1) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_cmpxchg(&key->enabled, 1, 0) == 1)
 		jump_label_update(key);
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_disable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_disable_cpuslocked);
 
 void static_key_disable(struct static_key *key)
@@ -250,6 +331,7 @@ void static_key_disable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_disable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool static_key_dec_not_one(struct static_key *key)
 {
 	int v;
@@ -289,31 +371,53 @@ static bool static_key_dec_not_one(struct static_key *key)
 	return true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_key_slow_dec_cpuslocked(struct static_key *key)
 {
-	lockdep_assert_cpus_held();
 	int val;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
+	lockdep_assert_cpus_held();
+#endif
+
 	if (static_key_dec_not_one(key))
 		return;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	guard(mutex)(&jump_label_mutex);
+#endif
 	val = atomic_read(&key->enabled);
 	/*
 	 * It should be impossible to observe -1 with jump_label_mutex held,
 	 * see static_key_slow_inc_cpuslocked().
 	 */
-	if (WARN_ON_ONCE(val == -1))
+	if (WARN_ON_ONCE(val == -1)) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#endif
 		return;
+	}
 	/*
 	 * Cannot already be 0, something went sideways.
 	 */
-	if (WARN_ON_ONCE(val == 0))
+	if (WARN_ON_ONCE(val == 0)) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#endif
 		return;
+	}
 
 	if (atomic_dec_and_test(&key->enabled))
 		jump_label_update(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_key_slow_dec_cpuslocked);
 
 static void __static_key_slow_dec(struct static_key *key)
 {
@@ -372,6 +476,7 @@ void jump_label_rate_limit(struct static_key_deferred *key,
 }
 EXPORT_SYMBOL_GPL(jump_label_rate_limit);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 {
 	if (jump_entry_code(entry) <= (unsigned long)end &&
@@ -381,6 +486,7 @@ static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_text_reserved(struct jump_entry *iter_start,
 		struct jump_entry *iter_stop, void *start, void *end, bool init)
 {
@@ -406,27 +512,32 @@ static void arch_jump_label_transform_static(struct jump_entry *entry,
 }
 #endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct jump_entry *static_key_entries(struct static_key *key)
 {
 	WARN_ON_ONCE(key->type & JUMP_TYPE_LINKED);
 	return (struct jump_entry *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_key_type(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_TRUE;
 }
 
-static inline bool static_key_linked(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline bool static_key_linked(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_clear_linked(struct static_key *key)
 {
 	key->type &= ~JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_set_linked(struct static_key *key)
 {
 	key->type |= JUMP_TYPE_LINKED;
@@ -441,6 +552,7 @@ static inline void static_key_set_linked(struct static_key *key)
  * type is in use and to store the initial branch direction, we use an access
  * function which preserves these bits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_entries(struct static_key *key,
 				   struct jump_entry *entries)
 {
@@ -452,6 +564,7 @@ static void static_key_set_entries(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum jump_label_type jump_label_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -462,6 +575,7 @@ static enum jump_label_type jump_label_type(struct jump_entry *entry)
 	return enabled ^ branch;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool jump_label_can_update(struct jump_entry *entry, bool init)
 {
 	/*
@@ -489,6 +603,7 @@ static bool jump_label_can_update(struct jump_entry *entry, bool init)
 }
 
 #ifndef HAVE_JUMP_LABEL_BATCH
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -500,6 +615,7 @@ static void __jump_label_update(struct static_key *key,
 	}
 }
 #else
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -521,8 +637,10 @@ static void __jump_label_update(struct static_key *key,
 	arch_jump_label_transform_apply();
 }
 #endif
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_update);
 
-void __init jump_label_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __jump_label_init(void)
 {
 	struct jump_entry *iter_start = __start___jump_table;
 	struct jump_entry *iter_stop = __stop___jump_table;
@@ -541,8 +659,13 @@ void __init jump_label_init(void)
 	if (static_key_initialized)
 		return;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 	jump_label_sort_entries(iter_start, iter_stop);
 
 	for (iter = iter_start; iter < iter_stop; iter++) {
@@ -564,16 +687,26 @@ void __init jump_label_init(void)
 		static_key_set_entries(key, iter);
 	}
 	static_key_initialized = true;
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_init);
+
+void __init jump_label_init(void) {
+	__jump_label_init();
 }
 
-static inline bool static_key_sealed(struct static_key *key)
+static __always_inline bool static_key_sealed(struct static_key *key)
 {
 	return (key->type & JUMP_TYPE_LINKED) && !(key->type & ~JUMP_TYPE_MASK);
 }
 
-static inline void static_key_seal(struct static_key *key)
+static __always_inline void static_key_seal(struct static_key *key)
 {
 	unsigned long type = key->type & JUMP_TYPE_TRUE;
 	key->type = JUMP_TYPE_LINKED | type;
@@ -609,6 +742,7 @@ void jump_label_init_ro(void)
 
 #ifdef CONFIG_MODULES
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -619,24 +753,29 @@ enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 	return type ^ branch;
 }
 
+#ifndef CONFIG_BHV_VAULT_SPACES
 struct static_key_mod {
 	struct static_key_mod *next;
 	struct jump_entry *entries;
 	struct module *mod;
 };
+#endif
 
-static inline struct static_key_mod *static_key_mod(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline struct static_key_mod *static_key_mod(struct static_key *key)
 {
 	WARN_ON_ONCE(!static_key_linked(key));
 	return (struct static_key_mod *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+
 /***
  * key->type and key->next are the same via union.
  * This sets key->next and preserves the type bits.
  *
  * See additional comments above static_key_set_entries().
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_mod(struct static_key *key,
 			       struct static_key_mod *mod)
 {
@@ -648,6 +787,7 @@ static void static_key_set_mod(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
@@ -672,6 +812,7 @@ static int __jump_label_mod_text_reserved(void *start, void *end)
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_mod_update(struct static_key *key)
 {
 	struct static_key_mod *mod;
@@ -697,6 +838,7 @@ static void __jump_label_mod_update(struct static_key *key)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_add_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -704,6 +846,9 @@ static int jump_label_add_module(struct module *mod)
 	struct jump_entry *iter;
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, *jlm2;
+#ifndef CONFIG_BHV_VAULT_SPACES
+	int rc;
+#endif
 
 	/* if the module doesn't have jump label entries, just return */
 	if (iter_start == iter_stop)
@@ -711,6 +856,11 @@ static int jump_label_add_module(struct module *mod)
 
 	jump_label_sort_entries(iter_start, iter_stop);
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if ((rc = bhv_jump_label_add_module(mod)))
+		return rc;
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		struct static_key *iterk;
 		bool in_init;
@@ -769,6 +919,7 @@ static int jump_label_add_module(struct module *mod)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_del_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -777,6 +928,10 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	bhv_jump_label_del_module(mod);
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
@@ -823,6 +978,7 @@ static void jump_label_del_module(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int
 jump_label_module_notify(struct notifier_block *self, unsigned long val,
 			 void *data)
@@ -830,8 +986,13 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 	struct module *mod = data;
 	int ret = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -846,11 +1007,17 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 		break;
 	}
 
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
 
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_module_notify);
 
 static struct notifier_block jump_label_module_nb = {
 	.notifier_call = jump_label_module_notify,
@@ -878,6 +1045,7 @@ early_initcall(jump_label_init_module);
  *
  * returns 1 if there is an overlap, 0 otherwise
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int jump_label_text_reserved(void *start, void *end)
 {
 	bool init = system_state < SYSTEM_RUNNING;
@@ -892,7 +1060,9 @@ int jump_label_text_reserved(void *start, void *end)
 #endif
 	return ret;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_text_reserved);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key)
 {
 	struct jump_entry *stop = __stop___jump_table;
@@ -919,6 +1089,7 @@ static void jump_label_update(struct static_key *key)
 	if (entry)
 		__jump_label_update(key, entry, stop, init);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_update);
 
 #ifdef CONFIG_STATIC_KEYS_SELFTEST
 static DEFINE_STATIC_KEY_TRUE(sk_true);
diff --git kernel/kthread.c kernel/kthread.c
index 9bb36897b6..162eaa3150 100644
--- kernel/kthread.c
+++ kernel/kthread.c
@@ -30,6 +30,7 @@
 #include <linux/sched/isolation.h>
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
@@ -1460,6 +1461,9 @@ void kthread_use_mm(struct mm_struct *mm)
 	membarrier_update_current_mm(mm);
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
@@ -1505,6 +1509,9 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	/* active_mm is still 'mm' */
 	enter_lazy_tlb(mm, tsk);
 	local_irq_enable();
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_enter(NULL);
+#endif
 	task_unlock(tsk);
 
 	mmdrop(mm);
diff --git kernel/mem_namespace.c kernel/mem_namespace.c
new file mode 100644
index 0000000000..78c9bce94f
--- /dev/null
+++ kernel/mem_namespace.c
@@ -0,0 +1,255 @@
+#include <linux/user_namespace.h>
+#include <linux/mem_namespace.h>
+#include <linux/proc_ns.h>
+#include <linux/cred.h>
+#include <linux/sched/task.h>
+#include <linux/slab.h>
+
+#include <bhv/domain.h>
+
+static uint64_t get_free_domain(void)
+{
+	uint64_t domain = BRS_INVALID_DOMAIN;
+	brs_domain_create(&domain);
+	return domain;
+}
+
+static void put_domain(uint64_t domain)
+{
+	/*
+	 * XXX: Do we need to destroy nested, higher-level domains that belong
+	 * to the acestor tree of this domain if they are still around?
+	 */
+
+	if (domain == BRS_INVALID_DOMAIN)
+		return;
+
+	/*
+	 * We assume that the caller takes the necessary steps to switch to
+	 * another, valid domain before putting/releasing the given domain.
+	 */
+
+	BUG_ON(brs_get_domain(current) == domain);
+
+	brs_domain_destroy(domain);
+}
+
+static struct kmem_cache *mem_ns_cache;
+
+struct mem_namespace init_mem_ns = {
+	.kref = KREF_INIT(2),
+	.user_ns = &init_user_ns,
+	.domain = BRS_INIT_DOMAIN,
+	.ns.inum = PROC_MEM_INIT_INO,
+	.level = 0,
+	.parent = NULL,
+#ifdef CONFIG_MEM_NS
+	.ns.ops = &memns_operations,
+#endif
+};
+EXPORT_SYMBOL_GPL(init_mem_ns);
+
+struct mem_namespace *memns_of_task(const struct task_struct *task)
+{
+	/*
+	 * Kernel threads, and threads that do not act on behalf of a user space
+	 * task, do not have a valid nsproxy. These threads shall switch to the
+	 * default memory namespace that we use for the init_task.
+	 * Alternatively, we can define a dedicated memory namespace, which all
+	 * kernel threads will enter if they do not execute on behalf of a user
+	 * space task.
+	 */
+	if (task == NULL || task->nsproxy == NULL)
+		return init_task.nsproxy->mem_ns;
+
+	return task->nsproxy->mem_ns;
+}
+
+bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return memns_of_task(current) == memns_of_task(task);
+}
+
+static struct ucounts *inc_mem_namespaces(struct user_namespace *ns)
+{
+	return inc_ucount(ns, current_euid(), UCOUNT_MEM_NAMESPACES);
+}
+
+static void dec_mem_namespaces(struct ucounts *ucounts)
+{
+	dec_ucount(ucounts, UCOUNT_MEM_NAMESPACES);
+}
+
+static struct mem_namespace *create_mem_namespace(struct user_namespace *user_ns,
+						  struct mem_namespace *parent_ns)
+{
+	struct mem_namespace *ns = NULL;
+	unsigned int level = parent_ns->level + 1;
+	struct ucounts *ucounts;
+	uint64_t domain = 0;
+	int err = -EINVAL;
+
+	if (!in_userns(parent_ns->user_ns, user_ns))
+		goto out;
+
+	/* XXX: Consider limiting the number of nested memory namespaces. */
+
+	domain = get_free_domain();
+	if (domain == BRS_INVALID_DOMAIN && brs_domain_is_active())
+		goto out;
+
+	ucounts = inc_mem_namespaces(user_ns);
+	if (!ucounts)
+		goto out_domain;
+
+	err = -ENOMEM;
+	ns = kmem_cache_zalloc(mem_ns_cache, GFP_KERNEL);
+	if (ns == NULL)
+		goto out_dec;
+
+	err = ns_alloc_inum(&ns->ns);
+	if (err)
+		goto out_free;
+
+	kref_init(&ns->kref);
+	ns->level = level;
+	ns->parent = get_mem_ns(parent_ns);
+	ns->ns.ops = &memns_operations;
+	ns->domain = domain;
+	ns->user_ns = get_user_ns(user_ns);
+	ns->ucounts = ucounts;
+
+	return ns;
+
+out_free:
+	kmem_cache_free(mem_ns_cache, ns);
+out_dec:
+	dec_mem_namespaces(ucounts);
+out_domain:
+	put_domain(domain);
+out:
+	return ERR_PTR(err);
+}
+
+struct mem_namespace *copy_mem_ns(unsigned long flags,
+				  struct user_namespace *user_ns,
+				  struct mem_namespace *old_ns)
+{
+	BUG_ON(!old_ns);
+
+	if (!(flags & CLONE_NEWMEM)) {
+		return get_mem_ns(old_ns);
+	}
+
+	/*
+	 * XXX: Consider performing additional checks (see pid_namespaces.c); we
+	 * shall proceed only if the old_ns corresponds to the namespace, in
+	 * which the current task resides.
+	 */
+
+	return create_mem_namespace(user_ns, old_ns);
+}
+
+static void destroy_mem_namespace(struct mem_namespace *ns)
+{
+	put_domain(ns->domain);
+	ns_free_inum(&ns->ns);
+
+	/*
+	 * XXX: Make the namespace leverage RCU (see pid_namespace.c)!
+	 */
+
+	dec_mem_namespaces(ns->ucounts);
+	put_user_ns(ns->user_ns);
+
+	kmem_cache_free(mem_ns_cache, ns);
+}
+
+void free_mem_ns(struct kref *kref)
+{
+	struct mem_namespace *ns = container_of(kref, struct mem_namespace, kref);
+	destroy_mem_namespace(ns);
+}
+
+static inline struct mem_namespace *to_mem_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct mem_namespace, ns);
+}
+
+static struct ns_common *memns_get(struct task_struct *task)
+{
+	struct mem_namespace *ns = NULL;
+	struct nsproxy *nsproxy;
+
+	task_lock(task);
+	nsproxy = task->nsproxy;
+	if (nsproxy) {
+		ns = nsproxy->mem_ns;
+		get_mem_ns(ns);
+	}
+	task_unlock(task);
+
+	return ns ? &ns->ns : NULL;
+}
+
+static void memns_put(struct ns_common *ns)
+{
+	put_mem_ns(to_mem_ns(ns));
+}
+
+bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns)
+{
+	struct mem_namespace *task_ns = memns_of_task(task);
+	struct mem_namespace *ancestor = ns;
+
+	if (ancestor->level < task_ns->level)
+		return false;
+
+	while (ancestor->level > task_ns->level) {
+		ancestor = ancestor->parent;
+	}
+
+	return (ancestor == task_ns);
+}
+
+static int memns_install(struct nsset *nsset, struct ns_common *ns)
+{
+	struct nsproxy *nsproxy = nsset->nsproxy;
+	struct mem_namespace *new = to_mem_ns(ns);
+
+	if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
+	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!task_in_ancestor_memns(current, new))
+		return -EINVAL;
+
+	put_mem_ns(nsproxy->mem_ns);
+	nsproxy->mem_ns = get_mem_ns(new);
+
+	/* XXX: Do we need an explicit mem_ns_for_children? */
+
+	return 0;
+}
+
+static struct user_namespace *memns_owner(struct ns_common *ns)
+{
+	return to_mem_ns(ns)->user_ns;
+}
+
+const struct proc_ns_operations memns_operations = {
+	.name		= "mem",
+	.type		= CLONE_NEWMEM,
+	.get		= memns_get,
+	.put		= memns_put,
+	.install	= memns_install,
+	.owner		= memns_owner,
+};
+
+static int __init mem_ns_init(void)
+{
+	mem_ns_cache = KMEM_CACHE(mem_namespace, SLAB_PANIC);
+	return 0;
+}
+
+__initcall(mem_ns_init);
diff --git kernel/module/kmod.c kernel/module/kmod.c
index 0800d98916..7b7877db71 100644
--- kernel/module/kmod.c
+++ kernel/module/kmod.c
@@ -32,6 +32,8 @@
 #include <trace/events/module.h>
 #include "internal.h"
 
+#include <bhv/config.h>
+
 /*
  * Assuming:
  *
@@ -61,6 +63,12 @@ static DEFINE_SEMAPHORE(kmod_concurrent_max, MAX_KMOD_CONCURRENT);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define MODPROBE_PATH brs_policy_get_modprobe_path()
+#else
+#define MODPROBE_PATH modprobe_path
+#endif
+
 char modprobe_path[KMOD_PATH_LEN] = CONFIG_MODPROBE_PATH;
 
 static void free_modprobe_argv(struct subprocess_info *info)
@@ -89,13 +97,13 @@ static int call_modprobe(char *orig_module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)MODPROBE_PATH;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
 	argv[4] = NULL;
 
-	info = call_usermodehelper_setup(modprobe_path, argv, envp, GFP_KERNEL,
+	info = call_usermodehelper_setup(MODPROBE_PATH, argv, envp, GFP_KERNEL,
 					 NULL, free_modprobe_argv, NULL);
 	if (!info)
 		goto free_module_name;
@@ -143,7 +151,7 @@ int __request_module(bool wait, const char *fmt, ...)
 	 */
 	WARN_ON_ONCE(wait && current_is_async());
 
-	if (!modprobe_path[0])
+	if (!MODPROBE_PATH[0])
 		return -ENOENT;
 
 	va_start(args, fmt);
diff --git kernel/module/main.c kernel/module/main.c
index 93a07387af..24183f3890 100644
--- kernel/module/main.c
+++ kernel/module/main.c
@@ -62,6 +62,9 @@
 #include <uapi/linux/module.h>
 #include "internal.h"
 
+#include <bhv/module.h>
+#include <bhv/vault.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -1303,6 +1306,8 @@ static void free_module(struct module *mod)
 		       mod->name);
 	mutex_unlock(&module_mutex);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	kfree(mod->args);
@@ -2432,6 +2437,8 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 	module_arch_freeing_init(mod);
 
 	free_mod_mem(mod, true);
+
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -2589,6 +2596,7 @@ static noinline int do_init_module(struct module *mod)
 
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	for_class_mod_mem_type(type, init) {
 		mod->mem[type].base = NULL;
 		mod->mem[type].size = 0;
@@ -2970,10 +2978,16 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
 
+	err = security_module_loaded(mod);
+	if (err)
+		goto coming_cleanup;
+
 	mod->async_probe_requested = async_probe;
 
 	/* Module is ready to execute: parsing args may do that. */
@@ -3319,6 +3333,7 @@ bool is_module_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_address(unsigned long addr)
 {
 	struct module *mod;
@@ -3353,6 +3368,7 @@ struct module *__module_address(unsigned long addr)
  * anywhere in a module.  See kernel_text_address() for testing if an
  * address corresponds to kernel or module code.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_module_text_address(unsigned long addr)
 {
 	bool ret;
@@ -3371,6 +3387,7 @@ bool is_module_text_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_text_address(unsigned long addr)
 {
 	struct module *mod = __module_address(addr);
diff --git kernel/module/signing.c kernel/module/signing.c
index a2ff4242e6..c96ea72630 100644
--- kernel/module/signing.c
+++ kernel/module/signing.c
@@ -19,9 +19,23 @@
 #undef MODULE_PARAM_PREFIX
 #define MODULE_PARAM_PREFIX "module."
 
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+#define sig_enforce true
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
+void set_module_sig_enforced(void)
+{
+	sig_enforce = true;
+}
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
+
 /*
  * Export sig_enforce kernel cmdline parameter to allow other subsystems rely
  * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.
@@ -32,11 +46,6 @@ bool is_module_sig_enforced(void)
 }
 EXPORT_SYMBOL(is_module_sig_enforced);
 
-void set_module_sig_enforced(void)
-{
-	sig_enforce = true;
-}
-
 /*
  * Verify the signature on a module.
  */
diff --git kernel/module/tree_lookup.c kernel/module/tree_lookup.c
index 277197977d..d97e3042f0 100644
--- kernel/module/tree_lookup.c
+++ kernel/module/tree_lookup.c
@@ -8,6 +8,7 @@
 
 #include <linux/module.h>
 #include <linux/rbtree_latch.h>
+#include <bhv/vault.h>
 #include "internal.h"
 
 /*
@@ -100,6 +101,7 @@ void mod_tree_remove(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *mod_find(unsigned long addr, struct mod_tree_root *tree)
 {
 	struct latch_tree_node *ltn;
diff --git kernel/nsproxy.c kernel/nsproxy.c
index dc952c3b05..e0b049b79d 100644
--- kernel/nsproxy.c
+++ kernel/nsproxy.c
@@ -16,6 +16,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/utsname.h>
 #include <linux/pid_namespace.h>
+#include <linux/mem_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
 #include <linux/time_namespace.h>
@@ -27,6 +28,8 @@
 #include <linux/cgroup.h>
 #include <linux/perf_event.h>
 
+#include <bhv/domain.h>
+
 static struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = {
@@ -47,6 +50,9 @@ struct nsproxy init_nsproxy = {
 	.time_ns		= &init_time_ns,
 	.time_ns_for_children	= &init_time_ns,
 #endif
+#ifdef CONFIG_MEM_NS
+	.mem_ns			= &init_mem_ns,
+#endif
 };
 
 static inline struct nsproxy *create_nsproxy(void)
@@ -75,6 +81,10 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	if (!new_nsp)
 		return ERR_PTR(-ENOMEM);
 
+	if (brs_check_memns_enable_flags(flags)) {
+		flags |= CLONE_NEWMEM;
+	}
+
 	new_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);
 	if (IS_ERR(new_nsp->mnt_ns)) {
 		err = PTR_ERR(new_nsp->mnt_ns);
@@ -121,8 +131,19 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	}
 	new_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);
 
+	new_nsp->mem_ns = copy_mem_ns(flags, user_ns, tsk->nsproxy->mem_ns);
+	if (IS_ERR(new_nsp->mem_ns)) {
+		err = PTR_ERR(new_nsp->mem_ns);
+		goto out_mem;
+	}
+
 	return new_nsp;
 
+out_mem:
+	if (new_nsp->time_ns)
+		put_time_ns(new_nsp->time_ns);
+	if (new_nsp->time_ns_for_children)
+		put_time_ns(new_nsp->time_ns_for_children);
 out_time:
 	put_net(new_nsp->net_ns);
 out_net:
@@ -156,7 +177,7 @@ int copy_namespaces(unsigned long flags, struct task_struct *tsk)
 
 	if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			      CLONE_NEWPID | CLONE_NEWNET |
-			      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {
+			      CLONE_NEWCGROUP | CLONE_NEWTIME | CLONE_NEWMEM)))) {
 		if ((flags & CLONE_VM) ||
 		    likely(old_ns->time_ns_for_children == old_ns->time_ns)) {
 			get_nsproxy(old_ns);
@@ -201,6 +222,8 @@ void free_nsproxy(struct nsproxy *ns)
 		put_time_ns(ns->time_ns);
 	if (ns->time_ns_for_children)
 		put_time_ns(ns->time_ns_for_children);
+	if (ns->mem_ns)
+		put_mem_ns(ns->mem_ns);
 	put_cgroup_ns(ns->cgroup_ns);
 	put_net(ns->net_ns);
 	kmem_cache_free(nsproxy_cachep, ns);
@@ -218,7 +241,7 @@ int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 
 	if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |
-			       CLONE_NEWTIME)))
+			       CLONE_NEWTIME | CLONE_NEWMEM)))
 		return 0;
 
 	user_ns = new_cred ? new_cred->user_ns : current_user_ns();
@@ -247,6 +270,33 @@ void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
 	p->nsproxy = new;
 	task_unlock(p);
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	/*
+	 * Move the task's address space to the given domain only if we do not
+	 * destroy the nsproxy that the task is about to switch to is valid.
+	 * Switching to an invalid nsproxy (nsproxy == NULL) means that the task
+	 * is about to be destroyed.
+	 */
+	if (new != NULL) {
+		brs_domain_transfer_mm(p->mm, ns, new);
+
+		// If we change the domain of the current process, we need to switch.
+		if (current == p) {
+			brs_domain_enter(p);
+		}
+	} else {
+		/*
+		* Note that  brs_domain_enter will automatically determine which domain
+		* to switch to (i.e., to the task's domain maintained by its nsproxy or
+		* to the default domain of init_task).
+		*
+		* XXX: If the task switches to an invalid nsproxy, we should consider
+		* switching to the parent's domain.
+		*/
+		brs_domain_enter(p);
+	}
+#endif
+
 	if (ns)
 		put_nsproxy(ns);
 }
@@ -277,7 +327,7 @@ static int check_setns_flags(unsigned long flags)
 {
 	if (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 				 CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |
-				 CLONE_NEWPID | CLONE_NEWCGROUP)))
+				 CLONE_NEWPID | CLONE_NEWCGROUP | CLONE_NEWMEM)))
 		return -EINVAL;
 
 #ifndef CONFIG_USER_NS
@@ -308,6 +358,10 @@ static int check_setns_flags(unsigned long flags)
 	if (flags & CLONE_NEWTIME)
 		return -EINVAL;
 #endif
+#ifndef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM)
+		return -EINVAL;
+#endif
 
 	return 0;
 }
@@ -490,6 +544,14 @@ static int validate_nsset(struct nsset *nsset, struct pid *pid)
 	}
 #endif
 
+#ifdef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM) {
+		ret = validate_ns(nsset, &nsp->mem_ns->ns);
+		if (ret)
+			goto out;
+	}
+#endif
+
 out:
 	if (pid_ns)
 		put_pid_ns(pid_ns);
@@ -559,6 +621,10 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 			err = -EINVAL;
 		flags = ns->ops->type;
 	} else if (!IS_ERR(pidfd_pid(fd_file(f)))) {
+		if (brs_check_memns_enable_flags(flags)) {
+			flags |= CLONE_NEWMEM;
+		}
+
 		err = check_setns_flags(flags);
 	} else {
 		err = -EINVAL;
@@ -571,12 +637,26 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 		goto out;
 
 	if (proc_ns_file(fd_file(f)))
+#ifdef CONFIG_MEM_NS
+		/*
+		 * XXX: Note that we cannot piggy-back memory namespaces onto
+		 * pid namespaces during nsset/nsenter if no pidfd is given.
+		 * This is because we cannot identify the associated memory
+		 * namespace without any additional links between the
+		 * pid_namespace and the mem_namespace data structures. Consider
+		 * adding links or using pid namespaces alone for creating
+		 * memory isolation domains.
+		 */
+#endif
 		err = validate_ns(&nsset, ns);
 	else
 		err = validate_nsset(&nsset, pidfd_pid(fd_file(f)));
 	if (!err) {
-		commit_nsset(&nsset);
-		perf_event_namespaces(current);
+		err = security_setns(current, &nsset);
+		if (!err) {
+			commit_nsset(&nsset);
+			perf_event_namespaces(current);
+		}
 	}
 	put_nsset(&nsset);
 out:
diff --git kernel/printk/printk.c kernel/printk/printk.c
index 3a91b739e8..db2ed2a9fe 100644
--- kernel/printk/printk.c
+++ kernel/printk/printk.c
@@ -980,7 +980,7 @@ static int devkmsg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations kmsg_fops = {
+const struct file_operations kmsg_fops __section(".rodata") = {
 	.open = devkmsg_open,
 	.read = devkmsg_read,
 	.write_iter = devkmsg_write,
diff --git kernel/rcu/tree.c kernel/rcu/tree.c
index 552464dcff..c2b8c9154c 100644
--- kernel/rcu/tree.c
+++ kernel/rcu/tree.c
@@ -66,6 +66,8 @@
 #include <linux/context_tracking.h>
 #include "../time/tick-internal.h"
 
+#include <bhv/vault.h>
+
 #include "tree.h"
 #include "rcu.h"
 
@@ -729,6 +731,7 @@ static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)
  * Make notrace because it can be called by the internal functions of
  * ftrace, and making this notrace removes unnecessary recursion calls.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 notrace bool rcu_is_watching(void)
 {
 	bool ret;
diff --git kernel/reboot.c kernel/reboot.c
index d6ee090eda..ad15b61f4f 100644
--- kernel/reboot.c
+++ kernel/reboot.c
@@ -19,6 +19,8 @@
 #include <linux/syscore_ops.h>
 #include <linux/uaccess.h>
 
+#include <bhv/config.h>
+
 /*
  * this indicates whether you can reboot with ctrl-alt-del: the default is yes
  */
@@ -824,8 +826,16 @@ void ctrl_alt_del(void)
 }
 
 #define POWEROFF_CMD_PATH_LEN  256
-static char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define POWEROFF_CMD brs_policy_get_poweroff_cmd()
+static const char reboot_cmd[] __section(".rodata") = "/sbin/reboot";
+#else
+#define POWEROFF_CMD poweroff_cmd
 static const char reboot_cmd[] = "/sbin/reboot";
+#endif
+
+static char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
 
 static int run_cmd(const char *cmd)
 {
@@ -866,7 +876,7 @@ static int __orderly_poweroff(bool force)
 {
 	int ret;
 
-	ret = run_cmd(poweroff_cmd);
+	ret = run_cmd(POWEROFF_CMD);
 
 	if (ret && force) {
 		pr_warn("Failed to start orderly shutdown: forcing the issue\n");
@@ -1286,8 +1296,13 @@ static struct ctl_table kern_reboot_table[] = {
 		.procname       = "poweroff_cmd",
 		.data           = &poweroff_cmd,
 		.maxlen         = POWEROFF_CMD_PATH_LEN,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode           = 0444,
+		.proc_handler	= proc_dopoweroff_cmd,
+#else
 		.mode           = 0644,
 		.proc_handler   = proc_dostring,
+#endif
 	},
 	{
 		.procname       = "ctrl-alt-del",
diff --git kernel/sched/core.c kernel/sched/core.c
index 4b1953b6c7..ff5f6dcdda 100644
--- kernel/sched/core.c
+++ kernel/sched/core.c
@@ -93,6 +93,10 @@
 #include "smp.h"
 #include "stats.h"
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+#include <bhv/integrity.h>
+
 #include "../workqueue_internal.h"
 #include "../../io_uring/io-wq.h"
 #include "../smpboot.h"
@@ -5314,6 +5318,8 @@ context_switch(struct rq *rq, struct task_struct *prev,
 			mmgrab_lazy_tlb(prev->active_mm);
 		else
 			prev->active_mm = NULL;
+
+		bhv_pt_protect_check_pgd(next->active_mm);
 	} else {                                        // to user
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
diff --git kernel/static_call_inline.c kernel/static_call_inline.c
index bb7d066a7c..2b67be451a 100644
--- kernel/static_call_inline.c
+++ kernel/static_call_inline.c
@@ -10,11 +10,21 @@
 #include <linux/processor.h>
 #include <asm/sections.h>
 
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER0(void, cpus_read_lock)
+BHV_VAULT_FN_WRAPPER0(void, cpus_read_unlock)
+
 extern struct static_call_site __start_static_call_sites[],
 			       __stop_static_call_sites[];
 extern struct static_call_tramp_key __start_static_call_tramp_key[],
 				    __stop_static_call_tramp_key[];
 
+/* XXX: CAN WE MOVE THIS INTO RO AFTER INIT? */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 int static_call_initialized;
 
 /*
@@ -31,49 +41,46 @@ void static_call_force_reinit(void)
 /* mutex to protect key modules/sites */
 static DEFINE_MUTEX(static_call_mutex);
 
-static void static_call_lock(void)
-{
-	mutex_lock(&static_call_mutex);
-}
-
-static void static_call_unlock(void)
-{
-	mutex_unlock(&static_call_mutex);
-}
-
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void *static_call_addr(struct static_call_site *site)
 {
 	return (void *)((long)site->addr + (long)&site->addr);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline unsigned long __static_call_key(const struct static_call_site *site)
 {
 	return (long)site->key + (long)&site->key;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_key *static_call_key(const struct static_call_site *site)
 {
 	return (void *)(__static_call_key(site) & ~STATIC_CALL_SITE_FLAGS);
 }
 
 /* These assume the key is word-aligned. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_is_init(struct static_call_site *site)
 {
 	return __static_call_key(site) & STATIC_CALL_SITE_INIT;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_is_tail(struct static_call_site *site)
 {
 	return __static_call_key(site) & STATIC_CALL_SITE_TAIL;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_call_set_init(struct static_call_site *site)
 {
 	site->key = (__static_call_key(site) | STATIC_CALL_SITE_INIT) -
 		    (long)&site->key;
 }
 
-static int static_call_site_cmp(const void *_a, const void *_b)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline int static_call_site_cmp(const void *_a, const void *_b)
 {
 	const struct static_call_site *a = _a;
 	const struct static_call_site *b = _b;
@@ -88,8 +95,10 @@ static int static_call_site_cmp(const void *_a, const void *_b)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_site_cmp);
 
-static void static_call_site_swap(void *_a, void *_b, int size)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline void static_call_site_swap(void *_a, void *_b, int size)
 {
 	long delta = (unsigned long)_a - (unsigned long)_b;
 	struct static_call_site *a = _a;
@@ -102,19 +111,22 @@ static void static_call_site_swap(void *_a, void *_b, int size)
 	b->addr = tmp.addr + delta;
 	b->key  = tmp.key  + delta;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_site_swap);
 
-static inline void static_call_sort_entries(struct static_call_site *start,
-					    struct static_call_site *stop)
+static noinline void static_call_sort_entries(struct static_call_site *start,
+					    	                   struct static_call_site *stop)
 {
 	sort(start, stop - start, sizeof(struct static_call_site),
 	     static_call_site_cmp, static_call_site_swap);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_key_has_mods(struct static_call_key *key)
 {
 	return !(key->type & 1);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_mod *static_call_key_next(struct static_call_key *key)
 {
 	if (!static_call_key_has_mods(key))
@@ -123,6 +135,7 @@ static inline struct static_call_mod *static_call_key_next(struct static_call_ke
 	return key->mods;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_site *static_call_key_sites(struct static_call_key *key)
 {
 	if (static_call_key_has_mods(key))
@@ -131,13 +144,19 @@ static inline struct static_call_site *static_call_key_sites(struct static_call_
 	return (struct static_call_site *)(key->type & ~1);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	struct static_call_site *site, *stop;
 	struct static_call_mod *site_mod, first;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 
 	if (key->func == func)
 		goto done;
@@ -212,11 +231,18 @@ void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 	}
 
 done:
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_update);
 EXPORT_SYMBOL_GPL(__static_call_update);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_init(struct module *mod,
 			      struct static_call_site *start,
 			      struct static_call_site *stop)
@@ -290,6 +316,7 @@ static int __static_call_init(struct module *mod,
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int addr_conflict(struct static_call_site *site, void *start, void *end)
 {
 	unsigned long addr = (unsigned long)static_call_addr(site);
@@ -301,6 +328,7 @@ static int addr_conflict(struct static_call_site *site, void *start, void *end)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_text_reserved(struct static_call_site *iter_start,
 				       struct static_call_site *iter_stop,
 				       void *start, void *end, bool init)
@@ -320,6 +348,7 @@ static int __static_call_text_reserved(struct static_call_site *iter_start,
 
 #ifdef CONFIG_MODULES
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
@@ -344,6 +373,7 @@ static int __static_call_mod_text_reserved(void *start, void *end)
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static unsigned long tramp_key_lookup(unsigned long addr)
 {
 	struct static_call_tramp_key *start = __start_static_call_tramp_key;
@@ -361,6 +391,7 @@ static unsigned long tramp_key_lookup(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int static_call_add_module(struct module *mod)
 {
 	struct static_call_site *start = mod->static_call_sites;
@@ -400,6 +431,7 @@ static int static_call_add_module(struct module *mod)
 	return __static_call_init(mod, start, stop);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_call_del_module(struct module *mod)
 {
 	struct static_call_site *start = mod->static_call_sites;
@@ -440,14 +472,20 @@ static void static_call_del_module(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int static_call_module_notify(struct notifier_block *nb,
 				     unsigned long val, void *data)
 {
 	struct module *mod = data;
 	int ret = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -462,11 +500,17 @@ static int static_call_module_notify(struct notifier_block *nb,
 		break;
 	}
 
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_module_notify);
 
 static struct notifier_block static_call_module_nb = {
 	.notifier_call = static_call_module_notify,
@@ -474,6 +518,7 @@ static struct notifier_block static_call_module_nb = {
 
 #else
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline int __static_call_mod_text_reserved(void *start, void *end)
 {
 	return 0;
@@ -481,6 +526,7 @@ static inline int __static_call_mod_text_reserved(void *start, void *end)
 
 #endif /* CONFIG_MODULES */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int static_call_text_reserved(void *start, void *end)
 {
 	bool init = system_state < SYSTEM_RUNNING;
@@ -492,27 +538,49 @@ int static_call_text_reserved(void *start, void *end)
 
 	return __static_call_mod_text_reserved(start, end);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_text_reserved);
 
-int __init static_call_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int _static_call_init(void)
 {
 	int ret;
 
 	/* See static_call_force_reinit(). */
 	if (static_call_initialized == 1)
-		return 0;
+		return -EEXIST;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 	ret = __static_call_init(NULL, __start_static_call_sites,
 				 __stop_static_call_sites);
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 
 	if (ret) {
 		pr_err("Failed to allocate memory for static_call!\n");
 		BUG();
 	}
 
+	return ret;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, _static_call_init);
+
+int __init static_call_init(void)
+{
+	int ret = _static_call_init();
+	if (ret)
+		return (ret == -EEXIST) ? 0 : ret;
+
 #ifdef CONFIG_MODULES
 	if (!static_call_initialized)
 		register_module_notifier(&static_call_module_nb);
diff --git kernel/sysctl.c kernel/sysctl.c
index 79e6cb1d5c..baff067b14 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -69,6 +69,8 @@
 #include <linux/uaccess.h>
 #include <asm/processor.h>
 
+#include <bhv/config.h>
+
 #ifdef CONFIG_X86
 #include <asm/nmi.h>
 #include <asm/stacktrace.h>
@@ -266,6 +268,41 @@ int proc_dostring(const struct ctl_table *table, int write,
 			ppos);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+static int proc_domodprobe_path(const struct ctl_table *table, int write,
+				void *buffer, size_t *lenp, loff_t *ppos)
+{
+	// No writes allowed.
+	if (write)
+		return -EPERM;
+
+	return _proc_do_string((char *)brs_policy_get_modprobe_path(),
+			       table->maxlen, 0, buffer, lenp, ppos);
+}
+
+int proc_dopoweroff_cmd(const struct ctl_table *table, int write,
+			void *buffer, size_t *lenp, loff_t *ppos)
+{
+	// No writes allowed.
+	if (write)
+		return -EPERM;
+
+	return _proc_do_string((char *)brs_policy_get_poweroff_cmd(),
+			       table->maxlen, 0, buffer, lenp, ppos);
+}
+
+int proc_docore_pattern(const struct ctl_table *table, int write,
+			void *buffer, size_t *lenp, loff_t *ppos)
+{
+	// No writes allowed.
+	if (write)
+		return -EPERM;
+
+	return _proc_do_string((char *)brs_policy_get_core_pattern(),
+			       table->maxlen, 0, buffer, lenp, ppos);
+}
+#endif
+
 static void proc_skip_spaces(char **buf, size_t *size)
 {
 	while (*size) {
@@ -1729,8 +1766,13 @@ static struct ctl_table kern_table[] = {
 		.procname	= "modprobe",
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+		.proc_handler	= proc_domodprobe_path,
+#else
 		.mode		= 0644,
 		.proc_handler	= proc_dostring,
+#endif
 	},
 	{
 		.procname	= "modules_disabled",
diff --git kernel/trace/Kconfig kernel/trace/Kconfig
index 721c3b2210..41382b87a2 100644
--- kernel/trace/Kconfig
+++ kernel/trace/Kconfig
@@ -183,6 +183,7 @@ menuconfig FTRACE
 	bool "Tracers"
 	depends on TRACING_SUPPORT
 	default y if DEBUG_KERNEL
+	depends on !BHV_LOCKDOWN
 	help
 	  Enable the kernel tracing infrastructure.
 
@@ -516,6 +517,7 @@ config MMIOTRACE
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
+	depends on !BHV_LOCKDOWN
 	select TRACING
 	help
 	  This tracer hooks to various trace points in the kernel,
diff --git kernel/trace/bpf_trace.c kernel/trace/bpf_trace.c
index 3ec7df7dbe..9052c82e2c 100644
--- kernel/trace/bpf_trace.c
+++ kernel/trace/bpf_trace.c
@@ -3246,7 +3246,8 @@ static int uprobe_prog_run(struct bpf_uprobe *uprobe,
 }
 
 static bool
-uprobe_multi_link_filter(struct uprobe_consumer *con, struct mm_struct *mm)
+uprobe_multi_link_filter(struct uprobe_consumer *con, enum uprobe_filter_ctx ctx,
+			 struct mm_struct *mm)
 {
 	struct bpf_uprobe *uprobe;
 
diff --git kernel/trace/ftrace.c kernel/trace/ftrace.c
index ad7db84b04..77e6f23bad 100644
--- kernel/trace/ftrace.c
+++ kernel/trace/ftrace.c
@@ -36,6 +36,8 @@
 #include <linux/rcupdate.h>
 #include <linux/kprobes.h>
 
+#include <bhv/vault.h>
+
 #include <trace/events/sched.h>
 
 #include <asm/sections.h>
@@ -1075,6 +1077,7 @@ struct ftrace_ops global_ops = {
 /*
  * Used by the stack unwinder to know about dynamic ftrace trampolines.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
 {
 	struct ftrace_ops *op = NULL;
@@ -1109,6 +1112,7 @@ struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
  * not return true for either core_kernel_text() or
  * is_module_text_address().
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_ftrace_trampoline(unsigned long addr)
 {
 	return ftrace_ops_trampoline(addr) != NULL;
diff --git kernel/trace/trace_uprobe.c kernel/trace/trace_uprobe.c
index 9916677acf..f452466664 100644
--- kernel/trace/trace_uprobe.c
+++ kernel/trace/trace_uprobe.c
@@ -1100,7 +1100,9 @@ print_uprobe_event(struct trace_iterator *iter, int flags, struct trace_event *e
 	return trace_handle_return(s);
 }
 
-typedef bool (*filter_func_t)(struct uprobe_consumer *self, struct mm_struct *mm);
+typedef bool (*filter_func_t)(struct uprobe_consumer *self,
+				enum uprobe_filter_ctx ctx,
+				struct mm_struct *mm);
 
 static int trace_uprobe_enable(struct trace_uprobe *tu, filter_func_t filter)
 {
@@ -1363,7 +1365,8 @@ static int uprobe_perf_open(struct trace_event_call *call,
 	return err;
 }
 
-static bool uprobe_perf_filter(struct uprobe_consumer *uc, struct mm_struct *mm)
+static bool uprobe_perf_filter(struct uprobe_consumer *uc,
+				enum uprobe_filter_ctx ctx, struct mm_struct *mm)
 {
 	struct trace_uprobe_filter *filter;
 	struct trace_uprobe *tu;
@@ -1453,7 +1456,7 @@ static void __uprobe_perf_func(struct trace_uprobe *tu,
 static int uprobe_perf_func(struct trace_uprobe *tu, struct pt_regs *regs,
 			    struct uprobe_cpu_buffer **ucbp)
 {
-	if (!uprobe_perf_filter(&tu->consumer, current->mm))
+	if (!uprobe_perf_filter(&tu->consumer, 0, current->mm))
 		return UPROBE_HANDLER_REMOVE;
 
 	if (!is_ret_probe(tu))
diff --git kernel/tracepoint.c kernel/tracepoint.c
index 8879da16ef..a5fb09a9d9 100644
--- kernel/tracepoint.c
+++ kernel/tracepoint.c
@@ -15,6 +15,8 @@
 #include <linux/sched/task.h>
 #include <linux/static_key.h>
 
+#include <bhv/vault.h>
+
 enum tp_func_state {
 	TP_FUNC_0,
 	TP_FUNC_1,
@@ -42,8 +44,10 @@ struct tp_transition_snapshot {
 };
 
 /* Protected by tracepoints_mutex */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct tp_transition_snapshot tp_transition_snapshot[_NR_TP_TRANSITION_SYNC];
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_get_state(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -54,6 +58,7 @@ static void tp_rcu_get_state(enum tp_transition_sync sync)
 	snapshot->ongoing = true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -67,6 +72,7 @@ static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 }
 
 /* Set to 1 to enable tracepoint debug output */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const int tracepoint_debug;
 
 #ifdef CONFIG_MODULES
@@ -104,6 +110,7 @@ static void tp_stub_func(void)
 	return;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void *allocate_probes(int count)
 {
 	struct tp_probes *p  = kmalloc(struct_size(p, probes, count),
@@ -139,6 +146,7 @@ static __init int release_early_probes(void)
 /* SRCU is initialized at core_initcall */
 postcore_initcall(release_early_probes);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void release_probes(struct tracepoint_func *old)
 {
 	if (old) {
@@ -165,6 +173,7 @@ static inline void release_probes(struct tracepoint_func *old)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void debug_print_probes(struct tracepoint_func *funcs)
 {
 	int i;
@@ -176,6 +185,7 @@ static void debug_print_probes(struct tracepoint_func *funcs)
 		printk(KERN_DEBUG "Probe %d : %p\n", i, funcs[i].func);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static struct tracepoint_func *
 func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	 int prio)
@@ -229,6 +239,7 @@ func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	return old;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *func_remove(struct tracepoint_func **funcs,
 		struct tracepoint_func *tp_func)
 {
@@ -294,6 +305,7 @@ static void *func_remove(struct tracepoint_func **funcs,
 /*
  * Count the number of functions (enum tp_func_state) in a tp_funcs array.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum tp_func_state nr_func_state(const struct tracepoint_func *tp_funcs)
 {
 	if (!tp_funcs)
@@ -320,6 +332,7 @@ static void tracepoint_update_call(struct tracepoint *tp, struct tracepoint_func
 /*
  * Add the probe function to a tracepoint.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_add_func(struct tracepoint *tp,
 			       struct tracepoint_func *func, int prio,
 			       bool warn)
@@ -386,6 +399,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_add_func);
 
 /*
  * Remove a probe function from a tracepoint.
@@ -393,6 +407,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
  * function insures that the original callback is not used anymore. This insured
  * by preempt_disable around the call site.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_remove_func(struct tracepoint *tp,
 		struct tracepoint_func *func)
 {
@@ -458,6 +473,7 @@ static int tracepoint_remove_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_remove_func);
 
 /**
  * tracepoint_probe_register_prio_may_exist -  Connect a probe to a tracepoint with priority
@@ -698,6 +714,7 @@ static void tracepoint_module_going(struct module *mod)
 	mutex_unlock(&tracepoint_module_list_mutex);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_module_notify(struct notifier_block *self,
 		unsigned long val, void *data)
 {
@@ -718,6 +735,7 @@ static int tracepoint_module_notify(struct notifier_block *self,
 	}
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_module_notify);
 
 static struct notifier_block tracepoint_module_nb = {
 	.notifier_call = tracepoint_module_notify,
diff --git kernel/ucount.c kernel/ucount.c
index 696406939b..f21b38ea22 100644
--- kernel/ucount.c
+++ kernel/ucount.c
@@ -87,6 +87,7 @@ static struct ctl_table user_table[] = {
 	UCOUNT_ENTRY("max_fanotify_groups"),
 	UCOUNT_ENTRY("max_fanotify_marks"),
 #endif
+	UCOUNT_ENTRY("max_mem_namespaces"),
 };
 #endif /* CONFIG_SYSCTL */
 
diff --git kernel/umh.c kernel/umh.c
index ff1f13a27d..6afaa58c28 100644
--- kernel/umh.c
+++ kernel/umh.c
@@ -32,6 +32,10 @@
 
 #include <trace/events/module.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 static kernel_cap_t usermodehelper_bset = CAP_FULL_SET;
 static kernel_cap_t usermodehelper_inheritable = CAP_FULL_SET;
 static DEFINE_SPINLOCK(umh_sysctl_lock);
@@ -107,6 +111,12 @@ static int call_usermodehelper_exec_async(void *data)
 	commit_creds(new);
 
 	wait_for_initramfs();
+
+	retval = security_kernel_exec(sub_info->path, sub_info->argv,
+				      sub_info->envp);
+	if (retval)
+		goto out;
+
 	retval = kernel_execve(sub_info->path,
 			       (const char *const *)sub_info->argv,
 			       (const char *const *)sub_info->envp);
diff --git lib/Kconfig.debug lib/Kconfig.debug
index b1d7c427bb..ec5f5405be 100644
--- lib/Kconfig.debug
+++ lib/Kconfig.debug
@@ -210,6 +210,7 @@ endmenu # "printk and dmesg options"
 
 config DEBUG_KERNEL
 	bool "Kernel debugging"
+        depends on !BHV_LOCKDOWN
 	help
 	  Say Y here if you are developing drivers or trying to debug and
 	  identify kernel problems.
diff --git lib/Kconfig.kgdb lib/Kconfig.kgdb
index 537e1b3f57..4ef0312748 100644
--- lib/Kconfig.kgdb
+++ lib/Kconfig.kgdb
@@ -28,6 +28,7 @@ config KGDB_HONOUR_BLOCKLIST
 	bool "KGDB: use kprobe blocklist to prohibit unsafe breakpoints"
 	depends on HAVE_KPROBES
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	select KPROBES
 	default y
 	help
diff --git lib/sort.c lib/sort.c
index 048b7a6ef9..e098e57b9b 100644
--- lib/sort.c
+++ lib/sort.c
@@ -14,6 +14,8 @@
 #include <linux/export.h>
 #include <linux/sort.h>
 
+#include <bhv/vault.h>
+
 /**
  * is_aligned - is this pointer & size okay for word-wide copying?
  * @base: pointer to data
@@ -53,6 +55,7 @@ static bool is_aligned(const void *base, size_t size, unsigned char align)
  * subtract (since the intervening mov instructions don't alter the flags).
  * Gcc 8.1.0 doesn't have that problem.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_32(void *a, void *b, size_t n)
 {
 	do {
@@ -78,6 +81,7 @@ static void swap_words_32(void *a, void *b, size_t n)
  * but it's possible to have 64-bit loads without 64-bit pointers (e.g.
  * x32 ABI).  Are there any cases the kernel needs to worry about?
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_64(void *a, void *b, size_t n)
 {
 	do {
@@ -106,6 +110,7 @@ static void swap_words_64(void *a, void *b, size_t n)
  *
  * This is the fallback if alignment doesn't allow using larger chunks.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_bytes(void *a, void *b, size_t n)
 {
 	do {
@@ -134,6 +139,15 @@ struct wrapper {
  * The function pointer is last to make tail calls most efficient if the
  * compiler decides not to inline this function.
  */
+
+/*
+ * XXX: Try to remove this!!
+ */
+#ifdef swap
+#undef swap
+#endif
+
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_swap(void *a, void *b, size_t size, swap_r_func_t swap_func, const void *priv)
 {
 	if (swap_func == SWAP_WRAPPER) {
@@ -153,6 +167,7 @@ static void do_swap(void *a, void *b, size_t size, swap_r_func_t swap_func, cons
 
 #define _CMP_WRAPPER ((cmp_r_func_t)0L)
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int do_cmp(const void *a, const void *b, cmp_r_func_t cmp, const void *priv)
 {
 	if (cmp == _CMP_WRAPPER)
@@ -205,6 +220,7 @@ static size_t parent(size_t i, unsigned int lsbit, size_t size)
  * O(n*n) worst-case behavior and extra memory requirements that make
  * it less suitable for kernel use.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort_r(void *base, size_t num, size_t size,
 	    cmp_r_func_t cmp_func,
 	    swap_r_func_t swap_func,
@@ -288,6 +304,7 @@ void sort_r(void *base, size_t num, size_t size,
 }
 EXPORT_SYMBOL(sort_r);
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort(void *base, size_t num, size_t size,
 	  cmp_func_t cmp_func,
 	  swap_func_t swap_func)
diff --git lib/string.c lib/string.c
index e657809fa7..22b5de4f3c 100644
--- lib/string.c
+++ lib/string.c
@@ -24,6 +24,7 @@
 #include <linux/stddef.h>
 #include <linux/string.h>
 #include <linux/types.h>
+#include <bhv/vault.h>
 
 #include <asm/page.h>
 #include <asm/rwonce.h>
@@ -520,6 +521,7 @@ EXPORT_SYMBOL(strsep);
  *
  * Do not use memset() to access IO space, use memset_io() instead.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset(void *s, int c, size_t count)
 {
 	char *xs = s;
@@ -542,6 +544,7 @@ EXPORT_SYMBOL(memset);
  * of a byte.  Remember that @count is the number of uint16_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset16(uint16_t *s, uint16_t v, size_t count)
 {
 	uint16_t *xs = s;
@@ -564,6 +567,7 @@ EXPORT_SYMBOL(memset16);
  * of a byte.  Remember that @count is the number of uint32_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset32(uint32_t *s, uint32_t v, size_t count)
 {
 	uint32_t *xs = s;
@@ -586,6 +590,7 @@ EXPORT_SYMBOL(memset32);
  * of a byte.  Remember that @count is the number of uint64_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset64(uint64_t *s, uint64_t v, size_t count)
 {
 	uint64_t *xs = s;
@@ -607,6 +612,7 @@ EXPORT_SYMBOL(memset64);
  * You should not use this function to access IO space, use memcpy_toio()
  * or memcpy_fromio() instead.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memcpy(void *dest, const void *src, size_t count)
 {
 	char *tmp = dest;
@@ -659,6 +665,7 @@ EXPORT_SYMBOL(memmove);
  * @count: The size of the area.
  */
 #undef memcmp
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 __visible int memcmp(const void *cs, const void *ct, size_t count)
 {
 	const unsigned char *su1, *su2;
diff --git mm/gup.c mm/gup.c
index e323843cc5..bc96792121 100644
--- mm/gup.c
+++ mm/gup.c
@@ -11,6 +11,7 @@
 #include <linux/rmap.h>
 #include <linux/swap.h>
 #include <linux/swapops.h>
+#include <linux/security.h>
 #include <linux/secretmem.h>
 
 #include <linux/sched/signal.h>
@@ -25,6 +26,8 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
+#include <linux/mem_namespace.h>
+
 #include "internal.h"
 
 struct follow_page_context {
@@ -1274,6 +1277,11 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 	int foreign = (gup_flags & FOLL_REMOTE);
 	bool vma_anon = vma_is_anonymous(vma);
 
+#ifdef CONFIG_MEM_NS
+	struct mm_struct *mm = vma->vm_mm;
+	struct task_struct *t = mm->owner;
+#endif
+
 	if (vm_flags & (VM_IO | VM_PFNMAP))
 		return -EFAULT;
 
@@ -1293,7 +1301,17 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 		if (!vma_anon &&
 		    !writable_file_mapping_allowed(vma, gup_flags))
 			return -EFAULT;
-
+			
+#ifdef CONFIG_MEM_NS
+		/*
+		 * Grant write access to remote address spaces only if both
+		 * processes are executing inside of the same mem_namespace.
+		 */
+		if (!current_in_same_mem_ns(t)) {
+			if(security_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
 		if (!(vm_flags & VM_WRITE) || (vm_flags & VM_SHADOW_STACK)) {
 			if (!(gup_flags & FOLL_FORCE))
 				return -EFAULT;
@@ -1311,17 +1329,38 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 			 */
 			if (!is_cow_mapping(vm_flags))
 				return -EFAULT;
+
+			if (!security_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
 		}
-	} else if (!(vm_flags & VM_READ)) {
-		if (!(gup_flags & FOLL_FORCE))
-			return -EFAULT;
+	} else {
+#ifdef CONFIG_MEM_NS
 		/*
-		 * Is there actually any vma we can reach here which does not
-		 * have VM_MAYREAD set?
+		 * Grant read access to remote address spaces only if both
+		 * processes are part of the same ancestor tree branch the
+		 * target mem_namespace.
 		 */
-		if (!(vm_flags & VM_MAYREAD))
-			return -EFAULT;
+		if (!task_in_ancestor_memns(current, memns_of_task(t))) {
+			if(security_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
+		if (!(vm_flags & VM_READ)) {
+			if (!(gup_flags & FOLL_FORCE))
+				return -EFAULT;
+			/*
+			 * Is there actually any vma we can reach here which does not
+			 * have VM_MAYREAD set?
+			 */
+			if (!(vm_flags & VM_MAYREAD))
+				return -EFAULT;
+
+			if (!security_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+		}
 	}
+
 	/*
 	 * gups are always data accesses, not instruction
 	 * fetches, so execute=false here
diff --git mm/khugepaged.c mm/khugepaged.c
index abd5764e48..20a7f1e9b7 100644
--- mm/khugepaged.c
+++ mm/khugepaged.c
@@ -569,6 +569,11 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 	int none_or_zero = 0, shared = 0, result = SCAN_FAIL, referenced = 0;
 	bool writable = false;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain = brs_get_active_domain();
+	brs_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 	     _pte++, address += PAGE_SIZE) {
 		pte_t pteval = ptep_get(_pte);
@@ -694,9 +699,16 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		result = SCAN_SUCCEED;
 		trace_mm_collapse_huge_page_isolate(&folio->page, none_or_zero,
 						    referenced, writable, result);
+#if defined(CONFIG_DOMAIN_SPACES)
+		brs_domain_switch(domain);
+#endif
 		return result;
 	}
 out:
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_switch(domain);
+#endif
+
 	release_pte_pages(pte, _pte, compound_pagelist);
 	trace_mm_collapse_huge_page_isolate(&folio->page, none_or_zero,
 					    referenced, writable, result);
@@ -723,6 +735,8 @@ static void __collapse_huge_page_copy_succeeded(pte_t *pte,
 				 * ptl mostly unnecessary.
 				 */
 				spin_lock(ptl);
+				brs_domain_clear_pte(vma->vm_mm, address, _pte,
+						     *_pte);
 				ptep_clear(vma->vm_mm, address, _pte);
 				spin_unlock(ptl);
 				ksm_might_unmap_zero_page(vma->vm_mm, pteval);
@@ -739,6 +753,7 @@ static void __collapse_huge_page_copy_succeeded(pte_t *pte,
 			 * inside folio_remove_rmap_pte().
 			 */
 			spin_lock(ptl);
+			brs_domain_clear_pte(vma->vm_mm, address, _pte, *_pte);
 			ptep_clear(vma->vm_mm, address, _pte);
 			folio_remove_rmap_pte(src, src_page, vma);
 			spin_unlock(ptl);
@@ -803,6 +818,11 @@ static int __collapse_huge_page_copy(pte_t *pte, struct folio *folio,
 	unsigned int i;
 	int result = SCAN_SUCCEED;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain = brs_get_active_domain();
+	brs_domain_enter(vma->vm_mm->owner);
+#endif
+
 	/*
 	 * Copying pages' contents is subject to memory poison at any iteration.
 	 */
@@ -830,6 +850,10 @@ static int __collapse_huge_page_copy(pte_t *pte, struct folio *folio,
 		__collapse_huge_page_copy_failed(pte, pmd, orig_pmd, vma,
 						 compound_pagelist);
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_switch(domain);
+#endif
+
 	return result;
 }
 
diff --git mm/maccess.c mm/maccess.c
index 518a256673..0729ea137a 100644
--- mm/maccess.c
+++ mm/maccess.c
@@ -6,6 +6,17 @@
 #include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
+#include <asm-generic/sections.h>
+
+__always_inline static bool is_vault(const void *addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
 
 bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 		size_t size)
@@ -28,6 +39,10 @@ long copy_from_kernel_nofault(void *dst, const void *src, size_t size)
 	if (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))
 		align = (unsigned long)dst | (unsigned long)src;
 
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(src, size))
 		return -ERANGE;
 
@@ -83,6 +98,10 @@ long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr, long count)
 
 	if (unlikely(count <= 0))
 		return 0;
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(unsafe_addr, count))
 		return -ERANGE;
 
diff --git mm/memory.c mm/memory.c
index b6daa0e673..38f94fa464 100644
--- mm/memory.c
+++ mm/memory.c
@@ -81,6 +81,8 @@
 
 #include <trace/events/kmem.h>
 
+#include <bhv/domain.h>
+
 #include <asm/io.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
@@ -2055,7 +2057,11 @@ static int insert_page_into_pte_locked(struct vm_area_struct *vma, pte_t *pte,
 		inc_mm_counter(vma->vm_mm, mm_counter_file(folio));
 		folio_add_file_rmap_pte(folio, page, vma);
 	}
+#ifdef CONFIG_BHV_VAS
+	brs_domain_set_pte_at_kernel(vma->vm_mm, addr, pte, pteval);
+#else // !defined CONFIG_BHV_VAS
 	set_pte_at(vma->vm_mm, addr, pte, pteval);
+#endif // defined CONFIG_BHV_VAS
 	return 0;
 }
 
@@ -2356,7 +2362,14 @@ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (pfn_t_devmap(pfn))
+		set_pte_at(mm, addr, pte, entry);
+	else
+		brs_domain_set_pte_at_kernel(mm, addr, pte, entry);
+#else // !defined CONFIG_BHV_VAS
 	set_pte_at(mm, addr, pte, entry);
+#endif // defined CONFIG_BHV_VAS
 	update_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */
 
 out_unlock:
@@ -6571,6 +6584,9 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 {
 	void *old_buf = buf;
 	int write = gup_flags & FOLL_WRITE;
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint64_t domain;
+#endif
 
 	if (mmap_read_lock_killable(mm))
 		return 0;
@@ -6582,6 +6598,11 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 	if (!vma_lookup(mm, addr) && !expand_stack(mm, addr))
 		return 0;
 
+#if defined(CONFIG_DOMAIN_SPACES)
+	domain = brs_get_active_domain();
+	brs_domain_enter(mm->owner);
+#endif
+
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, offset;
@@ -6637,6 +6658,11 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 		buf += bytes;
 		addr += bytes;
 	}
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_switch(domain);
+#endif
+
 	mmap_read_unlock(mm);
 
 	return buf - old_buf;
@@ -6789,7 +6815,12 @@ static void clear_gigantic_page(struct folio *folio, unsigned long addr_hint,
 	int i;
 
 	might_sleep();
+
 	for (i = 0; i < nr_pages; i++) {
+		brs_domain_map_kernel(current->mm,
+				      page_to_pfn(folio_page(folio, i)),
+				      PAGE_SIZE, true, true, false);
+
 		cond_resched();
 		clear_user_highpage(folio_page(folio, i), addr + i * PAGE_SIZE);
 	}
@@ -6803,6 +6834,15 @@ static int clear_subpage(unsigned long addr, int idx, void *arg)
 	return 0;
 }
 
+static int brs_domain_map_kernel_subpage(unsigned long, int idx, void *arg)
+{
+	struct folio *folio = arg;
+
+	brs_domain_map_kernel(current->mm, page_to_pfn(folio_page(folio, idx)),
+			      PAGE_SIZE, true, true, false);
+	return 0;
+}
+
 /**
  * folio_zero_user - Zero a folio which will be mapped to userspace.
  * @folio: The folio to zero.
@@ -6814,8 +6854,11 @@ void folio_zero_user(struct folio *folio, unsigned long addr_hint)
 
 	if (unlikely(nr_pages > MAX_ORDER_NR_PAGES))
 		clear_gigantic_page(folio, addr_hint, nr_pages);
-	else
+	else {
+		process_huge_page(addr_hint, nr_pages,
+				  brs_domain_map_kernel_subpage, folio);
 		process_huge_page(addr_hint, nr_pages, clear_subpage, folio);
+	}
 }
 
 static int copy_user_gigantic_page(struct folio *dst, struct folio *src,
diff --git mm/mm_init.c mm/mm_init.c
index 4ba5607aaf..df181a5160 100644
--- mm/mm_init.c
+++ mm/mm_init.c
@@ -36,6 +36,9 @@
 
 #include <asm/setup.h>
 
+#include <bhv/bhv.h>
+#include <bhv/init/mm_init.h>
+
 #ifdef CONFIG_DEBUG_MEMORY_INIT
 int __meminitdata mminit_loglevel;
 
@@ -2672,4 +2675,6 @@ void __init mm_core_init(void)
 	kmsan_init_runtime();
 	mm_cache_init();
 	execmem_init();
+
+	bhv_mm_init();
 }
diff --git mm/mmap.c mm/mmap.c
index 6183805f6f..433b2654f1 100644
--- mm/mmap.c
+++ mm/mmap.c
@@ -1226,6 +1226,7 @@ int expand_downwards(struct vm_area_struct *vma, unsigned long address)
 			}
 		}
 	}
+
 	anon_vma_unlock_write(vma->anon_vma);
 	vma_iter_free(&vmi);
 	validate_mm(mm);
diff --git mm/page_owner.c mm/page_owner.c
index 2d6360eacc..a180a6f4bc 100644
--- mm/page_owner.c
+++ mm/page_owner.c
@@ -848,7 +848,7 @@ static void init_early_allocated_pages(void)
 		init_zones_in_node(pgdat);
 }
 
-static const struct file_operations proc_page_owner_operations = {
+const struct file_operations proc_page_owner_operations __section(".rodata") = {
 	.read		= read_page_owner,
 	.llseek		= lseek_page_owner,
 };
diff --git mm/page_poison.c mm/page_poison.c
index 3e9037363c..d5a286b036 100644
--- mm/page_poison.c
+++ mm/page_poison.c
@@ -34,6 +34,10 @@ void __kernel_poison_pages(struct page *page, int n)
 {
 	int i;
 
+	brs_domain_map_kernel(current->mm != NULL ? current->mm :
+						    current->active_mm,
+			      page_to_pfn(page), n, true, true, false);
+
 	for (i = 0; i < n; i++)
 		poison_page(page + i);
 }
diff --git mm/pgtable-generic.c mm/pgtable-generic.c
index a78a4adf71..f965b5f10a 100644
--- mm/pgtable-generic.c
+++ mm/pgtable-generic.c
@@ -16,6 +16,10 @@
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 /*
  * If a p?d_bad entry is found while walking page tables, report
  * the error, before resetting entry to p?d_none.  Usually (but
@@ -199,7 +203,11 @@ pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 		     pmd_t *pmdp)
 {
 	VM_WARN_ON_ONCE(!pmd_present(*pmdp));
-	pmd_t old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
+	pmd_t old;
+#ifdef CONFIG_MEM_NS
+	brs_domain_clear_pmd(vma->vm_mm, address, pmdp, pmd_mkinvalid(*pmdp));
+#endif
+	old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return old;
 }
diff --git mm/shmem.c mm/shmem.c
index 88fd6e2a2d..67e7572375 100644
--- mm/shmem.c
+++ mm/shmem.c
@@ -261,8 +261,8 @@ static void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 }
 
 static const struct super_operations shmem_ops;
-static const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+const struct address_space_operations shmem_aops;
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -4806,7 +4806,7 @@ static int shmem_error_remove_folio(struct address_space *mapping,
 	return 0;
 }
 
-static const struct address_space_operations shmem_aops = {
+const struct address_space_operations shmem_aops = {
 	.writepage	= shmem_writepage,
 	.dirty_folio	= noop_dirty_folio,
 #ifdef CONFIG_TMPFS
@@ -4819,7 +4819,7 @@ static const struct address_space_operations shmem_aops = {
 	.error_remove_folio = shmem_error_remove_folio,
 };
 
-static const struct file_operations shmem_file_operations = {
+const struct file_operations shmem_file_operations __section(".rodata") = {
 	.mmap		= shmem_mmap,
 	.open		= shmem_file_open,
 	.get_unmapped_area = shmem_get_unmapped_area,
diff --git mm/vmalloc.c mm/vmalloc.c
index 3519c4e4f8..38d77f00a3 100644
--- mm/vmalloc.c
+++ mm/vmalloc.c
@@ -47,6 +47,8 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/vmalloc.h>
 
+#include <bhv/vault.h>
+
 #include "internal.h"
 #include "pgalloc-track.h"
 
@@ -76,6 +78,7 @@ early_param("nohugevmalloc", set_nohugevmalloc);
 static const bool vmap_allow_huge = false;
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMALLOC */
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_vmalloc_addr(const void *x)
 {
 	unsigned long addr = (unsigned long)kasan_reset_tag(x);
@@ -723,6 +726,7 @@ void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,
 	vunmap_range(start, end);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int is_vmalloc_or_module_addr(const void *x)
 {
 	/*
diff --git net/socket.c net/socket.c
index 042451f01c..8455647495 100644
--- net/socket.c
+++ net/socket.c
@@ -151,7 +151,7 @@ static void sock_show_fdinfo(struct seq_file *m, struct file *f)
  *	in the operation structures but are done directly via the socketcall() multiplexor.
  */
 
-static const struct file_operations socket_file_ops = {
+const struct file_operations socket_file_ops = {
 	.owner =	THIS_MODULE,
 	.read_iter =	sock_read_iter,
 	.write_iter =	sock_write_iter,
@@ -1953,6 +1953,10 @@ struct file *do_accept(struct file *file, struct proto_accept_arg *arg,
 	if (err < 0)
 		goto out_fd;
 
+	err = security_socket_accepted(sock, newsock);
+	if (err < 0)
+		goto out_fd;
+
 	if (upeer_sockaddr) {
 		len = ops->getname(newsock, (struct sockaddr *)&address, 2);
 		if (len < 0) {
diff --git net/unix/af_unix.c net/unix/af_unix.c
index 45f8e21829..9c4041a986 100644
--- net/unix/af_unix.c
+++ net/unix/af_unix.c
@@ -463,7 +463,7 @@ static inline struct sock *unix_find_socket_byname(struct net *net,
 	return s;
 }
 
-static struct sock *unix_find_socket_byinode(struct inode *i)
+struct sock *unix_find_socket_byinode(struct inode *i)
 {
 	unsigned int hash = unix_bsd_hash(i);
 	struct sock *s;
@@ -1186,9 +1186,8 @@ static struct sock *unix_find_abstract(struct net *net,
 	return sk;
 }
 
-static struct sock *unix_find_other(struct net *net,
-				    struct sockaddr_un *sunaddr,
-				    int addr_len, int type)
+struct sock *unix_find_other(struct net *net, struct sockaddr_un *sunaddr,
+							 int addr_len, int type)
 {
 	struct sock *sk;
 
diff --git scripts/Makefile.vmlinux_o scripts/Makefile.vmlinux_o
index 0b6e2ebf60..163ad419fb 100644
--- scripts/Makefile.vmlinux_o
+++ scripts/Makefile.vmlinux_o
@@ -33,12 +33,13 @@ endif
 # For LTO and IBT, objtool doesn't run on individual translation units.
 # Run everything on vmlinux instead.
 
-objtool-enabled := $(or $(delay-objtool),$(CONFIG_NOINSTR_VALIDATION))
+objtool-enabled := $(or $(delay-objtool),$(CONFIG_NOINSTR_VALIDATION),$(CONFIG_BHV_VAULT_SPACES))
 
 vmlinux-objtool-args-$(delay-objtool)			+= $(objtool-args-y)
 vmlinux-objtool-args-$(CONFIG_GCOV_KERNEL)		+= --no-unreachable
 vmlinux-objtool-args-$(CONFIG_NOINSTR_VALIDATION)	+= --noinstr \
 							   $(if $(or $(CONFIG_MITIGATION_UNRET_ENTRY),$(CONFIG_MITIGATION_SRSO)), --unret)
+vmlinux-objtool-args-$(CONFIG_BHV_VAULT_SPACES)	+= --vault
 
 objtool-args = $(vmlinux-objtool-args-y) --link
 
diff --git scripts/link-vmlinux.sh scripts/link-vmlinux.sh
index a9b3f34a78..ba9cc7fa3b 100755
--- scripts/link-vmlinux.sh
+++ scripts/link-vmlinux.sh
@@ -59,7 +59,7 @@ vmlinux_link()
 	# skip output file argument
 	shift
 
-	if is_enabled CONFIG_LTO_CLANG || is_enabled CONFIG_X86_KERNEL_IBT; then
+	if is_enabled CONFIG_LTO_CLANG || is_enabled CONFIG_X86_KERNEL_IBT || is_enabled CONFIG_BHV_VAULT_SPACES; then
 		# Use vmlinux.o instead of performing the slow LTO link again.
 		objs=vmlinux.o
 		libs=
diff --git scripts/tags.sh scripts/tags.sh
index 191e0461d6..a8e39a7d8e 100755
--- scripts/tags.sh
+++ scripts/tags.sh
@@ -81,6 +81,48 @@ find_other_sources()
 	       -name "$1" -not -type l -print;
 }
 
+__find_brs_files()
+{
+	local KERNEL_SRC="$(pwd)"
+
+	find -L $1 -type f -name $2 \
+		| while read -r F; do
+
+			# Convert the symlink to an absolute path
+			ABS="$(readlink -f "$F")"
+			if [ -z "$ABS" ]; then
+				# readlink -f failed or F doesn't exist -> skip
+				continue
+			fi
+
+			# Turn the absolute path into a path relative to $KERNEL_SRC
+			REL="$(realpath --relative-to="$KERNEL_SRC" "$ABS" 2>/dev/null || true)"
+			if [ -z "$REL" ]; then
+				REL="$ABS"
+			fi
+
+			echo "$REL"
+		done
+}
+
+find_brs_files()
+{
+	local BRS_FILES="security/bhv include/bhv"
+
+	for brs_file in $BRS_FILES; do
+		__find_brs_files $brs_file $1
+	done
+}
+
+find_brs_arch_files()
+{
+	local BRS_ARCH_FILES="arch/$1/bhv arch/$1/include/asm/bhv"
+
+	for brs_file in $BRS_ARCH_FILES; do
+		__find_brs_files $brs_file $2
+	done
+}
+
 all_sources()
 {
 	find_arch_include_sources ${SRCARCH} '*.[chS]'
@@ -91,8 +133,11 @@ all_sources()
 	for arch in $ALLSOURCE_ARCHS
 	do
 		find_arch_sources $arch '*.[chS]'
+		find_brs_arch_files $arch '*.[chS]'
 	done
 	find_other_sources '*.[chS]'
+
+	find_brs_files '*.[chS]'
 }
 
 all_compiled_sources()
diff --git security/Kconfig security/Kconfig
index 28e685f53b..fed149c970 100644
--- security/Kconfig
+++ security/Kconfig
@@ -215,6 +215,22 @@ config STATIC_USERMODEHELPER_PATH
 	  If you wish for all usermode helper programs to be disabled,
 	  specify an empty string here (i.e. "").
 
+config MEM_NS
+	bool "Enable memory namespaces"
+	depends on MEMCG
+	depends on BHV_VAS
+	default y
+	help
+	  Enable memory namespaces.
+
+config DOMAIN_SPACES
+	bool "Enable domain spaces"
+	depends on MEM_NS
+	default n
+	help
+	  Enable domain spaces.
+
+source "security/bhv/Kconfig"
 source "security/selinux/Kconfig"
 source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
@@ -264,11 +280,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "landlock,lockdown,yama,loadpin,safesetid,smack,selinux,tomoyo,apparmor,ipe,bpf" if DEFAULT_SECURITY_SMACK
-	default "landlock,lockdown,yama,loadpin,safesetid,apparmor,selinux,smack,tomoyo,ipe,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "landlock,lockdown,yama,loadpin,safesetid,tomoyo,ipe,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "landlock,lockdown,yama,loadpin,safesetid,ipe,bpf" if DEFAULT_SECURITY_DAC
-	default "landlock,lockdown,yama,loadpin,safesetid,selinux,smack,tomoyo,apparmor,ipe,bpf"
+	default "landlock,lockdown,yama,loadpin,safesetid,smack,selinux,tomoyo,apparmor,ipe,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "landlock,lockdown,yama,loadpin,safesetid,apparmor,selinux,smack,tomoyo,ipe,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "landlock,lockdown,yama,loadpin,safesetid,tomoyo,ipe,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "landlock,lockdown,yama,loadpin,safesetid,ipe,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "landlock,lockdown,yama,loadpin,safesetid,selinux,smack,tomoyo,apparmor,ipe,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list, except for those with order
diff --git security/Makefile security/Makefile
index cc0982214b..521c57038d 100644
--- security/Makefile
+++ security/Makefile
@@ -20,6 +20,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_BRS)			+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 0000000000..dae0599636
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,103 @@
+config BRS
+	bool "BlueRock LSM"
+	default y
+	depends on (X86_64) || (ARM64 && OF)
+	select RELAY
+	select TRACING
+	select SECURITY
+	select SECURITY_NETWORK
+	help
+	  Say Y if you want to enable the BlueRock LSM.
+
+config BHV_VAS_REALLY_ENABLE
+	bool "BHV guest support."
+	default n
+	depends on (X86_64) || (ARM64 && OF)
+	help
+	  Should currently only enabled for development. Not for production.
+
+config BHV_VAS
+	bool "BHV guest support"
+	default n
+	depends on BHV_VAS_REALLY_ENABLE && ((X86_64) || (ARM64 && OF))
+	select BLOCK
+	select EXT4_FS
+	select XFS_FS
+	select BRU
+	select VIRTIO_VSOCKETS
+	select VSOCKETS
+	select VIRTIO_VSOCKETS_COMMON
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on BHV and benefit 
+	  from Virtualization-assisted Security.
+
+config BHV_PANIC_ON_FAIL
+	bool "BHV guest panics on Hypercall failure"
+	default y
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	bool "Build BHV guest support with DEBUG information"
+	default n
+	depends on BHV_VAS || BRS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_TRACEPOINTS
+	bool "Enable BHV Tracepoints"
+	default n
+	depends on BHV_VAS_DEBUG && TRACEPOINTS
+	help
+	  Say Y if you want to enable BHV tracepoints. Note: do not enable this option in production systems.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	bool "Make all paths passed to call_usermodehelper in the kernel constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in the kernel are constant. This implies
+	  that these paths cannot be updated via sysctl. Paths include
+	  the modprobe path and the poweroff path. This setting is recommended,
+	  since these strings are often updated in exploits.
+
+config BHV_CONST_CALL_USERMODEHELPER_MODULES
+	bool "Make all paths passed to call_usermodehelper in modules constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in drivers constant. This implies that
+	  these paths cannot be updated via sysctl. This setting is recommended,
+	  to harden modules against exploits.
+
+config BHV_LOCKDOWN
+	bool "Enable the most secure BHV settings (Lockdown)"
+	default n
+	depends on BHV_VAS
+	select BHV_PANIC_ON_FAIL
+	select BHV_CONST_MODPROBE_PATH
+	help
+	  Say Y if you want to enable the most secure BHV settings
+
+config BHV_VAULT_SPACES
+    bool "Enable the spaces-based BHV vault to guard code patching"
+    default y
+    depends on BHV_VAS
+    depends on X86_64 || ARM64
+    help
+	  Say Y if you want to enable the spaces-based BHV vault
+
+
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 0000000000..8f292bf9c1
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,68 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+#          Tommaso Frassetto <tommaso@bedrocksystems.com>
+
+
+
+ifeq ($(CONFIG_BRS),y)
+    $(shell echo $(CONFIG_LSM) | grep -q bluerock)
+    ifneq ($(.SHELLSTATUS),0)
+        $(warning bluerock LSM not selected in CONFIG_LSM ($(CONFIG_LSM)))
+    endif
+endif
+
+
+ccflags-y+=-DKERNEL_COMMIT_HASH=\"961126b2e3ce16ac173e75ca135d098e5552e413+34d3739a718235560d6094e924f6502f269188ad\"
+
+ccflags-y+=-Werror
+
+obj-$(CONFIG_BHV_VAS)		+= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= abi_autogen.o
+obj-$(CONFIG_BRS)		+= brpol_autogen.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/mm_init.o
+obj-$(CONFIG_BHV_VAS)		+= init/late_start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_bpf.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BRS)		+= guestconn.o
+obj-$(CONFIG_BRS)		+= guestconn_send.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn_listen.o
+obj-$(CONFIG_BHV_VAS)		+= guestcmd.o
+obj-$(CONFIG_BRS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= patch_base.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_fops.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_integrity_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_version.o
+obj-$(CONFIG_BHV_VAS)		+= vmalloc_to_page.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= domain.o
+endif
+obj-$(CONFIG_BRS)		+= drift_detection.o
+obj-$(CONFIG_BRS)		+= reverse_shell_detection.o
+obj-$(CONFIG_BRS)		+= lsm.o
+obj-$(CONFIG_BHV_VAS)		+= memory_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= inode.o
+obj-$(CONFIG_BRS)		+= config.o
+ifeq ($(CONFIG_KEYS),y)
+obj-$(CONFIG_BHV_VAS)		+= keyring.o
+endif
+obj-$(CONFIG_BRS)		+= brs_fs.o
+obj-$(CONFIG_BRS)		+= brs_policy.o
+obj-$(CONFIG_BRS)		+= libinsight.o
diff --git security/bhv/abi_autogen.c security/bhv/abi_autogen.c
new file mode 100644
index 0000000000..c648f04f07
--- /dev/null
+++ security/bhv/abi_autogen.c
@@ -0,0 +1,345 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-10-29T15:55:25).
+ */
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__MemFlags: %s%s%s",
+                HypABI__Integrity__MemFlags__has_TRANSIENT(addr) ? "TRANSIENT " : "", 
+                HypABI__Integrity__MemFlags__has_MUTABLE(addr) ? "MUTABLE " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__Freeze__FreezeFlags: %s%s%s%s%s",
+                HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(addr) ? "CREATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(addr) ? "UPDATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(addr) ? "REMOVE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(addr) ? "PATCH " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        pr_info("HypABI__Guestlog__Init__GuestlogFlags: %s%s%s%s%s%s%s%s%s%s",
+                HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(addr) ? "PROCESS_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(addr) ? "DRIVER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(addr) ? "KERNEL_ACCESS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(addr) ? "UNKNOWN_FILEOPS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(addr) ? "KERNEL_EXEC_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(addr) ? "CONTAINER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_SOCKET_EVENTS(addr) ? "SOCKET_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_FILE_EVENTS(addr) ? "FILE_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_MMAP_EXEC_FILE_EVENTS(addr) ? "MMAP_EXEC_FILE_EVENTS " : "", 
+                ""
+        );
+}
+
+
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        pr_info("HypABI__FileProtection__Init__Config: %s%s%s%s",
+                HypABI__FileProtection__Init__Config__has_READ_ONLY(addr) ? "READ_ONLY " : "", 
+                HypABI__FileProtection__Init__Config__has_FILE_OPS(addr) ? "FILE_OPS " : "", 
+                HypABI__FileProtection__Init__Config__has_DIRTY_CRED(addr) ? "DIRTY_CRED " : "", 
+                ""
+        );
+}
+
+struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Remove__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__Patch__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Guestlog__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Configure__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Assign__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Commit__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Verification__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = NULL;
+struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab = NULL;
+struct kmem_cache *HypABI__GuestPolicy__GetPolicy__arg__slab = NULL;
+struct kmem_cache *HypABI__GuestPolicy__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Extend__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Delete__arg__slab = NULL;
+
+void HypABI__init_slabs(void)
+{
+        HypABI__Integrity__Create__Mem_Region__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__Mem_Region__slab",
+                sizeof(HypABI__Integrity__Create__Mem_Region__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__Mem_Region__slab)
+                panic("Could not create slab HypABI__Integrity__Create__Mem_Region__slab!\n");
+        HypABI__Integrity__Create__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__arg__slab",
+                sizeof(HypABI__Integrity__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Create__arg__slab!\n");
+        HypABI__Integrity__Update__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Update__arg__slab",
+                sizeof(HypABI__Integrity__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Update__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Update__arg__slab!\n");
+        HypABI__Integrity__Remove__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Remove__arg__slab",
+                sizeof(HypABI__Integrity__Remove__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Remove__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Remove__arg__slab!\n");
+        HypABI__Integrity__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Freeze__arg__slab",
+                sizeof(HypABI__Integrity__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Freeze__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Freeze__arg__slab!\n");
+        HypABI__Integrity__PtpgInit__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__PtpgInit__arg__slab",
+                sizeof(HypABI__Integrity__PtpgInit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__PtpgInit__arg__slab)
+                panic("Could not create slab HypABI__Integrity__PtpgInit__arg__slab!\n");
+        HypABI__Patch__Patch__arg__slab = kmem_cache_create(
+                "HypABI__Patch__Patch__arg__slab",
+                sizeof(HypABI__Patch__Patch__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__Patch__arg__slab)
+                panic("Could not create slab HypABI__Patch__Patch__arg__slab!\n");
+        HypABI__Patch__PatchNoClose__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchNoClose__arg__slab",
+                sizeof(HypABI__Patch__PatchNoClose__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchNoClose__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchNoClose__arg__slab!\n");
+        HypABI__Patch__PatchViolation__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchViolation__arg__slab",
+                sizeof(HypABI__Patch__PatchViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchViolation__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchViolation__arg__slab!\n");
+        HypABI__Acl__ProcessViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__ProcessViolation__arg__slab",
+                sizeof(HypABI__Acl__ProcessViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__ProcessViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__ProcessViolation__arg__slab!\n");
+        HypABI__Acl__DriverViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__DriverViolation__arg__slab",
+                sizeof(HypABI__Acl__DriverViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__DriverViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__DriverViolation__arg__slab!\n");
+        HypABI__Guestlog__Init__arg__slab = kmem_cache_create(
+                "HypABI__Guestlog__Init__arg__slab",
+                sizeof(HypABI__Guestlog__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Guestlog__Init__arg__slab)
+                panic("Could not create slab HypABI__Guestlog__Init__arg__slab!\n");
+        HypABI__Creds__Configure__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Configure__arg__slab",
+                sizeof(HypABI__Creds__Configure__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Configure__arg__slab)
+                panic("Could not create slab HypABI__Creds__Configure__arg__slab!\n");
+        HypABI__Creds__RegisterInitTask__arg__slab = kmem_cache_create(
+                "HypABI__Creds__RegisterInitTask__arg__slab",
+                sizeof(HypABI__Creds__RegisterInitTask__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__RegisterInitTask__arg__slab)
+                panic("Could not create slab HypABI__Creds__RegisterInitTask__arg__slab!\n");
+        HypABI__Creds__Assign__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Assign__arg__slab",
+                sizeof(HypABI__Creds__Assign__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Assign__arg__slab)
+                panic("Could not create slab HypABI__Creds__Assign__arg__slab!\n");
+        HypABI__Creds__AssignPriv__arg__slab = kmem_cache_create(
+                "HypABI__Creds__AssignPriv__arg__slab",
+                sizeof(HypABI__Creds__AssignPriv__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__AssignPriv__arg__slab)
+                panic("Could not create slab HypABI__Creds__AssignPriv__arg__slab!\n");
+        HypABI__Creds__Commit__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Commit__arg__slab",
+                sizeof(HypABI__Creds__Commit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Commit__arg__slab)
+                panic("Could not create slab HypABI__Creds__Commit__arg__slab!\n");
+        HypABI__Creds__Release__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Release__arg__slab",
+                sizeof(HypABI__Creds__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Release__arg__slab)
+                panic("Could not create slab HypABI__Creds__Release__arg__slab!\n");
+        HypABI__Creds__Verification__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Verification__arg__slab",
+                sizeof(HypABI__Creds__Verification__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Verification__arg__slab)
+                panic("Could not create slab HypABI__Creds__Verification__arg__slab!\n");
+        HypABI__Creds__Log__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Log__arg__slab",
+                sizeof(HypABI__Creds__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Log__arg__slab)
+                panic("Could not create slab HypABI__Creds__Log__arg__slab!\n");
+        HypABI__FileProtection__Init__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__Init__arg__slab",
+                sizeof(HypABI__FileProtection__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__Init__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__Init__arg__slab!\n");
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab!\n");
+        HypABI__FileProtection__ViolationFileOps__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationFileOps__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationFileOps__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationFileOps__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationFileOps__arg__slab!\n");
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab!\n");
+        HypABI__RegisterProtection__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__RegisterProtection__Freeze__arg__slab",
+                sizeof(HypABI__RegisterProtection__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__RegisterProtection__Freeze__arg__slab)
+                panic("Could not create slab HypABI__RegisterProtection__Freeze__arg__slab!\n");
+        HypABI__Inode__Register__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Register__arg__slab",
+                sizeof(HypABI__Inode__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Register__arg__slab)
+                panic("Could not create slab HypABI__Inode__Register__arg__slab!\n");
+        HypABI__Inode__Update__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Update__arg__slab",
+                sizeof(HypABI__Inode__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Update__arg__slab)
+                panic("Could not create slab HypABI__Inode__Update__arg__slab!\n");
+        HypABI__Inode__Release__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Release__arg__slab",
+                sizeof(HypABI__Inode__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Release__arg__slab)
+                panic("Could not create slab HypABI__Inode__Release__arg__slab!\n");
+        HypABI__Inode__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Verify__arg__slab",
+                sizeof(HypABI__Inode__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Verify__arg__slab)
+                panic("Could not create slab HypABI__Inode__Verify__arg__slab!\n");
+        HypABI__Inode__Log__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Log__arg__slab",
+                sizeof(HypABI__Inode__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Log__arg__slab)
+                panic("Could not create slab HypABI__Inode__Log__arg__slab!\n");
+        HypABI__Keyring__Register__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Register__arg__slab",
+                sizeof(HypABI__Keyring__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Register__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Register__arg__slab!\n");
+        HypABI__Keyring__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Verify__arg__slab",
+                sizeof(HypABI__Keyring__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Verify__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Verify__arg__slab!\n");
+        HypABI__Keyring__Log__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Log__arg__slab",
+                sizeof(HypABI__Keyring__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Log__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Log__arg__slab!\n");
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab",
+                sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab)
+                panic("Could not create slab HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab!\n");
+        HypABI__Confserver__StrictFileops__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__StrictFileops__arg__slab",
+                sizeof(HypABI__Confserver__StrictFileops__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__StrictFileops__arg__slab)
+                panic("Could not create slab HypABI__Confserver__StrictFileops__arg__slab!\n");
+        HypABI__GuestPolicy__GetPolicy__arg__slab = kmem_cache_create(
+                "HypABI__GuestPolicy__GetPolicy__arg__slab",
+                sizeof(HypABI__GuestPolicy__GetPolicy__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__GuestPolicy__GetPolicy__arg__slab)
+                panic("Could not create slab HypABI__GuestPolicy__GetPolicy__arg__slab!\n");
+        HypABI__GuestPolicy__Init__arg__slab = kmem_cache_create(
+                "HypABI__GuestPolicy__Init__arg__slab",
+                sizeof(HypABI__GuestPolicy__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__GuestPolicy__Init__arg__slab)
+                panic("Could not create slab HypABI__GuestPolicy__Init__arg__slab!\n");
+        HypABI__Wagner__Create__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Create__arg__slab",
+                sizeof(HypABI__Wagner__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Create__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Create__arg__slab!\n");
+        HypABI__Wagner__Extend__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Extend__arg__slab",
+                sizeof(HypABI__Wagner__Extend__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Extend__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Extend__arg__slab!\n");
+        HypABI__Wagner__Delete__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Delete__arg__slab",
+                sizeof(HypABI__Wagner__Delete__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Delete__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Delete__arg__slab!\n");
+}
diff --git security/bhv/acl.c security/bhv/acl.c
new file mode 100644
index 0000000000..b9f7cec43a
--- /dev/null
+++ security/bhv/acl.c
@@ -0,0 +1,316 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include <bhv/bhv.h>
+#include <bhv/context.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define PATH_DELIMITER '/'
+
+HypABI__Acl__ProcessInit__arg__T *ProcessInit_acl_config __ro_after_init = NULL;
+HypABI__Acl__DriverInit__arg__T *DriverInit_acl_config __ro_after_init = NULL;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+#define ACL_INIT(name)                                                         \
+	unsigned long r;                                                       \
+	HypABI__Acl__##name##Init__arg__T *acl_config =                        \
+		(HypABI__Acl__##name##Init__arg__T *)__get_free_pages(         \
+			GFP_KERNEL, 0);                                        \
+	if (acl_config == NULL) {                                              \
+		bhv_fail("Cannot allocate acl config");                        \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	acl_config->num_pages = 1;                                             \
+	r = HypABI__Acl__##name##Init__hypercall_noalloc(                      \
+		acl_config, PAGE_SIZE - HypABI__Acl__##name##Init__arg__SZ);   \
+	if (r) {                                                               \
+		pr_err("acl init fail");                                       \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	if (!acl_config->valid) {                                              \
+		uint16_t required_pages = acl_config->num_pages;               \
+		free_pages((unsigned long)acl_config, 0);                      \
+                                                                               \
+		acl_config =                                                   \
+			(HypABI__Acl__##name##Init__arg__T *)__get_free_pages( \
+				GFP_KERNEL, order_base_2(required_pages));     \
+                                                                               \
+		if (acl_config == NULL) {                                      \
+			bhv_fail("Cannot allocate acl config");                \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		r = HypABI__Acl__##name##Init__hypercall_noalloc(              \
+			acl_config,                                            \
+			required_pages * PAGE_SIZE -                           \
+				HypABI__Acl__##name##Init__arg__SZ);           \
+		if (r) {                                                       \
+			pr_err("acl init fail");                               \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		if (!acl_config->valid) {                                      \
+			bhv_fail("host returned invalid config");              \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	/* Protect memory */                                                   \
+	if (bhv_integrity_is_enabled()) {                                      \
+		HypABI__Integrity__Create__Mem_Region__T *region =             \
+			HypABI__Integrity__Create__Mem_Region__ALLOC();        \
+                                                                               \
+		region->start_addr = virt_to_phys(acl_config);                 \
+		region->size = acl_config->num_pages * PAGE_SIZE;              \
+		region->type = HypABI__Integrity__MemType__DATA_READ_ONLY;     \
+		region->flags = HypABI__Integrity__MemFlags__NONE;             \
+		region->next = BHV_INVALID_PHYS_ADDR;                          \
+		strscpy(region->label, "ACL CONFIG",                           \
+			HypABI__Integrity__MAX_LABEL_SIZE);                    \
+                                                                               \
+		r = bhv_create_kern_phys_mem_region_hyp(0, region);            \
+                                                                               \
+		HypABI__Integrity__Create__Mem_Region__FREE(region);           \
+                                                                               \
+		if (r) {                                                       \
+			pr_err("Cannot protect acl config");                   \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	name##Init_acl_config = acl_config;
+
+void __init bhv_mm_init_acl(void)
+{
+	if (bhv_acl_is_proc_acl_enabled()) {
+		ACL_INIT(Process);
+	}
+	if (bhv_acl_is_driver_acl_enabled()) {
+		ACL_INIT(Driver);
+	}
+}
+#undef ACL_INIT
+/************************************************************/
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+#define MATCHES(name)                                                         \
+	static bool _matches_##name(const char *target, bool strip_ext)       \
+	{                                                                     \
+		size_t target_len = 0;                                        \
+		size_t target_ext_len = 0;                                    \
+		uint16_t i;                                                   \
+                                                                              \
+		BUG_ON(target[0] != PATH_DELIMITER);                          \
+                                                                              \
+		/* Get target len */                                          \
+		target_len = strnlen(target, PATH_MAX);                       \
+		if (strip_ext) {                                              \
+			target_ext_len = _get_ext_len(target);                \
+			target_len -= target_ext_len;                         \
+		}                                                             \
+                                                                              \
+		if (target_len == 0 || target_len >= PATH_MAX)                \
+			return false;                                         \
+                                                                              \
+		for (i = 0; i < name##Init_acl_config->list_len; i++) {       \
+			const char *cur = ((char *)name##Init_acl_config) +   \
+					  name##Init_acl_config->list[i];     \
+			size_t cur_len = 0;                                   \
+                                                                              \
+			if (cur[0] != PATH_DELIMITER) {                       \
+				if (_match_names(cur, target, target_ext_len, \
+						 strip_ext))                  \
+					return true;                          \
+				else                                          \
+					continue;                             \
+			}                                                     \
+                                                                              \
+			cur_len = strnlen(cur, PATH_MAX);                     \
+			if (strip_ext) {                                      \
+				cur_len -= _get_ext_len(cur);                 \
+			}                                                     \
+                                                                              \
+			if (cur_len == 0 || cur_len >= PATH_MAX)              \
+				continue;                                     \
+                                                                              \
+			if (cur[cur_len - 1] == '*') {                        \
+				cur_len--;                                    \
+                                                                              \
+				if (target_len < cur_len)                     \
+					continue;                             \
+			} else if (target_len != cur_len)                     \
+				continue;                                     \
+                                                                              \
+			if (strncmp(cur, target, cur_len) == 0)               \
+				return true;                                  \
+		}                                                             \
+                                                                              \
+		return false;                                                 \
+	}
+
+MATCHES(Process)
+MATCHES(Driver)
+#undef MATCHES
+
+#define BLOCK_ENTITY(name)                                                     \
+	static bool _block_entity_##name(const char *target, bool strip_ext)   \
+	{                                                                      \
+		bool rv;                                                       \
+		unsigned long r;                                               \
+		size_t target_len = strlen(target);                            \
+		bool m;                                                        \
+		HypABI__Acl__##name##Violation__arg__T *violation;             \
+                                                                               \
+		if (name##Init_acl_config == NULL ||                           \
+		    !name##Init_acl_config->valid) {                           \
+			bhv_fail(                                              \
+				"unable to resolve entity due to init error"); \
+			return false;                                          \
+		}                                                              \
+                                                                               \
+		m = _matches_##name(target, strip_ext);                        \
+                                                                               \
+		/* Is this entity part of the allow list? */                   \
+		if (m && name##Init_acl_config->is_allow)                      \
+			return false;                                          \
+		/* Is this entity _NOT_ in the deny list? */                   \
+		if (!m && !name##Init_acl_config->is_allow)                    \
+			return false;                                          \
+                                                                               \
+		BUG_ON(target_len >= PAGE_SIZE);                               \
+                                                                               \
+		violation = HypABI__Acl__##name##Violation__arg__ALLOC();      \
+                                                                               \
+		/* Get Context */                                              \
+		r = populate_context(&violation->context, true);               \
+		if (r) {                                                       \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			bhv_fail("%s: BHV cannot retrieve event context",      \
+				 __FUNCTION__);                                \
+		}                                                              \
+                                                                               \
+		violation->name_len = target_len,                              \
+		violation->na##me = virt_to_phys((volatile void *)target);     \
+                                                                               \
+		/* Hypercall */                                                \
+		/* We cannot use the HL interface due to stack constraints */  \
+		r = HypABI__Acl__##name##Violation__hypercall_noalloc(         \
+			violation);                                            \
+		if (r) {                                                       \
+			pr_err("entity hypercall failed");                     \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			return true;                                           \
+		}                                                              \
+                                                                               \
+		rv = (bool)violation->block;                                   \
+		HypABI__Acl__##name##Violation__arg__FREE(violation);          \
+                                                                               \
+		return (bool)rv;                                               \
+	}
+
+BLOCK_ENTITY(Process)
+BLOCK_ENTITY(Driver)
+#undef BLOCK_ENTITY
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity_Driver(target, true);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity_Process(target, false);
+}
\ No newline at end of file
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 0000000000..f389c665d8
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_BHV_TRACEPOINTS
+#define CREATE_TRACE_POINTS
+#endif
+#include <bhv/bhv_trace.h>
+
+bool bhv_initialized __ro_after_init = false;
\ No newline at end of file
diff --git security/bhv/brpol_autogen.c security/bhv/brpol_autogen.c
new file mode 100644
index 0000000000..0feb9c7ce9
--- /dev/null
+++ security/bhv/brpol_autogen.c
@@ -0,0 +1,789 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_policy_c.py (2025-10-29T15:55:25).
+ */
+
+#include <linux/jump_label.h>
+#include <linux/printk.h>
+#include <linux/static_call.h>
+
+#include <bhv/base.h>
+#include <bhv/config_structs.h>
+#include <bhv/flast.h>
+#include <bhv/flast_ids_autogen.h>
+#include <bhv/guestlog_internal.h>
+#include <bhv/policy_sks_autogen.h>
+
+__section(".rodata") const flast_index FLAST__Version__ARR[1] = {0U};
+__section(".rodata") const flast_index FLAST__BrassPolicyOptionsHash__ARR[1] = {1U};
+__section(".rodata") const flast_index FLAST__DriverACL__enabled__ARR[2] = {2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__DriverACL__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__DriverACL__remediate__ARR[2] = {2U, 1U};
+__section(".rodata") const flast_index FLAST__DriverACL__type__ARR[2] = {2U, 2U};
+__section(".rodata") const flast_index FLAST__DriverACL__acl__ARR[2] = {2U, 3U};
+__section(".rodata") const flast_index FLAST__ProcessACL__enabled__ARR[2] = {3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__ProcessACL__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__ProcessACL__remediate__ARR[2] = {3U, 1U};
+__section(".rodata") const flast_index FLAST__ProcessACL__type__ARR[2] = {3U, 2U};
+__section(".rodata") const flast_index FLAST__ProcessACL__acl__ARR[2] = {3U, 3U};
+__section(".rodata") const flast_index FLAST__FileProtection__read_only__enabled__ARR[3] = {4U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__FileProtection__read_only__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__FileProtection__read_only__remediate__ARR[3] = {4U, 0U, 1U};
+__section(".rodata") const flast_index FLAST__FileProtection__file_ops__enabled__ARR[3] = {4U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__FileProtection__file_ops__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__FileProtection__file_ops__remediate__ARR[3] = {4U, 1U, 1U};
+__section(".rodata") const flast_index FLAST__FileProtection__file_ops__strict__ARR[3] = {4U, 1U, 2U};
+__section(".rodata") const flast_index FLAST__FileProtection__dirtycred__enabled__ARR[3] = {4U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__FileProtection__dirtycred__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__FileProtection__dirtycred__remediate__ARR[3] = {4U, 2U, 1U};
+__section(".rodata") const flast_index FLAST__Logging__driver__driver_load__enabled__ARR[4] = {5U, 0U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__driver__driver_load__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__driver__driver_load__synchronous__ARR[4] = {5U, 0U, 0U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__driver__driver_load__synchronous__sck, send_evt_msg);
+void *____brs__Logging__driver__driver_load__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__process__process_exec__enabled__ARR[4] = {5U, 1U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__process__process_exec__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__process__process_exec__synchronous__ARR[4] = {5U, 1U, 0U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__process__process_exec__synchronous__sck, send_evt_msg);
+void *____brs__Logging__process__process_exec__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__process__process_fork__enabled__ARR[4] = {5U, 1U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__process__process_fork__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__process__process_terminate__enabled__ARR[4] = {5U, 1U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__process__process_terminate__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__process__executable_exec_stack__enabled__ARR[4] = {5U, 1U, 3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__process__executable_exec_stack__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__process__executable_exec_stack__synchronous__ARR[4] = {5U, 1U, 3U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__process__executable_exec_stack__synchronous__sck, send_evt_msg);
+void *____brs__Logging__process__executable_exec_stack__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__kernel_access_violation__enabled__ARR[4] = {5U, 2U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__kernel__kernel_access_violation__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__kernel_exec__enabled__ARR[4] = {5U, 2U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__kernel__kernel_exec__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__kernel_exec__synchronous__ARR[4] = {5U, 2U, 1U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__kernel__kernel_exec__synchronous__sck, send_evt_msg);
+void *____brs__Logging__kernel__kernel_exec__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__forced_mem_access__enabled__ARR[4] = {5U, 2U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__kernel__forced_mem_access__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__forced_mem_access__synchronous__ARR[4] = {5U, 2U, 2U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__kernel__forced_mem_access__synchronous__sck, send_evt_msg);
+void *____brs__Logging__kernel__forced_mem_access__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__unsupported_file_operation__enabled__ARR[4] = {5U, 2U, 3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__kernel__unsupported_file_operation__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__kernel__unsupported_file_operation__synchronous__ARR[4] = {5U, 2U, 3U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck, send_evt_msg);
+void *____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__container__capable__enabled__ARR[4] = {5U, 3U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__container__capable__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__container__cgroup_create__enabled__ARR[4] = {5U, 3U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__container__cgroup_create__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__container__cgroup_destroy__enabled__ARR[4] = {5U, 3U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__container__cgroup_destroy__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__container__namespace_change__enabled__ARR[4] = {5U, 3U, 3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__container__namespace_change__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__container__namespace_change__synchronous__ARR[4] = {5U, 3U, 3U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__container__namespace_change__synchronous__sck, send_evt_msg);
+void *____brs__Logging__container__namespace_change__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__socket__interpreter_bound__enabled__ARR[4] = {5U, 4U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__socket__interpreter_bound__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__socket__interpreter_bound__synchronous__ARR[4] = {5U, 4U, 0U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__socket__interpreter_bound__synchronous__sck, send_evt_msg);
+void *____brs__Logging__socket__interpreter_bound__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__socket__interpreter_bound_transitive__enabled__ARR[4] = {5U, 4U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__socket__interpreter_bound_transitive__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__socket__interpreter_bound_transitive__synchronous__ARR[4] = {5U, 4U, 1U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck, send_evt_msg);
+void *____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__socket__socket_accept__enabled__ARR[4] = {5U, 4U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__socket__socket_accept__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__socket__socket_accept__synchronous__ARR[4] = {5U, 4U, 2U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__socket__socket_accept__synchronous__sck, send_evt_msg);
+void *____brs__Logging__socket__socket_accept__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__socket__socket_connection__enabled__ARR[4] = {5U, 4U, 3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__socket__socket_connection__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__socket__socket_connection__synchronous__ARR[4] = {5U, 4U, 3U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__socket__socket_connection__synchronous__sck, send_evt_msg);
+void *____brs__Logging__socket__socket_connection__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_open__enabled__ARR[4] = {5U, 5U, 0U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__file_open__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_open__synchronous__ARR[4] = {5U, 5U, 0U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__file__file_open__synchronous__sck, send_evt_msg);
+void *____brs__Logging__file__file_open__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__file__mmap_exec_file__enabled__ARR[4] = {5U, 5U, 1U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__mmap_exec_file__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__file__mmap_exec_file__synchronous__ARR[4] = {5U, 5U, 1U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__file__mmap_exec_file__synchronous__sck, send_evt_msg);
+void *____brs__Logging__file__mmap_exec_file__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_drift__enabled__ARR[4] = {5U, 5U, 2U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__file_drift__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_drift__synchronous__ARR[4] = {5U, 5U, 2U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__file__file_drift__synchronous__sck, send_evt_msg);
+void *____brs__Logging__file__file_drift__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_set_xattr__enabled__ARR[4] = {5U, 5U, 3U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__file_set_xattr__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__file__file_set_xattr__synchronous__ARR[4] = {5U, 5U, 3U, 1U};
+DEFINE_STATIC_CALL(__brs__Logging__file__file_set_xattr__synchronous__sck, send_evt_msg);
+void *____brs__Logging__file__file_set_xattr__synchronous__sck_state = send_evt_msg;
+
+__section(".rodata") const flast_index FLAST__Logging__file__lib_trace__enabled__ARR[4] = {5U, 5U, 4U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__lib_trace__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__file__skip_sym_hook__enabled__ARR[4] = {5U, 5U, 5U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__Logging__file__skip_sym_hook__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__Logging__filter_options__protected_unix_sockets__ARR[3] = {5U, 6U, 0U};
+__section(".rodata") const flast_index FLAST__Logging__filter_options__exempt_cgroups__ARR[3] = {5U, 6U, 1U};
+__section(".rodata") const flast_index FLAST__Logging__filter_options__trace_exclusion_list__ARR[3] = {5U, 6U, 2U};
+__section(".rodata") const flast_index FLAST__Hypercall__Timeout__enabled__ARR[2] = {6U, 0U};
+__section(".rodata") const flast_index FLAST__Hypercall__Timeout__remediate__ARR[2] = {6U, 1U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__enabled__ARR[2] = {7U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__IntegrityProtection__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__IntegrityProtection__remediate__ARR[2] = {7U, 1U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__kernel_integrity_mutable__ARR[2] = {7U, 2U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__kernel_integrity_dynamic__ARR[2] = {7U, 3U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__kernel_integrity_freeze_after_start__ARR[2] = {7U, 4U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__kernel_integrity_freeze_after_boot__ARR[2] = {7U, 5U};
+__section(".rodata") const flast_index FLAST__IntegrityProtection__pagetable_protection__enabled__ARR[3] = {7U, 6U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__IntegrityProtection__pagetable_protection__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__IntegrityProtection__pagetable_protection__remediate__ARR[3] = {7U, 6U, 1U};
+__section(".rodata") const flast_index FLAST__StrongIsolation__enabled__ARR[2] = {8U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__StrongIsolation__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__StrongIsolation__remediate__ARR[2] = {8U, 1U};
+DEFINE_STATIC_KEY_FALSE(__brs__StrongIsolation__remediate__sk);
+
+__section(".rodata") const flast_index FLAST__StrongIsolation__isolate__ARR[2] = {8U, 2U};
+DEFINE_STATIC_KEY_FALSE(__brs__StrongIsolation__isolate__sk);
+
+__section(".rodata") const flast_index FLAST__CredProtection__enabled__ARR[2] = {9U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__CredProtection__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__CredProtection__remediate__ARR[2] = {9U, 1U};
+__section(".rodata") const flast_index FLAST__GuestKernelConfig__userspace_force_nx_stack__ARR[2] = {10U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__GuestKernelConfig__userspace_force_nx_stack__sk);
+
+__section(".rodata") const flast_index FLAST__GuestKernelConfig__modprobe_path__ARR[2] = {10U, 1U};
+__section(".rodata") const flast_index FLAST__GuestKernelConfig__poweroff_cmd__ARR[2] = {10U, 2U};
+__section(".rodata") const flast_index FLAST__GuestKernelConfig__core_pattern__ARR[2] = {10U, 3U};
+__section(".rodata") const flast_index FLAST__RegisterProtect__filter__ARR[2] = {11U, 0U};
+__section(".rodata") const flast_index FLAST__RegisterProtect__counter__ARR[2] = {11U, 1U};
+__section(".rodata") const flast_index FLAST__RegisterProtectVAS__enabled__ARR[2] = {12U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__RegisterProtectVAS__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__RegisterProtectVAS__remediate__ARR[2] = {12U, 1U};
+__section(".rodata") const flast_index FLAST__GuestPolicy__enabled__ARR[2] = {13U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__GuestPolicy__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__HeartBeat__enabled__ARR[2] = {14U, 0U};
+__section(".rodata") const flast_index FLAST__HeartBeat__interval_sec__ARR[2] = {14U, 1U};
+__section(".rodata") const flast_index FLAST__DriftDetection__enabled__ARR[2] = {15U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__DriftDetection__enabled__sk);
+
+__section(".rodata") const flast_index FLAST__DriftDetection__container_host__ARR[2] = {15U, 1U};
+DEFINE_STATIC_KEY_FALSE(__brs__DriftDetection__container_host__sk);
+
+__section(".rodata") const flast_index FLAST__ReverseShellDetection__container_host__ARR[2] = {16U, 0U};
+DEFINE_STATIC_KEY_FALSE(__brs__ReverseShellDetection__container_host__sk);
+
+__section(".rodata") const flast_index FLAST__ReverseShellDetection__interpreter__ARR[2] = {16U, 1U};
+DEFINE_STATIC_KEY_FALSE(brs_retrieve_from_policy_sk);
+
+bool brs_policy_check(const char *_flast_data, size_t _size) {
+        int err;
+        size_t size;
+        flast_buf flast_data = {
+                .buf = (char *)_flast_data,
+                .size = _size
+        };
+
+        if (_flast_data[0] != 0x46 || _flast_data[1] != 0x4c || _flast_data[2] != 0x41
+            || _flast_data[3] != 0x53 || _flast_data[4] != 0x54 || _flast_data[5] != 0x0a
+            || _flast_data[6] != 0x0a || _flast_data[7] != 0x0a) {
+            pr_warn("Policy: Bad flast magic\n");
+            return false;
+        }
+
+        {
+                flast_index Version_arr[1] = {0U};
+                FLAST_CREATE_LOOKUP(Version_lk, &flast_data, Version_arr, 1);
+                const char *Version = flast_lookup_string(&Version_lk, &err, &size);
+                if (err != 0) {
+                        pr_warn("Could not read config: Version\n");
+                        return false;
+                }
+                if (size != 8 || 0 != strncmp(Version, "25.08.0", 8)) {
+                        pr_warn("Bad Version in attempted policy (expected: \"25.08.0\"/8, got: \"%s\"/%lu)\n", Version, size);
+                        return false;
+                }
+        }
+
+        {
+                flast_index BrassPolicyOptionsHash_arr[1] = {1U};
+                FLAST_CREATE_LOOKUP(BrassPolicyOptionsHash_lk, &flast_data, BrassPolicyOptionsHash_arr, 1);
+                const char *BrassPolicyOptionsHash = flast_lookup_string(&BrassPolicyOptionsHash_lk, &err, &size);
+                if (err != 0) {
+                        pr_warn("Could not read config: BrassPolicyOptionsHash\n");
+                        return false;
+                }
+                if (size != 41 || 0 != strncmp(BrassPolicyOptionsHash, "e00398b36fdcbc05dddec17069141556153a414c", 41)) {
+                        pr_warn("Bad BrassPolicyOptionsHash in attempted policy (expected: \"e00398b36fdcbc05dddec17069141556153a414c\"/41, got: \"%s\"/%lu)\n", BrassPolicyOptionsHash, size);
+                        return false;
+                }
+        }
+
+        {
+                flast_index enabled_arr[2] = {7U, 0U};
+                FLAST_CREATE_LOOKUP(enabled_lk, &flast_data, enabled_arr, 2);
+                bool enabled = flast_lookup_bool(&enabled_lk, &err);
+                if (err != 0) {
+                        pr_warn("Could not read config: enabled\n");
+                        return false;
+                }
+                if (!enabled) {
+                        pr_warn("Bad enabled in attempted policy (expected: true, got: false)\n");
+                        return false;
+                }
+        }
+
+        return true;
+}
+
+void brs_guestlog_disable_all(void) {
+        static_branch_disable(&__brs__Logging__driver__driver_load__enabled__sk);
+        static_branch_disable(&__brs__Logging__process__process_exec__enabled__sk);
+        static_branch_disable(&__brs__Logging__process__process_fork__enabled__sk);
+        static_branch_disable(&__brs__Logging__process__process_terminate__enabled__sk);
+        static_branch_disable(&__brs__Logging__process__executable_exec_stack__enabled__sk);
+        static_branch_disable(&__brs__Logging__kernel__kernel_access_violation__enabled__sk);
+        static_branch_disable(&__brs__Logging__kernel__kernel_exec__enabled__sk);
+        static_branch_disable(&__brs__Logging__kernel__forced_mem_access__enabled__sk);
+        static_branch_disable(&__brs__Logging__kernel__unsupported_file_operation__enabled__sk);
+        static_branch_disable(&__brs__Logging__container__capable__enabled__sk);
+        static_branch_disable(&__brs__Logging__container__cgroup_create__enabled__sk);
+        static_branch_disable(&__brs__Logging__container__cgroup_destroy__enabled__sk);
+        static_branch_disable(&__brs__Logging__container__namespace_change__enabled__sk);
+        static_branch_disable(&__brs__Logging__socket__interpreter_bound__enabled__sk);
+        static_branch_disable(&__brs__Logging__socket__interpreter_bound_transitive__enabled__sk);
+        static_branch_disable(&__brs__Logging__socket__socket_accept__enabled__sk);
+        static_branch_disable(&__brs__Logging__socket__socket_connection__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__file_open__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__mmap_exec_file__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__file_drift__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__file_set_xattr__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__lib_trace__enabled__sk);
+        static_branch_disable(&__brs__Logging__file__skip_sym_hook__enabled__sk);
+}
+
+void brs_disable_sync(void) {
+        if (____brs__Logging__driver__driver_load__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__driver__driver_load__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__driver__driver_load__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__process__process_exec__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__process__process_exec__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__process__process_exec__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__process__executable_exec_stack__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__process__executable_exec_stack__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__process__executable_exec_stack__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__kernel__kernel_exec__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__kernel__kernel_exec__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__kernel__kernel_exec__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__kernel__forced_mem_access__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__kernel__forced_mem_access__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__kernel__forced_mem_access__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__container__namespace_change__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__container__namespace_change__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__container__namespace_change__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__socket__interpreter_bound__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__socket__interpreter_bound__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__socket__interpreter_bound__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__socket__socket_accept__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__socket__socket_accept__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__socket__socket_accept__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__socket__socket_connection__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__socket__socket_connection__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__socket__socket_connection__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__file__file_open__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__file__file_open__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__file__file_open__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__file__mmap_exec_file__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__file__mmap_exec_file__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__file__mmap_exec_file__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__file__file_drift__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__file__file_drift__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__file__file_drift__synchronous__sck, send_evt_msg);
+        }
+
+        if (____brs__Logging__file__file_set_xattr__synchronous__sck_state == send_evt_msg_sync) {
+            ____brs__Logging__file__file_set_xattr__synchronous__sck_state = send_evt_msg;
+            static_call_update(__brs__Logging__file__file_set_xattr__synchronous__sck, send_evt_msg);
+        }
+
+}
+
+void brs_set_sks(flast_buf *buf) {
+        // Logging::driver::driver_load::enabled
+        if (FLAST__Logging__driver__driver_load__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__driver__driver_load__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__driver__driver_load__enabled__sk);
+        }
+
+        // Logging::driver::driver_load::synchronous
+        if (FLAST__Logging__driver__driver_load__synchronous(buf)) {
+                ____brs__Logging__driver__driver_load__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__driver__driver_load__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__driver__driver_load__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__driver__driver_load__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::process::process_exec::enabled
+        if (FLAST__Logging__process__process_exec__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__process__process_exec__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__process__process_exec__enabled__sk);
+        }
+
+        // Logging::process::process_exec::synchronous
+        if (FLAST__Logging__process__process_exec__synchronous(buf)) {
+                ____brs__Logging__process__process_exec__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__process__process_exec__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__process__process_exec__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__process__process_exec__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::process::process_fork::enabled
+        if (FLAST__Logging__process__process_fork__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__process__process_fork__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__process__process_fork__enabled__sk);
+        }
+
+        // Logging::process::process_terminate::enabled
+        if (FLAST__Logging__process__process_terminate__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__process__process_terminate__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__process__process_terminate__enabled__sk);
+        }
+
+        // Logging::process::executable_exec_stack::enabled
+        if (FLAST__Logging__process__executable_exec_stack__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__process__executable_exec_stack__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__process__executable_exec_stack__enabled__sk);
+        }
+
+        // Logging::process::executable_exec_stack::synchronous
+        if (FLAST__Logging__process__executable_exec_stack__synchronous(buf)) {
+                ____brs__Logging__process__executable_exec_stack__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__process__executable_exec_stack__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__process__executable_exec_stack__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__process__executable_exec_stack__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::kernel::kernel_access_violation::enabled
+        if (FLAST__Logging__kernel__kernel_access_violation__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__kernel__kernel_access_violation__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__kernel__kernel_access_violation__enabled__sk);
+        }
+
+        // Logging::kernel::kernel_exec::enabled
+        if (FLAST__Logging__kernel__kernel_exec__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__kernel__kernel_exec__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__kernel__kernel_exec__enabled__sk);
+        }
+
+        // Logging::kernel::kernel_exec::synchronous
+        if (FLAST__Logging__kernel__kernel_exec__synchronous(buf)) {
+                ____brs__Logging__kernel__kernel_exec__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__kernel__kernel_exec__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__kernel__kernel_exec__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__kernel__kernel_exec__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::kernel::forced_mem_access::enabled
+        if (FLAST__Logging__kernel__forced_mem_access__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__kernel__forced_mem_access__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__kernel__forced_mem_access__enabled__sk);
+        }
+
+        // Logging::kernel::forced_mem_access::synchronous
+        if (FLAST__Logging__kernel__forced_mem_access__synchronous(buf)) {
+                ____brs__Logging__kernel__forced_mem_access__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__kernel__forced_mem_access__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__kernel__forced_mem_access__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__kernel__forced_mem_access__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::kernel::unsupported_file_operation::enabled
+        if (FLAST__Logging__kernel__unsupported_file_operation__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__kernel__unsupported_file_operation__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__kernel__unsupported_file_operation__enabled__sk);
+        }
+
+        // Logging::kernel::unsupported_file_operation::synchronous
+        if (FLAST__Logging__kernel__unsupported_file_operation__synchronous(buf)) {
+                ____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__kernel__unsupported_file_operation__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::container::capable::enabled
+        if (FLAST__Logging__container__capable__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__container__capable__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__container__capable__enabled__sk);
+        }
+
+        // Logging::container::cgroup_create::enabled
+        if (FLAST__Logging__container__cgroup_create__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__container__cgroup_create__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__container__cgroup_create__enabled__sk);
+        }
+
+        // Logging::container::cgroup_destroy::enabled
+        if (FLAST__Logging__container__cgroup_destroy__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__container__cgroup_destroy__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__container__cgroup_destroy__enabled__sk);
+        }
+
+        // Logging::container::namespace_change::enabled
+        if (FLAST__Logging__container__namespace_change__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__container__namespace_change__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__container__namespace_change__enabled__sk);
+        }
+
+        // Logging::container::namespace_change::synchronous
+        if (FLAST__Logging__container__namespace_change__synchronous(buf)) {
+                ____brs__Logging__container__namespace_change__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__container__namespace_change__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__container__namespace_change__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__container__namespace_change__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::socket::interpreter_bound::enabled
+        if (FLAST__Logging__socket__interpreter_bound__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__socket__interpreter_bound__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__socket__interpreter_bound__enabled__sk);
+        }
+
+        // Logging::socket::interpreter_bound::synchronous
+        if (FLAST__Logging__socket__interpreter_bound__synchronous(buf)) {
+                ____brs__Logging__socket__interpreter_bound__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__socket__interpreter_bound__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__socket__interpreter_bound__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__socket__interpreter_bound__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::socket::interpreter_bound_transitive::enabled
+        if (FLAST__Logging__socket__interpreter_bound_transitive__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__socket__interpreter_bound_transitive__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__socket__interpreter_bound_transitive__enabled__sk);
+        }
+
+        // Logging::socket::interpreter_bound_transitive::synchronous
+        if (FLAST__Logging__socket__interpreter_bound_transitive__synchronous(buf)) {
+                ____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__socket__interpreter_bound_transitive__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::socket::socket_accept::enabled
+        if (FLAST__Logging__socket__socket_accept__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__socket__socket_accept__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__socket__socket_accept__enabled__sk);
+        }
+
+        // Logging::socket::socket_accept::synchronous
+        if (FLAST__Logging__socket__socket_accept__synchronous(buf)) {
+                ____brs__Logging__socket__socket_accept__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__socket__socket_accept__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__socket__socket_accept__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__socket__socket_accept__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::socket::socket_connection::enabled
+        if (FLAST__Logging__socket__socket_connection__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__socket__socket_connection__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__socket__socket_connection__enabled__sk);
+        }
+
+        // Logging::socket::socket_connection::synchronous
+        if (FLAST__Logging__socket__socket_connection__synchronous(buf)) {
+                ____brs__Logging__socket__socket_connection__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__socket__socket_connection__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__socket__socket_connection__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__socket__socket_connection__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::file::file_open::enabled
+        if (FLAST__Logging__file__file_open__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__file_open__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__file_open__enabled__sk);
+        }
+
+        // Logging::file::file_open::synchronous
+        if (FLAST__Logging__file__file_open__synchronous(buf)) {
+                ____brs__Logging__file__file_open__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__file__file_open__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__file__file_open__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__file__file_open__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::file::mmap_exec_file::enabled
+        if (FLAST__Logging__file__mmap_exec_file__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__mmap_exec_file__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__mmap_exec_file__enabled__sk);
+        }
+
+        // Logging::file::mmap_exec_file::synchronous
+        if (FLAST__Logging__file__mmap_exec_file__synchronous(buf)) {
+                ____brs__Logging__file__mmap_exec_file__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__file__mmap_exec_file__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__file__mmap_exec_file__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__file__mmap_exec_file__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::file::file_drift::enabled
+        if (FLAST__Logging__file__file_drift__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__file_drift__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__file_drift__enabled__sk);
+        }
+
+        // Logging::file::file_drift::synchronous
+        if (FLAST__Logging__file__file_drift__synchronous(buf)) {
+                ____brs__Logging__file__file_drift__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__file__file_drift__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__file__file_drift__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__file__file_drift__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::file::file_set_xattr::enabled
+        if (FLAST__Logging__file__file_set_xattr__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__file_set_xattr__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__file_set_xattr__enabled__sk);
+        }
+
+        // Logging::file::file_set_xattr::synchronous
+        if (FLAST__Logging__file__file_set_xattr__synchronous(buf)) {
+                ____brs__Logging__file__file_set_xattr__synchronous__sck_state = send_evt_msg_sync;
+                static_call_update(__brs__Logging__file__file_set_xattr__synchronous__sck, send_evt_msg_sync);
+        } else {
+                ____brs__Logging__file__file_set_xattr__synchronous__sck_state = send_evt_msg;
+                static_call_update(__brs__Logging__file__file_set_xattr__synchronous__sck, send_evt_msg);
+        }
+
+        // Logging::file::lib_trace::enabled
+        if (FLAST__Logging__file__lib_trace__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__lib_trace__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__lib_trace__enabled__sk);
+        }
+
+        // Logging::file::skip_sym_hook::enabled
+        if (FLAST__Logging__file__skip_sym_hook__enabled(buf)) {
+                static_branch_enable(&__brs__Logging__file__skip_sym_hook__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__Logging__file__skip_sym_hook__enabled__sk);
+        }
+
+        // StrongIsolation::remediate
+        if (FLAST__StrongIsolation__remediate(buf)) {
+                static_branch_enable(&__brs__StrongIsolation__remediate__sk);
+        } else {
+                static_branch_disable(&__brs__StrongIsolation__remediate__sk);
+        }
+
+        // GuestKernelConfig::userspace_force_nx_stack
+        if (FLAST__GuestKernelConfig__userspace_force_nx_stack(buf)) {
+                static_branch_enable(&__brs__GuestKernelConfig__userspace_force_nx_stack__sk);
+        } else {
+                static_branch_disable(&__brs__GuestKernelConfig__userspace_force_nx_stack__sk);
+        }
+
+        // DriftDetection::enabled
+        if (FLAST__DriftDetection__enabled(buf)) {
+                static_branch_enable(&__brs__DriftDetection__enabled__sk);
+        } else {
+                static_branch_disable(&__brs__DriftDetection__enabled__sk);
+        }
+
+        // DriftDetection::container_host
+        if (FLAST__DriftDetection__container_host(buf)) {
+                static_branch_enable(&__brs__DriftDetection__container_host__sk);
+        } else {
+                static_branch_disable(&__brs__DriftDetection__container_host__sk);
+        }
+
+        // ReverseShellDetection::container_host
+        if (FLAST__ReverseShellDetection__container_host(buf)) {
+                static_branch_enable(&__brs__ReverseShellDetection__container_host__sk);
+        } else {
+                static_branch_disable(&__brs__ReverseShellDetection__container_host__sk);
+        }
+
+}
+
+void brs_dump_sks(void) {
+        pr_info("SK state: DriverACL::enabled: %d\n", brs__is__DriverACL__enabled());
+        pr_info("SK state: ProcessACL::enabled: %d\n", brs__is__ProcessACL__enabled());
+        pr_info("SK state: FileProtection::read_only::enabled: %d\n", brs__is__FileProtection__read_only__enabled());
+        pr_info("SK state: FileProtection::file_ops::enabled: %d\n", brs__is__FileProtection__file_ops__enabled());
+        pr_info("SK state: FileProtection::dirtycred::enabled: %d\n", brs__is__FileProtection__dirtycred__enabled());
+        pr_info("SK state: Logging::driver::driver_load::enabled: %d\n", brs__is__Logging__driver__driver_load__enabled());
+        pr_info("SCK state: Logging::driver::driver_load::synchronous: %ps\n", ____brs__Logging__driver__driver_load__synchronous__sck_state);
+        pr_info("SK state: Logging::process::process_exec::enabled: %d\n", brs__is__Logging__process__process_exec__enabled());
+        pr_info("SCK state: Logging::process::process_exec::synchronous: %ps\n", ____brs__Logging__process__process_exec__synchronous__sck_state);
+        pr_info("SK state: Logging::process::process_fork::enabled: %d\n", brs__is__Logging__process__process_fork__enabled());
+        pr_info("SK state: Logging::process::process_terminate::enabled: %d\n", brs__is__Logging__process__process_terminate__enabled());
+        pr_info("SK state: Logging::process::executable_exec_stack::enabled: %d\n", brs__is__Logging__process__executable_exec_stack__enabled());
+        pr_info("SCK state: Logging::process::executable_exec_stack::synchronous: %ps\n", ____brs__Logging__process__executable_exec_stack__synchronous__sck_state);
+        pr_info("SK state: Logging::kernel::kernel_access_violation::enabled: %d\n", brs__is__Logging__kernel__kernel_access_violation__enabled());
+        pr_info("SK state: Logging::kernel::kernel_exec::enabled: %d\n", brs__is__Logging__kernel__kernel_exec__enabled());
+        pr_info("SCK state: Logging::kernel::kernel_exec::synchronous: %ps\n", ____brs__Logging__kernel__kernel_exec__synchronous__sck_state);
+        pr_info("SK state: Logging::kernel::forced_mem_access::enabled: %d\n", brs__is__Logging__kernel__forced_mem_access__enabled());
+        pr_info("SCK state: Logging::kernel::forced_mem_access::synchronous: %ps\n", ____brs__Logging__kernel__forced_mem_access__synchronous__sck_state);
+        pr_info("SK state: Logging::kernel::unsupported_file_operation::enabled: %d\n", brs__is__Logging__kernel__unsupported_file_operation__enabled());
+        pr_info("SCK state: Logging::kernel::unsupported_file_operation::synchronous: %ps\n", ____brs__Logging__kernel__unsupported_file_operation__synchronous__sck_state);
+        pr_info("SK state: Logging::container::capable::enabled: %d\n", brs__is__Logging__container__capable__enabled());
+        pr_info("SK state: Logging::container::cgroup_create::enabled: %d\n", brs__is__Logging__container__cgroup_create__enabled());
+        pr_info("SK state: Logging::container::cgroup_destroy::enabled: %d\n", brs__is__Logging__container__cgroup_destroy__enabled());
+        pr_info("SK state: Logging::container::namespace_change::enabled: %d\n", brs__is__Logging__container__namespace_change__enabled());
+        pr_info("SCK state: Logging::container::namespace_change::synchronous: %ps\n", ____brs__Logging__container__namespace_change__synchronous__sck_state);
+        pr_info("SK state: Logging::socket::interpreter_bound::enabled: %d\n", brs__is__Logging__socket__interpreter_bound__enabled());
+        pr_info("SCK state: Logging::socket::interpreter_bound::synchronous: %ps\n", ____brs__Logging__socket__interpreter_bound__synchronous__sck_state);
+        pr_info("SK state: Logging::socket::interpreter_bound_transitive::enabled: %d\n", brs__is__Logging__socket__interpreter_bound_transitive__enabled());
+        pr_info("SCK state: Logging::socket::interpreter_bound_transitive::synchronous: %ps\n", ____brs__Logging__socket__interpreter_bound_transitive__synchronous__sck_state);
+        pr_info("SK state: Logging::socket::socket_accept::enabled: %d\n", brs__is__Logging__socket__socket_accept__enabled());
+        pr_info("SCK state: Logging::socket::socket_accept::synchronous: %ps\n", ____brs__Logging__socket__socket_accept__synchronous__sck_state);
+        pr_info("SK state: Logging::socket::socket_connection::enabled: %d\n", brs__is__Logging__socket__socket_connection__enabled());
+        pr_info("SCK state: Logging::socket::socket_connection::synchronous: %ps\n", ____brs__Logging__socket__socket_connection__synchronous__sck_state);
+        pr_info("SK state: Logging::file::file_open::enabled: %d\n", brs__is__Logging__file__file_open__enabled());
+        pr_info("SCK state: Logging::file::file_open::synchronous: %ps\n", ____brs__Logging__file__file_open__synchronous__sck_state);
+        pr_info("SK state: Logging::file::mmap_exec_file::enabled: %d\n", brs__is__Logging__file__mmap_exec_file__enabled());
+        pr_info("SCK state: Logging::file::mmap_exec_file::synchronous: %ps\n", ____brs__Logging__file__mmap_exec_file__synchronous__sck_state);
+        pr_info("SK state: Logging::file::file_drift::enabled: %d\n", brs__is__Logging__file__file_drift__enabled());
+        pr_info("SCK state: Logging::file::file_drift::synchronous: %ps\n", ____brs__Logging__file__file_drift__synchronous__sck_state);
+        pr_info("SK state: Logging::file::file_set_xattr::enabled: %d\n", brs__is__Logging__file__file_set_xattr__enabled());
+        pr_info("SCK state: Logging::file::file_set_xattr::synchronous: %ps\n", ____brs__Logging__file__file_set_xattr__synchronous__sck_state);
+        pr_info("SK state: Logging::file::lib_trace::enabled: %d\n", brs__is__Logging__file__lib_trace__enabled());
+        pr_info("SK state: Logging::file::skip_sym_hook::enabled: %d\n", brs__is__Logging__file__skip_sym_hook__enabled());
+        pr_info("SK state: IntegrityProtection::enabled: %d\n", brs__is__IntegrityProtection__enabled());
+        pr_info("SK state: IntegrityProtection::pagetable_protection::enabled: %d\n", brs__is__IntegrityProtection__pagetable_protection__enabled());
+        pr_info("SK state: StrongIsolation::enabled: %d\n", brs__is__StrongIsolation__enabled());
+        pr_info("SK state: StrongIsolation::remediate: %d\n", brs__is__StrongIsolation__remediate());
+        pr_info("SK state: StrongIsolation::isolate: %d\n", brs__is__StrongIsolation__isolate());
+        pr_info("SK state: CredProtection::enabled: %d\n", brs__is__CredProtection__enabled());
+        pr_info("SK state: GuestKernelConfig::userspace_force_nx_stack: %d\n", brs__is__GuestKernelConfig__userspace_force_nx_stack());
+        pr_info("SK state: RegisterProtectVAS::enabled: %d\n", brs__is__RegisterProtectVAS__enabled());
+        pr_info("SK state: GuestPolicy::enabled: %d\n", brs__is__GuestPolicy__enabled());
+        pr_info("SK state: DriftDetection::enabled: %d\n", brs__is__DriftDetection__enabled());
+        pr_info("SK state: DriftDetection::container_host: %d\n", brs__is__DriftDetection__container_host());
+        pr_info("SK state: ReverseShellDetection::container_host: %d\n", brs__is__ReverseShellDetection__container_host());
+}
+
diff --git security/bhv/brs_fs.c security/bhv/brs_fs.c
new file mode 100644
index 0000000000..998d4298ad
--- /dev/null
+++ security/bhv/brs_fs.c
@@ -0,0 +1,932 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 BlueRock Security, Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+#include <linux/jump_label.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/relay.h>
+#include <linux/tracefs.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+#include <linux/xarray.h>
+
+#include <asm/atomic.h>
+
+#include <bhv/brs_fs.h>
+#include <bhv/brs_policy.h>
+#include <bhv/guestlog.h>
+#include <bhv/libinsight.h>
+#include <bhv/policy_sks_autogen.h>
+
+#if !defined(CONFIG_RELAY)
+#error "CONFIG_RELAY is not enabled"
+#error "CONFIG_TRACING is not enabled"
+#endif
+
+#define BRS_FS_DIRNAME "brs"
+#define BRS_FS_FILENAME_RELAY "events"
+#define BRS_FS_FILENAME_RESPONSE "waiting_events"
+
+static const size_t MAX_EVENT_SIZE = 4096;
+static const size_t RELAY_SUBBUF_COUNT = 256;
+static const size_t RELAY_SUBBUF_SIZE = MAX_EVENT_SIZE * 16;
+static const int RELAY_FLUSH_TIMEOUT_MS = 1;
+static const uint8_t GLOBAL_EVENT_TIMEOUT_SEC = 5;
+static const uint8_t SINGLE_EVENT_TIMEOUT_SEC = 10;
+
+static atomic_t timeout_in_progress = ATOMIC_INIT(0);
+DEFINE_STATIC_KEY_FALSE(brs_relay_sk_mode_mmap_enabled);
+
+enum brs_relay_mode {
+	BRS_RELAY_MODE_FILE_READ = 0,
+	BRS_RELAY_MODE_MMAP,
+};
+
+static int relay_mode = BRS_RELAY_MODE_FILE_READ;
+
+struct subbuf_header {
+	size_t padding;
+	atomic_t produced;
+};
+
+struct waiting_event {
+	bool block;
+	atomic_t complete;
+};
+
+struct brs_fs_cpu_private {
+	struct delayed_work work_flush;
+	struct dentry *file_wait;
+	struct delayed_work work_timeout;
+	atomic_t waiting_events; // Counter for the events waiting on this CPU.
+};
+
+struct brs_fs_event_channel {
+	struct rchan *rchan;
+	wait_queue_head_t waitqueue;
+	struct dentry *dir;
+	struct dentry *file_dropped;
+	struct dentry *file_control;
+	struct dentry *file_linux_commit;
+	struct xarray xa_waiting_events;
+	struct brs_fs_cpu_private __percpu *priv;
+	atomic_t dropped;
+};
+static struct brs_fs_event_channel *brs_chan = NULL;
+
+struct dentry *brs_fs_dir(void)
+{
+	return brs_chan->dir;
+}
+
+static void brs_fs_relay_flush_for_cpu(int cpu)
+{
+	struct rchan_buf *buf;
+
+	buf = *per_cpu_ptr(brs_chan->rchan->buf, cpu);
+	if (buf == NULL)
+		return;
+
+	/* Switch subbuffers only if there is data available. */
+	if (static_branch_likely(&brs_relay_sk_mode_mmap_enabled)) {
+		if (buf->offset > sizeof(struct subbuf_header))
+			relay_switch_subbuf(buf, 0);
+	} else {
+		if (buf->offset)
+			relay_switch_subbuf(buf, 0);
+	}
+}
+
+static void brs_fs_deferred_flush(struct work_struct *ws)
+{
+	int cpu;
+
+	preempt_disable();
+
+	cpu = smp_processor_id();
+	brs_fs_relay_flush_for_cpu(cpu);
+
+	preempt_enable();
+}
+
+static void brs_fs_flush_timer_run(int cpu, unsigned long expires)
+{
+	/* Modify delay or queue new deferred relay channel flush. */
+	mod_delayed_work_on(cpu, system_wq,
+			    &per_cpu_ptr(brs_chan->priv, cpu)->work_flush,
+			    expires);
+}
+
+static void brs_fs_flush_timer_stop(int cpu)
+{
+	/*
+	 * Note: the function cancel_delayed_work_sync can block, and thus must
+	 * not be called with preemption disabled.
+	 */
+	cancel_delayed_work_sync(&per_cpu_ptr(brs_chan->priv, cpu)->work_flush);
+}
+
+/**
+ * Disable all synchronous events in case we hit a timeout.
+ *
+ * This is essentially a safe guard against the kernel sensor crashing in
+ * userspace. If this happens, we will move all set all synchronous events
+ * to be asynchronous.
+ */
+static void brs_fs_disable_all_sync(void)
+{
+	unsigned long index;
+	struct waiting_event *event;
+	int expected = 0;
+
+	// Try to get the lock
+	if (!atomic_try_cmpxchg(&timeout_in_progress, &expected, 1)) {
+		// Somebody else won the race. Nothing to do.
+		return;
+	}
+
+	pr_warn("[BRS] synchronous event timeout triggered!\n");
+	WARN_ON(1);
+
+	// Disable all sync events
+	brs_disable_sync();
+
+	// Iterate over all events until all are cleared.
+	// This while loop should guarantee that we wake up all tasks
+	// and clear them. We could also get rid of them and wait for
+	// the individual event timeout.
+	while (wq_has_sleeper(&brs_chan->waitqueue)) {
+		// Mark all waiting events as complete.
+		xa_for_each(&brs_chan->xa_waiting_events, index, event) {
+			// Fail open.
+			event->block = 0;
+			atomic_set(&event->complete, 1);
+		}
+
+		// Wake the waiting threads up.
+		wake_up_all(&brs_chan->waitqueue);
+	}
+
+	// Reset timeout in progress.
+	atomic_set(&timeout_in_progress, 0);
+
+	pr_warn("[BRS] synchronous event timeout done!\n");
+	brs_guestlog_error(true, false);
+}
+
+static void brs_fs_deferred_disable_all_sync(struct work_struct *ws)
+{
+	int cpu;
+
+	preempt_disable();
+	cpu = smp_processor_id();
+
+	// Check if this is a valid timeout.
+	// We assume here that "mod_delayed_work_on" ensures that this function
+	// is either run before the work timer is updated or it will be run after
+	// the new time was set and has expired. That is, we assume that the
+	// timer cannot be updated while a work item is in flight.
+	if (atomic_read(&per_cpu_ptr(brs_chan->priv, cpu)->waiting_events) ==
+	    0) {
+		// No pending events. Nothing to do.
+		preempt_enable();
+		return;
+	}
+
+	preempt_enable();
+
+	// This is a valid timeout. All sync events will become async.
+	brs_fs_disable_all_sync();
+}
+
+static void brs_fs_deferred_disable_all_sync_reset(int cpu)
+{
+	mod_delayed_work_on(cpu, system_wq,
+			    &per_cpu_ptr(brs_chan->priv, cpu)->work_timeout,
+			    msecs_to_jiffies(GLOBAL_EVENT_TIMEOUT_SEC * 1000));
+}
+
+void brs_fs_policy_update()
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		// We will cancel all delayed work such that the policy update can continue.
+		// New events will renable the timeout. Stuck events will be resumed by the
+		// individual event timeout.
+		// Note that we cannot simply reschudle the timeout here. This will lead to
+		// deadlocks. The exact reason is unclear, but it seems that this interferes
+		// with the static call update during the policy update.
+		cancel_delayed_work_sync(
+			&per_cpu_ptr(brs_chan->priv, cpu)->work_timeout);
+	}
+}
+
+ssize_t brs_fs_relay_write(void *data, size_t size, bool sync)
+{
+	int cpu = 0;
+
+	if (brs_chan == NULL)
+		return -EEXIST;
+
+	if (size > MAX_EVENT_SIZE) {
+		BUG();
+		return -E2BIG;
+	}
+
+	preempt_disable();
+	cpu = smp_processor_id();
+
+	relay_write(brs_chan->rchan, data, size);
+	/*
+	 * XXX: we currently do not distinguish between sync and async events
+	 * when resetting the timer. The reason is that cancelling the timer for
+	 * synchronous events can wait (and thus cannot be executed with
+	 * preemption disabled). Instead, we always reset the timer, YET, we
+	 * must track and take into account the performance overhead of
+	 * unconditionally resetting the timer. Consider adjusting if this
+	 * becomes a bottleneck.
+	 */
+	brs_fs_flush_timer_run(cpu, msecs_to_jiffies(RELAY_FLUSH_TIMEOUT_MS));
+
+	if (sync) {
+		struct rchan_buf *buf = *per_cpu_ptr(brs_chan->rchan->buf, cpu);
+		if (buf == NULL) {
+			preempt_enable();
+			return -EEXIST;
+		}
+
+		relay_switch_subbuf(buf, 0);
+	}
+
+	preempt_enable();
+
+	return (ssize_t)size;
+}
+
+ssize_t brs_fs_relay_write_sync(void *data, size_t size, uint64_t id)
+{
+	ssize_t rc = 0;
+	struct waiting_event *event = NULL;
+	int cpu;
+
+	if (unlikely(atomic_read(&timeout_in_progress))) {
+		// There is currently a timeout in progress of a timeout happened
+		// Fail open.
+		return 0;
+	}
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event)
+		return -ENOMEM;
+
+	atomic_set(&event->complete, 0);
+
+	rc = xa_insert(&brs_chan->xa_waiting_events, id, event, GFP_KERNEL);
+	if (rc) {
+		pr_err("[BRS] Cannot insert event[%llu] into xa_array", id);
+		kfree(event);
+		return rc;
+	}
+
+	rc = brs_fs_relay_write(data, size, true);
+	if (rc < 0)
+		goto out;
+
+	// The order of the operations is important here. We reset the timer
+	// before we increase the count. This covers the following cases:
+	// - If the timer fires before we reset it, we know the event count is
+	//   zero and this will not lead to a timeout.
+	// - If the timer is reset, we now we can increase the count before it
+	//   expires.
+	// The only issue that could occur is that multiple tasks reset the event
+	// timer at the same time. This will only give slightly more time to the
+	// events in the queue and will not affect the overall process.
+	// This assumes that "mod_delayed_work_on" is race condition free. That is,
+	// the function guarantees that the work is not in flight while it is
+	// updated.
+	preempt_disable();
+	cpu = smp_processor_id();
+	if (atomic_read(&per_cpu_ptr(brs_chan->priv, cpu)->waiting_events) ==
+	    0) {
+		// Start/reset the timeout if there are no pending events.
+		brs_fs_deferred_disable_all_sync_reset(cpu);
+	}
+	// Increase waiting event after the timer has been reset.
+	atomic_inc(&per_cpu_ptr(brs_chan->priv, cpu)->waiting_events);
+	preempt_enable();
+
+	rc = wait_event_killable_timeout(
+		brs_chan->waitqueue, atomic_read(&event->complete),
+		msecs_to_jiffies(SINGLE_EVENT_TIMEOUT_SEC * 1000));
+
+	// The event returned.
+	// Decrease the waiting events and reset the timer.
+	// The order should not matter.
+	atomic_dec(&per_cpu_ptr(brs_chan->priv, cpu)->waiting_events);
+	brs_fs_deferred_disable_all_sync_reset(cpu);
+
+	if (!rc || rc == -ERESTARTSYS) {
+		// this case handles a time-out, otherwise this path is the
+		// result of a kill signal, in which case we simply return.
+		if (rc != -ERESTARTSYS) {
+			pr_warn("[BRS] Waitqueue timed-out (id=%llu rc=%ld)\n",
+				id, rc);
+			WARN_ON(true);
+			brs_guestlog_error(false, true);
+		}
+		rc = 0;
+		goto out;
+	}
+
+	/* Propagate event blocking decision to the caller. */
+	if (event->block)
+		rc = -EPERM;
+	else
+		rc = 0;
+
+out:
+	xa_erase(&brs_chan->xa_waiting_events, id);
+	kfree(event);
+
+	return rc;
+}
+
+static ssize_t brs_wait_write(struct file *filp, const char __user *buffer,
+			      size_t count, loff_t *ppos)
+{
+	ssize_t rc = 0;
+	ssize_t max_size = 0;
+	struct brs_fs_event_channel *chan = filp->private_data;
+	struct waiting_event *event = NULL;
+
+	struct event_remediation {
+		uint64_t id;
+		bool block;
+	} __attribute__((__packed__)) remediate = { 0 };
+
+	if (*ppos < 0)
+		return -EINVAL;
+
+	if (!count)
+		return 0;
+
+	max_size = min(count, sizeof(remediate));
+
+	rc = copy_from_user(&remediate, buffer, max_size);
+	if (rc < 0) {
+		return rc;
+	}
+
+	event = xa_load(&brs_chan->xa_waiting_events, remediate.id);
+	if (!event)
+		return rc;
+
+	event->block = remediate.block;
+	atomic_set(&event->complete, 1);
+
+	wake_up(&chan->waitqueue);
+
+	return rc;
+}
+
+static int brs_wait_open(struct inode *inode, struct file *file)
+{
+	if (!brs_current_can_configure())
+		return -EPERM;
+
+	return simple_open(inode, file);
+}
+
+static const struct file_operations brs_wait_fops = {
+	.owner = THIS_MODULE,
+	.open = brs_wait_open,
+	.write = brs_wait_write,
+};
+
+static ssize_t brs_control_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct brs_fs_event_channel *chan = filp->private_data;
+	struct rchan *rc = chan->rchan;
+	struct rchan_buf *rb = NULL;
+	int off = 0;
+
+#define BUFSIZE 256
+	char buf[BUFSIZE];
+
+	off += snprintf(buf + off, BUFSIZE - off, "subbuf size: %zu\n",
+			rc->subbuf_size);
+	off += snprintf(buf + off, BUFSIZE - off, "subbufs total: %zu\n\n",
+			rc->n_subbufs);
+
+	/* XXX: The following is valid just for the current CPU. */
+
+	rb = *get_cpu_ptr(rc->buf);
+
+	off += snprintf(buf + off, BUFSIZE - off, "\n");
+	off += snprintf(buf + off, BUFSIZE - off,
+			"CPU[%d] subbufs produced: %zu\n", rb->cpu,
+			rb->subbufs_produced);
+	off += snprintf(buf + off, BUFSIZE - off,
+			"CPU[%d] subbufs consumed: %zu\n", rb->cpu,
+			rb->subbufs_consumed);
+	off += snprintf(buf + off, BUFSIZE - off,
+			"CPU[%d] subbufs pages total: %u\n", rb->cpu,
+			rb->page_count);
+	off += snprintf(buf + off, BUFSIZE - off,
+			"CPU[%d] bytes consumed in cur subbuf: %zu\n", rb->cpu,
+			rb->bytes_consumed);
+#undef BUFSIZE
+
+	put_cpu_ptr(rc->buf);
+
+	buf[off] = '\0';
+
+	return simple_read_from_buffer(buffer, count, ppos, buf, strlen(buf));
+}
+
+struct brs_relay_metadata {
+	__u32 subbuf_size;
+	__u32 subbuf_count;
+};
+
+/*
+ * Note: avoid collisions with registered magic numbers.
+ *
+ * (Documentation/userspace-api/ioctl/ioctl-number.rst)
+ */
+#define BRS_RELAY_IO 0xBD
+#define BRS_RELAY_CONTROL_IOCTL_SET_MODE _IOW(BRS_RELAY_IO, 0, int)
+#define BRS_RELAY_CONTROL_IOCTL_GET_METADATA \
+	_IOR(BRS_RELAY_IO, 1, struct brs_relay_metadata)
+
+static long brs_control_ioctl(struct file *file, unsigned int cmd,
+			      unsigned long arg)
+{
+	int ret = -EFAULT;
+	void __user *argp = (void __user *)arg;
+
+	switch (cmd) {
+	case BRS_RELAY_CONTROL_IOCTL_SET_MODE: {
+		int old_mode = relay_mode;
+
+		if (get_user(relay_mode, (int __user *)argp))
+			return -EFAULT;
+
+		if (relay_mode == old_mode)
+			break;
+
+		if (relay_mode == BRS_RELAY_MODE_MMAP) {
+			pr_info("[BRS] Enabling mmap-based relay channels\n");
+			static_branch_enable(&brs_relay_sk_mode_mmap_enabled);
+		} else {
+			pr_info("[BRS] Enabling file-based relay channels\n");
+			static_branch_disable(&brs_relay_sk_mode_mmap_enabled);
+		}
+
+		/*
+		 * We must reset the entire relay channel when switching modes.
+		 * This is because the different modes of operation expect
+		 * different subbuf message formats (w/ and w/o subbuf
+		 * headers).
+		 */
+		relay_reset(brs_chan->rchan);
+
+		break;
+	}
+	case BRS_RELAY_CONTROL_IOCTL_GET_METADATA: {
+		struct brs_relay_metadata meta = {
+			.subbuf_size = RELAY_SUBBUF_SIZE,
+			.subbuf_count = RELAY_SUBBUF_COUNT,
+		};
+
+		if (copy_to_user(argp, &meta,
+				 sizeof(struct brs_relay_metadata)))
+			return ret;
+
+		break;
+	}
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static long brs_control_compat_ioctl(struct file *file, unsigned int cmd,
+				     unsigned long arg)
+{
+	return -ENOTSUPP;
+}
+#endif
+
+static const struct file_operations brs_control_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = brs_control_read,
+	.llseek = default_llseek,
+	.unlocked_ioctl = brs_control_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = brs_control_compat_ioctl,
+#endif
+};
+
+static ssize_t brs_dropped_read(struct file *filp, char __user *buffer,
+				size_t count, loff_t *ppos)
+{
+	struct brs_fs_event_channel *chan = filp->private_data;
+	char buf[16];
+
+	snprintf(buf, sizeof(buf), "%u\n", atomic_read(&chan->dropped));
+
+	return simple_read_from_buffer(buffer, count, ppos, buf, strlen(buf));
+}
+
+static const struct file_operations brs_dropped_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = brs_dropped_read,
+	.llseek = default_llseek,
+};
+
+static ssize_t brs_linux_commit_read(struct file *filp, char __user *buffer,
+				     size_t count, loff_t *ppos)
+{
+	static const char vers[] = KERNEL_COMMIT_HASH "\n";
+	return simple_read_from_buffer(buffer, count, ppos, vers, strlen(vers));
+}
+
+static const struct file_operations brs_linux_commit_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = brs_linux_commit_read,
+	.llseek = default_llseek,
+};
+
+static inline int _brs_fs_subbuf_start_mmap(struct rchan_buf *buf, void *subbuf,
+					    void *prev_subbuf,
+					    size_t prev_padding)
+{
+	struct brs_fs_event_channel *chan = NULL;
+	struct subbuf_header *prev_hdr = prev_subbuf;
+
+	if (prev_subbuf) {
+		prev_hdr = prev_subbuf;
+		prev_hdr->padding = prev_padding;
+
+		/*
+		 * XXX: do we need a call to smp_wmb() to make the padding
+		 * entry visible to user space first?
+		 */
+		smp_wmb();
+
+		atomic_set(&prev_hdr->produced, 1);
+	}
+
+	/*
+	 * We can rely on relay_buf_full to correctly return the channel's
+	 * state. We prefer using this function instead of atomically reading
+	 * the current subbuffer's header->produced value.
+	 */
+	if (!relay_buf_full(buf)) {
+		subbuf_start_reserve(buf, sizeof(struct subbuf_header));
+		return 1;
+	}
+
+	chan = buf->chan->private_data;
+	atomic_inc(&chan->dropped);
+
+	pr_warn_ratelimited("BRS buffer event%d is full (dropped=%u)!",
+			    smp_processor_id(), atomic_read(&chan->dropped));
+
+	return 0;
+}
+
+static int brs_fs_subbuf_start(struct rchan_buf *buf, void *subbuf,
+			       void *prev_subbuf, size_t prev_padding)
+{
+	struct brs_fs_event_channel *chan = NULL;
+
+	/*
+	 * XXX: Consider pulling the deferred timer logic in here. This way,
+	 * the (async) event hot-path does not need to reset the timer every
+	 * time it writes an event to the realy buffer. (Sync events will have
+	 * to bit the bullet of having to set the timer as the system currently
+	 * switches to a new subbuffer every time a sync event is written to
+	 * the relay buffer.)
+	 */
+
+	if (static_branch_likely(&brs_relay_sk_mode_mmap_enabled)) {
+		return _brs_fs_subbuf_start_mmap(buf, subbuf, prev_subbuf,
+						 prev_padding);
+	} else { /* BRS_RELAY_MODE_FILE_READ */
+		if (!relay_buf_full(buf))
+			return 1;
+	}
+
+	chan = buf->chan->private_data;
+	atomic_inc(&chan->dropped);
+
+	pr_warn_ratelimited("BRS buffer event%d is full (dropped=%u)!",
+			    smp_processor_id(), atomic_read(&chan->dropped));
+
+	return 0;
+}
+
+DEFINE_MUTEX(brs_relay_file_mutex);
+static u64 brs_relay_file_cntr = 0;
+
+static int brs_relay_file_open(struct inode *inode, struct file *file)
+{
+	int rv;
+
+	if (!brs_current_can_configure())
+		return -EPERM;
+
+	mutex_lock(&brs_relay_file_mutex);
+	rv = relay_file_operations.open(inode, file);
+	if (rv < 0)
+		goto out;
+	brs_relay_file_cntr++;
+out:
+	mutex_unlock(&brs_relay_file_mutex);
+	return rv;
+}
+
+static int brs_relay_release(struct inode *inode, struct file *file)
+{
+	int rv;
+	int cpu;
+	unsigned long index;
+	struct waiting_event *event;
+
+	mutex_lock(&brs_relay_file_mutex);
+	rv = relay_file_operations.release(inode, file);
+	if (rv < 0)
+		goto out;
+	if (--brs_relay_file_cntr == 0) {
+		pr_info("BRS: disabling events\n");
+		// Disable all events
+		brs_guestlog_disable_all();
+		brs_trace_disable();
+		brs_disable_sync();
+		// stop global timer
+		for_each_possible_cpu(cpu) {
+			cancel_delayed_work_sync(
+				&per_cpu_ptr(brs_chan->priv, cpu)->work_timeout);
+		}
+		// wake up any waiting threads
+		xa_for_each(&brs_chan->xa_waiting_events, index, event) {
+			event->block = 0;
+			atomic_set(&event->complete, 1);
+		}
+		wake_up(&brs_chan->waitqueue);
+	}
+out:
+	mutex_unlock(&brs_relay_file_mutex);
+	return rv;
+}
+
+#define BRS_RELAY_FILE_IOCTL_GET_PRODUCED_SUBBUFS _IO(BRS_RELAY_IO, 0)
+#define BRS_RELAY_FILE_IOCTL_GET_CONSUMED_SUBBUFS _IO(BRS_RELAY_IO, 1)
+#define BRS_RELAY_FILE_IOCTL_SET_CONSUMED_SUBBUFS \
+	_IOW(BRS_RELAY_IO, 2, uint64_t)
+
+static long brs_relay_ioctl(struct file *file, unsigned int cmd,
+			    unsigned long arg)
+{
+	struct rchan_buf *buf = file->private_data;
+
+	switch (cmd) {
+	case BRS_RELAY_FILE_IOCTL_GET_PRODUCED_SUBBUFS:
+		return (long)buf->subbufs_produced;
+	case BRS_RELAY_FILE_IOCTL_GET_CONSUMED_SUBBUFS:
+		return (long)buf->subbufs_consumed;
+	case BRS_RELAY_FILE_IOCTL_SET_CONSUMED_SUBBUFS: {
+		size_t subbufs_consumed = (size_t)arg;
+		relay_subbufs_consumed(buf->chan, buf->cpu, subbufs_consumed);
+		break;
+	}
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+struct file_operations brs_relay_file_operations __ro_after_init;
+
+static struct dentry *
+brs_fs_create_buf_file(const char *filename, struct dentry *parent,
+		       umode_t mode, struct rchan_buf *buf, int *is_global)
+{
+	struct dentry *d = tracefs_create_file(filename, mode, parent, buf,
+					       &brs_relay_file_operations);
+
+	/*
+	 * for global buffer set is_global to not-zero value
+	 * if (d != 0)
+	 *    *is_global = 1;
+	 */
+
+	return d;
+}
+
+static int brs_fs_remove_buf_file(struct dentry *dentry)
+{
+	tracefs_remove(dentry);
+	return 0;
+}
+
+static struct rchan_callbacks brs_fs_relay_cb = {
+	.subbuf_start = brs_fs_subbuf_start,
+	.create_buf_file = brs_fs_create_buf_file,
+	.remove_buf_file = brs_fs_remove_buf_file,
+};
+
+int __init brs_fs_relay_init(void)
+{
+	int cpu;
+	int rc = 0;
+
+	brs_relay_file_operations = relay_file_operations;
+	brs_relay_file_operations.open = brs_relay_file_open;
+	brs_relay_file_operations.release = brs_relay_release;
+	brs_relay_file_operations.unlocked_ioctl = brs_relay_ioctl;
+
+	brs_chan = kzalloc(sizeof(*brs_chan), GFP_KERNEL);
+	if (!brs_chan)
+		return -ENOMEM;
+
+	atomic_set(&brs_chan->dropped, 0);
+
+	brs_chan->dir = tracefs_create_dir(BRS_FS_DIRNAME, NULL);
+	if (IS_ERR_OR_NULL(brs_chan->dir)) {
+		rc = -ENOENT;
+		goto err;
+	}
+
+	brs_chan->file_dropped =
+		tracefs_create_file("dropped", S_IRUSR | S_IRGRP, brs_chan->dir,
+				    brs_chan, &brs_dropped_fops);
+	if (IS_ERR(brs_chan->file_dropped)) {
+		rc = -ENOENT;
+		goto err_dir;
+	}
+
+	brs_chan->file_control =
+		tracefs_create_file("control", S_IRUSR | S_IRGRP, brs_chan->dir,
+				    brs_chan, &brs_control_fops);
+	if (IS_ERR(brs_chan->file_control)) {
+		rc = -ENOENT;
+		goto err_file;
+	}
+
+	brs_chan->file_linux_commit =
+		tracefs_create_file("linux_commit", S_IRUSR | S_IRGRP, brs_chan->dir,
+				    brs_chan, &brs_linux_commit_fops);
+	if (IS_ERR(brs_chan->file_linux_commit)) {
+		rc = -ENOENT;
+		goto err_dir;
+	}
+
+	init_waitqueue_head(&brs_chan->waitqueue);
+
+	brs_chan->priv = alloc_percpu(struct brs_fs_cpu_private);
+	if (brs_chan->priv == NULL) {
+		rc = -ENOMEM;
+		goto err_file;
+	}
+
+	/*
+	 * XXX: Note that relay.c uses for_each_online_cpu to allocate relay
+	 * buffers. Consider switching the for loop for the sake of consistency.
+	 */
+	for_each_possible_cpu(cpu) {
+		char *file_name = NULL;
+		struct dentry *f = NULL;
+
+		file_name = kzalloc(NAME_MAX + 1, GFP_KERNEL);
+		if (!file_name) {
+			rc = -ENOMEM;
+			goto err_percpu;
+		}
+
+		snprintf(file_name, NAME_MAX, "%s%d", BRS_FS_FILENAME_RESPONSE,
+			 cpu);
+
+		f = tracefs_create_file(file_name, S_IWUSR | S_IWGRP,
+					brs_chan->dir, brs_chan,
+					&brs_wait_fops);
+		if (IS_ERR(f)) {
+			kfree(file_name);
+			rc = -ENOENT;
+			goto err_percpu;
+		}
+
+		per_cpu_ptr(brs_chan->priv, cpu)->file_wait = f;
+
+		kfree(file_name);
+
+		INIT_DELAYED_WORK(&per_cpu_ptr(brs_chan->priv, cpu)->work_flush,
+				  brs_fs_deferred_flush);
+
+		// Initialize waiting events counter & timeout
+		atomic_set(&per_cpu_ptr(brs_chan->priv, cpu)->waiting_events,
+			   0);
+		INIT_DELAYED_WORK(
+			&per_cpu_ptr(brs_chan->priv, cpu)->work_timeout,
+			brs_fs_deferred_disable_all_sync);
+	}
+
+	brs_chan->rchan = relay_open(BRS_FS_FILENAME_RELAY, brs_chan->dir,
+				     RELAY_SUBBUF_SIZE, RELAY_SUBBUF_COUNT,
+				     &brs_fs_relay_cb,
+				     brs_chan /* private data */);
+
+	if (brs_chan->rchan == NULL) {
+		rc = -EINVAL;
+		goto err_percpu;
+	}
+
+	xa_init(&brs_chan->xa_waiting_events);
+
+	return rc;
+
+err_percpu:
+	if (brs_chan->priv != NULL) {
+		int cpu;
+		for_each_possible_cpu(cpu)
+			tracefs_remove(
+				per_cpu_ptr(brs_chan->priv, cpu)->file_wait);
+
+		free_percpu(brs_chan->priv);
+	}
+err_file:
+	tracefs_remove(brs_chan->file_dropped);
+	tracefs_remove(brs_chan->file_control);
+err_dir:
+	tracefs_remove(brs_chan->dir);
+err:
+	kfree(brs_chan);
+	brs_chan = NULL;
+
+	return rc;
+}
+
+void brs_fs_relay_uninit(void)
+{
+	unsigned long index;
+	struct waiting_event *event = NULL;
+
+	if (brs_chan == NULL)
+		return;
+
+	if (brs_chan->rchan)
+		relay_close(brs_chan->rchan);
+
+	if (brs_chan->priv != NULL) {
+		int cpu;
+		for_each_possible_cpu(cpu) {
+			tracefs_remove(
+				per_cpu_ptr(brs_chan->priv, cpu)->file_wait);
+
+			brs_fs_flush_timer_stop(cpu);
+
+			// Cancel timeout
+			cancel_delayed_work_sync(
+				&per_cpu_ptr(brs_chan->priv, cpu)->work_timeout);
+		}
+
+		free_percpu(brs_chan->priv);
+	}
+
+	if (brs_chan->dir) {
+		tracefs_remove(brs_chan->file_dropped);
+		tracefs_remove(brs_chan->file_control);
+		tracefs_remove(brs_chan->dir);
+	}
+
+	/* Make sure to release any potentially blocking events. */
+	xa_for_each(&brs_chan->xa_waiting_events, index, event) {
+		event->block = 0;
+		atomic_set(&event->complete, 1);
+
+		/*
+		 * Note: we do not need to erase the index from the xa_array and
+		 * free the maintained event at this point; it will be taken
+		 * care of by the blocked thread.
+		 */
+	}
+	if (waitqueue_active(&brs_chan->waitqueue))
+		wake_up(&brs_chan->waitqueue);
+
+	xa_destroy(&brs_chan->xa_waiting_events);
+
+	kfree(brs_chan);
+	brs_chan = NULL;
+}
diff --git security/bhv/brs_policy.c security/bhv/brs_policy.c
new file mode 100644
index 0000000000..2683dd6342
--- /dev/null
+++ security/bhv/brs_policy.c
@@ -0,0 +1,172 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2025 BlueRock Security, Inc.
+ * All rights reserved.
+ */
+
+#define pr_fmt(fmt) "brs_policy: " fmt
+
+#include <bhv/bhv.h>
+#include <bhv/brs_fs.h>
+#include <bhv/brs_policy.h>
+#include <bhv/config.h>
+#include <linux/tracefs.h>
+#include <linux/vmalloc.h>
+
+static struct mutex brs_policy_lock;
+static struct dentry *brs_policy_file = NULL;
+static char *brs_current_policy = NULL;
+static size_t brs_current_policy_size = 0;
+static atomic64_t brs_configuring_task = ATOMIC64_INIT(0);
+
+bool brs_current_can_configure()
+{
+	// Get ptr to main thread
+	struct task_struct *leader = current->group_leader;
+	s64 old = 0;
+	if (atomic64_try_cmpxchg(&brs_configuring_task, &old, (s64)leader)) {
+		pr_info("Updated brs_configuring_task to point to PID %d\n",
+			task_pid_nr(leader));
+		return true;
+	} else if (old == (s64)leader) {
+		return true;
+	} else {
+		pr_warn("Warning: unexpected process %d accessing BR files\n",
+			task_pid_nr(leader));
+		return false;
+	}
+}
+
+void brs_configurator_process_terminated(struct task_struct *task)
+{
+	s64 old = (s64)task;
+	if (atomic64_try_cmpxchg(&brs_configuring_task, &old, 0)) {
+		pr_info("Cleared brs_configuring_task (was PID %d)\n",
+			task_pid_nr((struct task_struct *)old));
+	}
+}
+
+int brs_is_configurator_process(struct task_struct *task)
+{
+	return (s64)(task->group_leader) ==
+	       atomic64_read(&brs_configuring_task);
+}
+
+static int brs_policy_open(struct inode *inode, struct file *file)
+{
+	if (!brs_current_can_configure())
+		return -EPERM;
+
+	return simple_open(inode, file);
+}
+
+static ssize_t brs_policy_read(struct file *f, char __user *user_buf,
+			       size_t count, loff_t *ppos)
+{
+	ssize_t ret;
+	mutex_lock(&brs_policy_lock);
+	if (brs_current_policy == NULL) {
+		mutex_unlock(&brs_policy_lock);
+		return 0;
+	}
+	ret = simple_read_from_buffer(user_buf, count, ppos, brs_current_policy,
+				      brs_current_policy_size);
+
+	mutex_unlock(&brs_policy_lock);
+	return ret;
+}
+
+static ssize_t brs_policy_write(struct file *f, const char __user *user_buf,
+				size_t count, loff_t *ppos)
+{
+	/* continuous write is not supported */
+	static char *new_policy;
+	size_t new_policy_size;
+	if (*ppos)
+		return -EINVAL;
+
+	if (count > BRS_MAX_POLICY_SIZE) {
+		return -E2BIG;
+	}
+
+	new_policy = vmalloc(count);
+	if (new_policy == NULL) {
+		return -ENOMEM;
+	}
+
+	new_policy_size = simple_write_to_buffer(new_policy, count, ppos,
+						 user_buf, count);
+
+	if (new_policy_size != count) {
+		pr_warn("Failed to write BRS policy %ld < %ld\n",
+			new_policy_size, count);
+		vfree(new_policy);
+		return 0;
+	}
+
+	if (!brs_policy_check(new_policy, new_policy_size)) {
+		pr_warn("Bad policy!\n");
+		vfree(new_policy);
+		return -EINVAL;
+	}
+
+	mutex_lock(&brs_policy_lock);
+
+	// Notify event system of policy update.
+	// brs_fs_policy_update();
+
+	// Will free any old policy
+	if (!brs_policy_update(new_policy, new_policy_size)) {
+		mutex_unlock(&brs_policy_lock);
+		return -EINVAL;
+	}
+
+	brs_current_policy = new_policy;
+	brs_current_policy_size = new_policy_size;
+
+	/* update file size */
+	d_inode(brs_policy_file)->i_size = new_policy_size;
+
+	mutex_unlock(&brs_policy_lock);
+
+	return count;
+}
+
+static const struct file_operations brs_policy_file_operations = {
+	.owner = THIS_MODULE,
+	.open = brs_policy_open,
+	.read = brs_policy_read,
+	.write = brs_policy_write,
+};
+
+int __init brs_policy_init(void)
+{
+	if (!brs_is_standalone())
+		return 0;
+
+	mutex_init(&brs_policy_lock);
+
+	if (brs_fs_dir() == NULL) {
+		return -ENOENT;
+	}
+
+	brs_policy_file =
+		tracefs_create_file(BRS_POLICY_FILENAME, S_IRUSR | S_IWUSR,
+				    brs_fs_dir(), NULL,
+				    &brs_policy_file_operations);
+
+	if (brs_policy_file == NULL) {
+		pr_err("Failed to initialize tracefs policy file\n");
+		return -EINVAL;
+	}
+	return 0;
+}
+late_initcall_sync(brs_policy_init);
+
+void brs_policy_deinit(void)
+{
+	if (brs_policy_file != NULL) {
+		tracefs_remove(brs_policy_file);
+		brs_policy_file = NULL;
+	}
+}
diff --git security/bhv/config.c security/bhv/config.c
new file mode 100644
index 0000000000..ee1121e01a
--- /dev/null
+++ security/bhv/config.c
@@ -0,0 +1,423 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#define pr_fmt(fmt) "BR config: " fmt
+
+#include <linux/cache.h>
+#include <linux/cgroup.h>
+#include <linux/vmalloc.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/config.h>
+#include <bhv/config_structs.h>
+#include <bhv/events_autogen.h>
+#include <bhv/flast_ids_autogen.h>
+#include <bhv/flast_lists.h>
+#include <bhv/libinsight.h>
+#include <bhv/policy_sks_autogen.h>
+
+/***********************************************************************
+ * Variables
+ ***********************************************************************/
+static uint32_t _gpolicy_counter = 0; // Counter for the policy objects.
+struct brs_policy *brs_policy = NULL; // Global pointer to the policy object
+
+/***********************************************************************
+ * Hypercalls
+ ***********************************************************************/
+/*
+static inline int
+get_policy_hyp(enum HypABI__GuestPolicy__GetPolicy__PolicyType type)
+{
+	int r;
+	void *addr;
+	uint32_t num_pages;
+	HypABI__GuestPolicy__GetPolicy__arg__T *brs_arg;
+
+	switch (type) {
+	case HypABI__GuestPolicy__GetPolicy__PolicyType__BOOT:
+		addr = (void *)_boot_policy.buf;
+		num_pages = _boot_policy.size * PAGE_SIZE;
+		break;
+	case HypABI__GuestPolicy__GetPolicy__PolicyType__RUNTIME:
+		// Only use runtime address if it is different from boot.
+		// We never want to free the boot policy. See while loop below.
+		addr = _runtime_policy.buf == _boot_policy.buf ?
+			       NULL :
+			       (void *)_runtime_policy.buf;
+		num_pages = _runtime_policy.buf == _boot_policy.buf ?
+				    0 :
+				    _runtime_policy.size * PAGE_SIZE;
+		break;
+	default:
+		BUG();
+		break;
+	}
+
+	brs_arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC();
+	brs_arg->valid = false;
+	brs_arg->dest = 0;
+	brs_arg->dest_sz = num_pages * PAGE_SIZE;
+
+	while (!brs_arg->valid) {
+		if (addr == NULL) {
+			num_pages = (brs_arg->dest_sz / PAGE_SIZE) +
+				    (brs_arg->dest_sz % PAGE_SIZE != 0 ||
+						     brs_arg->dest_sz == 0 ?
+					     1 :
+					     0);
+
+			addr = (void *)__get_free_pages(
+				GFP_KERNEL, order_base_2(num_pages));
+			if (addr == NULL) {
+				pr_err("Could not allocate memory!");
+				break;
+			}
+		}
+
+		brs_arg->dest = bhv_virt_to_phys(addr, num_pages * PAGE_SIZE);
+		brs_arg->dest_gva = (uint64_t)addr;
+		brs_arg->dest_sz = num_pages * PAGE_SIZE;
+		r = HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(brs_arg);
+		if (r) {
+			pr_err("Config get policy hypercall failed");
+			free_pages((unsigned long)addr,
+				   order_base_2(num_pages));
+			addr = NULL;
+			break;
+		}
+
+		// Free old memory area, if it was too small.
+		// We do this at the end, to guarantee that the memory area is
+		// not freed while it is still protected by the host.
+		if (!brs_arg->valid && addr != NULL) {
+			free_pages((unsigned long)addr,
+				   order_base_2(num_pages));
+			addr = NULL;
+		}
+	}
+
+	HypABI__GuestPolicy__GetPolicy__arg__FREE(brs_arg);
+	return r;
+}
+
+static inline int init_policy_hyp(void)
+{
+	int r;
+	HypABI__GuestPolicy__Init__arg__T *brs_arg;
+
+	brs_arg = HypABI__GuestPolicy__Init__arg__ALLOC();
+	brs_arg->boot_policy_struct_addr =
+		bhv_virt_to_phys(&_boot_policy, sizeof(flast_buf));
+	brs_arg->runtime_policy_struct_addr =
+		bhv_virt_to_phys(&_runtime_policy, sizeof(flast_buf));
+	r = HypABI__GuestPolicy__Init__hypercall_noalloc(brs_arg);
+	if (r) {
+		pr_err("Config init policy hypercall failed");
+	}
+
+	HypABI__GuestPolicy__Init__arg__FREE(brs_arg);
+	return r;
+}
+*/
+
+/***********************************************************************
+ * Functions
+ ***********************************************************************/
+static inline const char *
+_runtime_policy_get_cstring(const flast_lookup *lookup, const char *def)
+{
+	int err;
+	const char *rv;
+
+	rv = flast_lookup_cstring(lookup, &err);
+	if (err != 0) {
+		pr_err_once("Could not read config: %ps", lookup);
+		rv = def;
+	}
+
+	return rv;
+}
+
+#ifdef CONFIG_BHV_VAS
+void bhv_policy_update(void)
+{
+	panic("Updated not implemented");
+}
+#endif // defined CONFIG_BHV_VAS
+
+bool brs_policy_update(void *policy, size_t sz)
+{
+	struct brs_policy *new;
+	struct brs_policy *old;
+
+	if (!brs_is_standalone())
+		panic("Not running in standalone mode!");
+
+	new = kmalloc(sizeof(struct brs_policy), GFP_KERNEL);
+	if (new == NULL) {
+		pr_err("No memory left for policy update!");
+		return false;
+	}
+	new->flast.buf = policy;
+	new->flast.size = sz;
+	new->id = _gpolicy_counter;
+	refcount_set(&new->refcount, 1);
+
+	// Get old policy
+	old = brs_policy;
+
+	// Mark the beginning of a policy update.
+	if (likely(old != NULL)) {
+		static_branch_enable(&brs_retrieve_from_policy_sk);
+
+		// Disable all static keys.
+		brs_guestlog_disable_all();
+		brs_trace_disable();
+	}
+
+	// Switch policy
+	rcu_assign_pointer(brs_policy, new);
+	synchronize_rcu();
+
+	// New policy now available. Release our reference to old.
+	if (likely(old != NULL))
+		brs_put_policy(old);
+
+	// Update static keys
+	brs_set_sks(&new->flast);
+
+	// Remove update in progress
+	if (likely(old != NULL))
+		static_branch_disable(&brs_retrieve_from_policy_sk);
+
+	// Increase counter
+	_gpolicy_counter++;
+
+	// Print new policy
+	pr_info("Loaded a policy of %lu bytes\n", sz);
+#ifdef CONFIG_BHV_VAS_DEBUG
+	brs_dump_sks();
+#endif
+
+	return true;
+}
+
+void __init brs_mm_init_config(void)
+{
+	// int r;
+
+	if (!brs_is_standalone()) {
+		panic("Policy updated not implemented!");
+		/*
+		if (!is_bhv_initialized()) {
+			panic("BHV not initialized!");
+			return;
+		}
+
+		// Hypervisor-based init.
+		r = init_policy_hyp();
+		if (r != 0) {
+			panic("Could not initialize config!");
+			return;
+		}
+
+		r = get_policy_hyp(
+			HypABI__GuestPolicy__GetPolicy__PolicyType__BOOT);
+		if (r != 0) {
+			panic("Could not get configuration");
+			return;
+		}
+
+		brs_set_boot_sks();
+		*/
+	}
+}
+
+const char *brs_policy_get_modprobe_path(void)
+{
+	const char *def = "/sbin/modprobe";
+	struct brs_policy *cur = brs_get_policy();
+
+	if (unlikely(cur == NULL))
+		return def;
+
+	{
+		FLAST_CREATE_CONST_LOOKUP(
+			lookup, &cur->flast,
+			(flast_index *)
+				FLAST__GuestKernelConfig__modprobe_path__ARR,
+			ARRAY_SIZE(
+				FLAST__GuestKernelConfig__modprobe_path__ARR));
+		const char *rv = _runtime_policy_get_cstring(&lookup, def);
+		brs_put_policy(cur);
+		return rv;
+	}
+}
+
+const char *brs_policy_get_poweroff_cmd(void)
+{
+	const char *def = "/sbin/poweroff";
+	struct brs_policy *cur = brs_get_policy();
+
+	if (unlikely(cur == NULL))
+		return def;
+
+	{
+		FLAST_CREATE_CONST_LOOKUP(
+			lookup, &cur->flast,
+			(flast_index *)
+				FLAST__GuestKernelConfig__poweroff_cmd__ARR,
+			ARRAY_SIZE(
+				FLAST__GuestKernelConfig__poweroff_cmd__ARR));
+		const char *rv = _runtime_policy_get_cstring(&lookup, def);
+		brs_put_policy(cur);
+		return rv;
+	}
+}
+
+const char *brs_policy_get_core_pattern(void)
+{
+	const char *def = "core";
+	struct brs_policy *cur = brs_get_policy();
+
+	if (unlikely(cur == NULL))
+		return def;
+
+	{
+		FLAST_CREATE_CONST_LOOKUP(
+			lookup, &cur->flast,
+			(flast_index *)
+				FLAST__GuestKernelConfig__core_pattern__ARR,
+			ARRAY_SIZE(
+				FLAST__GuestKernelConfig__core_pattern__ARR));
+		const char *rv = _runtime_policy_get_cstring(&lookup, def);
+		brs_put_policy(cur);
+		return rv;
+	}
+}
+
+static inline bool _brs_policy_is_s_interpreter(const flast_lookup *lookup,
+						const char *path)
+{
+	const char *name;
+
+	name = strrchr(path, '/');
+
+	if (name == NULL)
+		name = path;
+
+	return flast_cstringarray_contains_str(lookup, name);
+}
+
+bool brs_policy_reverse_shell_is_interpreter(struct brs_policy *policy,
+					     const char *path)
+{
+	FLAST_CREATE_CONST_LOOKUP(
+		lookup, &policy->flast,
+		(flast_index *)FLAST__ReverseShellDetection__interpreter__ARR,
+		ARRAY_SIZE(FLAST__ReverseShellDetection__interpreter__ARR));
+	return _brs_policy_is_s_interpreter(&lookup, path);
+}
+
+struct brs_protected_unix_socket_filter_arg_t {
+	brs_protected_unix_socket_filter_t func;
+	void *arg;
+};
+
+static bool _brs_protected_unix_socket_filter(flast_index _, const char *s,
+					      size_t __, void *_arg)
+{
+	struct brs_protected_unix_socket_filter_arg_t *arg =
+		(struct brs_protected_unix_socket_filter_arg_t *)_arg;
+	return arg->func(s, arg->arg);
+}
+
+bool brs_policy_guestlog_protected_unix_sockets_is_empty(
+	struct brs_policy *policy)
+{
+	FLAST_CREATE_CONST_LOOKUP(
+		lookup, &policy->flast,
+		(flast_index *)
+			FLAST__Logging__filter_options__protected_unix_sockets__ARR,
+		ARRAY_SIZE(
+			FLAST__Logging__filter_options__protected_unix_sockets__ARR));
+
+	return !flast_cstringarray_is_not_empty(&lookup);
+}
+
+bool brs_policy_guestlog_match_each_protected_unix_sockets(
+	struct brs_policy *policy, brs_protected_unix_socket_filter_t f,
+	void *arg)
+{
+	struct brs_protected_unix_socket_filter_arg_t _arg = { .func = f,
+							       .arg = arg };
+	int error;
+	bool rv;
+
+	FLAST_CREATE_CONST_LOOKUP(
+		lookup, &policy->flast,
+		(flast_index *)
+			FLAST__Logging__filter_options__protected_unix_sockets__ARR,
+		ARRAY_SIZE(
+			FLAST__Logging__filter_options__protected_unix_sockets__ARR));
+
+	rv = flast_iterate_string_dict(&lookup, &error, NULL, NULL,
+				       _brs_protected_unix_socket_filter,
+				       &_arg);
+
+	if (error != 0) {
+		pr_err("Unexpected failure %d at %s:%d\n", error, __FILE__,
+		       __LINE__);
+		return false;
+	}
+
+	return rv;
+}
+
+bool brs_is_cgroup_exempt(struct brs_policy *policy, struct task_struct *tsk)
+{
+	int r;
+	struct cgroup *cgrp;
+	char str[brs_evt_CGroupAttributes_cgroup_name_MAX];
+	bool rv;
+
+	FLAST_CREATE_CONST_LOOKUP(
+		lookup, &policy->flast,
+		(flast_index *)
+			FLAST__Logging__filter_options__exempt_cgroups__ARR,
+		ARRAY_SIZE(
+			FLAST__Logging__filter_options__exempt_cgroups__ARR));
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	r = cgroup_path(cgrp, str, brs_evt_CGroupAttributes_cgroup_name_MAX);
+	rcu_read_unlock();
+
+	if (r <= 0) {
+		pr_warn("Cannot get cgroup name!\n");
+		return false;
+	} else if (r >= brs_evt_CGroupAttributes_cgroup_name_MAX)
+		pr_err("BRU: cgroup path being truncated! (sz=%d)\n", r);
+
+	rv = flast_cstringarray_contains_str(&lookup, str);
+	return rv;
+}
+
+bool brs_is_process_excluded_from_trace(struct brs_policy *policy,
+					const struct task_struct *task)
+{
+	FLAST_CREATE_CONST_LOOKUP(
+		lookup, &policy->flast,
+		(flast_index *)
+			FLAST__Logging__filter_options__trace_exclusion_list__ARR,
+		ARRAY_SIZE(
+			FLAST__Logging__filter_options__trace_exclusion_list__ARR));
+
+	return flast_cstringarray_contains_str(&lookup, task->comm);
+}
diff --git security/bhv/creds.c security/bhv/creds.c
new file mode 100644
index 0000000000..b3675bcaf8
--- /dev/null
+++ security/bhv/creds.c
@@ -0,0 +1,456 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/gfp.h>
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/context.h>
+#include <bhv/inode.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/keyring.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_CRED_USAGE_SET(arg1, arg2)         \
+	_Generic((arg1), atomic_t *            \
+		 : atomic_set, atomic_long_t * \
+		 : atomic_long_set)(arg1, arg2)
+
+siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	BHV_CRED_USAGE_SET(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+#undef MAX_BUF_SIZE
+}
+
+#define LOG_PREPARE(logarg)                                                    \
+	HypABI__Creds__Log__arg__T *logarg = HypABI__Creds__Log__arg__ALLOC(); \
+	rc = populate_context(&logarg->context, true);                         \
+	if (rc) {                                                              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}
+
+#define LOG_SEND(rc, logarg)                                                  \
+	rc = HypABI__Creds__Log__hypercall_noalloc(logarg);                   \
+	if (rc) {                                                             \
+		pr_err("%s: BHV Cannot log event with type=%d", __FUNCTION__, \
+		       type);                                                 \
+	}
+
+#define LOG_FREE(logarg) HypABI__Creds__Log__arg__FREE(logarg)
+
+static int __bhv_cred_assign(struct task_struct *t,
+			     struct task_struct *_current, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Assign__arg__T *assarg = NULL;
+	struct task_struct *parent = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(_current);
+	if (rc)
+		return -EPERM;
+
+	assarg = HypABI__Creds__Assign__arg__ALLOC();
+	if (assarg == NULL)
+		return -ENOMEM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = _current->real_parent;
+	else
+		parent = _current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	assarg->new_task.addr = (uint64_t)t;
+	assarg->new_task.cred = (uint64_t)t->cred;
+	assarg->new_task.hmac = hmac;
+	assarg->parent.addr = (uint64_t)parent;
+	assarg->parent.cred = (uint64_t)parent->cred;
+
+	rc = HypABI__Creds__Assign__hypercall_noalloc(assarg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = assarg->ret;
+
+	HypABI__Creds__Assign__arg__FREE(assarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		/* Note that we currently log only the parent's information. */
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)parent;
+		logarg->task_cred = (uint64_t)parent->cred;
+		logarg->task_pid = parent->pid;
+		strscpy(logarg->task_name, parent->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return __bhv_cred_assign(t, current, clone_flags);
+}
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	HypABI__Creds__AssignPriv__arg__T *aparg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	aparg = HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK();
+	if (aparg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	aparg->cred = (uint64_t)c;
+	aparg->daemon = (uint64_t)daemon;
+
+	rc = HypABI__Creds__AssignPriv__hypercall_noalloc(aparg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = aparg->ret;
+	HypABI__Creds__AssignPriv__arg__FREE(aparg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Commit__arg__T *commarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	commarg = HypABI__Creds__Commit__arg__ALLOC_NOCHECK();
+	if (commarg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	commarg->currnt.cred = (uint64_t)c;
+	commarg->currnt.addr = (uint64_t)current;
+	commarg->currnt.hmac = hmac;
+
+	rc = HypABI__Creds__Commit__hypercall_noalloc(commarg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = commarg->ret;
+	HypABI__Creds__Commit__arg__FREE(commarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+
+		LOG_FREE(logarg);
+	}
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Verification__arg__T *verarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	verarg = HypABI__Creds__Verification__arg__ALLOC_NOCHECK();
+	if (verarg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	verarg->task.cred = (uint64_t)t->cred;
+	verarg->task.addr = (uint64_t)t;
+	verarg->task.hmac = hmac;
+
+	rc = HypABI__Creds__Verification__hypercall_noalloc(verarg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = verarg->ret;
+	HypABI__Creds__Verification__arg__FREE(verarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)t;
+		logarg->task_cred = (uint64_t)t->cred;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, t->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	HypABI__Creds__Release__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->cred = (uint64_t)c;
+
+	rc = HypABI__Creds__Release__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	HypABI__Creds__Release__arg__FREE(arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	HypABI__Creds__RegisterInitTask__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK();
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->init_task.addr = (uint64_t)t;
+	arg->init_task.cred = (uint64_t)c;
+	arg->init_task.hmac = siphash_cred_context(c, t);
+
+	rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	HypABI__Creds__RegisterInitTask__arg__FREE(arg);
+}
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_cred(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_cred_register_init_task(&init_cred, &init_task);
+}
+/***********************************************************************/
+
+/***********************************************************************
+ * init
+ ***********************************************************************/
+int __init bhv_init_cred(void)
+{
+	int rc = 0;
+
+	static HypABI__Creds__Configure__arg__T fbarg;
+	HypABI__Creds__Configure__arg__T *arg;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(fbarg);
+
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = HypABI__Creds__Configure__hypercall_noalloc(arg);
+
+	if (!rc) {
+		static_assert(HypABI__Creds__Configure__arg__SZ ==
+			      sizeof(siphash_key_t));
+		memcpy(&bhv_siphash_key, arg->key, sizeof(siphash_key_t));
+		memset(arg->key, 0, sizeof(siphash_key_t));
+	}
+
+	HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(arg, fbarg);
+	if (rc)
+		return -EINVAL;
+
+	rc = bhv_inode_init();
+	if (rc)
+		return rc;
+	rc = bhv_init_keyring();
+	if (rc)
+		return rc;
+
+	return rc;
+}
+/***********************************************************************/
diff --git security/bhv/domain.c security/bhv/domain.c
new file mode 100644
index 0000000000..abdb2b9095
--- /dev/null
+++ security/bhv/domain.c
@@ -0,0 +1,853 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - 2025 BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bluerock.io>
+ *           Sebastian Vogl <sebastian@bluerock.io>
+ */
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/nsproxy.h>
+#include <linux/mem_namespace.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+#include <linux/hugetlb.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain.h>
+#include <bhv/guestlog.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+#include <bhv/config.h>
+
+#include <asm/bhv/domain.h>
+
+#ifdef BRS_VAS_DOMAIN_DEBUG
+#include <asm/stacktrace.h>
+#include <asm/unwind.h>
+#endif
+
+DEFINE_PER_CPU(uint64_t, brs_domain_current_domain);
+EXPORT_PER_CPU_SYMBOL_GPL(brs_domain_current_domain);
+
+static DEFINE_XARRAY_ALLOC(xa_domids);
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static brs_domain_batched_arg_t _batch_area;
+#endif
+
+bool brs_domain_initialized __ro_after_init = false;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static bool isolate __ro_after_init = false;
+#endif
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static int brs_domain_create_isolated_view(uint64_t domid)
+{
+	int rc;
+	brs_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BRS_DOMAIN_CREATE_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BRS cannot create new domain\n", __FUNCTION__);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+int brs_domain_create(uint64_t *domid)
+{
+	int rc = 0;
+	unsigned int id;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	if (domid == NULL)
+		return -EINVAL;
+
+	/*
+	 * Start allocating domain IDs from ID 1. Domain with ID 0 is reserved.
+	 *
+	 * NOTE: This implementation allocates domain IDs inside of the guest.
+	 * The allocations are done sequentially, yet (unless otherwise required
+	 * by BHV), do not necessarily have to be. These can be passed to BHV
+	 * for management purposes or, if needed, allocated directly by BHV
+	 * instead.
+	 *
+	 * XXX: Consider binding struct mem_namespace (or another datastructure)
+	 * to the allocated ID.
+	 */
+	rc = xa_alloc(&xa_domids, &id, NULL, xa_limit_32b, GFP_KERNEL);
+	if (rc) {
+		pr_err("%s: Cannot allocate new domain ID\n", __FUNCTION__);
+		*domid = BRS_INVALID_DOMAIN;
+		return rc;
+	}
+
+	*domid = (uint64_t)id;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	if (isolate) {
+		rc = brs_domain_create_isolated_view(*domid);
+		if (rc) {
+			xa_release(&xa_domids, id);
+			*domid = BRS_INVALID_DOMAIN;
+		}
+	}
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+	return 0;
+}
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static int brs_domain_destroy_isolated_view(uint64_t domid)
+{
+	int rc;
+	brs_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BRS_DOMAIN_DESTROY_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BRS cannot destroy domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+void brs_domain_destroy(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!brs_domain_is_active())
+		return;
+
+	if (domid == BRS_INVALID_DOMAIN)
+		return;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	if (isolate)
+		rc = brs_domain_destroy_isolated_view(domid);
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+	/*
+	 * Release the allocated domain IDs only if BRS did not return an error,
+	 * or if guest isolation was not enabled in the first place.
+	 */
+	if (!rc)
+		xa_release(&xa_domids, domid);
+}
+
+#if defined(CONFIG_DOMAIN_SPACES)
+static int brs_domain_switch_isolated_view(uint64_t domid)
+{
+	int rc = 0;
+	brs_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BRS_DOMAIN_SWITCH_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BRS cannot switch to domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+int brs_domain_switch(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	if (domid == BRS_INVALID_DOMAIN)
+		return -EINVAL;
+
+	if (domid == this_cpu_read(brs_domain_current_domain))
+		return 0;
+
+	if (isolate) {
+		rc = brs_domain_switch_isolated_view(domid);
+		if (rc)
+			return rc;
+	}
+
+	this_cpu_write(brs_domain_current_domain, domid);
+
+	return rc;
+}
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+#if defined(CONFIG_DOMAIN_SPACES)
+int brs_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns)
+{
+	int rc = 0;
+	brs_domain_arg_t arg;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	if (mm == NULL)
+		return -EINVAL;
+
+	if (old_ns == NULL || new_ns == NULL)
+		return -EINVAL;
+
+	arg.domain.id = old_ns->mem_ns->domain;
+	arg.domain.pgd = virt_to_phys(brs_domain_get_user_pgd(mm->pgd));
+	arg.id = new_ns->mem_ns->domain;
+
+	rc = BRS_DOMAIN_TRANSFER_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BRS cannot transfer PGD 0x%llx to domain[%llu]\n",
+		       __FUNCTION__, arg.domain.pgd, new_ns->mem_ns->domain);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+static inline void brs_domain_batch_from_info(uint64_t info, uint32_t *head,
+					      uint32_t *tail)
+{
+	(*tail) = (info & 0xffffffff);
+	(*head) = (info >> 32);
+}
+
+static inline uint64_t brs_domain_batch_to_info(uint32_t head, uint32_t tail)
+{
+	return (((uint64_t)head << 32) | tail);
+}
+
+static inline uint32_t brs_domain_get_nr_entries(uint32_t head, uint32_t tail)
+{
+	return head <= tail ? tail - head :
+			      BRS_DOMAIN_MAX_ENTRIES - head + tail;
+}
+
+static int brs_domain_batch_send_locked(void)
+{
+	// LOCK MUST BE HELD!
+	int rc = 0;
+	brs_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	rc = BRS_DOMAIN_BATCH_HYP(batch_area);
+	if (rc) {
+		uint32_t head;
+		uint32_t tail;
+		uint64_t info = atomic_long_read(&_batch_area.info);
+		brs_domain_batch_from_info(info, &head, &tail);
+		pr_err("%s: BRS could not process %u batched hypercalls!\n",
+		       __FUNCTION__, brs_domain_get_nr_entries(head, tail));
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+// DISCLAIMER: Disabled for now, since it may change the order of operations.
+#define BRS_BATCH_STEAL_PROCESSED 0
+
+static inline uint32_t brs_domain_batch_get_slot(void)
+{
+	static atomic_t sending = { false };
+	uint32_t in_send = false;
+
+	uint64_t info = atomic64_read(&_batch_area.info);
+	uint64_t new_info;
+	uint32_t head;
+	uint32_t tail;
+	uint32_t nr_entries;
+
+#if BRS_BATCH_STEAL_PROCESSED
+	uint32_t i;
+#endif
+
+	while (true) {
+		// Update head and tail
+		brs_domain_batch_from_info(info, &head, &tail);
+		// Calculate the current number of entries
+		nr_entries = brs_domain_get_nr_entries(head, tail);
+
+		// Check if we need to send the batch area down because it is full
+		// Note that we check whether we reached BRS_DOMAIN_MAX_ENTRIES - 1
+		// as the tail always points to the next free entry
+		if (nr_entries >= BRS_DOMAIN_MAX_ENTRIES - 1) {
+			// Make sure in send is always false when we try to win
+			// the race.
+			in_send = false;
+
+			if (atomic_try_cmpxchg(&sending, &in_send, true)) {
+				// Hooray! We are a winner! Send it down.
+				brs_domain_batch_send_locked();
+				// Reset sending afterwards.
+				atomic_set(&sending, false);
+			} else {
+#if BRS_BATCH_STEAL_PROCESSED
+				// Lets try to steal an existing entry.
+				// This works as processed entries always come after an INVALID entry
+				// Thus we can try to reuse them in case little space is left in our
+				// batch area.
+				for (i = head; i < nr_entries; i++) {
+					uint32_t cur =
+						i % BRS_DOMAIN_MAX_ENTRIES;
+					uint32_t expected =
+						BRS_VAS_DOMAIN_BATCH_STATE_PROCESSED;
+
+					if (atomic_try_cmpxchg(
+						    &_batch_area.entries[cur]
+							     .state,
+						    &expected,
+						    BRS_VAS_DOMAIN_BATCH_STATE_INVALID)) {
+						// We manged to steal an entry. Lets use it.
+						return cur;
+					}
+				}
+#endif
+			}
+
+			// Reread the the info and try again
+			info = atomic64_read(&_batch_area.info);
+			continue;
+		}
+
+		// There are still free entries! Lets see if we can get one
+		new_info = brs_domain_batch_to_info(
+			head, (tail + 1) % BRS_DOMAIN_MAX_ENTRIES);
+		if (atomic64_try_cmpxchg(&_batch_area.info, &info, new_info)) {
+			// We won the race!
+			if (atomic_read(&_batch_area.entries[tail].state) !=
+			    BRS_VAS_DOMAIN_BATCH_STATE_INVALID) {
+				pr_err("Found valid/processed slot! (state: %u, head: %u, tail: %u, info: 0x%llx)",
+				       atomic_read(
+					       &_batch_area.entries[tail].state),
+				       head, tail,
+				       atomic64_read(&_batch_area.info));
+				panic("Batching failed!");
+			}
+			return tail;
+		}
+	}
+}
+
+#ifdef BRS_VAS_DOMAIN_DEBUG
+static void brs_domain_add_stack_trace(brs_domain_batched_entry_arg_t *target)
+{
+	struct unwind_state state;
+	struct stack_info stack_info = { 0 };
+	unsigned long visit_mask = 0;
+	unsigned long *stack = get_stack_pointer(current, NULL);
+	char *cur = &target->stack_trace[0];
+	size_t stack_buf_len = BRS_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+
+	unwind_start(&state, current, NULL, stack);
+
+	stack = PTR_ALIGN(stack, sizeof(long));
+	if (get_stack_info(stack, current, &stack_info, &visit_mask)) {
+		pr_err("Could not get stack info!");
+		target->stack_trace[0] = '\0';
+		return;
+	}
+
+	for (; stack < stack_info.end && stack_buf_len > 0 &&
+	       stack_buf_len <= BRS_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+	     stack++) {
+		unsigned long addr = READ_ONCE_NOCHECK(*stack);
+		unsigned long *ret_addr_p =
+			unwind_get_return_address_ptr(&state);
+		int reliable = 0;
+		int written;
+
+		if (!__kernel_text_address(addr))
+			continue;
+
+		if (stack == ret_addr_p)
+			reliable = 1;
+
+		// Skip unreliable for now.
+		if (!reliable)
+			continue;
+
+		written = snprintf(cur, stack_buf_len, "%c%pB\n",
+				   reliable ? ' ' : '?', (void *)addr);
+		if (written >= stack_buf_len)
+			break;
+		stack_buf_len -= written;
+		cur += written;
+
+		if (!reliable)
+			continue;
+
+		unwind_next_frame(&state);
+	}
+}
+#endif
+
+static int brs_domain_batch_op(brs_domain_batched_entry_arg_t *arg)
+{
+	int rc = 0;
+	uint32_t dest;
+	uint32_t invalid_state = BRS_VAS_DOMAIN_BATCH_STATE_INVALID;
+	unsigned long flags = 0;
+	brs_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg->state, BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	dest = brs_domain_batch_get_slot();
+
+	// disable interrupts for the completion of the entry.
+	local_irq_save(flags);
+	BUG_ON(atomic_read(&batch_area->entries[dest].state) !=
+	       BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	memcpy(&batch_area->entries[dest], arg,
+	       sizeof(brs_domain_batched_entry_arg_t));
+
+#ifdef BRS_VAS_DOMAIN_DEBUG
+	brs_domain_add_stack_trace(&batch_area->entries[dest]);
+#endif
+
+	if (!atomic_try_cmpxchg(&batch_area->entries[dest].state,
+				&invalid_state,
+				BRS_VAS_DOMAIN_BATCH_STATE_VALID)) {
+		panic("Could not set valid state!");
+	}
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+
+static int brs_domain_batch_map(uint32_t op, struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages, bool read, bool write,
+				bool exec, bool kernel)
+{
+	brs_domain_batched_entry_arg_t arg;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg.state, BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = op;
+	arg.domain.id = brs_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(brs_domain_get_user_pgd(mm->pgd));
+	arg.map.read = read;
+	arg.map.write = write;
+	arg.map.exec = exec;
+	arg.map.kernel = kernel;
+	arg.map.pfn.pfn = pfn;
+	arg.map.pfn.count = nr_pages;
+
+	return brs_domain_batch_op(&arg);
+}
+
+int brs_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec)
+{
+	if (!isolate)
+		return 0;
+
+	return brs_domain_batch_map(BRS_VAS_DOMAIN_OP_MAP, mm, pfn, nr_pages,
+				    read, write, exec, true);
+}
+
+void brs_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pte(pte))
+		return;
+
+	if (pte_special(pte) && !pte_exec(pte)) {
+		brs_domain_batch_map(BRS_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), true);
+	} else {
+		uint32_t brs_domain_op = BRS_VAS_DOMAIN_OP_MAP;
+
+		if (pte_present(*ptep))
+			brs_domain_op = BRS_VAS_DOMAIN_OP_UPDATE;
+
+		brs_domain_batch_map(brs_domain_op, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), false);
+	}
+}
+EXPORT_SYMBOL(brs_domain_set_pte_at);
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+void brs_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte)) {
+		set_pte(ptep, pte);
+		return;
+	}
+#endif
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	brs_domain_batch_map(BRS_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+			     pte_read(pte), pte_write(pte), pte_exec(pte),
+			     true);
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+	set_pte(ptep, pte);
+}
+
+#if defined(CONFIG_DOMAIN_SPACES)
+void brs_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd)
+{
+	uint32_t brs_domain_op = BRS_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pmd(pmd))
+		return;
+
+	if (!pmd_large(pmd))
+		return;
+
+	if (pmd_present(*pmdp))
+		brs_domain_op = BRS_VAS_DOMAIN_OP_UPDATE;
+
+	brs_domain_batch_map(brs_domain_op, mm, pmd_pfn(pmd), 512,
+			     pmd_read(pmd), pmd_write(pmd), pmd_exec(pmd),
+			     false);
+}
+EXPORT_SYMBOL(brs_domain_set_pmd_at);
+
+void brs_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud)
+{
+	uint32_t brs_domain_op = BRS_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pud(pud))
+		return;
+
+	if (!pud_large(pud))
+		return;
+
+	if (pud_present(*pudp))
+		brs_domain_op = BRS_VAS_DOMAIN_OP_UPDATE;
+
+	brs_domain_batch_map(brs_domain_op, mm, pud_pfn(pud), 512 * 512,
+			     pud_read(pud), pud_write(pud), pud_exec(pud),
+			     false);
+}
+EXPORT_SYMBOL(brs_domain_set_pud_at);
+
+static int brs_domain_unmap_pte(struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages)
+{
+	brs_domain_batched_entry_arg_t arg;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+	atomic_set(&arg.state, BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BRS_VAS_DOMAIN_OP_UNMAP;
+	arg.domain.id = brs_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(brs_domain_get_user_pgd(mm->pgd));
+	arg.unmap.pfn.pfn = pfn;
+	arg.unmap.pfn.count = nr_pages;
+
+	return brs_domain_batch_op(&arg);
+}
+
+void brs_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pte(*ptep))
+		return;
+
+	brs_domain_unmap_pte(mm, pte_pfn(*ptep), 1);
+}
+EXPORT_SYMBOL(brs_domain_clear_pte);
+
+void brs_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pmd(*pmdp))
+		return;
+
+	if (!pmd_large(*pmdp))
+		return;
+
+	brs_domain_unmap_pte(mm, pmd_pfn(*pmdp), 512);
+}
+EXPORT_SYMBOL(brs_domain_clear_pmd);
+
+void brs_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!brs_domain_is_user_pud(*pudp))
+		return;
+
+	if (!pud_large(*pudp))
+		return;
+
+	brs_domain_unmap_pte(mm, pud_pfn(*pudp), 512 * 512);
+}
+EXPORT_SYMBOL(brs_domain_clear_pud);
+
+#ifdef BRS_VAS_DOMAIN_DEBUG
+void brs_domain_debug_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	int rc = 0;
+	brs_domain_debug_arg_t arg = {};
+
+	if (!brs_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	arg.msg_type = BRS_VAS_DOMAIN_DEBUG_DESTROY_PGD;
+	arg.domain.id = brs_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(brs_domain_get_user_pgd(mm->pgd));
+
+	rc = BRS_DOMAIN_DEBUG_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BRS an error occurred during DEBUG destroy pgd hypercall\n",
+		       __FUNCTION__);
+	}
+}
+#endif
+
+void brs_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	brs_domain_batched_entry_arg_t arg;
+
+	if (!brs_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	atomic_set(&arg.state, BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BRS_VAS_DOMAIN_OP_PGD_DESTROY;
+	arg.domain.id = brs_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(brs_domain_get_user_pgd(mm->pgd));
+
+	brs_domain_batch_op(&arg);
+}
+#endif // defined(CONFIG_DOMAIN_SPACES)
+
+static inline int brs_domain_log_report(const struct task_struct *t,
+					const struct mm_struct *mm_target,
+					const struct vm_area_struct *vma,
+					unsigned int gup_flags, bool blocked)
+{
+	struct brs_policy *policy = brs_get_policy();
+	int rc = brs_guestlog_log_strong_isolation_report(
+		policy, brs_get_domain(t), brs_get_domain(mm_target->owner),
+		vma->vm_start, vma->vm_end,
+		((gup_flags & FOLL_WRITE) || (gup_flags & FOLL_FORCE)),
+		blocked);
+	if (rc)
+		pr_err("Cannot log forced memory access event!\n");
+
+	brs_put_policy(policy);
+	return rc;
+}
+
+int brs_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags)
+{
+	bool block = false;
+
+	if (!brs_domain_is_active())
+		return 0;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return 0;
+#endif
+
+	if (!brs__is__StrongIsolation__enabled())
+		return 0;
+
+	block = brs__is__StrongIsolation__remediate();
+
+	brs_domain_log_report(t, mm_target, vma, gup_flags, block);
+
+	return (block) ? -EPERM : 0;
+}
+
+static void __init __brs_domain_init_virt_assisted(void)
+{
+	int rc;
+	bool pti_active = false;
+	uint64_t pa_batched_region = BHV_INVALID_PHYS_ADDR;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	// We use the CPU region before the initialization!
+	// This is important, otherwise this would overwrite nr_entries!!!
+	brs_domain_arg_t *arg = (brs_domain_arg_t *)&_batch_area;
+#else /* !defined(CONFIG_DOMAIN_SPACES) */
+	static HypABI__Domain__Configure__arg__T args = { 0 };
+	HypABI__Domain__Configure__arg__T *arg = &args;
+#endif /* defined(CONFIG_DOMAIN_SPACES) */
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		pti_active = true;
+#endif
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	isolate = !!brs__is__StrongIsolation__isolate();
+
+	arg->batched_region = bhv_virt_to_phys(&_batch_area);
+#endif
+	arg->batched_region = pa_batched_region;
+	arg->pti = (uint8_t)pti_active;
+
+	rc = HypABI__Domain__Configure__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("[BRS] cannot configure StrongIsolation\n");
+		return;
+	}
+}
+
+static int __init brs_domain_init(void)
+{
+	int cpu;
+	int rc;
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	uint32_t i;
+#endif
+
+	/*
+	 * XXX: At the moment, the unplugged BRS setting requires memory
+	 * namespaces to be able to allocate and use BRS domain IDs, independent
+	 * of whether or not StrongIsolation was enabled by the policy. The
+	 * reason for this is that the policy will be initialized only after the
+	 * system has booted. At the same time, if the respective
+	 * StrongIsolation-related data structures were not allocated and
+	 * initialized already during boot, StrongIsolation will not be able to
+	 * function correctly because the created processes, will not use valid
+	 * domain IDs.
+	 *
+	 * TODO: To avoid unnecessary resource allocations, e.g., when
+	 * StrongIsolation remains disabled during run-time, consider porting
+	 * the hook-based Strong Isolation functionality to PID namespaces. PID
+	 * namespaces are always present, and will not consume additional
+	 * resources, even if the hook-based StrongIsolation remains disabled.
+	 */
+	if (!brs_is_standalone() && !brs_domain_is_enabled())
+		return -EPERM;
+
+	if (brs_domain_initialized)
+		return -EFAULT;
+
+	if (!brs_is_standalone())
+		__brs_domain_init_virt_assisted();
+
+	rc = xa_reserve_bh(&xa_domids, BRS_INIT_DOMAIN, GFP_KERNEL);
+	if (rc) {
+		bhv_fail("[BRS] Cannot reserve domain ID %lu", BRS_INIT_DOMAIN);
+		return -EFAULT;
+	}
+
+	for_each_possible_cpu (cpu) {
+		per_cpu(brs_domain_current_domain, cpu) = 0;
+	}
+
+#if defined(CONFIG_DOMAIN_SPACES)
+	/* Reserve the ID of the first, static memory namespace. */
+	atomic64_set(&_batch_area.info, 0);
+
+	for (i = 0; i < BRS_DOMAIN_MAX_ENTRIES; i++) {
+		atomic_set(&_batch_area.entries[i].state,
+			   BRS_VAS_DOMAIN_BATCH_STATE_INVALID);
+	}
+#endif
+
+	brs_domain_initialized = true;
+
+	return 0;
+}
+
+__initcall(brs_domain_init);
diff --git security/bhv/drift_detection.c security/bhv/drift_detection.c
new file mode 100644
index 0000000000..fa9ca37daa
--- /dev/null
+++ security/bhv/drift_detection.c
@@ -0,0 +1,207 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024-2025 - BlueRock Security Inc.
+ * Authors:  Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 
+ */
+#include <bhv/bhv_print.h>
+
+#include <uapi/linux/magic.h>
+#include <linux/dcache.h>
+#include <linux/xattr.h>
+#include <linux/cgroup.h>
+#include <linux/version.h>
+#include <linux/namei.h>
+#include <linux/binfmts.h>
+#include <linux/file.h>
+#include <linux/sched/mm.h>
+#include <linux/vmalloc.h>
+
+#include <bhv/drift_detection.h>
+#include <bhv/guestlog.h>
+#include <bhv/util.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define DEBUG 0
+
+/***********************************************************************
+ * CONFIG
+ ***********************************************************************/
+// Keep this value in sync with the rego rule!!!
+#define BRS_XATTR_PREFIX "security.brs."
+#define BRS_XATTR_INFO BRS_XATTR_PREFIX "info"
+/***********************************************************************/
+
+static inline void drift_detection_debug_print_context(struct dentry *dentry)
+{
+#if DEBUG
+	const char *path;
+	char *buf;
+
+	if (dentry == NULL)
+		return;
+
+	buf = brs_aa_kmalloc(PATH_MAX);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	path = brs_get_dentry_path(dentry, buf, PATH_MAX);
+	pr_debug("-> Dentry: %s (%p), Task: %s\n", path, dentry, current->comm);
+
+	path = brs_get_dentry_path(d_real(dentry, NULL), buf, PATH_MAX);
+	pr_debug("-> Underlying path: %s (%p)\n", path, d_real(dentry, NULL));
+
+	kfree(buf);
+#endif
+}
+
+struct brs_xattr_info {
+	bool new_file;
+};
+
+static inline int drift_detection_set_xattr(struct dentry *d,
+					    struct brs_xattr_info *info)
+{
+	int rv;
+
+	inode_lock(d->d_inode);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+	rv = __vfs_setxattr_noperm(&nop_mnt_idmap, d, BRS_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#else
+	rv = __vfs_setxattr_noperm(&init_user_ns, d, BRS_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#endif
+	inode_unlock(d->d_inode);
+
+	return rv;
+}
+
+static inline ssize_t drift_detection_get_xattr(struct dentry *d,
+						struct brs_xattr_info *info)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+	return vfs_getxattr(&nop_mnt_idmap, d, BRS_XATTR_INFO, info,
+			    sizeof(*info));
+#else
+	return vfs_getxattr(&init_user_ns, d, BRS_XATTR_INFO, info,
+			    sizeof(*info));
+#endif
+}
+
+static inline bool is_new_file_by_dentry(struct dentry *dentry)
+{
+	struct brs_xattr_info info = { .new_file = false };
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
+	ssize_t sz =
+		drift_detection_get_xattr(d_real(dentry, D_REAL_DATA), &info);
+#else // LINUX_VERSION_CODE < 6.9
+	ssize_t sz = drift_detection_get_xattr(d_real(dentry, NULL), &info);
+#endif // LINUX_VERSION_CODE <> 6.9
+
+	if (sz != sizeof(info) && sz != 0) {
+		return false;
+	}
+
+	if (info.new_file) {
+		return true;
+	}
+
+	return false;
+}
+
+bool drift_detection_is_new_file_by_file(struct file *file)
+{
+	return is_new_file_by_dentry(file->f_path.dentry);
+}
+
+bool drift_detection_is_new_file_by_filename(const char *filename)
+{
+	bool rv;
+	struct path path;
+
+	if (kern_path(filename, LOOKUP_FOLLOW, &path)) {
+		// Could not lookup path
+		return false;
+	}
+
+	rv = is_new_file_by_dentry(path.dentry);
+	path_put(&path);
+	return rv;
+}
+
+static bool drift_detection_consider_file(struct file *file)
+{
+	if (!d_is_reg(file->f_path.dentry)) {
+		return false;
+	}
+
+	// Filer special file systems
+	switch (file->f_inode->i_sb->s_magic) {
+	case PROC_SUPER_MAGIC:
+		return false;
+	case SYSFS_MAGIC:
+		return false;
+	default:
+		break;
+	}
+
+	return true;
+}
+
+// These functions check whether we should consider the current
+// task for the individual mechanism. In particular, we verify
+// whether "container_only" is set and whether the current task
+// is in a container (if container_only=true)
+static inline bool drift_detection_proceed(void)
+{
+	bool container_host;
+
+	if (!brs_drift_detection_is_enabled())
+		return false;
+
+	container_host = brs__is__DriftDetection__container_host();
+
+	// If host is false, we only consider container
+	if (!container_host && !brs_task_in_container(current))
+		return false;
+
+	return true;
+}
+
+int brs_drift_detection_file_permission(struct brs_policy *policy,
+					struct file *file, int mask)
+{
+	if ((mask & MAY_WRITE) == MAY_WRITE && drift_detection_proceed() &&
+	    drift_detection_consider_file(file)) {
+		int rv;
+		struct brs_xattr_info info = { .new_file = true };
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
+		struct dentry *d = d_real(file->f_path.dentry, D_REAL_DATA);
+#else // LINUX_VERSION_CODE < 6.9
+		struct dentry *d = d_real(file->f_path.dentry, NULL);
+#endif // LINUX_VERSION_CODE <> 6.9
+
+		if (BRS_EVT_ENABLED(&policy->flast, file_drift)) {
+			rv = brs_guestlog_log_file_drift(policy, file, true);
+			if (rv)
+				return rv;
+		}
+
+		rv = drift_detection_set_xattr(d, &info);
+		if (rv && rv != -EOPNOTSUPP) {
+			pr_err("Could not set xattr on file (%d)!\n", rv);
+			drift_detection_debug_print_context(
+				file->f_path.dentry);
+		} else if (rv == 0) {
+			pr_debug("Set xattr on:\n");
+			drift_detection_debug_print_context(
+				file->f_path.dentry);
+		}
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git security/bhv/file_protection.c security/bhv/file_protection.c
new file mode 100644
index 0000000000..2f4be9cd92
--- /dev/null
+++ security/bhv/file_protection.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/context.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/file_protection.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init = HypABI__FileProtection__Init__Config__NONE;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void)
+{
+	unsigned long r;
+
+	HypABI__FileProtection__Init__arg__T *bhv_arg;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+
+	r = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+	if (r) {
+		pr_err("File protection init failed");
+	} else {
+		bhv_file_protection_config = bhv_arg->feature_bitmap;
+	}
+
+	HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+}
+/***********************************************************************/
+
+#define READ_ONLY_FUNC(T)                                                      \
+	bool bhv_block_read_only_file_write_##T(const char *target)            \
+	{                                                                      \
+		unsigned long r;                                               \
+		bool rv;                                                       \
+		HypABI__FileProtection__##T##__arg__T *volatile violation =    \
+			HypABI__FileProtection__##T##__arg__ALLOC();           \
+                                                                               \
+		rv = populate_context(&violation->context, true);              \
+		if (rv) {                                                      \
+			bhv_fail("%s: BHV cannot retrieve event context",      \
+				 __FUNCTION__);                                \
+		}                                                              \
+                                                                               \
+		/* Setup arg */                                                \
+		violation->name_len =                                          \
+			strlen(target) + 1 /* NULL terminator */;              \
+		violation->name =                                              \
+			bhv_virt_to_phys((void *)target, violation->name_len); \
+                                                                               \
+		/* Hypercall */                                                \
+		r = HypABI__FileProtection__##T##__hypercall_noalloc(          \
+			violation);                                            \
+		if (r) {                                                       \
+			pr_err("file protection violation hypercall failed");  \
+			rv = true;                                             \
+		} else {                                                       \
+			/* Note: in case of dirtycred, "block" halts */        \
+			/* the guest with a panic */                           \
+			/* Read block and free */                              \
+			rv = (bool)violation->block;                           \
+		}                                                              \
+		HypABI__FileProtection__##T##__arg__FREE(violation);           \
+                                                                               \
+		return rv;                                                     \
+	}
+
+READ_ONLY_FUNC(ViolationWriteReadOnlyFile)
+READ_ONLY_FUNC(ViolationDirtyCredWrite)
+
+#undef READ_ONLY_FUNC
\ No newline at end of file
diff --git security/bhv/fileops_protection.c security/bhv/fileops_protection.c
new file mode 100644
index 0000000000..8cdfc8783d
--- /dev/null
+++ security/bhv/fileops_protection.c
@@ -0,0 +1,326 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <uapi/linux/magic.h>
+#include <linux/sort.h>
+#include <linux/bsearch.h>
+#include <linux/moduleparam.h>
+
+#include <bhv/bhv.h>
+#include <bhv/context.h>
+#include <bhv/file_protection.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/fileops_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/guestlog.h>
+
+#include <asm/sections.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+static bool bhv_strict_fileops __ro_after_init = false;
+
+bool bhv_strict_fileops_enforced(void)
+{
+	return bhv_strict_fileops;
+}
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+u8 bhv_fileops_type(u32 fs_magic)
+{
+	switch (fs_magic) {
+#if defined CONFIG_EXT4_FS
+	case EXT4_SUPER_MAGIC:
+		return FT(EXT4);
+#endif
+#if defined CONFIG_XFS_FS
+	case XFS_SUPER_MAGIC:
+		return FT(XFS);
+#endif
+	case TMPFS_MAGIC:
+		return FT(TMPFS);
+	case PROC_SUPER_MAGIC:
+		return FT(PROC);
+	case CGROUP2_SUPER_MAGIC:
+		fallthrough;
+	case SYSFS_MAGIC:
+		return FT(SYSFS);
+	case DEBUGFS_MAGIC:
+		return FT(DEBUGFS);
+	case RAMFS_MAGIC:
+		return FT(RAMFS);
+	default:
+		return FT(UNSUPPORTED);
+	}
+}
+#undef FT
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod && ((unsigned long)mod->mem[MOD_RODATA].base <= addr) &&
+	       ((unsigned long)mod->mem[MOD_RODATA].base +
+			mod->mem[MOD_RODATA].size >
+		addr);
+}
+#else
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.text_size <=
+		addr) &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.ro_size >
+		addr);
+}
+#endif
+
+bool bhv_fileops_is_ro(u64 f_op)
+{
+	struct module *mod;
+	if (f_op >= (unsigned long)__start_rodata &&
+	    f_op < (unsigned long)__end_rodata)
+		return true;
+
+	preempt_disable();
+	mod = __module_address(f_op);
+	preempt_enable();
+	if (mod == NULL)
+		return false;
+
+	if (_is_module_ro_data(f_op, mod))
+		return true;
+
+	return false;
+}
+
+bool bhv_block_fileops(const char *target, u8 fops_type, bool is_dir,
+		       const void *fops_ptr)
+{
+	unsigned long err;
+	bool retval;
+	HypABI__FileProtection__ViolationFileOps__arg__T *volatile violation;
+	size_t path_sz;
+	int rv;
+
+	if (!bhv_fileops_file_protection_is_enabled())
+		return false;
+
+	violation = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+
+	rv = populate_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ) {
+		path_sz =
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ -
+			1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->fops_type = fops_type;
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+	violation->fops_ptr = (uint64_t)fops_ptr;
+
+	// pr_err("Bad fops %d %s %d %px %pS\n", violation->fops_type, violation->path_name, violation->is_dir, (void*)violation->fops_ptr, (void*)violation->fops_ptr);
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+
+	return retval;
+}
+
+const fops_t fileops_map[] __section(".rodata") = {
+#define FOPS_MAP(_, idx, file_ops, dir_ops) [idx] = { &file_ops, &dir_ops },
+#define FOPS_MAP_DIRNULL(_, idx, file_ops) [idx] = { &file_ops, NULL },
+#include <bhv/fileops_internal_fopsmap.h>
+};
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[] __ro_after_init = {
+#define FOPS(_)
+#define FOPS_PROC(sym) &sym,
+#include <bhv/fileops_internal_symlist.h>
+};
+
+#define init_fileops_data()
+
+static int cmp_fileops(const void *a_ptr, const void *b_ptr)
+{
+	u64 a = *(u64 *)a_ptr;
+	u64 b = *(u64 *)b_ptr;
+	return ((a == b) ? 0 : (a > b ? 1 : -1));
+}
+
+/***************************************************************
+ * init
+ ***************************************************************/
+void __init bhv_init_fileops(void)
+{
+	static HypABI__Confserver__StrictFileops__arg__T bhv_arg_fb;
+	int rc;
+
+	HypABI__Confserver__StrictFileops__arg__T *bhv_arg =
+		HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(
+			bhv_arg_fb);
+
+	init_fileops_data();
+
+	rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+
+	if (rc == 0) {
+		bhv_strict_fileops = bhv_arg->strict_fileops;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(bhv_arg,
+								    bhv_arg_fb);
+
+	if (rc == 0 && bhv_file_protection_is_enabled()) {
+		sort(proc_fops, ARRAY_SIZE(proc_fops), sizeof(proc_fops[0]),
+		     cmp_fileops, NULL);
+	}
+}
+/***************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **fop_ptr)
+{
+	if (bsearch(fop_ptr, proc_fops, ARRAY_SIZE(proc_fops),
+		    sizeof(proc_fops[0]), cmp_fileops))
+		return true;
+
+	return false;
+}
+
+extern int full_proxy_release(struct inode *inode, struct file *filp);
+extern loff_t full_proxy_llseek(struct file *, loff_t, int);
+extern ssize_t full_proxy_read(struct file *, char __user *, size_t, loff_t *);
+extern ssize_t full_proxy_write(struct file *, const char __user *, size_t,
+				loff_t *);
+extern __poll_t full_proxy_poll(struct file *, struct poll_table_struct *);
+extern long full_proxy_unlocked_ioctl(struct file *, unsigned int,
+				      unsigned long);
+
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr)
+{
+	size_t i;
+
+	if (fop_ptr == NULL)
+		return false;
+
+	// Check whether the fop points to one of three proxy fops
+	// see fs/debugfs/file.c
+	if (fop_ptr == &debugfs_noop_file_operations)
+		return true;
+	else if (fop_ptr == &debugfs_open_proxy_file_operations)
+		return true;
+	else if (fop_ptr == &debugfs_full_proxy_file_operations)
+		return true;
+
+	// Allow directory operations. Might be set by debugfs_create_automount()
+	// (see  fs/debugfs/inode.c)
+	if (fop_ptr == &empty_dir_operations ||
+	    fop_ptr == &simple_dir_operations) {
+		return true;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+	// simple_offset_dir_operations available in versions >= 6.6
+	if (fop_ptr == &simple_offset_dir_operations) {
+		return true;
+	}
+#endif
+
+	// No. This should be a dynamically allocated fops struct.
+	// We will validate the pointer within fops.
+	// See __full_proxy_fops_init in fs/debugfs/file.c
+	if (fop_ptr->llseek != NULL &&
+	    (u64)fop_ptr->llseek != (unsigned long)full_proxy_llseek)
+		return false;
+
+	if (fop_ptr->read != NULL &&
+	    (u64)fop_ptr->read != (unsigned long)full_proxy_read)
+		return false;
+
+	if (fop_ptr->write != NULL &&
+	    (u64)fop_ptr->write != (unsigned long)full_proxy_write)
+		return false;
+
+	if (fop_ptr->read_iter != NULL)
+		return false;
+
+	if (fop_ptr->write_iter != NULL)
+		return false;
+
+	if (fop_ptr->iopoll != NULL)
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 5, 0)
+	if (fop_ptr->iterate != NULL)
+		return false;
+#endif
+
+	if (fop_ptr->iterate_shared != NULL)
+		return false;
+
+	if (fop_ptr->poll != NULL &&
+	    (u64)fop_ptr->poll != (unsigned long)full_proxy_poll)
+		return false;
+
+	if (fop_ptr->unlocked_ioctl != NULL &&
+	    (u64)fop_ptr->unlocked_ioctl !=
+		    (unsigned long)full_proxy_unlocked_ioctl)
+		return false;
+
+	if (fop_ptr->compat_ioctl != NULL)
+		return false;
+
+	if (fop_ptr->mmap != NULL)
+		return false;
+
+	if (fop_ptr->open != NULL)
+		return false;
+
+	if (fop_ptr->flush != NULL)
+		return false;
+
+	if ((u64)fop_ptr->release != (unsigned long)full_proxy_release)
+		return false;
+
+	// Remainder of pointers must be NULL as well
+	for (i = offsetof(struct file_operations, fsync);
+	     i < sizeof(struct file_operations); i++) {
+		if (((uint8_t *)fop_ptr)[i] != '\0') {
+			return false;
+		}
+	}
+
+	return true;
+}
diff --git security/bhv/guestcmd.c security/bhv/guestcmd.c
new file mode 100644
index 0000000000..caea7806b7
--- /dev/null
+++ security/bhv/guestcmd.c
@@ -0,0 +1,149 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+
+#include <linux/cgroup.h>
+
+#include <bhv/guestconn.h>
+
+#include <bhv/guestcmd.h>
+
+#include <bhv/config.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+static void bhv_guestcmd_kill_proc_handler(void *buf, size_t len)
+{
+	int r;
+	SendConnABI__CmdKillProc__T *cmd;
+	struct task_struct *tsk;
+
+	if (len != sizeof(SendConnABI__CmdKillProc__T)) {
+		pr_err("bhv: kill proc command length mismatch!\n");
+		return;
+	}
+	cmd = buf;
+
+	tsk = find_task_by_pid_ns(cmd->pid, &init_pid_ns);
+	if (tsk == NULL) {
+		pr_err("bhv: kill proc command failed: no task with pid %llu!\n",
+		       cmd->pid);
+		return;
+	}
+
+	r = send_sig(SIGKILL, tsk, 0);
+	if (r) {
+		pr_err("bhv: kill proc command failed with %d!\n", r);
+		return;
+	}
+}
+
+static bool bhv_guestcmd_is_target_container(struct cgroup *cur,
+					     const char *target)
+{
+	static char buf[128];
+
+	buf[0] = '\0';
+	cgroup_name(cur, buf, sizeof(buf));
+
+	return strstr(buf, target) != NULL;
+}
+
+// Kill containers is only supported in the VAS kernel.
+extern bool cgroup_is_threaded(struct cgroup *cgrp);
+extern void cgroup_kill(struct cgroup *cgrp);
+static void bhv_guestcmd_kill_container_handler(void *buf, size_t len)
+{
+	struct cgroup_subsys_state *pos;
+	struct cgroup *cur;
+	SendConnABI__CmdKillContainer__T *cmd;
+
+	if (len != sizeof(SendConnABI__CmdKillContainer__T)) {
+		pr_err("bhv: kill container command length mismatch!\n");
+		return;
+	}
+	cmd = buf;
+
+	rcu_read_lock();
+	css_for_each_descendant_pre (pos, &cgrp_dfl_root.cgrp.self) {
+		cur = container_of(pos, struct cgroup, self);
+		if (bhv_guestcmd_is_target_container(cur, cmd->container_id))
+			break;
+
+		cur = NULL;
+	}
+	rcu_read_unlock();
+
+	if (cur == NULL) {
+		pr_err("bhv: kill container command failed! Could not find container with ID '%s'!\n",
+		       cmd->container_id);
+		return;
+	}
+
+	if (cgroup_is_threaded(cur)) {
+		pr_err("bhv: kill container command failed! Cannot kill threaded container!\n");
+		return;
+	}
+
+	cgroup_kill(cur);
+}
+
+static void bhv_guestcmd_policy_update_handler(void *buf, size_t len)
+{
+	if (len != sizeof(SendConnABI__CmdPolicyUpdate__T)) {
+		pr_err("bhv: policy update command length mismatch!\n");
+		return;
+	}
+
+	bhv_policy_update();
+}
+
+static const __section(".rodata") brs_guestconn_backend_handler_t
+	bhv_guestcmd_cmd_handlers[SendConnABI__NUM_CMDS] = {
+		bhv_guestcmd_kill_proc_handler,
+		bhv_guestcmd_kill_container_handler,
+		bhv_guestcmd_policy_update_handler,
+	};
+
+static int bhv_guestcmd_get_cmd(void *buf, size_t len, uint64_t *cmd)
+{
+	if (len < sizeof(uint64_t))
+		return -EOVERFLOW;
+
+	*cmd = *(uint64_t *)buf;
+
+	if (*cmd >= SendConnABI__NUM_CMDS)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void bhv_guestcmd_handler(void *buf, size_t len)
+{
+	int r;
+	uint64_t cmd;
+
+	r = bhv_guestcmd_get_cmd(buf, len, &cmd);
+	if (r) {
+		pr_err("BHV: Guestcmd: Invalid message! (%d)\n", r);
+		return;
+	}
+
+	bhv_guestcmd_cmd_handlers[cmd](buf, len);
+}
+
+/*********************************************************
+ * init
+ *********************************************************/
+void __init bhv_init_guestcmd(void)
+{
+	int r;
+
+	r = brs_guestconn_register_backend(bhv_guestcmd_handler);
+	if (r) {
+		bhv_fail("bhv: unable to register guestcmd handler!\n");
+		return;
+	}
+}
+/*********************************************************/
diff --git security/bhv/guestconn.c security/bhv/guestconn.c
new file mode 100644
index 0000000000..d7a3eae33c
--- /dev/null
+++ security/bhv/guestconn.c
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include "guestconn.h"
+#include <bhv/guestconn.h>
+
+uint32_t brs_guestconn_cid __ro_after_init = 0;
+uint32_t brs_guestconn_port __ro_after_init = 0;
+
+static int brs_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	brs_guestconn_send_reboot();
+
+	if (is_bhv_initialized()) {
+		brs_guestconn_listen_reboot();
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block brs_guestconn_reboot_notifier = {
+	.notifier_call = brs_guestconn_reboot,
+	.priority = 0,
+};
+
+void __init brs_guestconn_init(void)
+{
+	pr_info("%s: [BRS] bhv_initialized=%d brs_standalone=%d", __FUNCTION__,
+		is_bhv_initialized(), brs_is_standalone());
+
+	if (!is_bhv_initialized() && !brs_is_standalone())
+		return;
+
+	if (is_bhv_initialized()) {
+		BUG_ON(brs_guestconn_cid == 0 && brs_guestconn_port == 0);
+	}
+
+	register_reboot_notifier(&brs_guestconn_reboot_notifier);
+	brs_guestconn_send_alloc();
+}
+
+/**********************************************************
+ * init
+ **********************************************************/
+int __init brs_init_guestconn(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	brs_guestconn_cid = cid;
+	brs_guestconn_port = port;
+	return 0;
+}
+/**********************************************************/
diff --git security/bhv/guestconn.h security/bhv/guestconn.h
new file mode 100644
index 0000000000..084026f216
--- /dev/null
+++ security/bhv/guestconn.h
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BRS_GUESTCONN_INTERNAL_H__
+#define __BRS_GUESTCONN_INTERNAL_H__
+void brs_guestconn_send_reboot(void);
+void brs_guestconn_listen_reboot(void);
+void brs_guestconn_send_alloc(void);
+extern uint32_t brs_guestconn_cid;
+extern uint32_t brs_guestconn_port;
+#endif /* __BRS_GUESTCONN_INTERNAL_H__ */
diff --git security/bhv/guestconn_listen.c security/bhv/guestconn_listen.c
new file mode 100644
index 0000000000..ec6171f4c0
--- /dev/null
+++ security/bhv/guestconn_listen.c
@@ -0,0 +1,231 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+// #include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/kthread.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+#include <net/vsock_addr.h>
+#include <net/sock.h>
+
+#include <bhv/bhv.h>
+
+#include "guestconn.h"
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+typedef struct {
+	u32 cid;
+	u32 port;
+} guestconn_conn_t;
+
+static struct task_struct *guestconn_listen_tsk;
+
+static brs_guestconn_backend_handler_t brs_guestconn_backend_handler
+    __ro_after_init = NULL;
+
+
+static int guestconn_recv(struct socket *vsock, u8 *buf, size_t sz)
+{
+	size_t bytes_read = 0;
+	struct msghdr msg = { .msg_flags = 0 };
+	int len;
+	struct kvec iov;
+
+	while (bytes_read < sz) {
+		iov.iov_base = (void *)(buf + bytes_read);
+		iov.iov_len = sz - bytes_read;
+		len = kernel_recvmsg(vsock, &msg, &iov, 1, iov.iov_len,
+				     msg.msg_flags);
+		if (unlikely(kthread_should_stop()))
+			return -ERESTARTSYS;
+		if (unlikely((len < 0))) {
+			if (len == -EAGAIN || len == -ETIMEDOUT)
+				continue;
+			return len;
+		}
+		bytes_read += len;
+	}
+	return 0;
+}
+
+static int guestconn_get_hdr(struct socket *vsock, SendConnABI__Header__T *hdr)
+{
+	return guestconn_recv(vsock, (u8 *)hdr, sizeof(SendConnABI__Header__T));
+}
+
+static int guestconn_get_body(struct socket *vsock, void *buf, size_t read_len)
+{
+	return guestconn_recv(vsock, (u8 *)buf, read_len);
+}
+
+static void deliver_body(void *buf, size_t len)
+{
+	BUG_ON(buf == NULL);
+	if (len == 0)
+		return;
+
+	if (brs_guestconn_backend_handler == NULL) {
+		pr_err("no handler registered\n");
+		return;
+	}
+
+	brs_guestconn_backend_handler(buf, len);
+}
+
+static int guestconn_listen_kthread_main(void *arg)
+{
+	int rv = 0;
+	int err;
+	struct sockaddr_vm addr;
+	struct socket *vsock;
+	SendConnABI__Header__T hdr;
+	void *body_buf;
+	int body_buf_order = 0;
+	size_t body_buf_read_len;
+	uint32_t cid = ((guestconn_conn_t *)arg)->cid;
+	uint32_t port = ((guestconn_conn_t *)arg)->port;
+
+	kfree((guestconn_conn_t *)arg);
+
+	body_buf = (void *)__get_free_pages(GFP_KERNEL, body_buf_order);
+	if (body_buf == NULL) {
+		bhv_fail("GuestConn: Listen: Could allocate body buffer");
+		return -ENOMEM;
+	}
+
+	vsock_addr_init(&addr, cid, port);
+	pr_info("bhv guestconn listener started with cid %u, port %u", cid,
+		port);
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		free_pages((unsigned long)body_buf, body_buf_order);
+		bhv_fail(
+			"GuestConn: Listen: Could not create kernel socket (%d)",
+			err);
+		return err;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		free_pages((unsigned long)body_buf, body_buf_order);
+		bhv_fail("GuestConn: Listen: Could not connect to host (%d)",
+			 err);
+		return err;
+	}
+
+	while (!kthread_should_stop()) {
+		err = guestconn_get_hdr(vsock, &hdr);
+		if (err) {
+			if (err != -ERESTARTSYS) {
+				bhv_fail(
+					"GuestConn: Listen: error during recv (%d)",
+					err);
+				rv = err;
+			}
+			break;
+		}
+
+		if (hdr.sz < sizeof(SendConnABI__Header__T)) {
+			bhv_fail("Guestconn: Listen: protocol error");
+			return -EPROTO;
+		}
+
+		body_buf_read_len = hdr.sz - sizeof(SendConnABI__Header__T);
+
+		if (body_buf_read_len == 0) {
+			pr_warn("recieved msg of size 0\n");
+			continue;
+		}
+
+		if (((1ULL << body_buf_order) * PAGE_SIZE) <
+		    body_buf_read_len) {
+			free_pages((unsigned long)body_buf, body_buf_order);
+			body_buf_order = get_order(body_buf_read_len);
+			body_buf = (void *)__get_free_pages(GFP_KERNEL,
+							    body_buf_order);
+			if (body_buf == NULL) {
+				bhv_fail(
+					"GuestConn: Listen: Could allocate body buffer");
+				rv = -ENOMEM;
+				break;
+			}
+		}
+
+		err = guestconn_get_body(vsock, body_buf, body_buf_read_len);
+		if (err) {
+			if (err != -ERESTARTSYS) {
+				bhv_fail(
+					"GuestConn: Listen: error during recv (%d)",
+					err);
+				rv = err;
+			}
+			break;
+		}
+
+		deliver_body(body_buf, body_buf_read_len);
+	}
+
+	sock_release(vsock);
+	if (body_buf != NULL)
+		free_pages((unsigned long)body_buf, body_buf_order);
+	return rv;
+}
+
+void brs_guestconn_listen_reboot(void)
+{
+	kthread_stop(guestconn_listen_tsk);
+}
+
+int __init brs_guestconn_register_backend(brs_guestconn_backend_handler_t cb)
+{
+	if (brs_guestconn_backend_handler != NULL) {
+		pr_err("BHV: registration attempt with existing handler\n");
+		return -EINVAL;
+	}
+	brs_guestconn_backend_handler = cb;
+	return 0;
+}
+
+static int __init bhv_guestconn_listen_init(void)
+{
+	guestconn_conn_t *guestconn_conn;
+	int rc = 0;
+
+	if (!is_bhv_initialized())
+		return rc;
+
+	guestconn_conn = kzalloc(sizeof(guestconn_conn_t), GFP_KERNEL);
+	if (guestconn_conn == NULL) {
+		bhv_fail(
+			"GuestConn: Could not allocate memory for guestconn arg\n");
+		return -ENOMEM;
+	}
+
+	guestconn_conn->cid = brs_guestconn_cid;
+	guestconn_conn->port = brs_guestconn_port;
+
+	guestconn_listen_tsk =
+		kthread_create(guestconn_listen_kthread_main, guestconn_conn,
+			       "guestconn_listen");
+	rc = IS_ERR(guestconn_listen_tsk);
+	if (rc) {
+		bhv_fail("GuestConn: Could not create kthread (%ld)\n",
+			 PTR_ERR(guestconn_listen_tsk));
+		guestconn_listen_tsk = NULL;
+		kfree(guestconn_conn);
+		return rc;
+	}
+
+	wake_up_process(guestconn_listen_tsk);
+	return rc;
+}
+
+device_initcall_sync(bhv_guestconn_listen_init);
\ No newline at end of file
diff --git security/bhv/guestconn_send.c security/bhv/guestconn_send.c
new file mode 100644
index 0000000000..3a04b0282e
--- /dev/null
+++ security/bhv/guestconn_send.c
@@ -0,0 +1,520 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022-2025 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/cpumask.h>
+#include <linux/mutex.h>
+#include <linux/mmzone.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/smp.h>
+#include <linux/static_call.h>
+#include <linux/types.h>
+#include <linux/genalloc.h>
+#include <linux/workqueue.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+#include <bhv/brs_fs.h>
+#include <bhv/guestlog.h>
+#include <bhv/libinsight.h>
+
+#include "guestconn.h"
+#include <bhv/guestconn.h>
+
+#define brs_guestconn_send_configured true
+
+typedef struct {
+	struct work_struct wq_link;
+	size_t to_send;
+	size_t sz;
+	uint8_t *msg;
+} brs_guestconn_work_t;
+
+struct brs_gen_pool {
+	struct gen_pool *pool;
+	unsigned long mem;
+};
+
+static DEFINE_MUTEX(vsock_mutex);
+static struct socket *vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+
+static struct workqueue_struct *brs_guestconn_wq = NULL;
+static struct kmem_cache *brs_guestconn_work_cache;
+static struct brs_gen_pool __percpu *brs_guestconn_gen_pool;
+
+#define BRS_GUESTCONN_GEN_POOL_SZ_ORDER 10 // pool size, per CPU, in pages: 4MB
+#define POOL_ALIGN_ORDER 8 // allocation granularity: 256 bytes
+_Static_assert(1<<POOL_ALIGN_ORDER >= sizeof(uint64_t));
+
+static void __brs_guestconn_sendmsg_vsock(brs_guestconn_work_t *);
+static void __brs_guestconn_sendmsg_relay(brs_guestconn_work_t *);
+
+DEFINE_STATIC_CALL(__brs_guestconn_sendmsg, __brs_guestconn_sendmsg_vsock);
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define ALLOC_MAGIC 0xA10CA73D
+#endif //CONFIG_BHV_VAS_DEBUG
+
+struct brs_guestconn_allocation {
+	size_t size;
+	unsigned cpu;
+#ifdef ALLOC_MAGIC
+	uint32_t magic;
+#endif // ALLOC_MAGIC
+	uint8_t data[] __aligned(16);
+};
+_Static_assert(sizeof(struct brs_guestconn_allocation) == 16);
+
+
+#define ALLOC_FROM_MSG(MSG)                                                    \
+	container_of((void*)MSG, struct brs_guestconn_allocation, data)
+
+void *brs_guestconn_alloc_msg(size_t sz)
+{
+	struct brs_guestconn_allocation *alloc;
+	unsigned cpu;
+
+	// XXX: We may want to save this data elsewhere for better
+	// resistance to memory corruption.
+	preempt_disable();
+	cpu = smp_processor_id();
+	alloc = (struct brs_guestconn_allocation *)gen_pool_alloc(
+		this_cpu_ptr(brs_guestconn_gen_pool)->pool,
+		sz + sizeof(struct brs_guestconn_allocation));
+	preempt_enable();
+	if (!alloc) {
+#ifdef CONFIG_BHV_VAS_DEBUG
+		pr_err("%s returning NULL!\n", __FUNCTION__);
+#endif //CONFIG_BHV_VAS_DEBUG
+		return NULL;
+	}
+	alloc->size = sz;
+	alloc->cpu = cpu;
+#ifdef ALLOC_MAGIC
+	alloc->magic = ALLOC_MAGIC;
+#endif // ALLOC_MAGIC
+	return alloc->data;
+}
+
+void brs_guestconn_free_msg(void *msg)
+{
+	struct brs_guestconn_allocation *alloc = ALLOC_FROM_MSG(msg);
+	BUG_ON(!cpu_possible(alloc->cpu));
+#ifdef ALLOC_MAGIC
+	BUG_ON(alloc->magic != ALLOC_MAGIC);
+#endif // ALLOC_MAGIC
+	BUG_ON(!gen_pool_has_addr(
+		per_cpu_ptr(brs_guestconn_gen_pool, alloc->cpu)->pool,
+		(unsigned long)(alloc),
+		alloc->size + sizeof(struct brs_guestconn_allocation)));
+	gen_pool_free(per_cpu_ptr(brs_guestconn_gen_pool, alloc->cpu)->pool,
+		      (unsigned long)(alloc),
+		      alloc->size + sizeof(struct brs_guestconn_allocation));
+}
+
+static inline void vsock_lock(void)
+{
+	mutex_lock(&vsock_mutex);
+}
+
+static inline void vsock_unlock(void)
+{
+	mutex_unlock(&vsock_mutex);
+}
+
+static inline size_t __brs_send_vsock(void *data, size_t size, size_t to_send,
+				      struct msghdr *msghdr)
+{
+	int r;
+	struct kvec vec;
+
+	/* XXX: Remove the lock as soon as we use a vsock per CPU. */
+	vsock_lock();
+
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+
+		r = kernel_sendmsg(vsock, msghdr, &vec, 1, vec.iov_len);
+		if (r == -EAGAIN)
+			goto unlock;
+
+		if (r < 0) {
+			pr_err("[BRS] GuestLog: Send Failed (%d)", r);
+			to_send = 0;
+			goto unlock;
+		}
+
+		to_send -= r;
+	}
+
+unlock:
+	vsock_unlock();
+
+	return to_send;
+}
+
+static inline size_t brs_send_blocking_vsock(void *data, size_t size,
+					     size_t to_send)
+{
+	struct msghdr msghdr = { .msg_flags = 0 };
+	return __brs_send_vsock(data, size, to_send, &msghdr);
+}
+
+static inline size_t brs_send_non_blocking_vsock(void *data, size_t size,
+						 size_t to_send)
+{
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	return __brs_send_vsock(data, size, to_send, &msghdr);
+}
+
+static void __brs_guestconn_sendmsg_vsock(brs_guestconn_work_t *work)
+{
+	work->to_send =
+		brs_send_non_blocking_vsock(work->msg, work->sz, work->to_send);
+	if (work->to_send != 0) {
+		work->to_send = brs_send_blocking_vsock(work->msg, work->sz,
+							work->to_send);
+	}
+}
+
+static void __brs_guestconn_sendmsg_relay(brs_guestconn_work_t *work)
+{
+	ssize_t rc = 0;
+
+	rc = brs_fs_relay_write(work->msg, work->to_send, false);
+	if (rc < 0) {
+		bhv_fail("[BRS] Cannot send data via relay channels (%lu)", rc);
+		return;
+	}
+
+	work->to_send -= rc;
+}
+
+static void brs_guestconn_sendmsg(struct work_struct *w)
+{
+	brs_guestconn_work_t *work =
+		container_of(w, brs_guestconn_work_t, wq_link);
+
+	static_call(__brs_guestconn_sendmsg)(work);
+
+	BUG_ON(work->to_send != 0);
+
+	brs_guestconn_free_msg(work->msg);
+	kmem_cache_free(brs_guestconn_work_cache, work);
+}
+
+int brs_guestconn_send_sync(uint8_t *msg, size_t size, uint64_t id)
+{
+	struct brs_guestconn_allocation *alloc = ALLOC_FROM_MSG(msg);
+	ssize_t rc = 0;
+
+	if (!brs_guestconn_send_configured) {
+		brs_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		brs_guestconn_free_msg(msg);
+		return 0;
+	}
+
+#ifdef ALLOC_MAGIC
+	BUG_ON(alloc->magic != ALLOC_MAGIC);
+#endif // ALLOC_MAGIC
+	BUG_ON(size > alloc->size);
+
+	/*
+	 * The function brs_fs_relay_write_sync determines how to proceed:
+	 * - rc == 0: do not block event
+	 * - rc != 0: error or permission to block the event (-EPERM)
+	 *
+	 * XXX: Create a static call key to select between relay and vsock.
+	 */
+	rc = brs_fs_relay_write_sync(msg, size, id);
+
+	/* Propagate the return value to the caller. */
+
+	brs_guestconn_free_msg(msg);
+
+	return rc;
+}
+
+int brs_guestconn_send(uint8_t *msg, size_t size)
+{
+	struct brs_guestconn_allocation *alloc = ALLOC_FROM_MSG(msg);
+	brs_guestconn_work_t *work = NULL;
+
+	if (!brs_guestconn_send_configured) {
+		brs_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		brs_guestconn_free_msg(msg);
+		return 0;
+	}
+
+#ifdef ALLOC_MAGIC
+	BUG_ON(alloc->magic != ALLOC_MAGIC);
+#endif // ALLOC_MAGIC
+	BUG_ON(size > alloc->size);
+	BUG_ON(!brs_guestconn_work_cache);
+
+	pr_debug("[BRS] GuestConn: Queuing msg with size %lu", size);
+
+	work = kmem_cache_alloc(brs_guestconn_work_cache, GFP_ATOMIC);
+	if (work == NULL) {
+		brs_guestconn_free_msg(msg);
+		bhv_fail("[BRS]: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	work->msg = msg;
+	work->sz = size;
+	work->to_send = size;
+
+	INIT_WORK(&work->wq_link, brs_guestconn_sendmsg);
+
+	/*
+	 * XXX: Eliminate the following if statement! We must ensure that
+	 * sending events is possible only if GuestCon was initialized.
+	 * Otherwise, this function should never be called.
+	 */
+
+	if (!atomic_read(&workqueue_ready)) {
+		/*
+		 * XXX: For now, if the workqueue was not initialized, we drop
+		 * the event. Investigate whether we should maintain premature
+		 * events in a list, until the workqueue becomes ready.
+		 */
+		kmem_cache_free(brs_guestconn_work_cache, work);
+
+		return 0;
+	}
+
+	queue_work(brs_guestconn_wq, &work->wq_link);
+
+	return 0;
+}
+
+void brs_guestconn_send_reboot(void)
+{
+	int cpu;
+
+	atomic_set(&reboot_in_progress, 1);
+
+	brs_guestlog_disable_all();
+	brs_trace_disable();
+
+	if (atomic_read(&workqueue_ready)) {
+		atomic_set(&workqueue_ready, 0);
+		drain_workqueue(brs_guestconn_wq);
+	}
+
+	brs_fs_relay_uninit();
+
+	if (vsock != NULL) {
+		/* We assume all messages are gone: shut down the socket. */
+		sock_release(vsock);
+		vsock = NULL;
+	}
+
+	if (brs_guestconn_work_cache != NULL) {
+		kmem_cache_destroy(brs_guestconn_work_cache);
+		brs_guestconn_work_cache = NULL;
+	}
+
+	if (brs_guestconn_gen_pool) {
+		for_each_possible_cpu (cpu) {
+			struct brs_gen_pool *tmp_gen_pool =
+				per_cpu_ptr(brs_guestconn_gen_pool, cpu);
+			if (tmp_gen_pool->pool != NULL) {
+				gen_pool_destroy(tmp_gen_pool->pool);
+				tmp_gen_pool->pool = NULL;
+			}
+			if (tmp_gen_pool->mem != 0) {
+				free_pages(tmp_gen_pool->mem,
+					   BRS_GUESTCONN_GEN_POOL_SZ_ORDER);
+				tmp_gen_pool->mem = 0;
+			}
+		}
+		free_percpu(brs_guestconn_gen_pool);
+	}
+}
+
+static inline int __init brs_init_workqueue(void)
+{
+	/*
+	 * Prepare a workqueue that is capable of handling both the BRS-assisted
+	 * and the bare Linux kernel setup.
+	 *
+	 * WQ_UNBOUND:     worker threads may run on any CPU.
+	 * WQ_MEM_RECLAIM: worker threds should produce events, even if the
+	 *                 system is under memory pressure.
+	 */
+	brs_guestconn_wq = alloc_workqueue("brs_guestconn_wq",
+					   WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+	if (brs_guestconn_wq == NULL) {
+		bhv_fail("[BRS]: Cannot allocate work queue!");
+		return -EINVAL;
+	}
+
+	atomic_inc(&workqueue_ready);
+
+	return 0;
+}
+
+static inline int __init bhv_init_vsock(uint32_t brs_guestconn_cid,
+					uint32_t brs_guestconn_port)
+{
+	int rc;
+	struct sockaddr_vm addr;
+
+	BUG_ON(brs_guestconn_cid == 0 && brs_guestconn_port == 0);
+
+	vsock_addr_init(&addr, brs_guestconn_cid, brs_guestconn_port);
+
+	pr_info("[BRS] GuestConn sender started with cid:port %u:%u",
+		brs_guestconn_cid, brs_guestconn_port);
+
+	rc = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (rc < 0) {
+		bhv_fail("GuestConn: Cannot create kernel socket (%d)", rc);
+		return rc;
+	}
+
+	rc = kernel_connect(vsock, (struct sockaddr *)&addr,
+			    sizeof(struct sockaddr_vm), 0);
+	if (rc < 0) {
+		sock_release(vsock);
+		bhv_fail("GuestConn: Cannot connect to host (%d)", rc);
+		return rc;
+	}
+
+	return 0;
+}
+
+static inline int __init brs_init_relay(void)
+{
+	int rc = 0;
+
+	pr_info("[BRS] GuestConn: initializing relay event channels");
+
+	rc = brs_fs_relay_init();
+	if (rc != 0) {
+		bhv_fail("GuestConn: Cannot init relay channels (%d)", rc);
+		return rc;
+	}
+
+	/* Update the static call if the kernel is not assisted by BRS. */
+	static_call_update(__brs_guestconn_sendmsg,
+			   __brs_guestconn_sendmsg_relay);
+	return rc;
+}
+
+void __init brs_guestconn_send_alloc(void)
+{
+#if defined(MAX_PAGE_ORDER)
+	_Static_assert(BRS_GUESTCONN_GEN_POOL_SZ_ORDER <= MAX_PAGE_ORDER,
+		       "BRS_GUESTCONN_GEN_POOL_SZ_ORDER > MAX_PAGE_ORDER");
+#else
+	_Static_assert(BRS_GUESTCONN_GEN_POOL_SZ_ORDER <= MAX_ORDER,
+		       "BRS_GUESTCONN_GEN_POOL_SZ_ORDER > MAX_ORDER");
+#endif
+
+	int cpu;
+
+	brs_guestconn_work_cache = kmem_cache_create(
+		"brs_guestconn_work_cache", sizeof(brs_guestconn_work_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (brs_guestconn_work_cache == NULL) {
+		bhv_fail("[BRS]: Cannot create kmem_cache for work items!");
+		return;
+	}
+
+	brs_guestconn_gen_pool = alloc_percpu(struct brs_gen_pool);
+	for_each_possible_cpu (cpu) {
+		int rv;
+		unsigned long tmp_mem;
+		struct gen_pool *tmp_gen_pool =
+			gen_pool_create(POOL_ALIGN_ORDER, -1);
+		if (tmp_gen_pool == NULL) {
+			bhv_fail(
+				"[BRS]: Cannot create gen_pool for guestconn!");
+			goto clear_pool;
+		}
+		tmp_mem = __get_free_pages(GFP_KERNEL,
+					   BRS_GUESTCONN_GEN_POOL_SZ_ORDER);
+		if (tmp_mem == 0) {
+			gen_pool_destroy(tmp_gen_pool);
+			bhv_fail(
+				"[BRS]: Cannot create gen_pool pages for guestconn!");
+			goto clear_pool;
+		}
+		rv = gen_pool_add(
+			tmp_gen_pool, tmp_mem,
+			(1 << BRS_GUESTCONN_GEN_POOL_SZ_ORDER) * PAGE_SIZE, -1);
+		if (rv) {
+			gen_pool_destroy(tmp_gen_pool);
+			free_pages(tmp_mem, BRS_GUESTCONN_GEN_POOL_SZ_ORDER);
+			bhv_fail(
+				"[BRS]: Cannot add pages to gen_pool for guestconn!");
+			goto clear_pool;
+		}
+		per_cpu_ptr(brs_guestconn_gen_pool, cpu)->pool = tmp_gen_pool;
+		per_cpu_ptr(brs_guestconn_gen_pool, cpu)->mem = tmp_mem;
+	}
+	return;
+clear_pool:
+	for_each_possible_cpu (cpu) {
+		struct brs_gen_pool *tmp_gen_pool =
+			per_cpu_ptr(brs_guestconn_gen_pool, cpu);
+		if (tmp_gen_pool->pool != NULL) {
+			gen_pool_destroy(tmp_gen_pool->pool);
+			tmp_gen_pool->pool = NULL;
+		}
+		if (tmp_gen_pool->mem != 0) {
+			free_pages(tmp_gen_pool->mem,
+				   BRS_GUESTCONN_GEN_POOL_SZ_ORDER);
+			tmp_gen_pool->mem = 0;
+		}
+	}
+	free_percpu(brs_guestconn_gen_pool);
+}
+
+static int __init brs_guestconn_send_init(void)
+{
+	int rc = 0;
+	/*
+	 * Since this function gets called as one of the system's initcalls, we need
+	 * to make sure that we do not continue initializing relay channels if the
+	 * system is not in a standalone setting. Similarly, do not initialize
+	 * vsockets when not running on bhv.
+	 */
+	if (!is_bhv_initialized() && !brs_is_standalone())
+		return rc;
+
+	if (is_bhv_initialized()) {
+		rc = bhv_init_vsock(brs_guestconn_cid, brs_guestconn_port);
+	} else if (brs_is_standalone()) {
+		rc = brs_init_relay();
+	}
+
+	if (rc)
+		return rc;
+
+	return brs_init_workqueue();
+}
+
+device_initcall_sync(brs_guestconn_send_init);
diff --git security/bhv/guestlog.c security/bhv/guestlog.c
new file mode 100644
index 0000000000..30063dfb92
--- /dev/null
+++ security/bhv/guestlog.c
@@ -0,0 +1,1099 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/types.h>
+#include <linux/ctype.h>
+
+#include <linux/cgroup.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/time_namespace.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/namei.h>
+#include <linux/fs_struct.h>
+#include <linux/base64.h>
+#include <net/net_namespace.h>
+#include <net/sock.h>
+#include <net/inet_common.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+
+#include <bhv/brs_policy.h>
+#include <bhv/capability.h>
+#include <bhv/config.h>
+#include <bhv/context.h>
+#include <bhv/drift_detection.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestlog_internal.h>
+#include <bhv/json_gen.h>
+#include <bhv/policy_scks_autogen.h>
+#include <bhv/util.h>
+
+#include <bhv/events_autogen.h>
+
+#include "../../fs/internal.h"
+
+struct sock *unix_find_other(struct net *net, struct sockaddr_un *sunaddr,
+			     int addr_len, int type);
+struct sock *unix_find_socket_byinode(struct inode *i);
+
+static atomic64_t brs_event_id_ctr = ATOMIC_INIT(0);
+
+static inline bool brs_is_exempt(struct brs_policy *policy)
+{
+	if (policy == NULL)
+		return false;
+	if (brs_is_configurator_process(current))
+		return true;
+	return brs_is_cgroup_exempt(policy, current);
+}
+
+static inline void brs_set_meta(struct brs_evt_EventMeta *meta)
+{
+	meta->source_event_id = atomic64_fetch_inc(&brs_event_id_ctr);
+}
+
+#define EVT_PROLOGUE(POLICY, EVT, DATA)                                        \
+	int __rc;                                                              \
+	struct CONCATENATE(brs_evt_, EVT) * DATA;                              \
+	struct CONCATENATE(brs_evt_buf_, EVT) * __buf;                         \
+                                                                               \
+	if (brs_is_exempt(POLICY))                                             \
+		return 0;                                                      \
+                                                                               \
+	if ((__rc = CONCATENATE(brs_evt_alloc_, EVT)(&DATA, &__buf)))          \
+		return __rc;                                                   \
+                                                                               \
+	brs_set_meta(&DATA->meta);                                             \
+	CONCATENATE(CONCATENATE(BRS_EVT_, EVT), _ADD_CONTEXT)(&data->context)
+
+#define EVT_EPILOGUE(EVT, DATA)                                                \
+	return CONCATENATE(brs_evt_send_, EVT)(DATA, __buf)
+
+#define EVT_ABORT(EVT, DATA, ERR)                                              \
+	{                                                                      \
+		pr_err("%s: Failure sending brs event (%s:%d)!", __FUNCTION__, \
+		       __FILE__, __LINE__);                                    \
+		brs_guestconn_free_msg(&data);                                 \
+		brs_guestconn_free_msg(&__buf);                                \
+		return ERR;                                                    \
+	}
+
+int send_evt_msg_sync(evt_writer_t *writer)
+{
+	int msg_sz = brs_evt_finalize(writer, BRS_EVENT_TYPE_SYNC);
+	if (msg_sz < 0) {
+		pr_err("%s: Failure finalizing brs event!", __FUNCTION__);
+		brs_guestconn_free_msg(writer->buf);
+		return msg_sz;
+	}
+	return brs_guestconn_send_sync(writer->buf, msg_sz,
+				       writer->data->header.id);
+}
+
+int send_evt_msg(evt_writer_t *writer)
+{
+	int msg_sz = brs_evt_finalize(writer, BRS_EVENT_TYPE_ASYNC);
+	if (msg_sz < 0) {
+		pr_err("%s: Failure finalizing brs event!", __FUNCTION__);
+		brs_guestconn_free_msg(writer->buf);
+		return msg_sz;
+	}
+	return brs_guestconn_send(writer->buf, msg_sz);
+}
+
+static inline int16_t get_file_path_noalloc(struct path *f_path, char *buf,
+					    size_t max_size)
+{
+	char *path = NULL;
+
+	BUG_ON(f_path == NULL);
+	BUG_ON(buf == NULL);
+
+	path = d_path(f_path, buf, max_size);
+	if (IS_ERR(path)) {
+		BUG_ON(PTR_ERR(path) >= 0);
+		return PTR_ERR(path);
+	}
+
+	buf[max_size - 1] = '\0';
+
+	BUG_ON(path - buf > S16_MAX);
+	return path - buf;
+}
+
+static inline int16_t get_dentry_path_noalloc(struct dentry *dentry, char *buf,
+					      size_t max_size)
+{
+	const char *path;
+	BUG_ON(buf == NULL);
+
+	path = dentry_path(dentry, buf, max_size);
+	if (IS_ERR(path)) {
+		BUG_ON(PTR_ERR(path) >= 0);
+		return PTR_ERR(path);
+	}
+
+	buf[max_size - 1] = '\0';
+
+	BUG_ON(path - buf > S16_MAX);
+	return path - buf;
+}
+
+#define CLS Error
+int brs_guestlog_error(bool global_timeout, bool event_timeout)
+{
+	EVT_PROLOGUE(NULL, CLS, data);
+
+	data->global_event_timeout = global_timeout;
+	data->single_event_timeout = event_timeout;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS Log
+int brs_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+	EVT_PROLOGUE(NULL, CLS, data);
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(data->BES_FLD_MAX(CLS, message), fmt, args);
+	va_end(args);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS NamespaceAttributes
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _brs_get_incoming_ns_inums(struct task_struct *tsk,
+				       struct nsset *nsset, BES(CLS) * bes)
+{
+	if (nsset->nsproxy) {
+		bes->cgroup_ns_inum = NS_DEREF(nsset->nsproxy->cgroup_ns);
+		bes->ipc_ns_inum = NS_DEREF(nsset->nsproxy->ipc_ns);
+		bes->mnt_ns_inum = from_mnt_ns(nsset->nsproxy->mnt_ns)->inum;
+		bes->net_ns_inum = NS_DEREF(nsset->nsproxy->net_ns);
+		bes->time_ns_inum = NS_DEREF(nsset->nsproxy->time_ns);
+		bes->time_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->time_ns_for_children);
+		bes->uts_ns_inum = NS_DEREF(nsset->nsproxy->uts_ns);
+		bes->pid_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->pid_ns_for_children);
+	} else {
+		bes->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		bes->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		bes->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		bes->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		bes->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		bes->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		bes->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+		bes->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+	}
+
+	bes->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+
+	if (nsset->flags & CLONE_NEWUSER) {
+		bes->user_ns_inum = NS_DEREF(nsset->cred->user_ns);
+	} else {
+		rcu_read_lock();
+		bes->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+	}
+}
+#undef NS_DEREF
+#undef CLS
+
+#define CLS ProcessFork
+int brs_guestlog_log_process_fork(struct brs_policy *policy, uint32_t child_pid,
+				  const char *child_comm, uint32_t parent_pid,
+				  const char *parent_comm)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->child.pid = child_pid;
+	strscpy(data->child.comm, child_comm,
+		BES_MAX(BasicProcessAttributes, comm));
+	data->parent.pid = parent_pid;
+	strscpy(data->parent.comm, parent_comm,
+		BES_MAX(BasicProcessAttributes, comm));
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+static inline struct page *brs_get_page(struct linux_binprm *bprm,
+					unsigned long addr)
+{
+	struct page *page;
+#ifdef CONFIG_MMU
+	/*
+		 * This is called at execve() time in order to dig around
+		 * in the argv/environment of the new proceess
+		 * (represented by bprm).  'current' is the process doing
+		 * the execve().
+		 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0)
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL) <=
+	    0)
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL,
+				  NULL) <= 0)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+		return NULL;
+#else /* CONFIG_MMU */
+	page = bprm->page[addr / PAGE_SIZE];
+#endif /* CONFIG_MMU */
+	return page;
+}
+
+static uint32_t brs_get_args_env(struct linux_binprm *bprm,
+				 uint32_t start_offset, uint32_t count,
+				 char *dst, size_t dst_sz, bool *newfile,
+				 char **newfile_ptr)
+{
+	uint32_t rv = 0;
+	char *kaddr;
+	struct page *page;
+	int cur = 0;
+	unsigned long i, j;
+	unsigned long pos = bprm->p + start_offset;
+	unsigned int offset = pos % PAGE_SIZE;
+	char *last_arg = NULL;
+	char *next_arg = NULL;
+
+	if (dst_sz <= 0)
+		return 0;
+
+	if (count <= 0) {
+		dst[0] = '\0';
+		return 1;
+	}
+
+#ifdef CONFIG_MMU
+	mmap_read_lock(bprm->mm);
+#endif
+	page = brs_get_page(bprm, pos);
+	if (page == NULL) {
+		pr_err("BHV: unable to find user page\n");
+		goto out;
+	}
+	kaddr = kmap_local_page(page);
+
+	for (i = 0, j = 0; i < dst_sz; i++) {
+		dst[i] = *(char *)(kaddr + (offset + j));
+
+		if (dst[i] == '\0') {
+			cur++;
+			if (cur == count) {
+				rv = i + 1;
+				break;
+			}
+
+			dst[i] = ' ';
+		}
+
+		if ((offset + j + 1) >= PAGE_SIZE) {
+			kunmap_local(kaddr);
+#ifdef CONFIG_MMU
+			put_page(page);
+#endif
+
+			page = brs_get_page(bprm, pos + i);
+			if (page == NULL) {
+				pr_err("BHV: unable to find user page\n");
+				dst[dst_sz - 1] = '\0';
+				rv = i + 1;
+				goto out;
+			}
+			kaddr = kmap_local_page(page);
+
+			offset = 0;
+			j = 0;
+		} else {
+			j++;
+		}
+	}
+	dst[dst_sz - 1] = '\0';
+
+	kunmap_local(kaddr);
+#ifdef CONFIG_MMU
+	put_page(page);
+#endif
+
+#ifdef CONFIG_MMU
+	mmap_read_unlock(bprm->mm);
+#endif
+
+	if (newfile == NULL || newfile_ptr == NULL)
+		return rv;
+
+	*newfile = false;
+	last_arg = dst;
+	while ((next_arg = strchr(last_arg, ' '))) {
+		*next_arg = '\0';
+
+		*newfile = drift_detection_is_new_file_by_filename(last_arg);
+
+		*next_arg = ' ';
+
+		if (*newfile) {
+			(*newfile_ptr) = last_arg;
+			break;
+		}
+
+		last_arg = next_arg + 1;
+	}
+
+	// Consider final argument
+	if (!(*newfile) && last_arg) {
+		*newfile = drift_detection_is_new_file_by_filename(last_arg);
+		if (*newfile) {
+			(*newfile_ptr) = last_arg;
+		}
+	}
+
+	return rv;
+
+out:
+#ifdef CONFIG_MMU
+	mmap_read_unlock(bprm->mm);
+#endif
+
+	return rv;
+}
+
+static inline void file_to_type_string(struct file *f, char *dst,
+				       size_t dst_size)
+{
+	size_t written;
+
+	if (!f) {
+		strscpy(dst, "NULL", dst_size);
+		return;
+	}
+
+	switch (f->f_path.dentry->d_inode->i_mode & S_IFMT) {
+	case S_IFIFO:
+		written = strscpy(dst, "PIPE: ", dst_size);
+		break;
+	case S_IFSOCK:
+		written = strscpy(dst, "SOCKET: ", dst_size);
+		break;
+	case S_IFBLK:
+		written = strscpy(dst, "BLOCKDEV: ", dst_size);
+		break;
+	case S_IFCHR:
+		written = strscpy(dst, "CHARDEV: ", dst_size);
+		break;
+	default:
+		written = strscpy(dst, "UNKNOWN: ", dst_size);
+		break;
+	}
+
+	if (written == -E2BIG) {
+		return;
+	}
+
+	get_file_path(&f->f_path, dst + written, dst_size - written);
+}
+
+#define CLS ProcessExec
+int brs_guestlog_log_process_exec(struct brs_policy *policy,
+				  struct linux_binprm *bprm,
+				  struct task_struct *task, const char *path)
+{
+	uint32_t env_offset;
+	struct file *f;
+	char *newfile_arg_ptr = NULL;
+
+	EVT_PROLOGUE(policy, CLS, data);
+
+	BES_STRSCPY(CLS, data->, file_path, path);
+
+	env_offset = brs_get_args_env(bprm, 0, bprm->argc,
+				      data->BES_FLD_MAX(CLS, args),
+				      &data->newfile_arg, &newfile_arg_ptr);
+	brs_get_args_env(bprm, env_offset, bprm->envc,
+			 data->BES_FLD_MAX(CLS, env), NULL, NULL);
+
+	// Add FD information
+	f = fget_task(current, 0);
+	file_to_type_string(f, data->BES_FLD_MAX(CLS, fd0));
+	if (f)
+		fput(f);
+
+	f = fget_task(current, 1);
+	file_to_type_string(f, data->BES_FLD_MAX(CLS, fd1));
+	if (f)
+		fput(f);
+
+	f = fget_task(current, 2);
+	file_to_type_string(f, data->BES_FLD_MAX(CLS, fd2));
+	if (f)
+		fput(f);
+
+	// Add whether this is a new file.
+	// We do not add the path if it is a new file as it is the same as the file_path.
+	data->newfile = drift_detection_is_new_file_by_file(bprm->file);
+
+	// Add whether a new file was passed as an argument.
+	if (data->newfile_arg) {
+		const char *space = strchr(newfile_arg_ptr, ' ');
+		size_t newfile_arg_len = space != NULL ?
+						 space - newfile_arg_ptr + 1 :
+						 strlen(newfile_arg_ptr) + 1;
+
+		strscpy(data->newfile_arg_path, newfile_arg_ptr,
+			min(newfile_arg_len, BES_MAX(CLS, newfile_arg_path)));
+		data->has_newfile_arg_path = true;
+	}
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS ProcessExit
+int brs_guestlog_log_process_exit(struct brs_policy *policy,
+				  struct task_struct *target)
+{
+	u32 exit_code = target->exit_code;
+	u64 process_signal = (exit_code & 0xff);
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->pid = target->tgid;
+	get_comm_from_task(target, data->BES_FLD_MAX(CLS, comm));
+	data->exit_signal = process_signal;
+	data->exit_code = exit_code;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS ExectuableStack
+int brs_guestlog_log_elf_load_exec_stack(struct brs_policy *policy,
+					 struct linux_binprm *bprm)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	snprintf(data->BES_FLD_MAX(CLS, file_path), "%pD4",
+		 bprm->file /* binary */);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS CgroupCreate
+int brs_guestlog_log_cgroup_create(struct brs_policy *policy,
+				   struct cgroup *cgrp)
+{
+	int r;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->cgroup_id = cgroup_id(cgrp);
+	r = cgroup_path(cgrp, data->BES_FLD_MAX(CLS, cgroup_name));
+	if (r <= 0)
+		pr_err("BRU: cgroup path fetch failed! (sz=%d)\n", r);
+	else if (r >= BES_MAX(CLS, cgroup_name))
+		pr_err("BRU: cgroup path being truncated! (sz=%d)\n", r);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS CgroupDestroy
+int brs_guestlog_log_cgroup_destroy(struct brs_policy *policy,
+				    struct cgroup *cgrp)
+{
+	int r;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->cgroup_id = cgroup_id(cgrp);
+	cgroup_path(cgrp, data->BES_FLD_MAX(CLS, cgroup_name));
+	if (r <= 0)
+		pr_err("BRU: cgroup path fetch failed! (sz=%d)\n", r);
+	if (r >= BES_MAX(CLS, cgroup_name))
+		pr_err("BRU: cgroup path being truncated! (sz=%d)\n", r);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS NamespaceChange
+int brs_guestlog_log_namespace_change(struct brs_policy *policy,
+				      struct task_struct *tsk,
+				      struct nsset *nsset)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->pid = tsk->pid;
+	get_comm_from_task(tsk, data->BES_FLD_MAX(CLS, comm));
+	_brs_get_incoming_ns_inums(tsk, nsset, &data->namespace);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS Capable
+int brs_guestlog_log_capable(struct brs_policy *policy, int cap)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->cap_num = (u64)cap;
+	strscpy(data->cap_name, cap2str(cap), BES_MAX(CLS, cap_name));
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS DriverLoad
+int brs_guestlog_log_driver_load(struct brs_policy *policy, const char *name)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	BES_STRSCPY(CLS, data->, file_path, name);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS KernelAccessViolation
+int brs_guestlog_log_kaccess(struct brs_policy *policy, uint64_t addr,
+			     uint8_t type)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->gva = addr;
+	data->type = (u64)type;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS UnsupportedFileOperation
+int brs_guestlog_log_fops_unknown(struct brs_policy *policy, uint32_t magic,
+				  const char *pathname, uint8_t type,
+				  uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->magic = (u64)magic;
+	data->filesystem = (u64)type;
+	data->major = (u64)major;
+	data->minor = (u64)minor;
+	data->fops_gva = (u64)fops_ptr;
+	BES_STRSCPY(CLS, data->, file_path, pathname);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+static void brs_guestlog_char_array_into_buf(char *dst, size_t dst_size,
+					     char **arr)
+{
+	char *cur;
+	unsigned int i;
+	ssize_t cur_len = 0;
+	ssize_t total_len = 0;
+
+	BUG_ON(dst == NULL);
+
+	if (arr == NULL) {
+		if (dst_size > 0)
+			dst[0] = '\0';
+
+		return;
+	}
+
+	for (i = 0, cur = arr[0]; cur != NULL; i++, cur = arr[i]) {
+		if (i != 0) {
+			// Replace NULL of previous round with a space
+			dst[total_len] = ' ';
+			// Increase len since strscpy returns len without NULL
+			total_len++;
+		}
+
+		cur_len = strscpy(&dst[total_len], cur, dst_size);
+
+		// No more room. We are done
+		if (cur_len == -E2BIG)
+			return;
+
+		// Update size
+		dst_size -= cur_len;
+		total_len += cur_len;
+
+		// No more room. We are done. The last character is '\0'
+		if (dst_size <= 1)
+			return;
+	}
+}
+
+#define CLS KernelExec
+int brs_guestlog_log_kernel_exec(struct brs_policy *policy, const char *path,
+				 char **argv, char **envp)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	BES_STRSCPY(CLS, data->, file_path, path);
+	brs_guestlog_char_array_into_buf(data->BES_FLD_MAX(CLS, args), argv);
+	brs_guestlog_char_array_into_buf(data->BES_FLD_MAX(CLS, env), envp);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+static inline void _brs_guestlog_add_ip_info_sockaddr(char *dest,
+						      size_t dest_sz,
+						      struct sockaddr *addr)
+{
+	snprintf(dest, dest_sz, "%pISpc", addr);
+}
+
+static inline void _brs_guestlog_add_ip_info_socket(char *dest, size_t dest_sz,
+						    struct socket *sock,
+						    bool dest_addr)
+{
+	if (sock->sk->sk_family == AF_INET || sock->sk->sk_family == AF_INET6) {
+		struct sockaddr addr;
+		inet_getname(sock, &addr, dest_addr);
+		_brs_guestlog_add_ip_info_sockaddr(dest, dest_sz, &addr);
+	} else {
+		strscpy(dest, "UNKNOWN", dest_sz);
+		dest[dest_sz - 1] = '\0';
+	}
+}
+
+static inline void _brs_guestlog_add_ip_info_file(char *dest, size_t dest_sz,
+						  struct file *file,
+						  bool dest_addr)
+{
+	struct socket *sock;
+
+	if (file && (sock = sock_from_file(file))) {
+		_brs_guestlog_add_ip_info_socket(dest, dest_sz, sock,
+						 dest_addr);
+	} else {
+		strscpy(dest, "UNKNOWN", dest_sz);
+		dest[dest_sz - 1] = '\0';
+	}
+}
+
+#define CLS InterpreterBound
+int brs_guestlog_log_reverse_shell_detection_interpreter_bound(
+	struct brs_policy *policy, pid_t interpreter_pid,
+	const char *interpreter_path, uint32_t socket_fd, struct file *socket)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->interpreter.pid = (u64)interpreter_pid;
+
+	BES_STRSCPY(IBE_InterpreterAttributes, data->interpreter., file_path,
+		    interpreter_path);
+
+	data->interpreter.socket_fd = (u64)socket_fd;
+
+	_brs_guestlog_add_ip_info_file(data->BES_FLD_MAX(CLS, remote_addr),
+				       socket, true);
+
+	_brs_guestlog_add_ip_info_file(data->BES_FLD_MAX(CLS, source_addr),
+				       socket, false);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS InterpreterTransitive
+int brs_guestlog_log_reverse_shell_detection_interpreter_transitive(
+	struct brs_policy *policy, struct task_struct *interpreter,
+	uint32_t interpreter_fd, const char *interpreter_path,
+	struct task_struct *transitive, uint32_t transitive_pipe_fd,
+	uint32_t transitive_socket_fd, struct sockaddr *dest_address)
+{
+	struct file *file;
+
+	EVT_PROLOGUE(policy, CLS, data);
+
+	// interpreter
+	data->interpreter.pid = (u64)interpreter->tgid;
+
+	if (interpreter_path != NULL)
+		BES_STRSCPY(ITE_InterpreterAttributes, data->interpreter.,
+			    file_path, interpreter_path);
+	else
+		get_path_from_task(
+			interpreter,
+			data->interpreter.BES_FLD_MAX(ITE_InterpreterAttributes,
+						      file_path),
+			true);
+
+	data->interpreter.pipe_fd = (u64)interpreter_fd;
+
+	// piper
+	data->piper.pid = (u64)transitive->tgid;
+	get_path_from_task(
+		transitive,
+		data->piper.BES_FLD_MAX(ITE_PiperAttributes, file_path), true);
+
+	data->piper.pipe_fd = (u64)transitive_pipe_fd;
+	data->piper.socket_fd = (u64)transitive_socket_fd;
+
+	file = fget_task(transitive, transitive_socket_fd);
+
+	_brs_guestlog_add_ip_info_file(data->BES_FLD_MAX(CLS, source_addr),
+				       file, false);
+
+	if (dest_address) {
+		_brs_guestlog_add_ip_info_sockaddr(
+			data->BES_FLD_MAX(CLS, remote_addr), dest_address);
+	} else {
+		_brs_guestlog_add_ip_info_file(
+			data->BES_FLD_MAX(CLS, remote_addr), file, true);
+	}
+	fput(file);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS SocketConnection
+int brs_guestlog_log_net_socket_connection(struct brs_policy *policy,
+					   bool local, struct sockaddr *address)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->local = local;
+	data->domain = false;
+	data->protected = false;
+	_brs_guestlog_add_ip_info_sockaddr(data->BES_FLD_MAX(CLS, addr),
+					   address);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+static struct sock *_brs_get_unix_sock(struct sockaddr_un *sunaddr,
+				       int addr_len)
+{
+	struct inode *inode;
+	struct path path;
+	struct sock *sk;
+	int err;
+
+	err = vfs_path_lookup(init_task.fs->root.dentry, init_task.fs->root.mnt,
+			      sunaddr->sun_path,
+			      LOOKUP_FOLLOW | LOOKUP_AUTOMOUNT, &path);
+	if (err)
+		return ERR_PTR(err);
+
+	inode = d_backing_inode(path.dentry);
+	if (!S_ISSOCK(inode->i_mode)) {
+		path_put(&path);
+		return ERR_PTR(-ECONNREFUSED);
+	}
+
+	sk = unix_find_socket_byinode(inode);
+	if (!sk) {
+		path_put(&path);
+		return ERR_PTR(-ECONNREFUSED);
+	}
+
+	path_put(&path);
+	return sk;
+}
+
+static bool _brs_guestlog_runtime_sock_filter(const char *path, void *arg)
+{
+	struct sockaddr_un cur_addr;
+	struct sock *target, *cur_sock;
+
+	target = arg;
+
+	cur_addr.sun_family = AF_UNIX;
+	strscpy(cur_addr.sun_path, path, UNIX_PATH_MAX);
+	cur_addr.sun_path[UNIX_PATH_MAX - 1] = '\0';
+	cur_sock = _brs_get_unix_sock(&cur_addr, sizeof(cur_addr));
+	if (!IS_ERR(cur_sock)) {
+		sock_put(cur_sock);
+		if (cur_sock == target) {
+			return true;
+		}
+	}
+	return false;
+}
+
+static bool _brs_guestlog_is_runtime_sock(struct brs_policy *policy,
+					  struct socket *sock,
+					  struct sockaddr_un *address,
+					  int addrlen)
+{
+	struct sock *target;
+	bool found;
+
+	if (address->sun_family != AF_UNIX ||
+	    brs_policy_guestlog_protected_unix_sockets_is_empty(policy))
+		return false;
+
+	target = unix_find_other(sock_net(sock->sk), address, addrlen,
+				 sock->type);
+	if (IS_ERR(target))
+		return false;
+
+	found = brs_policy_guestlog_match_each_protected_unix_sockets(
+		policy, _brs_guestlog_runtime_sock_filter, target);
+	sock_put(target);
+	return found;
+}
+
+#define CLS SocketConnection
+int brs_guestlog_log_unix_socket_connection(struct brs_policy *policy,
+					    struct socket *sock,
+					    struct sockaddr_un *address,
+					    int addrlen)
+{
+	size_t size = 0;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	if (addrlen < sizeof(sa_family_t))
+		EVT_ABORT(CLS, data, -EPERM);
+
+	size = min((unsigned int)addrlen - sizeof(sa_family_t) + 1,
+		   BES_MAX(CLS, addr));
+
+	data->local = false;
+	data->domain = true;
+	data->protected =
+		_brs_guestlog_is_runtime_sock(policy, sock, address, addrlen);
+
+	strscpy(data->addr, address->sun_path, size);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS SocketAccept
+int brs_guestlog_log_socket_accept(struct brs_policy *policy,
+				   struct socket *sock)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	_brs_guestlog_add_ip_info_socket(data->BES_FLD_MAX(CLS, source_addr),
+					 sock, false);
+	_brs_guestlog_add_ip_info_socket(data->BES_FLD_MAX(CLS, remote_addr),
+					 sock, true);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS FileOpen
+int brs_guestlog_log_file_open(struct brs_policy *policy, struct file *file)
+{
+	uint16_t skip;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	skip = get_file_path_noalloc(&file->f_path,
+				     data->BES_FLD_MAX(CLS, file_path));
+	if (skip < 0)
+		EVT_ABORT(CLS, data, skip);
+	data->skipbytes_file_path = skip;
+
+	data->is_link = S_ISLNK(file->f_inode->i_mode);
+	data->is_dir = S_ISDIR(file->f_inode->i_mode);
+	data->is_fifo = S_ISFIFO(file->f_inode->i_mode);
+	data->is_socket = S_ISSOCK(file->f_inode->i_mode);
+	data->is_writeable = (bool)(file->f_mode & FMODE_WRITE);
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS FileDrift
+int brs_guestlog_log_file_drift(struct brs_policy *policy, struct file *file,
+				bool writebable)
+{
+	uint16_t skip;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	skip = get_file_path_noalloc(&file->f_path,
+				     data->BES_FLD_MAX(CLS, file_path));
+	if (skip < 0)
+		EVT_ABORT(CLS, data, skip);
+	data->skipbytes_file_path = skip;
+
+	data->is_link = S_ISLNK(file->f_inode->i_mode);
+	data->is_dir = S_ISDIR(file->f_inode->i_mode);
+	data->is_fifo = S_ISFIFO(file->f_inode->i_mode);
+	data->is_socket = S_ISSOCK(file->f_inode->i_mode);
+	data->is_writeable = writebable;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS FileSetXattr
+int brs_guestlog_log_file_set_xattr(struct brs_policy *policy,
+				    struct dentry *dentry, const char *name,
+				    const void *value, size_t value_sz)
+{
+	uint16_t skip;
+	size_t max_sz;
+	int len;
+	EVT_PROLOGUE(policy, CLS, data);
+
+	skip = get_dentry_path_noalloc(dentry,
+				       data->BES_FLD_MAX(CLS, file_path));
+	if (skip < 0)
+		EVT_ABORT(CLS, data, skip);
+	data->skipbytes_file_path = skip;
+
+	BES_STRSCPY(CLS, data->, name, name);
+
+	// We encode the value as base64 as it may be binary data
+	// First calculate the maximal number of characters that we can encode
+	// given the size of our buffer.
+	max_sz = round_down(BES_MAX(CLS, value), 4) * 3 / 4;
+
+	// encode the string.
+	len = base64_encode(value, min(value_sz, max_sz), data->value);
+
+	// zero terminate
+	if (len < 0 || len >= BES_MAX(CLS, value))
+		len = BES_MAX(CLS, value) - 1;
+
+	data->value[len] = '\0';
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS StrongIsolation
+int brs_guestlog_log_strong_isolation_report(
+	struct brs_policy *policy, uint64_t domain_src, uint64_t domain_target,
+	uint64_t gva_start, uint64_t gva_end, bool write, bool blocked)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->attacker_domain = domain_src;
+	data->target_domain = domain_target;
+	data->gva_start = gva_start;
+	data->gva_end = gva_end;
+	data->is_write_access = write;
+	data->blocked = blocked;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS ForcedMemAccess
+int brs_guestlog_log_forced_mem_access(struct brs_policy *policy,
+				       uint64_t gva_start, uint64_t gva_end,
+				       bool write)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	data->gva_start = gva_start;
+	data->gva_end = gva_end;
+	data->is_write_access = write;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+// https://elixir.bootlin.com/linux/v6.1.131/source/mm/memfd.c#L260
+#define MFD_PREFIX "memfd:"
+#define MFD_PREFIX_LEN (sizeof(MFD_PREFIX) - 1)
+
+static inline bool is_mem_fd(struct file *file)
+{
+	if (strlen(file->f_path.dentry->d_name.name) >= MFD_PREFIX_LEN &&
+	    !memcmp(file->f_path.dentry->d_name.name, MFD_PREFIX,
+		    MFD_PREFIX_LEN)) {
+		return true;
+	}
+
+	return false;
+}
+
+#define CLS MmapExecFile
+int brs_guestlog_log_mmap_exec_file(struct brs_policy *policy,
+				    struct file *file, unsigned long prot,
+				    unsigned long flags)
+{
+	uint16_t skip;
+
+	/* Do we have a memfd (fileless) mapping */
+	bool mem_fd = is_mem_fd(file);
+	/* For file-backed mappings we are not interested in the
+	 * main executable itself. */
+	if (file == current->mm->exe_file && !mem_fd)
+		return 0;
+
+	EVT_PROLOGUE(policy, CLS, data);
+
+	skip = get_file_path_noalloc(&file->f_path,
+				     data->BES_FLD_MAX(CLS, file_path));
+	if (skip < 0)
+		EVT_ABORT(CLS, data, skip);
+	data->skipbytes_file_path = skip;
+
+	data->is_link = S_ISLNK(file_inode(file)->i_mode);
+
+	// Add whether this is a new file.
+	data->is_newfile = drift_detection_is_new_file_by_file(file);
+
+	// Add whether this is a file-less memory-backed mapping
+	data->is_memfd = mem_fd;
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS LibTrace
+int brs_guestlog_log_lib_trace(struct brs_policy *policy, const char *lib_name,
+			       const char *func_name)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	/* XXX: Change file_path to be of the PathStr in the model. */
+	strscpy(data->file_path, lib_name, BES_MAX(CLS, file_path));
+	strscpy(data->func_name, func_name, BES_MAX(CLS, func_name));
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
+
+#define CLS SkipSymHook
+int brs_guestlog_log_skip_sym_hook(struct brs_policy *policy,
+				   const char *lib_name,
+				   const char *func_name)
+{
+	EVT_PROLOGUE(policy, CLS, data);
+
+	/* XXX: Change file_path to be of the PathStr in the model. */
+	strscpy(data->file_path, lib_name, BES_MAX(CLS, file_path));
+	strscpy(data->func_name, func_name, BES_MAX(CLS, func_name));
+
+	EVT_EPILOGUE(CLS, data);
+}
+#undef CLS
diff --git security/bhv/init/init.c security/bhv/init/init.c
new file mode 100644
index 0000000000..d52d3cb4ee
--- /dev/null
+++ security/bhv/init/init.c
@@ -0,0 +1,541 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include "bhv/bhv.h"
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/syscall.h>
+#include <linux/jump_label.h>
+#include <linux/kmod.h>
+#include <linux/mm.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+
+#include <bhv/bhv_print.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestcmd.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/creds.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/creds.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_comm_start[];
+extern char __bhv_vault_comm_end[];
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_text_jump_label_start[];
+extern char __bhv_vault_text_jump_label_end[];
+
+extern char __start_static_call_sites[];
+extern char __stop_static_call_sites[];
+extern char __start_static_call_tramp_key[];
+extern char __stop_static_call_tramp_key[];
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0)
+extern char __alt_instructions[];
+extern char __alt_instructions_end[];
+#endif // LINUX_VERSION_CODE < 6.8
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+extern char __start_bhv_tp_vault[];
+extern char __end_bhv_tp_vault[];
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+bool __bhv_init_done __ro_after_init = false;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define BHV_VAULT_PRINT_DEBUG_INFO(v, vault_name)                                  \
+	{                                                                          \
+		int i = 0;                                                         \
+                                                                                   \
+		pr_info("Registering vault '%s': ep=%u rp=%u:\n", #vault_name,     \
+			v->nr_entry_points, v->nr_return_points);                  \
+                                                                                   \
+		for (i = 0; i < v->nr_entry_points; ++i) {                         \
+			pr_info("\t Entry point @ 0x%llx\n",                       \
+				(uint64_t)(&v->transit_points)[i]);                \
+		}                                                                  \
+                                                                                   \
+		pr_info("\t CODE Region     @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+			v->code.gpa, v->code.gpa + v->code.size,                   \
+			__bhv_vault_text_##vault_name##_start,                     \
+			__bhv_vault_text_##vault_name##_end);                      \
+		pr_info("\t REF_CODE Region @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+			v->ref_code.gpa, v->ref_code.gpa + v->ref_code.size,       \
+			__bhv_vault_ref_text_##vault_name##_start,                 \
+			__bhv_vault_ref_text_##vault_name##_end);                  \
+		pr_info("\t DATA Region     @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+			v->data.gpa, v->data.gpa + v->data.size,                   \
+			__bhv_vault_data_##vault_name##_start,                     \
+			__bhv_vault_data_##vault_name##_end);                      \
+		pr_info("\t RODATA Region   @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+			v->ro_data.gpa, v->ro_data.gpa + v->ro_data.size,          \
+			__bhv_vault_ro_data_##vault_name##_start,                  \
+			__bhv_vault_ro_data_##vault_name##_end);                   \
+	}
+#else /* !CONFIG_BHV_VAS_DEBUG */
+#define BHV_VAULT_PRINT_DEBUG_INFO(v, vault_name)
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 9, 0)
+#ifdef CONFIG_MITIGATION_RETPOLINE
+#define RETPOLINE
+#endif // CONFIG_MITIGATION_RETPOLINE
+#else // LINUX_VERSION_CODE < 6.9
+#ifdef CONFIG_RETPOLINE
+#define RETPOLINE
+#endif // CONFIG_RETPOLINE
+#endif // LINUX_VERSION_CODE <> 6.9
+
+#ifdef RETPOLINE
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)                              \
+	{                                                                      \
+		extern char __indirect_thunk_start[];                          \
+		extern char __indirect_thunk_end[];                            \
+                                                                               \
+		vault->thunks.gpa =                                            \
+			bhv_virt_to_phys_single(__indirect_thunk_start);       \
+		vault->thunks.size =                                           \
+			__indirect_thunk_end - __indirect_thunk_start;         \
+	}
+#else /* !RETPOLINE */
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)                              \
+	{                                                                      \
+		vault->thunks.gpa = 0;                                         \
+		vault->thunks.size = 0;                                        \
+	}
+#endif /* RETPOLINE */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_ALTINSTR_AUX(vault)                                 \
+	{                                                                      \
+		extern char __altinstr_aux_start[];                            \
+		extern char __altinstr_aux_end[];                              \
+                                                                               \
+		vault->altinstr_aux.gpa =                                      \
+			bhv_virt_to_phys_single(__altinstr_aux_start);         \
+		vault->altinstr_aux.size =                                     \
+			__altinstr_aux_end - __altinstr_aux_start;             \
+	}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_ALTINSTR_AUX(vault)                                 \
+	{                                                                      \
+		vault->altinstr_aux.gpa = 0;                                   \
+		vault->altinstr_aux.size = 0;                                  \
+	}
+#endif /* CONFIG_X86_64 */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_SHARED_CODE(vault, vault_name)                      \
+	{                                                                      \
+		extern char __bhv_vault_shared_text_##vault_name##_start[];    \
+		extern char __bhv_vault_shared_text_##vault_name##_end[];      \
+                                                                               \
+		vault->shared_code.gpa = bhv_virt_to_phys_single(              \
+			__bhv_vault_shared_text_##vault_name##_start);         \
+		vault->shared_code.size =                                      \
+			__bhv_vault_shared_text_##vault_name##_end -           \
+			__bhv_vault_shared_text_##vault_name##_start;          \
+	}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_SHARED_CODE(vault, vault_name)                      \
+	{                                                                      \
+		vault->shared_code.gpa = 0;                                    \
+		vault->shared_code.size = 0;                                   \
+	}
+#endif /* CONFIG_X86_64 */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_NOINSTR_CODE(vault)                                 \
+	{                                                                      \
+		extern char __noinstr_text_start[];                            \
+		extern char __noinstr_text_end[];                              \
+                                                                               \
+		vault->noinstr_text.gpa =                                      \
+			bhv_virt_to_phys_single(__noinstr_text_start);         \
+		vault->noinstr_text.size =                                     \
+			__noinstr_text_end - __noinstr_text_start;             \
+	}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_NOINSTR_CODE(vault)                                 \
+	{                                                                      \
+		vault->noinstr_text.gpa = 0;                                   \
+		vault->noinstr_text.size = 0;                                  \
+	}
+#endif /* CONFIG_X86_64 */
+
+#define BHV_VAULT_REGISTER(vault, buf, buf_size)                               \
+	{                                                                      \
+		extern char __bhv_vault_text_##vault##_start[];                \
+		extern char __bhv_vault_text_##vault##_end[];                  \
+		extern char __bhv_vault_ref_text_##vault##_start[];            \
+		extern char __bhv_vault_ref_text_##vault##_end[];              \
+		extern char __bhv_vault_data_##vault##_start[];                \
+		extern char __bhv_vault_data_##vault##_end[];                  \
+		extern char __bhv_vault_ro_data_##vault##_start[];             \
+		extern char __bhv_vault_ro_data_##vault##_end[];               \
+		uint32_t ep_ctr = 0;                                           \
+		uint32_t rp_ctr = 0;                                           \
+		uint32_t __cur_ctr = 0;                                        \
+		int __r = 0;                                                   \
+                                                                               \
+		HypABI__Wagner__Create__arg__T *_vault =                       \
+			(HypABI__Wagner__Create__arg__T *)buf;                 \
+                                                                               \
+		bhv_vault_return_point_helper_t *ret;                          \
+		bhv_vault_entry_point_helper_t *entry;                         \
+		BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                   \
+		{                                                              \
+			ep_ctr++;                                              \
+		}                                                              \
+                                                                               \
+		BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +               \
+			sizeof(uint64_t) * ep_ctr) > buf_size);                \
+                                                                               \
+		_vault->nr_entry_points = ep_ctr;                              \
+                                                                               \
+		BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                   \
+		{                                                              \
+			uint64_t *tps = (uint64_t *)&_vault->transit_points +  \
+					sizeof(uint64_t);                      \
+			tps[__cur_ctr] = bhv_virt_to_phys_single(entry->ep);   \
+			__cur_ctr++;                                           \
+		}                                                              \
+                                                                               \
+		BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                    \
+		{                                                              \
+			rp_ctr++;                                              \
+		}                                                              \
+                                                                               \
+		BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                   \
+		{                                                              \
+			rp_ctr++;                                              \
+		}                                                              \
+                                                                               \
+		BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +               \
+			(sizeof(uint64_t) * ep_ctr) +                          \
+			(sizeof(uint64_t) * rp_ctr)) > buf_size);              \
+                                                                               \
+		__cur_ctr = _vault->nr_entry_points;                           \
+		_vault->nr_return_points = rp_ctr;                             \
+                                                                               \
+		BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                    \
+		{                                                              \
+			uint64_t *tps = (uint64_t *)&_vault->transit_points +  \
+					sizeof(uint64_t);                      \
+			tps[__cur_ctr] = bhv_virt_to_phys_single(ret->rp);     \
+			__cur_ctr++;                                           \
+		}                                                              \
+                                                                               \
+		BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                   \
+		{                                                              \
+			uint64_t *tps = (uint64_t *)&_vault->transit_points +  \
+					sizeof(uint64_t);                      \
+			tps[__cur_ctr] = bhv_virt_to_phys_single(ret->rp);     \
+			__cur_ctr++;                                           \
+		}                                                              \
+		_vault->transit_points = bhv_virt_to_phys_single(              \
+			(uint64_t *)&_vault->transit_points +                  \
+			sizeof(uint64_t));                                     \
+                                                                               \
+		_vault->code.gpa = bhv_virt_to_phys_single(                    \
+			__bhv_vault_text_##vault##_start);                     \
+		_vault->code.size = __bhv_vault_text_##vault##_end -           \
+				    __bhv_vault_text_##vault##_start;          \
+		_vault->ref_code.gpa = bhv_virt_to_phys_single(                \
+			__bhv_vault_ref_text_##vault##_start);                 \
+		_vault->ref_code.size = __bhv_vault_ref_text_##vault##_end -   \
+					__bhv_vault_ref_text_##vault##_start;  \
+		_vault->data.gpa = bhv_virt_to_phys_single(                    \
+			__bhv_vault_data_##vault##_start);                     \
+		_vault->data.size = __bhv_vault_data_##vault##_end -           \
+				    __bhv_vault_data_##vault##_start;          \
+		_vault->ro_data.gpa = bhv_virt_to_phys_single(                 \
+			__bhv_vault_ro_data_##vault##_start);                  \
+		_vault->ro_data.size = __bhv_vault_ro_data_##vault##_end -     \
+				       __bhv_vault_ro_data_##vault##_start;    \
+		_vault->entry_text.start = (uint64_t)__entry_text_start;       \
+		_vault->entry_text.end = (uint64_t)__entry_text_end;           \
+                                                                               \
+		BHV_VAULT_REGISTER_ALTINSTR_AUX(_vault)                        \
+		BHV_VAULT_REGISTER_INDIRECT_THUNKS(_vault)                     \
+		BHV_VAULT_REGISTER_SHARED_CODE(_vault, vault)                  \
+		BHV_VAULT_REGISTER_NOINSTR_CODE(_vault)                        \
+                                                                               \
+		BHV_VAULT_PRINT_DEBUG_INFO(_vault, vault)                      \
+                                                                               \
+		__r = HypABI__Wagner__Create__hypercall_noalloc(_vault);       \
+	}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+static inline void
+bhv_section_run_ctor(HypABI__Init__Init__BHVSectionRun__T *curr_item,
+		     HypABI__Init__Init__BHVSectionRun__T *prev_item,
+		     uint64_t gpa_start, uint64_t size, uint8_t type)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (HypABI__Init__Init__BHVSectionRun__T){
+		.gpa_start = gpa_start,
+		.size = size,
+		.type = type,
+		.next = BHV_INVALID_PHYS_ADDR,
+	};
+
+	if (prev_item)
+		prev_item->next = bhv_virt_to_phys_single(curr_item);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static int __init bhv_vault_extend(void)
+{
+	int r = 0;
+	HypABI__Wagner__Extend__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return 0;
+
+	/*
+	 * Note: jump lables ((__start|__stop)___jump_table) and static calls
+	 * ((__start|__stop)_static_call_sites) are part of the __ro_after_init
+	 * section. As such, they will be protected by BRASS integrity and do
+	 * not need to be explicitly propagated to the spaces-based BRASS vault.
+	 */
+
+	/* Alternative instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end -
+			 (unsigned long)__alt_instructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+#if defined CONFIG_PARAVIRT && LINUX_VERSION_CODE < KERNEL_VERSION(6, 8, 0)
+	/* Paravirt instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__parainstructions);
+	vault.mem.size = (unsigned long)__parainstructions_end -
+			 (unsigned long)__parainstructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif // CONFIG_PARAVIRT && LINUX_VERSION_CODE < 6.8
+
+#ifdef RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end -
+			 (unsigned long)__retpoline_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end -
+			 (unsigned long)__return_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif //RETPOLINE
+
+#ifdef CONFIG_TRACEPOINTS
+	/* Tracepoints */
+	vault.mem.gpa = bhv_virt_to_phys_single(__start_bhv_tp_vault);
+	vault.mem.size = (unsigned long)__end_bhv_tp_vault -
+			 (unsigned long)__start_bhv_tp_vault;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+	return 0;
+}
+
+#endif
+
+static int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size)
+{
+	unsigned long r;
+	unsigned int region_counter = 0;
+
+	struct {
+		HypABI__Init__Init__arg__T init_arg;
+		bhv_mem_region_t mem_regions[BHV_INIT_MAX_REGIONS];
+		HypABI__Init__Init__BHVSectionRun__T bhv_section_runs[];
+	} *arg = bhv_data;
+	BUG_ON((void *)&arg->bhv_section_runs[2] - bhv_data > +bhv_data_size);
+
+#define BI_ALIGN_START_SIZE(start, end)                                        \
+	(bhv_virt_to_phys_single((void *)(unsigned long)(start))),             \
+		(round_up((unsigned long)(end), PAGE_SIZE) -                   \
+		 (unsigned long)(start))
+#define BI_LL_FIRST(ll) &(ll)[region_counter], NULL
+#define BI_LL_NEXT(ll) &(ll)[region_counter], &(ll)[region_counter - 1]
+
+	bhv_mem_region_create_ctor(BI_LL_FIRST(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_stext, _etext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sinittext, _einittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL INIT TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sexittext, _eexittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL EXIT TEXT SECTION");
+	region_counter++;
+
+	bhv_init_hyp_arch(arg->mem_regions, &region_counter);
+
+	region_counter = 0;
+	BUG_ON((unsigned long)bhv_data & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_FIRST(arg->bhv_section_runs),
+		bhv_virt_to_phys(bhv_data, bhv_data_size), bhv_data_size,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA);
+	region_counter++;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		BI_ALIGN_START_SIZE(__bhv_text_start, __bhv_text_end),
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+	region_counter++;
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#ifdef BHV_CONST_MODPROBE_PATH
+	arg->init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	arg->init_arg.modprobe_path =
+		bhv_virt_to_phys((void *)&modprobe_path, KMOD_PATH_LEN);
+#else
+	arg->init_arg.modprobe_path_sz = 0;
+	arg->init_arg.modprobe_path = BHV_INVALID_PHYS_ADDR;
+#endif /* BHV_CONST_MODPROBE_PATH */
+
+	arg->init_arg.owner = 0;
+	arg->init_arg.region_head = BHV_VIRT_TO_PHYS_SIZEOF(&arg->mem_regions);
+	arg->init_arg.bhv_region_head =
+		bhv_virt_to_phys_single(&arg->bhv_section_runs);
+
+	r = HypABI__Init__Init__hypercall_noalloc(&arg->init_arg);
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+
+void __init bhv_init_platform(void)
+{
+	int rv;
+	uint32_t cid, port;
+	void *bhv_data_ptr = NULL;
+	HypABI__Init__Init__BHVData__T *data_ptr = NULL;
+	static_assert(sizeof(uint64_t) ==
+		      sizeof(unsigned long)); //for pointer cast below
+
+    if (!bhv_init_arch()) {
+        return;
+    }
+
+	rv = bhv_init_hyp(__bhv_data_start, __bhv_data_end - __bhv_data_start);
+	bhv_data_ptr = __bhv_data_start;
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return;
+	}
+
+	bhv_initialized = true;
+	data_ptr = bhv_data_ptr;
+	cid = data_ptr->vsocket_cid;
+	port = data_ptr->vsocket_port;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_vault_is_enabled()) {
+		rv = (BHV_VAULT_REGISTER(jump_label, __bhv_vault_comm_start,
+					 __bhv_vault_comm_end -
+						 __bhv_vault_comm_start));
+		if (rv) {
+			bhv_fail("BHV: Cannot create spaces-based Vault");
+			return;
+		}
+
+		rv = bhv_vault_extend();
+		if (rv) {
+			bhv_fail("BHV: Cannot extend the spaces-based Vault");
+			return;
+		}
+	}
+#endif
+
+	rv = brs_init_guestconn(cid, port);
+	if (rv) {
+		bhv_fail(
+			"BHV: Cannot configure the BHV guest connection subsystem");
+		return;
+	}
+
+	bhv_init_guestcmd();
+
+	bhv_init_fileops();
+
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	/*
+	 * Initialize static keys.
+	 *
+	 * XXX: Consider moving this to another place.
+	 */
+	bhv_init_jump_label();
+	bhv_init_alternatives();
+	bhv_init_static_call();
+#endif
+
+	__bhv_init_done = true;
+}
diff --git security/bhv/init/late_start.c security/bhv/init/late_start.c
new file mode 100644
index 0000000000..8b909500c3
--- /dev/null
+++ security/bhv/init/late_start.c
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+#include <bhv/init/late_start.h>
+
+void bhv_late_start(void)
+{
+	int r;
+
+	if (!is_bhv_initialized())
+		return;
+
+	r = bhv_late_start_init_ptpg();
+	if (r) {
+		bhv_fail("ptpg init failed");
+	}
+	r = bhv_late_start_arch();
+	if (r) {
+		bhv_fail("bhv_late_start_arch failed");
+	}
+}
diff --git security/bhv/init/mm_init.c security/bhv/init/mm_init.c
new file mode 100644
index 0000000000..32dbe0c0ba
--- /dev/null
+++ security/bhv/init/mm_init.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/file_protection.h>
+#include <bhv/config.h>
+
+#include <bhv/acl.h>
+
+#include <bhv/init/mm_init.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+void __init bhv_mm_init(void)
+{
+	HypABI__init_slabs();
+
+	brs_mm_init_config();
+	bhv_mm_init_integrity();
+	bhv_mm_init_acl();
+	int rv = bhv_init_cred();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV creds subsystem");
+		return;
+	}
+	bhv_mm_init_cred();
+	bhv_mm_init_file_protection();
+}
diff --git security/bhv/init/start.c security/bhv/init/start.c
new file mode 100644
index 0000000000..4315dd9a74
--- /dev/null
+++ security/bhv/init/start.c
@@ -0,0 +1,206 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <linux/types.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/guestconn.h>
+
+#include <bhv/init/start.h>
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+#include <linux/kprobes.h>
+#endif
+
+#ifdef CONFIG_SECURITY_SELINUX
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+static int disable_patching_vault_regions(void)
+{
+	extern char __bhv_vault_text_jump_label_start[];
+	extern char __bhv_vault_text_jump_label_end[];
+	extern char __bhv_vault_ref_text_jump_label_start[];
+	extern char __bhv_vault_ref_text_jump_label_end[];
+
+	int rc = 0;
+
+	/*
+	 * Add the vault .text and .ref.text segments into the kprobes blacklist
+	 * to prohibit setting unauthorized kprobes inside of the vault.
+	 * This function must be called after initializing kmem_caches.
+	 *
+	 * Note: we do not prohibit patching the vault's shared code segment.
+	 */
+
+	rc = kprobe_add_area_blacklist((unsigned long)__bhv_vault_text_jump_label_start,
+				       (unsigned long)__bhv_vault_text_jump_label_end);
+	if (rc)
+		return rc;
+
+	rc = kprobe_add_area_blacklist((unsigned long)__bhv_vault_ref_text_jump_label_start,
+				       (unsigned long)__bhv_vault_ref_text_jump_label_end);
+
+	return rc;
+}
+#endif
+
+static inline void do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	HypABI__Init__Start__arg__T *config =
+		(HypABI__Init__Start__arg__T *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = HypABI__Init__Start__hypercall_noalloc(
+		config, num_pages * PAGE_SIZE - HypABI__Init__Start__arg__SZ);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (HypABI__Init__Start__arg__T *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = HypABI__Init__Start__hypercall_noalloc(
+			config,
+			num_pages * PAGE_SIZE - HypABI__Init__Start__arg__SZ);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#ifdef CONFIG_SECURITY_SELINUX
+		if ((sizeof(HypABI__Init__Start__arg__T) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#endif // CONFIG_SECURITY_SELINUX
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+bool bhv_start(void)
+{
+	int rc;
+	bhv_mem_region_node_t *n[2];
+
+	if (!is_bhv_initialized())
+		return false;
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_ptpg();
+
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return false;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region.remove));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_start_delete_alternatives();
+	}
+#elif defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+	rc = disable_patching_vault_regions();
+	if (rc)
+		pr_err("BHV: Cannot disable patching the vault's code segments.\n");
+#endif
+
+	do_start();
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_sysfs();
+	}
+
+	if (bhv_reg_protect_is_enabled()) {
+		bhv_start_reg_protect();
+	}
+
+	return true;
+}
diff --git security/bhv/inode.c security/bhv/inode.c
new file mode 100644
index 0000000000..17c68b889d
--- /dev/null
+++ security/bhv/inode.c
@@ -0,0 +1,393 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fs.h>
+#include <linux/siphash.h>
+#include <linux/uidgid.h>
+#include <linux/version.h>
+
+#include <bhv/creds.h>
+#include <bhv/context.h>
+#include <bhv/inode.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_INODE_DEBUG 0
+
+static bool bhv_inode_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_inode_is_active(void)
+{
+	if (!bhv_inode_initialized)
+		return false;
+
+	return bhv_inode_is_enabled();
+}
+
+static size_t collect_inode_invariants(char *buf, const struct inode *inode,
+				       size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t i_addr = (uint64_t)inode;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(umode_t) +
+				       sizeof(kuid_t) + sizeof(kgid_t) +
+				       sizeof(unsigned long);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	/*
+	 * Consider tracking the integrity of the remaining fields as well (not
+	 * just the below). Determine, which of them are subject to change
+	 * during the inode's life time. Otherwise, tracking the integrity of
+	 * those would require an inode tag update every time the fields change.
+	 *
+	 * NOTE: The difficulty in tracking these fields is that we need to make
+	 * sure that we track every time any of the below fields change at
+	 * run-time.
+	 */
+
+	_buf = memcpy(_buf, &i_addr, sizeof(i_addr));
+	_buf += sizeof(i_addr);
+
+	_buf = memcpy(_buf, &inode->i_mode, sizeof(umode_t));
+	_buf += sizeof(umode_t);
+
+	_buf = memcpy(_buf, &inode->i_uid, sizeof(kuid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_gid, sizeof(kgid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_ino, sizeof(unsigned long));
+	_buf += sizeof(unsigned long);
+
+	return buf_size;
+}
+
+static uint64_t siphash_inode_state(const struct inode *inode)
+{
+#define MAX_BUF_SIZE                                                           \
+	sizeof(uint64_t) + sizeof(umode_t) + sizeof(kuid_t) + sizeof(kgid_t) + \
+		sizeof(unsigned long)
+
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_inode_invariants(buf, inode, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static inline bool __is_setuid(struct cred *new, const struct cred *old)
+{
+	return !uid_eq(new->euid, old->uid);
+}
+
+static inline bool __is_setgid(struct cred *new, const struct cred *old)
+{
+	return !gid_eq(new->egid, old->gid);
+}
+
+static inline void bhv_inode_register(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Register__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Register inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_register.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_update(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Update__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the UPDATE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Update inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_update.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_release(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Release__HYPERCALL(.inode = { .addr = (uint64_t)
+								  inode });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the RELEASE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu\n", __FUNCTION__, inode->i_ino);
+#endif
+}
+
+static inline int bhv_inode_verify(struct inode *inode, struct file *file)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Inode__EventType__EVENT_NONE;
+	int block;
+
+	if (!inode)
+		return 0;
+
+	rc = HypABI__Inode__Verify__HYPERCALL(
+		&type, .inode = { .addr = (uint64_t)inode,
+				  .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Verify inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_verify.inode.hmac);
+#endif
+
+	if (!rc && type != HypABI__Inode__EventType__EVENT_NONE) {
+		HypABI__Inode__Log__arg__T *log_arg =
+			HypABI__Inode__Log__arg__ALLOC();
+		populate_context(&log_arg->context, true);
+		rc = get_file_path(&file->f_path, log_arg->file_path,
+				   HypABI__Context__MAX_PATH_SZ);
+		if (rc) {
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		log_arg->event_type = type;
+		log_arg->inode_addr = (uint64_t)(inode);
+		log_arg->inode_uid = (uint32_t)(__kuid_val(inode->i_uid));
+		log_arg->inode_gid = (uint32_t)(__kgid_val(inode->i_gid));
+		log_arg->inode_mode = (uint16_t)(inode->i_mode);
+
+		rc = HypABI__Inode__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[-BHV-] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Inode__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   const struct file *file)
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+{
+	int rc = 0;
+	bool is_setxid = false;
+	const struct cred *old = current_cred();
+	struct cred *new = bprm->cred;
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return 0;
+
+	is_setxid = __is_setuid(new, old) || __is_setgid(new, old);
+	if (!is_setxid)
+		return 0;
+
+	inode = d_backing_inode(file->f_path.dentry);
+
+	/*
+	 * We consider the following cases if is_setxid == true:
+	 * - The new executable has the SXID bits set: verify its inode.
+	 * - The new executable does not have the SXID bits set: verify
+	 *   credentials of the current process.
+	 */
+
+	inode_lock(inode);
+
+	if (is_sxid(inode->i_mode)) {
+		// For simplicity, we make sure the pointer is not const.
+		// This makes it easier since from 6.7 this function gets a
+		// const pointer as an argument.
+		rc = bhv_inode_verify(inode, (struct file *)file);
+		inode_unlock(inode);
+	} else {
+		inode_unlock(inode);
+		rc = bhv_cred_verify(current);
+	}
+
+	return rc;
+}
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+/* Caller holds semaphore inode->i_rwsem. */
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid,
+			    umode_t old_mode)
+{
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return;
+
+	/*
+	 * Unfortunately, we cannot verify the current task's credentials at
+	 * this point. This function is sometimes called after having
+	 * temporarily overwritten the current tasks' credentials via
+	 * override_creds; this is done e.g., by the overlay fs to temporarily
+	 * elevate permissions to the filesystem's owner so allow ops on the
+	 * filesystem. As long as we do not track the temporarily overwritten
+	 * credentials, we cannot verify the current task's credentials.
+	 */
+
+	/* We are interested only in mode, UID, and GID changes. */
+	if (!(ia_valid & (ATTR_MODE | ATTR_UID | ATTR_GID)))
+		return;
+
+	/* Note that the caller of this function locked the inode. */
+	inode = d_backing_inode(dentry);
+
+	/* Mode changes should be updated only if the SXID bits are set. */
+	if ((old_mode & (S_ISUID | S_ISGID)) ==
+	    (inode->i_mode & ((S_ISUID | S_ISGID)))) {
+		if (!is_sxid(inode->i_mode))
+			return;
+	}
+
+	/* Register inode if the previous mode did not have SXID bits set. */
+	if (!is_sxid(old_mode) && is_sxid(inode->i_mode)) {
+		bhv_inode_register(inode);
+		return;
+	}
+
+	/* Release tracked inode if setattr removed SXID bits. */
+	if (is_sxid(old_mode) && !is_sxid(inode->i_mode)) {
+		bhv_inode_release(inode);
+		return;
+	}
+
+	/* In any other case, the inode is tracked: update the changes */
+	bhv_inode_update(inode);
+}
+
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+	if (!is_sxid(inode->i_mode))
+		return;
+
+	/*
+	 * Note: we could check whether the inode has any existing dentry
+	 * aliases, via d_find_any_alias, and if so return. Yet, we chose to do
+	 * this check from within BHV to ensure that an attacker cannot modify
+	 * this value to skip inode registrations.
+	 */
+
+	spin_lock(&inode->i_lock);
+	bhv_inode_register(inode);
+	spin_unlock(&inode->i_lock);
+}
+
+/* Caller holds the spinlock inode->i_lock. */
+void bhv_inode_iput_final(struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu i_count=%d i_nlink=%d\n",
+		__FUNCTION__, inode->i_ino, atomic_read(&inode->i_count),
+		inode->i_nlink);
+#endif
+
+	/*
+	 * Note: when releasing inodes, we unconditionally call into BHV (i.e.,
+	 * disregarding whether the inode is privileged or not) to avoid
+	 * potential reuse attacks.  Otherwise, the attacker could maliciously
+	 * modify a privileged (and by BHV registered) inode, cause the kernel
+	 * to drop the inode in  the kernel, and then re-allocate an
+	 * unprivileged file, the meta data of which can be adjusted to match
+	 * the previous meta information that were incorporated into the HMAC of
+	 * the previously privileged inode.
+	 *
+	 * XXX: Consider conditionally calling into BHV at this point if the
+	 * above renders to be not critical (every new inode allocation receives
+	 * a new inode->i_ino). This would reduce the number of hypercalls.
+	 */
+
+	bhv_inode_release(inode);
+}
+
+int __init bhv_inode_init(void)
+{
+	if (!bhv_inode_is_enabled())
+		return -EPERM;
+
+	if (bhv_inode_initialized)
+		return -EPERM;
+
+	bhv_inode_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/inode.h security/bhv/inode.h
new file mode 100644
index 0000000000..c2a9a02903
--- /dev/null
+++ security/bhv/inode.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __SECURITY_BHV_INODE_H__
+#define __SECURITY_BHV_INODE_H__
+
+#include <linux/lsm_hooks.h>
+#include <linux/sched.h>
+
+#include "libinsight_types.h"
+
+extern struct lsm_blob_sizes brs_blob_sizes;
+
+struct brs_inode_security {
+	struct lib_node *lib;
+};
+
+static inline struct brs_inode_security *
+brs_inode(const struct inode *const inode) {
+	return inode->i_security + brs_blob_sizes.lbs_inode;
+}
+
+#endif /* __SECURITY_BHV_INODE_H__ */
diff --git security/bhv/integrity.c security/bhv/integrity.c
new file mode 100644
index 0000000000..2fc9b869d7
--- /dev/null
+++ security/bhv/integrity.c
@@ -0,0 +1,209 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/filter.h>
+#include <linux/module.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+/************************************************************/
+
+int bhv_integrity_freeze_events(uint64_t flags)
+{
+	if (HypABI__Integrity__Freeze__HYPERCALL(.flags = flags))
+		return -EINVAL;
+	return 0;
+}
+
+extern struct mutex module_mutex;
+extern struct list_head modules;
+
+static void lock_all_modules(void)
+{
+	static bool all_modules_locked = false;
+	struct module *mptr;
+
+	if (all_modules_locked)
+		return;
+
+	mutex_lock(&module_mutex);
+
+	list_for_each_entry (mptr, &modules, list) {
+		if (mptr != THIS_MODULE) {
+			if (!try_module_get(mptr)) {
+				printk(KERN_WARNING
+				       "%s: Cannot lock module %s\n",
+				       __FUNCTION__, mptr->name);
+			}
+		}
+	}
+
+	mutex_unlock(&module_mutex);
+	all_modules_locked = true;
+	return;
+}
+
+bool bhv_allow_kmod_loads = true;
+bool bhv_allow_patch = true;
+bool bhv_integrity_freeze_create_currently_frozen = false;
+bool bhv_integrity_freeze_update_currently_frozen = false;
+bool bhv_integrity_freeze_remove_currently_frozen = false;
+bool bhv_integrity_freeze_patch_currently_frozen = false;
+
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks)
+{
+	int ret;
+	if (!skip_locks) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_kmod_loads = false;
+			lock_all_modules();
+		}
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_patch = false;
+		}
+	} else {
+		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+		       __FUNCTION__);
+	}
+
+	ret = bhv_integrity_freeze_events(flags);
+
+	if (!ret) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE)
+			bhv_integrity_freeze_create_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE)
+			bhv_integrity_freeze_update_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE)
+			bhv_integrity_freeze_remove_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH)
+			bhv_integrity_freeze_patch_currently_frozen = true;
+	}
+
+	return ret;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Create__HYPERCALL(.owner = owner,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Update__HYPERCALL(.region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) false,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) true,
+						    .owner = owner);
+}
+
+static uint64_t _ptpg_pgd_offset __ro_after_init;
+static uint64_t _ptpg_pgd_value __ro_after_init;
+static atomic_t _ptpg_ready __ro_after_init = ATOMIC_INIT(0);
+
+/**************************************************************
+ * start
+ **************************************************************/
+void bhv_start_ptpg(void)
+{
+	_ptpg_pgd_offset = 0;
+	_ptpg_pgd_value = 0;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+
+	bhv_start_get_pt_protect_pgd_data(&_ptpg_pgd_offset, &_ptpg_pgd_value);
+
+	atomic_inc(&_ptpg_ready);
+}
+/**************************************************************/
+
+/**************************************************************
+ * late_start
+ **************************************************************/
+int bhv_late_start_init_ptpg(void)
+{
+	unsigned long r;
+	HypABI__Integrity__PtpgInit__arg__T *arg;
+
+	if (!bhv_integrity_pt_prot_is_enabled() || !atomic_read(&_ptpg_ready)) {
+		return 0;
+	}
+
+	BUG_ON(sizeof(HypABI__Integrity__PtpgInit__arg__T) > PAGE_SIZE);
+
+	arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+
+	bhv_late_start_get_pt_protect_data(arg);
+
+	r = HypABI__Integrity__PtpgInit__hypercall_noalloc(arg);
+
+	HypABI__Integrity__PtpgInit__arg__FREE(arg);
+
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+/**************************************************************/
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+	unsigned long r;
+	bool success;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+	success = bhv_pt_protect_check_pgd_arch(mm, _ptpg_pgd_offset,
+						_ptpg_pgd_value);
+	if (!success) {
+		r = HypABI__Integrity__PtpgReport__hypercall_noalloc();
+		if (r) {
+			pr_err("BHV: error reporting pt violation to host!");
+		}
+	}
+}
diff --git security/bhv/keyring.c security/bhv/keyring.c
new file mode 100644
index 0000000000..963aed5c6b
--- /dev/null
+++ security/bhv/keyring.c
@@ -0,0 +1,198 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/key-type.h>
+#include <linux/siphash.h>
+
+#include <bhv/context.h>
+#include <bhv/keyring.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_KEYRING_DEBUG 0
+
+static bool bhv_keyring_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_keyring_is_active(void)
+{
+	if (!bhv_keyring_initialized)
+		return false;
+
+	return bhv_keyring_is_enabled();
+}
+
+static size_t collect_keyring_invariants(char *buf, const struct key *keyring,
+					 uint64_t anchor, size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct key keyring_copy;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct key);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&keyring_copy, keyring, sizeof(struct key));
+
+	/* Exclude mutable fields from the keyring to be hashed. */
+
+	refcount_set(&keyring_copy.usage, 0);
+	memset(&keyring_copy.graveyard_link, 0, sizeof(struct list_head));
+	memset(&keyring_copy.serial_node, 0, sizeof(struct rb_node));
+#ifdef CONFIG_KEY_NOTIFICATIONS
+	keyring_copy.watchers = NULL;
+#endif
+	memset(&keyring_copy.sem, 0, sizeof(struct rw_semaphore));
+	keyring_copy.last_used_at = 0;
+	keyring_copy.datalen = 0;
+	memset(&keyring_copy.payload, 0, sizeof(union key_payload));
+
+	bound_context = (uint64_t)keyring ^ anchor;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &keyring_copy, sizeof(struct key));
+
+	return buf_size;
+}
+
+static uint64_t siphash_keyring_state(const struct key *keyring,
+				      uint64_t anchor)
+{
+#define MAX_BUF_SIZE sizeof(uint64_t) + sizeof(struct key)
+	char buf[MAX_BUF_SIZE];
+	size_t size =
+		collect_keyring_invariants(buf, keyring, anchor, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Keyring__EventType__EVENT_NONE;
+	int block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	rc = HypABI__Keyring__Verify__HYPERCALL(
+		&type, .keyring = { .addr = (uint64_t)anchor,
+				    .hmac = siphash_keyring_state(
+					    keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	if (!rc && type != HypABI__Keyring__EventType__EVENT_NONE) {
+		HypABI__Keyring__Log__arg__T *log_arg =
+			HypABI__Keyring__Log__arg__ALLOC();
+		populate_context(&log_arg->context, true);
+
+		log_arg->event_type = type;
+		log_arg->keyring_addr = (uint64_t)(keyring);
+		log_arg->keyring_uid = (uint32_t)(__kuid_val(keyring->uid));
+		log_arg->keyring_gid = (uint32_t)(__kgid_val(keyring->gid));
+		log_arg->keyring_perm = (uint32_t)(keyring->perm);
+		log_arg->keyring_serial = (uint32_t)(keyring->serial);
+
+		strncpy(log_arg->keyring_desc, keyring->description,
+			HypABI__Context__MAX_PATH_SZ);
+
+		rc = HypABI__Keyring__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[BHV] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Keyring__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Keyring__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+int bhv_keyring_verify(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	down_read(&keyring->sem);
+	rc = bhv_keyring_verify_locked(keyring, anchor);
+	up_read(&keyring->sem);
+
+	return rc;
+}
+
+static int bhv_keyring_register(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	/* Note: we index system-trusted keyrings by its global anchor. */
+
+	rc = HypABI__Keyring__Register__HYPERCALL(
+		&block, .keyring = { .addr = (uint64_t)anchor,
+				     .hmac = siphash_keyring_state(
+					     keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	return block ? -EPERM : 0;
+}
+
+int bhv_keyring_register_system_trusted(struct key **k)
+{
+	void *anchor = k;
+	return bhv_keyring_register(*k, anchor);
+}
+
+int __init bhv_init_keyring(void)
+{
+	if (!bhv_keyring_is_enabled())
+		return -EPERM;
+
+	if (bhv_keyring_initialized)
+		return -EPERM;
+
+	bhv_keyring_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/libinsight.c security/bhv/libinsight.c
new file mode 100644
index 0000000000..8b41272dfc
--- /dev/null
+++ security/bhv/libinsight.c
@@ -0,0 +1,880 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#include <linux/elf.h>
+#include <linux/rculist.h>
+#include <linux/mm_types.h>
+#include <linux/path.h>
+#include <linux/rbtree.h>
+#include <linux/refcount.h>
+#include <linux/sched.h>
+#include <linux/sched/coredump.h>
+#include <linux/slab.h>
+#include <linux/uprobes.h>
+#include <linux/workqueue.h>
+
+#include <bhv/brs_policy.h>
+#include <bhv/config.h>
+#include <bhv/events_autogen.h>
+#include <bhv/guestlog.h>
+#include <bhv/libinsight.h>
+#include <bhv/util.h>
+
+#include "inode.h"
+
+#if !defined(CONFIG_UPROBES)
+#error "CONFIG_UPROBES is not enabled"
+#endif
+
+#if ELF_CLASS == ELFCLASS32
+#define elf_sym Elf32_Sym
+#else
+#define elf_sym Elf64_Sym
+#endif
+
+enum lib_state { LIB_STATE_PREPARING = 0, LIB_STATE_READY };
+
+/*
+ * Add a ref count into the lib_node so that it can be deleted only after the
+ * last reference was dropped.
+ */
+struct lib_node {
+	refcount_t refcount;
+	/* Red-Black tree maintaining all library nodes. */
+	struct rb_node rb;
+	/* RCU-protected list maintaining uprobes in this library. */
+	struct list_head uprobes;
+	/* Reference-counted inode pointer representing this library. */
+	struct inode *inode;
+	/* Buffer for the path of the library. */
+	char path_buf[brs_evt_SkipSymHook_file_path_MAX];
+	/* Path of the library. */
+	const char *path;
+	/* The state of this library node (XXX: consider removing it) */
+	enum lib_state state;
+	/* Required to defer releasing objects. */
+	struct rcu_head rcu_head;
+	/* Deferred teardown of the entire lib_node. */
+	struct work_struct wq_unreg;
+	/* Is this lib_node queued for teardown? */
+	atomic_t teardown_in_progress;
+};
+
+struct brs_trace_uprobe {
+	struct list_head list;
+	struct uprobe_consumer uc;
+	loff_t file_offset;
+	char *sym_name;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 12, 0)
+	struct uprobe *uprobe;
+#endif
+	/* Reference to the associated library node holding its path. */
+	struct lib_node *lib;
+	/* Required to defer releasing objects. */
+	struct rcu_head rcu_head;
+};
+
+/*
+ * XXX: Instead of using a global rb_tree to maintain inodes-related security
+ * information, consider maintaining private information in the
+ * inode->i_security field. This would eliminate the need for a global rb_tree
+ * and the associated mutex.
+ */
+static struct rb_root lib_tree = RB_ROOT;
+static DEFINE_MUTEX(lib_tree_mutex);
+
+static inline struct lib_node *get_lib_node(struct lib_node *node)
+{
+	if (node == NULL)
+		return NULL;
+
+	if (!refcount_inc_not_zero(&node->refcount))
+		return NULL;
+
+	return node;
+}
+
+static inline void put_lib_node(struct lib_node *node)
+{
+	if (node == NULL)
+		return;
+
+	if (refcount_dec_and_test(&node->refcount))
+		kfree_rcu(node, rcu_head);
+}
+
+static bool brs_uprobe_filter(struct uprobe_consumer *uc,
+			      enum uprobe_filter_ctx ctx, struct mm_struct *mm)
+{
+	bool rv = false;
+	struct brs_policy *policy = NULL;
+
+	/*
+	 * XXX: BIG FAT AND UGLY COMMENT:
+	 *
+	 * The changes between the Linux kernel v6.1 and v6.12 removed the
+	 * uprobe_filter_ctx enum, which unfortunately made the implementation
+	 * of a one-shot uprobe handler less convenient. Given that the changes
+	 * are not at all critical, we have reverted the commit
+	 * 59da880afed211c989ef65da577b24215ce57774. Please consider this in
+	 * future implementations and adjust the code appropriately, if the
+	 * requirements demand the elimination of the context enum after all.
+	 */
+
+	if (ctx == UPROBE_FILTER_UNREGISTER)
+		return false;
+
+	policy = brs_get_policy();
+
+	if (brs_is_configurator_process(current) ||
+	    brs_is_cgroup_exempt(policy, current)) {
+		//pr_debug("Do not set uprobes for task: %s (ctx=%d)",
+		//	 current->comm, ctx);
+		goto out;
+	}
+
+	if (brs_is_process_excluded_from_trace(policy, current)) {
+		//pr_debug("Do not set uprobes for task: %s (ctx=%d)",
+		//	  current->comm, ctx);
+		goto out;
+	}
+
+	rv = true;
+out:
+	brs_put_policy(policy);
+
+	return rv;
+}
+
+static int brs_uprobe_handler(struct uprobe_consumer *uc, struct pt_regs *)
+{
+	struct brs_trace_uprobe *tu = NULL;
+	struct brs_policy *policy = brs_get_policy();
+
+	/*
+	 * RCU guarantees that the memory behind brs_trace_uprobe cannot be
+	 * freed during the execution of this handler.
+	 */
+	rcu_read_lock();
+
+	tu = container_of(uc, struct brs_trace_uprobe, uc);
+
+	brs_guestlog_log_lib_trace(policy, tu->lib->path, tu->sym_name);
+
+	rcu_read_unlock();
+
+	/*
+	 * Note: If we wanted to have more control, we could use current->utask
+	 * and the function set_orig_insn() to patch the instruction into the
+	 * current->mm only. This strategy would ensure that, even in presence
+	 * of other uprobe consumers, the uprobe would be patched out of the
+	 * current process' address space (do not forget to set the flag
+	 * MMF_RECALC_UPROBES in its mm). For now, by returning the flag
+	 * UPROBE_HANDLER_REMOVE to the handler's dispatcher, we ensure that
+	 * the uprobe gets properly removed from the current process' address
+	 * space, as long as no other uprobe consumer is active and handles the
+	 * uprobe differently.
+	 */
+
+	brs_put_policy(policy);
+
+	return UPROBE_HANDLER_REMOVE;
+}
+
+static struct brs_trace_uprobe *
+alloc_brs_uprobe(struct lib_node *lib, elf_sym *sym, const char *sym_name)
+{
+	struct brs_trace_uprobe *tu = NULL;
+
+	if ((tu = kzalloc(sizeof(*tu), GFP_KERNEL)) == NULL)
+		return ERR_PTR(-ENOMEM);
+
+	if ((tu->sym_name = kstrdup(sym_name ?: "<unknown>", GFP_KERNEL)) ==
+	    NULL) {
+		kfree(tu);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	tu->file_offset = sym->st_value;
+	tu->uc.handler = brs_uprobe_handler;
+	tu->uc.filter = brs_uprobe_filter;
+	tu->lib = get_lib_node(lib);
+
+	if (tu->lib == NULL) {
+		kfree(tu->sym_name);
+		kfree(tu);
+		return ERR_PTR(-ENOENT);
+	}
+
+	INIT_LIST_HEAD(&tu->list);
+
+	return tu;
+}
+
+static int rb_compare_inodes(const struct inode *a, const struct inode *b)
+{
+	/*
+	 * We assume that we maintain a unique reference to an inode.
+	 */
+	if (a < b)
+		return -1;
+	if (a > b)
+		return 1;
+
+	/*
+	 * To make sure that the inode reference was not reused, we
+	 * additionally check its inode number.
+	 */
+	if (a->i_ino < b->i_ino)
+		return -1;
+	if (a->i_ino > b->i_ino)
+		return 1;
+
+	/*
+	 * Note: consider additionally comparing the inode->i_sb to completely
+	 * rule out a potentially reused inode.
+	 */
+
+	return 0;
+}
+
+/*
+ * XXX: This function currently remains unused; do not delete it until we are
+ * completely sure we son't need it.
+ */
+#if 0
+static struct lib_node *__maybe_unused rb_get_lib_node(struct rb_root *root,
+						       struct inode *inode)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct lib_node *cur = rb_entry(node, struct lib_node, rb);
+		int cmp = rb_compare_inodes(inode, cur->inode);
+
+		if (cmp < 0)
+			node = node->rb_left;
+		else if (cmp > 0)
+			node = node->rb_right;
+		else
+			return cur;
+	}
+
+	return NULL;
+}
+#endif
+
+static int rb_insert_lib_node(struct rb_root *root,
+			      struct lib_node *node_to_add)
+{
+	struct rb_node *parent = NULL;
+	struct rb_node **new = &(root->rb_node);
+
+	while (*new) {
+		struct lib_node *cur = rb_entry(*new, struct lib_node, rb);
+		int cmp = rb_compare_inodes(node_to_add->inode, cur->inode);
+
+		parent = *new;
+
+		if (cmp < 0)
+			new = &((*new)->rb_left);
+		else if (cmp > 0)
+			new = &((*new)->rb_right);
+		else
+			return -EEXIST;
+	}
+
+	rb_link_node(&node_to_add->rb, parent, new);
+	rb_insert_color(&node_to_add->rb, root);
+
+	return 0;
+}
+
+static void rb_remove_lib_node(struct rb_root *root,
+			       struct lib_node *node_to_rm)
+{
+	if (node_to_rm == NULL)
+		return;
+
+	rb_erase(&node_to_rm->rb, root);
+}
+
+static int brs_inode_set_lib(struct inode *inode, struct lib_node *lib)
+{
+	struct brs_inode_security *security = brs_inode(inode);
+	struct lib_node *old_lib = NULL;
+
+	if (security == NULL || lib == NULL)
+		return -EINVAL;
+
+	lib = get_lib_node(lib);
+	if (lib == NULL)
+		return -ENOENT;
+
+	old_lib = rcu_dereference_protected(security->lib,
+					    lockdep_is_held(&lib_tree_mutex));
+	if (old_lib != NULL) {
+		if (old_lib != lib) {
+			put_lib_node(lib);
+			return -EEXIST;
+		}
+
+		/* The security blob points to the same lib; drop extra ref. */
+		put_lib_node(lib);
+		return 0;
+	}
+
+	rcu_assign_pointer(security->lib, lib);
+
+	return 0;
+}
+
+static void brs_inode_clear_lib(struct inode *inode, struct lib_node *expected)
+{
+	struct brs_inode_security *security = brs_inode(inode);
+	struct lib_node *old_lib = NULL;
+
+	if (security == NULL)
+		return;
+
+	old_lib = rcu_dereference_protected(security->lib,
+					    lockdep_is_held(&lib_tree_mutex));
+
+	if (old_lib == NULL)
+		return;
+
+	/* We only clear an expected object. */
+	if (expected && old_lib != expected) {
+		pr_err("[BRS] Unexpected inode security blob exists!");
+		return;
+	}
+
+	RCU_INIT_POINTER(security->lib, NULL);
+
+	put_lib_node(old_lib);
+}
+
+static void brs_release_lib_node(struct lib_node *lib)
+{
+	rb_remove_lib_node(&lib_tree, lib);
+	brs_inode_clear_lib(lib->inode, lib);
+	iput(lib->inode);
+
+	if (!list_empty(&lib->uprobes))
+		pr_warn("[BRS] Deleting lib node with non-empty uprobes list!");
+
+	put_lib_node(lib);
+}
+
+static void brs_deferred_release_uprobes_in_lib(struct work_struct *w);
+static struct lib_node *track_inode(struct rb_root *root, struct file *file,
+				    struct inode *inode)
+{
+	struct lib_node *node = NULL;
+
+	/* XXX: Consider adding a custom kmem cache. */
+	node = kzalloc(sizeof(*node), GFP_KERNEL);
+	if (node == NULL)
+		return NULL;
+
+	INIT_LIST_HEAD(&node->uprobes);
+	INIT_WORK(&node->wq_unreg, brs_deferred_release_uprobes_in_lib);
+
+	refcount_set(&node->refcount, 1);
+	atomic_set(&node->teardown_in_progress, 0);
+	node->state = LIB_STATE_PREPARING;
+	node->inode = igrab(inode);
+	node->path =
+		d_path(&file->f_path, node->path_buf, sizeof(node->path_buf));
+
+	if (IS_ERR(node->path)) {
+		struct dentry *dentry = file_dentry(file);
+		strscpy(node->path_buf, dentry->d_name.name,
+			sizeof(node->path_buf));
+		node->path = node->path_buf;
+	}
+
+	if (rb_insert_lib_node(root, node)) {
+		iput(node->inode);
+		kfree(node);
+		return NULL;
+	}
+
+	if (brs_inode_set_lib(inode, node)) {
+		brs_release_lib_node(node);
+		return NULL;
+	}
+
+	return node;
+}
+
+static void brs_release_uprobe_rcu(struct rcu_head *rcu)
+{
+	struct brs_trace_uprobe *tu;
+
+	tu = container_of(rcu, struct brs_trace_uprobe, rcu_head);
+
+	put_lib_node(tu->lib);
+	kfree(tu->sym_name);
+	kfree(tu);
+}
+
+static inline void brs_release_uprobe(struct brs_trace_uprobe *tu)
+{
+	list_del_rcu(&tu->list);
+	call_rcu(&tu->rcu_head, brs_release_uprobe_rcu);
+}
+
+static void brs_release_uprobes_in_lib(struct lib_node *lib)
+{
+	struct brs_trace_uprobe *tu, *tmp;
+
+	list_for_each_entry_safe(tu, tmp, &lib->uprobes, list) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 12, 0)
+		if (!IS_ERR_OR_NULL(tu->uprobe))
+			uprobe_unregister_nosync(tu->uprobe, &tu->uc);
+#else
+		uprobe_unregister(lib->inode, tu->file_offset, &tu->uc);
+#endif
+		brs_release_uprobe(tu);
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 12, 0)
+	uprobe_unregister_sync();
+#else
+	synchronize_rcu();
+#endif
+}
+
+static inline void brs_skip_and_release_uprobe(struct lib_node *lib,
+					       struct brs_trace_uprobe *tu)
+{
+	struct brs_policy *policy = brs_get_policy();
+	brs_guestlog_log_skip_sym_hook(policy, lib->path, tu->sym_name);
+	brs_put_policy(policy);
+	brs_release_uprobe(tu);
+}
+
+/*
+ * This function runs in a deferred context ensuring that unregistering uprobes
+ * cannot fail.
+ */
+static void brs_deferred_release_uprobes_in_lib(struct work_struct *w)
+{
+	struct lib_node *lib = container_of(w, struct lib_node, wq_unreg);
+
+	mutex_lock(&lib_tree_mutex);
+	brs_release_uprobes_in_lib(lib);
+	brs_release_lib_node(lib);
+	mutex_unlock(&lib_tree_mutex);
+
+	/* Drop extra ref taken when we queued the work. */
+	put_lib_node(lib);
+}
+
+static void brs_enqueue_unregister_lib(struct lib_node *lib)
+{
+	if (lib == NULL)
+		return;
+
+	if (atomic_xchg(&lib->teardown_in_progress, 1))
+		return;
+
+	/* Take ref until the worker finishes unregistering the uprobes. */
+	if (get_lib_node(lib) == NULL)
+		return;
+
+	/* The system_unbound_wq is not bound to any CPU. */
+	queue_work(system_unbound_wq, &lib->wq_unreg);
+}
+
+/* Inspired by elf_read() in binfmt_elf.c. */
+static int brs_elf_read(struct file *file, void *buf, size_t len, loff_t pos)
+{
+	ssize_t rv;
+
+	/*
+	 * XXX: file to be mmap'ed is likely already in the page cache.
+	 * Consider reading directly from the page_cache to avoid unnecessary
+	 * copies.
+	 */
+	rv = kernel_read(file, buf, len, &pos);
+	if (unlikely(rv != len)) {
+		return (rv < 0) ? rv : -EIO;
+	}
+
+	return 0;
+}
+
+static struct elf_shdr *brs_load_elf_shdrs(const struct elfhdr *elf_hdr,
+					   struct file *elf_file)
+{
+	struct elf_shdr *elf_shdrs = NULL;
+	size_t size = 0;
+
+	if (elf_hdr->e_shentsize != sizeof(struct elf_shdr))
+		return NULL;
+
+	if (elf_hdr->e_shnum == 0)
+		return NULL;
+
+	elf_shdrs = kmalloc_array(elf_hdr->e_shnum, sizeof(struct elf_shdr),
+				  GFP_KERNEL);
+	if (elf_shdrs == NULL)
+		goto out;
+
+	size = elf_hdr->e_shnum * sizeof(struct elf_shdr);
+
+	if (brs_elf_read(elf_file, elf_shdrs, size, elf_hdr->e_shoff))
+		goto out;
+
+	return elf_shdrs;
+
+out:
+	kfree(elf_shdrs);
+	elf_shdrs = NULL;
+
+	return NULL;
+}
+
+static struct elf_shdr *brs_find_elf_shdr(const struct elfhdr *elf_hdr,
+					  struct elf_shdr *elf_shdrs,
+					  int sh_type)
+{
+	int i = 0;
+	struct elf_shdr *elf_shdr = &elf_shdrs[1];
+
+	/* Note: the first section header is reserved. */
+	for (i = 1; i < elf_hdr->e_shnum; i++, elf_shdr++) {
+		if (elf_shdr->sh_type != sh_type)
+			continue;
+
+		return elf_shdr;
+	}
+
+	return NULL;
+}
+
+static char *brs_load_elf_dynstrs(struct elf_shdr *sh_dynstr,
+				  struct file *elf_file)
+{
+	char *dyn_strs = NULL;
+	size_t size = sh_dynstr->sh_size;
+
+	if (size == 0)
+		return NULL;
+
+	if ((dyn_strs = kmalloc(size, GFP_KERNEL)) == NULL)
+		goto out;
+
+	if (brs_elf_read(elf_file, dyn_strs, size, sh_dynstr->sh_offset))
+		goto out;
+
+	return dyn_strs;
+
+out:
+	kfree(dyn_strs);
+	dyn_strs = NULL;
+
+	return NULL;
+}
+
+static elf_sym *brs_load_elf_dynsyms(struct elf_shdr *sh_dynsym,
+				     struct file *elf_file, size_t *num_syms)
+{
+	elf_sym *dyn_syms = NULL;
+
+	if (sh_dynsym->sh_entsize != sizeof(elf_sym))
+		return NULL;
+
+	*num_syms = sh_dynsym->sh_size / sh_dynsym->sh_entsize;
+
+	dyn_syms = kmalloc_array(*num_syms, sizeof(elf_sym), GFP_KERNEL);
+	if (dyn_syms == NULL)
+		goto out;
+
+	if (brs_elf_read(elf_file, dyn_syms, *num_syms * sizeof(elf_sym),
+			 sh_dynsym->sh_offset))
+		goto out;
+
+	return dyn_syms;
+
+out:
+	kfree(dyn_syms);
+	dyn_syms = NULL;
+
+	return NULL;
+}
+
+static int brs_set_uprobes_in_lib(struct file *elf_file, elf_sym *dyn_syms,
+				  struct elf_shdr *sh_dynstr, char *dyn_strs,
+				  size_t num_syms, struct lib_node *lib)
+{
+	size_t i = 0;
+
+	for (i = 0; i < num_syms; ++i) {
+		const char *name = NULL;
+		unsigned char bind, type;
+		elf_sym *sym = &dyn_syms[i];
+		struct brs_trace_uprobe *tu = NULL;
+
+		/* Skip invalid/anonymous symbols. */
+		if (sym->st_name == 0 || sym->st_shndx == SHN_UNDEF)
+			continue;
+
+		bind = ELF_ST_BIND(sym->st_info);
+		type = ELF_ST_TYPE(sym->st_info);
+
+		/* We are interested only in exported functions. */
+		if (type != STT_FUNC)
+			continue;
+
+		if (bind != STB_GLOBAL && bind != STB_WEAK)
+			continue;
+
+		if (sym->st_name >= sh_dynstr->sh_size)
+			continue;
+
+		name = &dyn_strs[sym->st_name];
+
+		tu = alloc_brs_uprobe(lib, sym, name);
+		if (IS_ERR(tu))
+			goto cleanup_uprobes;
+
+		list_add_tail_rcu(&tu->list, &lib->uprobes);
+
+		/* Skip potentially terminating processes. */
+		if (fatal_signal_pending(current))
+			goto cleanup_uprobes;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 12, 0)
+		tu->uprobe =
+			uprobe_register(d_real_inode(file_dentry(elf_file)),
+					tu->file_offset, 0, &tu->uc);
+		if (IS_ERR(tu->uprobe)) {
+			/*
+			 * Processes can receive signals asynchronously. If the
+			 * current process receives a (fatal) signal while
+			 * registering the uprobe, we skip further
+			 * registration attempts.
+			 */
+			if (PTR_ERR(tu->uprobe) == -EINTR)
+				goto cleanup_uprobes;
+
+			/*
+			 * Skip this uprobe only; we may be overwriting atomic
+			 * instructions.
+			 */
+			brs_skip_and_release_uprobe(lib, tu);
+		}
+#else
+		if (uprobe_register(d_real_inode(file_dentry(elf_file)),
+				    tu->file_offset, &tu->uc) != 0) {
+			if (fatal_signal_pending(current))
+				goto cleanup_uprobes;
+
+			brs_skip_and_release_uprobe(lib, tu);
+		}
+#endif
+	}
+
+	pr_debug("[BRS] %s @ 0x%px exported %lu functions",
+		 file_dentry(elf_file)->d_name.name,
+		 d_real_inode(file_dentry(elf_file)), num_syms);
+
+	return 0;
+
+cleanup_uprobes:
+	/* Defer teardown to avoid -EINTR when unregistering uprobes. */
+	brs_enqueue_unregister_lib(lib);
+
+	return -EINVAL;
+}
+
+int brs_trace_lib(struct file *file)
+{
+	int rc = -EINVAL;
+	struct elfhdr elf_hdr;
+	struct elf_shdr *elf_shdrs = NULL;
+	struct elf_shdr *sh_dynsym = NULL;
+	struct elf_shdr *sh_dynstr = NULL;
+	elf_sym *dynsyms = NULL;
+	char *dynstrs = NULL;
+	size_t index_sh_dynstr = 0;
+	size_t num_syms = 0;
+
+	struct lib_node *node = NULL;
+	struct lib_node *ref = NULL;
+	struct inode *inode = NULL;
+	struct brs_inode_security *security = NULL;
+
+	/* Get real inode in case of container. */
+	inode = d_real_inode(file_dentry(file));
+
+	security = brs_inode(inode);
+	if (security == NULL) {
+		pr_warn("[BRS] Inode does not have a security blob field!");
+		return -ENOENT;
+	}
+
+	/* Quickly check whether the lib_node is already being tracked. */
+	if (rcu_access_pointer(security->lib) != NULL)
+		return 0;
+
+	mutex_lock(&lib_tree_mutex);
+
+	/* Just to make sure we don't race. */
+	node = rcu_dereference_protected(security->lib,
+					 lockdep_is_held(&lib_tree_mutex));
+	if (node != NULL) {
+		mutex_unlock(&lib_tree_mutex);
+		return 0;
+	}
+
+	if ((node = track_inode(&lib_tree, file, inode)) == NULL) {
+		mutex_unlock(&lib_tree_mutex);
+		return -EFAULT;
+	}
+
+	/* Take a reference so that the node cannot be freed underneath us. */
+	ref = get_lib_node(node);
+
+	mutex_unlock(&lib_tree_mutex);
+
+	if (ref == NULL) {
+		rc = -ENOENT;
+		goto free_node;
+	}
+
+	if ((rc = brs_elf_read(file, &elf_hdr, sizeof(struct elfhdr), 0)))
+		goto free_node;
+
+	if (memcmp(elf_hdr.e_ident, ELFMAG, SELFMAG) != 0) {
+		rc = -EINVAL;
+		goto free_node;
+	}
+	if (elf_hdr.e_type != ET_EXEC && elf_hdr.e_type != ET_DYN) {
+		rc = -EINVAL;
+		goto free_node;
+	}
+	if (!elf_check_arch(&elf_hdr)) {
+		rc = -EINVAL;
+		goto free_node;
+	}
+
+	/* Load ELF section headers. */
+	if ((elf_shdrs = brs_load_elf_shdrs(&elf_hdr, file)) == NULL) {
+		rc = -EINVAL;
+		goto free_node;
+	}
+
+	if ((sh_dynsym = brs_find_elf_shdr(&elf_hdr, elf_shdrs, SHT_DYNSYM)) ==
+	    NULL) {
+		rc = -EFAULT;
+		goto free_shdrs;
+	}
+
+	/* Note: sh_dynsym->sh_link points to the section header of .dynstr. */
+	index_sh_dynstr = sh_dynsym->sh_link;
+	if (index_sh_dynstr >= elf_hdr.e_shnum) {
+		rc = -EFAULT;
+		goto free_shdrs;
+	}
+
+	sh_dynstr = &elf_shdrs[index_sh_dynstr];
+	if (sh_dynstr->sh_type != SHT_STRTAB) {
+		rc = -EFAULT;
+		goto free_shdrs;
+	}
+
+	if ((dynstrs = brs_load_elf_dynstrs(sh_dynstr, file)) == NULL) {
+		rc = -EFAULT;
+		goto free_shdrs;
+	}
+
+	if ((dynsyms = brs_load_elf_dynsyms(sh_dynsym, file, &num_syms)) ==
+	    NULL) {
+		rc = -EFAULT;
+		goto free_dynstrs;
+	}
+
+	if ((brs_set_uprobes_in_lib(file, dynsyms, sh_dynstr, dynstrs, num_syms,
+				    node)) != 0) {
+		rc = -EFAULT;
+		goto free_dynsyms;
+	}
+
+	/* No need to take the lock as no one depends on the lib state. */
+	node->state = LIB_STATE_READY;
+
+free_dynsyms:
+	kfree(dynsyms);
+free_dynstrs:
+	kfree(dynstrs);
+free_shdrs:
+	kfree(elf_shdrs);
+free_node:
+	if (rc) {
+		pr_err("[BRS] Cannot trace library: %s | mapped in %s",
+		       file_dentry(file)->d_name.name, current->comm);
+
+		brs_enqueue_unregister_lib(node);
+	}
+
+	if (ref != NULL)
+		put_lib_node(ref);
+
+	return rc;
+}
+
+void brs_trace_disable(void)
+{
+	struct rb_node *node, *next;
+
+	mutex_lock(&lib_tree_mutex);
+
+	node = rb_first(&lib_tree);
+	while (node) {
+		struct lib_node *lib = rb_entry(node, struct lib_node, rb);
+		next = rb_next(node);
+
+		brs_release_uprobes_in_lib(lib);
+		brs_release_lib_node(lib);
+
+		node = next;
+	}
+
+	lib_tree = RB_ROOT;
+
+	mutex_unlock(&lib_tree_mutex);
+
+	/* Ensure that all rcu-protected memory is freed. */
+	synchronize_rcu();
+}
+
+void brs_trace_inode_free(struct inode *inode)
+{
+	struct lib_node *lib = NULL;
+	struct brs_inode_security *security = NULL;
+
+	security = brs_inode(inode);
+	if (security == NULL)
+		return;
+
+	if (rcu_access_pointer(security->lib) == NULL)
+		return;
+
+	mutex_lock(&lib_tree_mutex);
+
+	lib = rcu_dereference_protected(security->lib,
+					lockdep_is_held(&lib_tree_mutex));
+	if (lib != NULL) {
+		brs_release_uprobes_in_lib(lib);
+		brs_release_lib_node(lib);
+	}
+
+	mutex_unlock(&lib_tree_mutex);
+}
diff --git security/bhv/libinsight_types.h security/bhv/libinsight_types.h
new file mode 100644
index 0000000000..99aaa1552b
--- /dev/null
+++ security/bhv/libinsight_types.h
@@ -0,0 +1,12 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __BHV_LIBINSIGHT_TYPES__
+#define __BHV_LIBINSIGHT_TYPES__
+
+struct lib_node;
+
+#endif /* __BHV_LIBINSIGHT_TYPES__ */
diff --git security/bhv/lsm.c security/bhv/lsm.c
new file mode 100644
index 0000000000..00e4127c6f
--- /dev/null
+++ security/bhv/lsm.c
@@ -0,0 +1,1139 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#define pr_fmt(fmt) "BR LSM: " fmt
+
+#include <linux/binfmts.h>
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fdtable.h>
+#include <linux/highmem.h>
+#include <linux/kernel.h>
+#include <linux/mman.h>
+#include <linux/module.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <uapi/linux/magic.h>
+#include <linux/net.h>
+#include <net/sock.h>
+#include <net/inet_common.h>
+
+#include <bhv/acl.h>
+#include <bhv/brs.h>
+#include <bhv/brs_policy.h>
+#include <bhv/drift_detection.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/inode.h>
+#include <bhv/integrity.h>
+#include <bhv/libinsight.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/reverse_shell_detection.h>
+#include <bhv/util.h>
+
+#include <linux/lsm_hooks.h>
+
+#include "inode.h"
+#include "task.h"
+
+#define __BRS_COUNT_ARGS(_0, _t1, _1, _t2, _2, _t3, _3, _t4, _4, _t5, _5, _t6, \
+			 _6, _t7, _7, _t8, _8, _n, X...)                       \
+	_n
+#define BRS_COUNT_ARGS(X...)                                                 \
+	__BRS_COUNT_ARGS(, ##X, 8, 8, 7, 7, 6, 6, 5, 5, 4, 4, 3, 3, 2, 2, 1, \
+			 1, 0)
+
+#define NAMEOFARG0()
+#define NAMEOFARG1(t, a) a
+#define NAMEOFARG2(t, a, ...) a, NAMEOFARG1(__VA_ARGS__)
+#define NAMEOFARG3(t, a, ...) a, NAMEOFARG2(__VA_ARGS__)
+#define NAMEOFARG4(t, a, ...) a, NAMEOFARG3(__VA_ARGS__)
+#define NAMEOFARG5(t, a, ...) a, NAMEOFARG4(__VA_ARGS__)
+#define NAMEOFARG6(t, a, ...) a, NAMEOFARG5(__VA_ARGS__)
+#define NAMEOFARG7(t, a, ...) a, NAMEOFARG6(__VA_ARGS__)
+
+#define TYPEANDNAMEARG0()
+#define TYPEANDNAMEARG1(t, a) t a
+#define TYPEANDNAMEARG2(t, a, ...) t a, TYPEANDNAMEARG1(__VA_ARGS__)
+#define TYPEANDNAMEARG3(t, a, ...) t a, TYPEANDNAMEARG2(__VA_ARGS__)
+#define TYPEANDNAMEARG4(t, a, ...) t a, TYPEANDNAMEARG3(__VA_ARGS__)
+#define TYPEANDNAMEARG5(t, a, ...) t a, TYPEANDNAMEARG4(__VA_ARGS__)
+#define TYPEANDNAMEARG6(t, a, ...) t a, TYPEANDNAMEARG5(__VA_ARGS__)
+#define TYPEANDNAMEARG7(t, a, ...) t a, TYPEANDNAMEARG6(__VA_ARGS__)
+
+#define NAMEOFARG(...) \
+	CONCATENATE(NAMEOFARG, BRS_COUNT_ARGS(__VA_ARGS__))(__VA_ARGS__)
+#define TYPEANDNAMEARG(...) \
+	CONCATENATE(TYPEANDNAMEARG, BRS_COUNT_ARGS(__VA_ARGS__))(__VA_ARGS__)
+
+#define BRS_LSM_HOOK(RET, FN, POLICY, ...)                                     \
+	static RET FN(struct brs_policy *POLICY, TYPEANDNAMEARG(__VA_ARGS__)); \
+	static RET WRAPPER_##FN(TYPEANDNAMEARG(__VA_ARGS__))                   \
+	{                                                                      \
+		RET rv;                                                        \
+		struct brs_policy *POLICY = brs_get_policy();                  \
+		rv = FN(POLICY, NAMEOFARG(__VA_ARGS__));                       \
+		brs_put_policy(POLICY);                                        \
+		return rv;                                                     \
+	}                                                                      \
+	static RET FN(struct brs_policy *POLICY, TYPEANDNAMEARG(__VA_ARGS__))
+
+#define BRS_LSM_HOOK_VOID(FN, POLICY, ...)                    \
+	static void FN(struct brs_policy *POLICY,             \
+		       TYPEANDNAMEARG(__VA_ARGS__));          \
+	static void WRAPPER_##FN(TYPEANDNAMEARG(__VA_ARGS__)) \
+	{                                                     \
+		struct brs_policy *POLICY = brs_get_policy(); \
+		FN(POLICY, NAMEOFARG(__VA_ARGS__));           \
+		brs_put_policy(POLICY);                       \
+	}                                                     \
+	static void FN(struct brs_policy *POLICY, TYPEANDNAMEARG(__VA_ARGS__))
+
+#define BRS_LSM_HOOK_INIT(HOOK, FN) LSM_HOOK_INIT(HOOK, WRAPPER_##FN)
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+#define __lsm_ro_after_init __ro_after_init
+#endif // LINUX_VERSION_CODE >= 6.4
+
+bool _brs_is_standalone __ro_after_init = true;
+
+#ifdef CONFIG_BHV_VAS
+BRS_LSM_HOOK(int, bhv_read_file, policy, struct file *, file,
+	     enum kernel_read_file_id, id, bool, whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+BRS_LSM_HOOK(int, brs_load_data, policy, enum kernel_load_data_id, id, bool,
+	     contents)
+{
+	const char *origin = kernel_load_data_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+#ifdef CONFIG_BHV_VAS
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+#endif // defined CONFIG_BHV_VAS
+
+		if (BRS_EVT_ENABLED(&policy->flast, driver_load)) {
+			brs_guestlog_log_driver_load(policy,
+						     "[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_task_alloc, policy, struct task_struct *, target,
+	     long unsigned int, clone_flags)
+{
+	struct brs_task_security *task_security = brs_task(target);
+	struct bhv_vas *vas = &task_security->vas;
+
+	BUILD_BUG_ON_MSG((CAP_LAST_CAP + 1) > (sizeof(vas->cap_raised) << 3),
+			 "Number of capabilities exceeds size of bitmask.");
+
+	vas->cap_raised = 0UL;
+
+#ifdef CONFIG_BHV_VAS
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+#endif // defined CONFIG_BHV_VAS
+
+	if (BRS_EVT_ENABLED(&policy->flast, process_fork)) {
+		brs_guestlog_log_process_fork(policy, target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK_VOID(brs_task_free, policy, struct task_struct *, target)
+{
+	brs_configurator_process_terminated(target);
+
+	if (BRS_EVT_ENABLED(&policy->flast, process_terminate)) {
+		brs_guestlog_log_process_exit(policy, target);
+	}
+}
+
+static inline const char *brs_bprm_get_file_name(struct linux_binprm *bprm,
+						 char **buf)
+{
+	char *_buf;
+
+	_buf = brs_aa_kmalloc(PATH_MAX);
+	if (_buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	(*buf) = _buf;
+	// Can be called with when _buf is NULL.
+	return brs_get_file_path(bprm->file, _buf, PATH_MAX);
+}
+
+BRS_LSM_HOOK(int, brs_bprm_check_security, policy, struct linux_binprm *, bprm)
+{
+	int rv = 0;
+	char *buf = NULL;
+	const char *path = NULL;
+
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+#ifdef CONFIG_BHV_VAS
+	if (bhv_acl_is_proc_acl_enabled()) {
+		path = brs_bprm_get_file_name(bprm, &buf);
+
+		// check executed filename (name on cli)
+		if (bhv_block_process(bprm->filename)) {
+			rv = -EPERM;
+			goto out;
+		}
+
+		// check underlying file (e.g., busybox or interpreter)
+		if (bhv_block_process(path)) {
+			rv = -EPERM;
+			goto out;
+		}
+	}
+#endif // defined CONFIG_BHV_VAS
+
+	if (brs_reverse_shell_detetection_is_enabled(policy)) {
+		if (path == NULL)
+			path = brs_bprm_get_file_name(bprm, &buf);
+
+		rv = brs_reverse_shell_exec(policy, bprm, path);
+		if (rv)
+			goto out;
+	}
+
+	if (BRS_EVT_ENABLED(&policy->flast, process_exec)) {
+		if (path == NULL)
+			path = brs_bprm_get_file_name(bprm, &buf);
+
+		rv = brs_guestlog_log_process_exec(policy, bprm, current, path);
+		if (rv)
+			goto out;
+	}
+
+out:
+	if (buf)
+		kfree(buf);
+
+	return rv;
+}
+
+static inline const char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+
+#ifdef CONFIG_BHV_VAS
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+static bool bhv_perform_check_fileops(struct brs_policy *policy,
+				      struct file *file, u8 bhv_fops,
+				      bool is_dir)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+	bool block = false;
+
+	if (bhv_fops != FT(UNSUPPORTED)) {
+		if (file->f_op ==
+		    fileops_map[bhv_fops][is_dir == true ? 1 : 0]) {
+			// fops matches
+			return false;
+		}
+	}
+
+	switch (bhv_fops) {
+	case FT(UNSUPPORTED): {
+		if (bhv_strict_fileops_enforced()) {
+			/*
+			 * strict mode configured or forced by kernel boot option
+			 * continue to report and optionally block
+			 */
+			break;
+		}
+
+		/*
+		 * fallback in case of unknown fops:
+		 * check if pointer points to ro section
+		 */
+		if (bhv_fileops_is_ro((u64)file->f_op)) {
+			uint64_t struct_type = 0;
+			uint32_t minor = 0;
+			uint64_t major = 0;
+
+			if (!BRS_EVT_ENABLED(&policy->flast,
+					     unsupported_file_operation)) {
+				return false;
+			}
+
+			// set the type for logging
+			if (d_is_reg(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE;
+			} else if (d_can_lookup(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY;
+			} else if (d_is_special(file->f_path.dentry)) {
+				// save i_rdev which contains major and minor
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL;
+				major = MAJOR(file->f_inode->i_rdev);
+				minor = MINOR(file->f_inode->i_rdev);
+			}
+
+			// log info about unsupported
+			pathname_buf = kzalloc(
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+				GFP_KERNEL);
+			pathname = get_pathname(
+				file, pathname_buf,
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+
+			brs_guestlog_log_fops_unknown(
+				policy, file->f_inode->i_sb->s_magic, pathname,
+				struct_type, major, minor,
+				(uint64_t)file->f_op);
+
+			pathname = NULL;
+			kfree(pathname_buf);
+			// OK
+			return false;
+		}
+
+		break;
+	}
+	// additional check for empty dir ops
+	case FT(SYSFS):
+		if (file->f_op == &empty_dir_operations) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	// additional check for dummy fops
+	case FT(DEV_TTY):
+		if (file->f_op == &hung_up_tty_fops) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	// additional check for not-allowed-to-open file ops
+	case FT(SOCKFS):
+		if (file->f_op == &no_open_fops) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	case FT(DEV_NULL):
+	case FT(DEV_URANDOM):
+	case FT(DEV_RANDOM):
+	case FT(DEV_CONSOLE):
+	case FT(DEV_KMSG):
+	case FT(DEV_MEM):
+	case FT(DEV_ZERO):
+		if (file->f_op == &def_chr_fops) {
+			// we're all set
+			return false;
+		}
+		break;
+	case FT(PROC):
+		// additional check for proc fops:
+		if (is_valid_proc_fop(&(file->f_op)))
+			return false;
+		break;
+	case FT(DEBUGFS):
+		if (is_valid_debugfs_fop(file->f_op))
+			return false;
+		break;
+	case FT(RAMFS):
+		if (file->f_op == &simple_dir_operations)
+			return false;
+		else if (file->f_op == &ramfs_file_operations)
+			return false;
+		else if (file->f_op == &def_blk_fops)
+			return false;
+#ifdef CONFIG_INOTIFY_USER
+		else if (file->f_op == &inotify_fops)
+			return false;
+#endif
+		break;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+	case FT(TMPFS):
+		if (file->f_op == &simple_offset_dir_operations)
+			return false;
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+
+	default:
+		break;
+	}
+
+	// by now we have decided that the fops ptr is bad
+
+	pathname_buf =
+		kzalloc(HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+	pathname = get_pathname(
+		file, pathname_buf,
+		HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+	if (bhv_block_fileops(pathname, bhv_fops, is_dir, file->f_op)) {
+		// block file operation
+		block = true;
+	}
+
+	pathname = NULL;
+	kfree(pathname_buf);
+
+	return block;
+}
+
+static bool bhv_check_fileops(struct brs_policy *policy, struct file *file)
+{
+	// set up fops check data
+	u8 bhv_fops = FT(UNSUPPORTED);
+	bool is_dir = false;
+
+	unsigned dev_major = imajor(file->f_inode);
+	unsigned dev_minor = iminor(file->f_inode);
+
+	if (d_can_lookup(file->f_path.dentry)) {
+		/* directory S_ISDIR(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+		is_dir = true;
+	} else if (d_is_reg(file->f_path.dentry)) {
+		/* regular file  S_ISREG(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+	} else if (d_is_special(file->f_path.dentry)) {
+		// DCACHE_SPECIAL_TYPE
+		if (S_ISSOCK(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == SOCKFS_MAGIC
+			bhv_fops = FT(SOCKFS);
+		} else if (S_ISFIFO(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == PIPEFS_MAGIC
+			bhv_fops = FT(PIPEFS);
+		} else if (S_ISCHR(file->f_inode->i_mode)) {
+			// character device
+			if (dev_major == 1) {
+				// mem
+				switch (dev_minor) {
+				case 1: // DEVMEM_MINOR
+					// /dev/mem
+					bhv_fops = FT(DEV_MEM);
+					break;
+				case 3:
+					// /dev/null
+					bhv_fops = FT(DEV_NULL);
+					break;
+				case 4:
+					// /dev/port
+					bhv_fops = FT(DEV_PORT);
+					break;
+				case 5:
+					// /dev/zero
+					bhv_fops = FT(DEV_ZERO);
+					break;
+				case 7:
+					// /dev/full
+					bhv_fops = FT(DEV_FULL);
+					break;
+				case 8:
+					// /dev/random
+					bhv_fops = FT(DEV_RANDOM);
+					break;
+				case 9:
+					// /dev/urandom
+					bhv_fops = FT(DEV_URANDOM);
+					break;
+				case 11:
+					// /dev/kmsg
+					bhv_fops = FT(DEV_KMSG);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 5) {
+				switch (dev_minor) {
+				case 0:
+					// /dev/tty
+					bhv_fops = FT(DEV_TTY);
+					break;
+				case 1:
+					// /dev/console
+					bhv_fops = FT(DEV_CONSOLE);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 4 && dev_minor == 64) {
+				// /dev/ttyS0
+				bhv_fops = FT(DEV_TTY);
+			} else if (dev_major == 229 && dev_minor == 0) {
+				// /dev/hvc0
+				bhv_fops = FT(DEV_TTY);
+			}
+		}
+	}
+
+#ifdef DEBUG
+	if (bhv_fops == FT(UNSUPPORTED)) {
+		char *pathname_buf = kzalloc(
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+		const char *pathname = get_pathname(
+			file, pathname_buf,
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+		pr_debug(
+			"name: %s magic: 0x%lx dentry_type: 0x%x stat: 0o%o path: %s fops: 0x%px",
+			file->f_inode->i_sb->s_type->name,
+			file->f_inode->i_sb->s_magic,
+			file->f_path.dentry->d_flags & DCACHE_ENTRY_TYPE,
+			(file->f_inode->i_mode & S_IFMT), pathname, file->f_op);
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+#endif
+
+	// perform fops check
+	return bhv_perform_check_fileops(policy, file, bhv_fops, is_dir);
+}
+#undef FT
+#endif // defined CONFIG_BHV_VAS
+
+#ifdef CONFIG_BHV_VAS
+#define BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ 256
+
+static int bhv_check_files_dirty_pipe(const void *address_space,
+				      struct file *file, unsigned int number)
+{
+	char *filename_buf = NULL;
+	const char *filename = NULL;
+	int rv;
+
+	struct inode *ipipe =
+		address_space != NULL ?
+			((struct address_space *)address_space)->host :
+			NULL;
+	struct inode *ifile = d_real_inode(file->f_path.dentry);
+
+	if (ipipe == NULL || ifile == NULL || !virt_addr_valid(ipipe))
+		return 0;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (ifile->i_ino == ipipe->i_ino && ifile->i_sb == ipipe->i_sb &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Allocate memory
+		filename_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+
+		if (filename_buf == NULL) {
+			pr_err("Could not allocate file buffer");
+			filename = "UNKNOWN";
+		} else {
+			// Get Path of the file we are trying to write
+			filename = d_path(&file->f_path, filename_buf,
+					  BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+			if (IS_ERR(filename)) {
+				pr_err("Could not retrieve file name (%ld)",
+				       PTR_ERR(filename));
+				filename = "UNKNOWN";
+			}
+		}
+
+		// Ask the HOST whether we should block this attempt.
+		rv = bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+			filename);
+
+		if (filename_buf != NULL) {
+			kfree(filename_buf);
+		}
+
+		return rv;
+	}
+
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+#ifdef CONFIG_BHV_VAS
+/*
+ * Dirty cred detection
+ * When a write happens this checks whether the struct file object is opened for
+ * writing. If this is not the case notify and optionally stop the write
+ * attempt immediately.
+ */
+void bhv_check_file_dirty_cred(struct file *file, int mask)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+
+	if (!bhv_dirtycred_file_protection_is_enabled())
+		return;
+
+	// No need to continue if no write attempt is ongoing.
+	if (!(mask & MAY_WRITE))
+		return;
+
+	if (!(file->f_mode & FMODE_WRITE) ||
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		pathname_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+		pathname = get_pathname(file, pathname_buf,
+					BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+		pr_err("Write to read-only file \"%s\" detected!", pathname);
+		// At this point a struct file object was already placed illegitimately
+		// at the location of another. When we're be called from a LSM file
+		// permission hook, we could still bail out gracefully. However, as
+		// illegitimately placing struct file objects often happens after
+		// permission and LSM hooks, we need to check deep inside write attempts
+		// (e.g. in mm/filemap.c:generic_perform_write()). There, we do not have
+		// much choice as the attempts might have already corrupted the
+		// read-only file. Hence, on a block, we panic after having informed the
+		// HOST.
+		if (bhv_block_read_only_file_write_ViolationDirtyCredWrite(
+			    pathname)) {
+			panic("possible dirtycred compromise detected");
+		}
+
+		kfree(pathname_buf);
+	}
+}
+#endif // defined CONFIG_BHV_VAS
+
+BRS_LSM_HOOK(int, brs_file_open, policy, struct file *, file)
+{
+#ifdef CONFIG_BHV_VAS
+	if (bhv_fileops_file_protection_is_enabled() &&
+	    bhv_check_fileops(policy, file))
+		return -EFAULT;
+#endif // defined CONFIG_BHV_VAS
+
+	if (BRS_EVT_ENABLED(&policy->flast, file_open))
+		return brs_guestlog_log_file_open(policy, file);
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_file_permission, policy, struct file *, file, int, mask)
+{
+	int rv;
+
+#ifdef CONFIG_BHV_VAS
+	if (bhv_fileops_file_protection_is_enabled() &&
+	    bhv_check_fileops(policy, file))
+		return -EFAULT;
+
+	/* check for dirty cred inside LSM hook (e.g. for aio_write()) */
+	bhv_check_file_dirty_cred(file, mask);
+#endif // defined CONFIG_BHV_VAS
+
+	if (brs_drift_detection_is_enabled() &&
+	    (rv = brs_drift_detection_file_permission(policy, file, mask)) != 0)
+		return rv;
+
+#ifdef CONFIG_BHV_VAS
+	if (bhv_read_only_file_protection_is_enabled()) {
+		struct pipe_inode_info *info;
+		struct pipe_buffer *buf;
+		/*
+		* Dirty pipe detection. Whenever we write to a file and this file is
+		* a pipe, we are going to check whether this pipe points to a read-only
+		* file. This check happens in `bhv_check_files_dirty_pipe` above.
+		*/
+		if ((mask & MAY_WRITE) == MAY_WRITE &&
+		    (info = get_pipe_info(file, false))) {
+			// Check current buffer in the pipe for dirty pipe
+			buf = &info->bufs[(info->head - 1) &
+					  (info->ring_size - 1)];
+			// Iterate over all open files and see whether the pipe points to the same file.
+			if (buf && buf->page && current->files &&
+			    virt_addr_valid(buf->page->mapping)) {
+				if (iterate_fd(current->files, 0,
+					       bhv_check_files_dirty_pipe,
+					       buf->page->mapping)) {
+					return -EACCES;
+				}
+			}
+		}
+	}
+#endif // defined CONFIG_BHV_VAS
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_cgroup_mkdir, policy, struct cgroup *, cgrp)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, cgroup_create))
+		brs_guestlog_log_cgroup_create(policy, cgrp);
+	return 0;
+}
+
+BRS_LSM_HOOK_VOID(brs_cgroup_rmdir, policy, struct cgroup *, cgrp)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, cgroup_destroy))
+		brs_guestlog_log_cgroup_destroy(policy, cgrp);
+}
+
+BRS_LSM_HOOK(int, brs_unshare, policy, struct task_struct *, tsk,
+	     struct nsset *, nsset)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, namespace_change))
+		return brs_guestlog_log_namespace_change(policy, tsk, nsset);
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_setns, policy, struct task_struct *, tsk, struct nsset *,
+	     nsset)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, namespace_change))
+		return brs_guestlog_log_namespace_change(policy, tsk, nsset);
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_socket_connect, policy, struct socket *, sock,
+	     struct sockaddr *, address, int, addrlen)
+{
+	int rv;
+
+	if (addrlen < offsetofend(struct sockaddr, sa_family))
+		return -EINVAL;
+
+	/*
+	 * This is a documented way to disconnect the socket; no need to emit
+	 * a socket_connect event.
+	 */
+	if (address->sa_family == AF_UNSPEC)
+		return 0;
+
+	if (BRS_EVT_ENABLED(&policy->flast, socket_connection) && sock) {
+		if (address->sa_family == AF_INET ||
+		    address->sa_family == AF_INET6) {
+			rv = brs_guestlog_log_net_socket_connection(
+				policy, false, address);
+			if (rv != 0)
+				return rv;
+		} else if (address->sa_family == AF_UNIX) {
+			rv = brs_guestlog_log_unix_socket_connection(
+				policy, sock, (struct sockaddr_un *)address,
+				addrlen);
+			if (rv != 0)
+				return rv;
+		}
+	}
+
+	if (brs_reverse_shell_detetection_is_enabled(policy)) {
+		rv = brs_reverse_shell_detection_socket_connect(
+			policy, sock, address, addrlen);
+		if (rv != 0)
+			return rv;
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_socket_bind, policy, struct socket *, sock,
+	     struct sockaddr *, address, int, addrlen)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, socket_connection) && sock &&
+	    (sock->sk->sk_family == AF_INET || sock->sk->sk_family == AF_INET6))
+		brs_guestlog_log_net_socket_connection(policy, true, address);
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_socket_accepted, policy, struct socket *, sock,
+	     struct socket *, newsock)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, socket_accept) && newsock &&
+	    (newsock->sk->sk_family == AF_INET ||
+	     newsock->sk->sk_family == AF_INET6)) {
+		return brs_guestlog_log_socket_accept(policy, newsock);
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_fd_dup, policy, unsigned int, fd, struct file *, file)
+{
+	int rv;
+
+	if (brs_reverse_shell_detetection_is_enabled(policy)) {
+		rv = brs_reverse_shell_fd_dup(policy, fd, file);
+		if (rv != 0)
+			return rv;
+	}
+
+	return 0;
+}
+
+static void _brs_capable_check(struct brs_policy *policy,
+			       const struct cred *cred,
+			       struct user_namespace *ns, int cap,
+			       unsigned int opts)
+{
+	bool task_is_null;
+	rcu_read_lock();
+	task_is_null = task_dfl_cgroup(current) == NULL;
+	rcu_read_unlock();
+
+	if (task_is_null)
+		return;
+
+	if (cap_raised(current_real_cred()->cap_permitted, cap))
+		if (!test_and_set_bit(cap, &brs_task(current)->vas.cap_raised))
+			brs_guestlog_log_capable(policy, cap);
+
+	return;
+}
+
+BRS_LSM_HOOK(int, brs_capable, policy, const struct cred *, cred,
+	     struct user_namespace *, ns, int, cap, unsigned int, opts)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, capable))
+		_brs_capable_check(policy, cred, ns, cap, opts);
+	return 0;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+BRS_LSM_HOOK(int, brs_inode_setxattr, policy, struct mnt_idmap *, mnt_idmap,
+	     struct dentry *, dentry, const char *, name, const void *, value,
+	     size_t, size, int, flags)
+#else
+BRS_LSM_HOOK(int, brs_inode_setxattr, policy, struct user_namespace *,
+	     mnt_userns, struct dentry *, dentry, const char *, name,
+	     const void *, value, size_t, size, int, flags)
+#endif
+{
+	if (BRS_EVT_ENABLED(&policy->flast, file_set_xattr)) {
+		return brs_guestlog_log_file_set_xattr(policy, dentry, name,
+						       value, size);
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_module_loaded, policy, struct module *, mod)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, driver_load)) {
+		return brs_guestlog_log_driver_load(policy, mod->name);
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK_VOID(brs_kaccess, policy, uint64_t, addr, uint8_t, event_id)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, kernel_access_violation)) {
+		brs_guestlog_log_kaccess(policy, addr, event_id);
+	}
+}
+
+BRS_LSM_HOOK(int, brs_elf_load_exec_stack, policy, struct linux_binprm *, bprm)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, executable_exec_stack)) {
+		return brs_guestlog_log_elf_load_exec_stack(policy, bprm);
+	}
+
+	return 0;
+}
+
+BRS_LSM_HOOK(int, brs_kernel_exec, policy, const char *, path, char **, argv,
+	     char **, envp)
+{
+	if (BRS_EVT_ENABLED(&policy->flast, kernel_exec)) {
+		return brs_guestlog_log_kernel_exec(policy, path, argv, envp);
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_MEM_NS
+static int brs_lsm_domain_report(const struct task_struct *t,
+				 const struct mm_struct *mm_target,
+				 const struct vm_area_struct *vma,
+				 unsigned int gup_flags)
+{
+	if (brs__is__StrongIsolation__enabled()) {
+		return brs_domain_report(t, mm_target, vma, gup_flags);
+	}
+	return 0;
+}
+#endif // CONFIG_MEM_NS
+
+BRS_LSM_HOOK(int, brs_lsm_forced_mem_access_permitted, policy,
+	     struct vm_area_struct *, vma, bool, write, bool, foreign)
+{
+	bool block = false;
+
+	if (BRS_EVT_ENABLED(&policy->flast, forced_mem_access)) {
+		int rc = 0;
+
+		/* Retrieve inode security blob only if VMA is file-backed. */
+		if (vma->vm_file != NULL) {
+			struct inode *inode =
+				d_real_inode(file_dentry(vma->vm_file));
+			struct brs_inode_security *security = brs_inode(inode);
+
+			/*
+			 * Do not log foced memory access requests when the VMA
+			 * in question holds active uprobes.
+			 */
+			if (security &&
+			    rcu_access_pointer(security->lib) != NULL)
+				return 1;
+		}
+
+		rc = brs_guestlog_log_forced_mem_access(policy, vma->vm_start,
+							vma->vm_end, write);
+		if (rc) {
+			if (rc == -EPERM)
+				block = true;
+			else
+				pr_err("Cannot log forced memory access event!\n");
+		}
+	}
+
+	return (int)(!block);
+}
+
+static bool is_file_backed_executable(struct file *file, unsigned long prot)
+{
+	/* We are not interested in anonymous mappings. */
+	if (!file)
+		return false;
+
+	/* We are interested in files to be mapped for execution. */
+	if (!(prot & PROT_EXEC))
+		return false;
+
+	return true;
+}
+
+BRS_LSM_HOOK(int, brs_mmap_file, policy, struct file *, file, unsigned long,
+	     reqprot, unsigned long, prot, unsigned long, flags)
+{
+	int rc = 0;
+
+	if (BRS_EVT_ENABLED(&policy->flast, mmap_exec_file)) {
+		if (!is_file_backed_executable(file, prot))
+			return 0;
+
+		rc = brs_guestlog_log_mmap_exec_file(policy, file, prot, flags);
+		if (rc != 0)
+			goto out;
+	}
+
+	if (BRS_EVT_ENABLED(&policy->flast, lib_trace)) {
+		if (!is_file_backed_executable(file, prot))
+			return 0;
+
+		/* We are not interested in the main executable itself. */
+		if (file == current->mm->exe_file)
+			return 0;
+
+		/* Do not set uprobes on terminating tasks. */
+		if (fatal_signal_pending(current))
+			return 0;
+
+		/*
+		 * Do not return potential error values; tracing-related issues
+		 * must not block process execution.
+		 */
+		brs_trace_lib(file);
+	}
+
+out:
+	return rc;
+}
+
+BRS_LSM_HOOK_VOID(brs_trace_inode_free_security, policy, struct inode *, inode)
+{
+	/*
+	 * XXX: We must disable all uprobes when a policy update changes the
+	 * configuration for lib_trace. Otherwise, inode updates will not be
+	 * considered and may lead to invalid states.
+	 */
+	if (BRS_EVT_ENABLED(&policy->flast, lib_trace)) {
+		brs_trace_inode_free(inode);
+	}
+}
+
+static struct security_hook_list bluerock_hooks[] __lsm_ro_after_init = {
+#ifdef CONFIG_BHV_VAS
+	BRS_LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+#endif // defined CONFIG_BHV_VAS
+	BRS_LSM_HOOK_INIT(kernel_load_data, brs_load_data), // init_module
+	BRS_LSM_HOOK_INIT(task_alloc, brs_task_alloc), // fork
+	BRS_LSM_HOOK_INIT(task_free, brs_task_free), // exit
+	BRS_LSM_HOOK_INIT(bprm_check_security,
+			  brs_bprm_check_security), // execve
+	BRS_LSM_HOOK_INIT(file_permission,
+			  brs_file_permission), // file read/write
+	BRS_LSM_HOOK_INIT(file_open, brs_file_open), // file open
+	BRS_LSM_HOOK_INIT(module_loaded,
+			  brs_module_loaded), // module has successfully loaded
+
+#ifdef CONFIG_BHV_VAS
+	/* Inode protection */
+	LSM_HOOK_INIT(bprm_creds_from_file, bhv_inode_bprm_creds_from_file),
+	LSM_HOOK_INIT(task_fix_setuid, bhv_inode_task_fix_setuid),
+	LSM_HOOK_INIT(task_fix_setgid, bhv_inode_task_fix_setgid),
+	LSM_HOOK_INIT(d_instantiate, bhv_inode_d_instantiate),
+#endif // defined CONFIG_BHV_VAS
+
+	/* Container visibility */
+	BRS_LSM_HOOK_INIT(cgroup_mkdir, brs_cgroup_mkdir),
+	BRS_LSM_HOOK_INIT(cgroup_rmdir, brs_cgroup_rmdir),
+	BRS_LSM_HOOK_INIT(unshare, brs_unshare),
+	BRS_LSM_HOOK_INIT(setns, brs_setns),
+	BRS_LSM_HOOK_INIT(capable, brs_capable),
+
+	/* Xattr */
+	BRS_LSM_HOOK_INIT(inode_setxattr, brs_inode_setxattr),
+
+	/* Socket */
+	BRS_LSM_HOOK_INIT(socket_connect, brs_socket_connect),
+	BRS_LSM_HOOK_INIT(socket_bind, brs_socket_bind),
+	BRS_LSM_HOOK_INIT(socket_accepted, brs_socket_accepted),
+
+	/* Dup */
+	BRS_LSM_HOOK_INIT(fd_dup, brs_fd_dup),
+
+	BRS_LSM_HOOK_INIT(kaccess, brs_kaccess),
+	BRS_LSM_HOOK_INIT(elf_load_exec_stack, brs_elf_load_exec_stack),
+	BRS_LSM_HOOK_INIT(kernel_exec, brs_kernel_exec),
+
+#ifdef CONFIG_MEM_NS
+	LSM_HOOK_INIT(domain_report, brs_lsm_domain_report),
+#endif // CONFIG_MEM_NS
+
+	BRS_LSM_HOOK_INIT(forced_mem_access_permitted,
+			  brs_lsm_forced_mem_access_permitted),
+
+	/* Dynamic linker and library visibility. */
+	BRS_LSM_HOOK_INIT(mmap_file, brs_mmap_file),
+
+	/* BRS Trace */
+	BRS_LSM_HOOK_INIT(inode_free_security, brs_trace_inode_free_security),
+};
+
+struct lsm_blob_sizes brs_blob_sizes __ro_after_init = {
+	.lbs_inode = sizeof(struct brs_inode_security),
+	.lbs_task = sizeof(struct brs_task_security),
+};
+
+static void __init __brs_lsm_init(void)
+{
+	brs_guestconn_init();
+}
+
+#define br_lsm_id_str "bluerock"
+
+static int __init bluerock_lsm_init(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 8, 0)
+	static const struct lsm_id br_lsm_id = { br_lsm_id_str,
+						 LSM_ID_BLUEROCK };
+#endif
+
+	__brs_lsm_init();
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 8, 0)
+	security_add_hooks(bluerock_hooks, ARRAY_SIZE(bluerock_hooks),
+			   &br_lsm_id);
+#else // LINUX_VERSION_CODE < 6.8
+	security_add_hooks(bluerock_hooks, ARRAY_SIZE(bluerock_hooks),
+			   br_lsm_id_str);
+#endif // LINUX_VERSION_CODE <> 6.8
+
+	pr_info("[bluerock] LSM active");
+	return 0;
+}
+
+DEFINE_LSM(bluerock) = {
+	.name = br_lsm_id_str,
+	.init = bluerock_lsm_init,
+	.blobs = &brs_blob_sizes,
+};
diff --git security/bhv/memory_freeze.c security/bhv/memory_freeze.c
new file mode 100644
index 0000000000..4255858b12
--- /dev/null
+++ security/bhv/memory_freeze.c
@@ -0,0 +1,104 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/printk.h>
+#include <linux/umh.h>
+
+#include <bhv/integrity.h>
+#include <bhv/guestlog.h>
+#include <bhv/memory_freeze.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+static int __maybe_unused bhv_memory_freeze_main(void *_)
+{
+	static const __section(".rodata") char *const argv[] = {
+		"/usr/bin/systemctl", "is-system-running", NULL
+	};
+	static const __section(".rodata") char *const envp[] = {
+		"HOME=/", "PATH=/sbin:/bin:/usr/sbin:/usr/bin", NULL
+	};
+	int ret;
+
+	while (true) {
+		msleep(500);
+
+		ret = call_usermodehelper(
+			argv[0], (char **)argv, (char **)envp,
+			UMH_WAIT_PROC /* wait for the process to complete */);
+
+		if (ret == -ENOENT) {
+			pr_info("%s: Non-systemd filesystem\n", __FUNCTION__);
+			msleep(120000);
+			pr_info("%s: Assuming system is up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+
+		if (ret == 0) {
+			pr_info("%s: System up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+	}
+}
+
+void bhv_memory_freeze_init(void)
+{
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg;
+	int rc;
+	bool fmab = false;
+
+	if (!is_bhv_initialized())
+		return;
+
+	arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+
+	rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(arg);
+
+	if (rc == 0) {
+		fmab = arg->freeze_memory_after_boot;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(arg);
+
+	if (fmab) {
+#ifdef CONFIG_STATIC_USERMODEHELPER
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.\n",
+		       __FUNCTION__);
+		brs_guestlog_log_str(
+			"Error: freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.");
+#endif
+#ifdef CONFIG_BPF_JIT
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_BPF_JIT.\n",
+		       __FUNCTION__);
+		brs_guestlog_log_str(
+			"Error: freezing memory is incompatible with CONFIG_BPF_JIT.");
+#endif
+
+#ifndef NO_FREEZE
+		if (ERR_PTR(-ENOMEM) == kthread_run(bhv_memory_freeze_main,
+						    NULL, "memory_freeze")) {
+			panic("%s: Could not create memory_freeze thread",
+			      __FUNCTION__);
+		}
+		pr_err("%s: started thread", __FUNCTION__);
+#endif
+	}
+}
\ No newline at end of file
diff --git security/bhv/module.c security/bhv/module.c
new file mode 100644
index 0000000000..a898027e23
--- /dev/null
+++ security/bhv/module.c
@@ -0,0 +1,557 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/bhv_print.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/module.h>
+#include <bhv/patch.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t, const char *);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags, label);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t type,
+				    uint64_t flags, const char *unused2)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3, const char *unused4)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags,
+				  label);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags, label);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags,
+			       const char *label)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, label,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	type &= ~HypABI__Integrity__MemFlags__MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, "INVALID",
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size,
+				HypABI__Integrity__MemType__UNKNOWN,
+				HypABI__Integrity__MemFlags__NONE, "INVALID",
+				_bhv_link_node_op_remove);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0))
+static void bhv_prepare_mod_init(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_INIT_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_INIT_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static void bhv_prepare_mod_core(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_TEXT].base,
+				   mod->mem[MOD_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_RODATA].base,
+				   mod->mem[MOD_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (mod->mem[MOD_RO_AFTER_INIT].size) {
+		bhv_create_section(head, mod->mem[MOD_RO_AFTER_INIT].base,
+				   mod->mem[MOD_RO_AFTER_INIT].size,
+				   HypABI__Integrity__MemType__DATA,
+				   base_flags |
+					   HypABI__Integrity__MemFlags__MUTABLE,
+				   "MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_DATA].base,
+				   mod->mem[MOD_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout,
+				   unsigned long base_flags)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   HypABI__Integrity__MemType__CODE_PATCHABLE,
+			   base_flags, "MODULE TEXT SECTION");
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			HypABI__Integrity__MemType__DATA,
+			base_flags | HypABI__Integrity__MemFlags__MUTABLE,
+			"MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static inline void bhv_prepare_mod_init(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout, base_flags);
+}
+
+static inline void bhv_prepare_mod_core(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->core_layout, base_flags);
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_init(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+
+	bhv_prepare_mod_core(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	/* Prepare the module region's .text section. */
+	if (mod->mem[MOD_INIT_TEXT].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size);
+
+	/* Prepare the module region's .rodata section. */
+	if (mod->mem[MOD_INIT_RODATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size);
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size);
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	const struct module_layout *layout = &mod->init_layout;
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_complete_free_init(const struct module *mod)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	_bhv_complete_free_init(mod, &bhv_region_list_head);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod,
+				     unsigned long base_flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+	void *base = mod->mem[MOD_RO_AFTER_INIT].base;
+	unsigned int size = mod->mem[MOD_RO_AFTER_INIT].size;
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   HypABI__Integrity__MemType__DATA_READ_ONLY,
+			   base_flags);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region.update);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_update_ro_after_init(mod, HypABI__Integrity__MemFlags__TRANSIENT);
+	bhv_complete_free_init(mod);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags,
+			   "BPF SECTION");
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__CODE_PATCHABLE,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+	bhv_add_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT,
+			       ((size + PAGE_SIZE - 1) >> PAGE_SHIFT));
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_rm_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT);
+	bhv_release_memory_by_owner((uint64_t)base);
+}
diff --git security/bhv/patch_alternative.c security/bhv/patch_alternative.c
new file mode 100644
index 0000000000..a59c3a6c24
--- /dev/null
+++ security/bhv/patch_alternative.c
@@ -0,0 +1,239 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+struct mm_struct; // Referenced by the following include
+#include <linux/sync_core.h>
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/patch.h>
+#include <bhv/kversion.h>
+#include <bhv/vault.h>
+
+#include <asm/bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+/**************************************************/
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		pr_err("BHV: %s: Unknown module!\n", __FUNCTION__);
+		rv = -EACCES;
+		goto out;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+out:
+	// Close vault.
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(&search, arch,
+					  bhv_alternatives_find_by_alt);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+// CONFIG_MITIGATION_RETPOLINE is used in 6.12 and CONFIG_RETPOLINE in prev
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) &&     \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+// CONFIG_MITIGATION_RETHUNK is used in 6.12 and CONFIG_RETHUNK in prev
+#if (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK))
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* (defined(CONFIG_MITIGATION_RETHUNK) || defined(CONFIG_RETHUNK)) */
+
+#endif /* (defined(CONFIG_RETPOLINE) || defined(CONFIG_MITIGATION_RETPOLINE)) */
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+/* XXX: CONSIDER MOVING THIS PART INTO TEXT_POKE_EARLY!! */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len)
+{
+	int rc;
+	unsigned long flags;
+
+	/* XXX: Do we still need this? */
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(addr, opcode, len);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+}
+
+#endif
diff --git security/bhv/patch_base.c security/bhv/patch_base.c
new file mode 100644
index 0000000000..9405d5cdbb
--- /dev/null
+++ security/bhv/patch_base.c
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bluerock.io>
+ */
+
+#include <bhv/patch_base.h>
+#include <bhv/vault.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_patch(void *addr, const void *data, size_t len)
+{
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_apply_alternatives(addr, data, len);
+#else // CONFIG_BHV_VAULT_SPACES
+	bhv_patch_hypercall(addr, (uint8_t *)data, len, false);
+#endif // CONFIG_BHV_VAULT_SPACES
+}
\ No newline at end of file
diff --git security/bhv/patch_bpf.c security/bhv/patch_bpf.c
new file mode 100644
index 0000000000..62002b13c2
--- /dev/null
+++ security/bhv/patch_bpf.c
@@ -0,0 +1,216 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com> 
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+void init_xmem(void);
+
+#include <linux/version.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+struct bhv_bpf_code_range {
+	uint64_t start_pfn;
+	size_t num_pages;
+	struct list_head next;
+};
+
+static DEFINE_MUTEX(bhv_bpf_mutex);
+static LIST_HEAD(bhv_bpf_code_ranges_head);
+
+static __always_inline void bhv_bpf_lock(void)
+{
+	mutex_lock(&bhv_bpf_mutex);
+}
+
+static __always_inline void bhv_bpf_unlock(void)
+{
+	mutex_unlock(&bhv_bpf_mutex);
+}
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages)
+{
+	struct bhv_bpf_code_range *n =
+		kzalloc(sizeof(struct bhv_bpf_code_range), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->start_pfn = pfn;
+	n->num_pages = num_pages;
+
+	bhv_bpf_lock();
+	list_add(&(n->next), &bhv_bpf_code_ranges_head);
+	bhv_bpf_unlock();
+}
+
+void bhv_rm_bpf_code_range(uint64_t pfn)
+{
+	struct bhv_bpf_code_range *i, *tmp;
+
+	bhv_bpf_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_bpf_code_ranges_head, next) {
+		if (i->start_pfn == pfn) {
+			list_del(&(i->next));
+			kfree(i);
+			break;
+		}
+	}
+	bhv_bpf_unlock();
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool __bhv_text bhv_bpf_check_write(void *dst, size_t sz)
+{
+	struct bhv_bpf_code_range *i;
+	uint64_t start_pfn = ((uint64_t)dst) >> PAGE_SHIFT;
+	size_t num_pages = ((sz + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	uint64_t end_pfn = (start_pfn + num_pages) - 1;
+
+	list_for_each_entry(i, &bhv_bpf_code_ranges_head, next) {
+		if (start_pfn >= i->start_pfn &&
+		    start_pfn < (i->start_pfn + i->num_pages)) {
+			if (end_pfn >= i->start_pfn &&
+			    end_pfn < (i->start_pfn + i->num_pages))
+				return true;
+		}
+	}
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_write_vault(void *dst, void *src, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall(dst, src, sz);
+#else
+	rv = bhv_patch_hypercall(dst, src, sz, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_write_vault);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_invalidate_vault(void *dst, uint8_t b, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall_memset(dst, sz, b);
+#else
+	rv = bhv_patch_hypercall_memset(dst, sz, b, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_invalidate_vault);
+
+int bhv_bpf_write(void *dst, void *src, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_write_vault(dst, src, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_invalidate_vault(dst, b, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
diff --git security/bhv/patch_jump_label.c security/bhv/patch_jump_label.c
new file mode 100644
index 0000000000..9c41e37735
--- /dev/null
+++ security/bhv/patch_jump_label.c
@@ -0,0 +1,552 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static __always_inline void bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static __always_inline void bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+static enum jump_label_type __bhv_text
+bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+typedef enum {
+	FAIL = 0,
+	SKIP,
+	SUCCESS,
+} jump_label_validation_t;
+
+static jump_label_validation_t __bhv_text
+validate_jmp_labels(struct jump_entry *entry, const void *opcode, size_t len,
+		    char *error_msg)
+{
+	struct bhv_static_key_mod *i;
+	unsigned long addr = (unsigned long)jump_entry_code(entry);
+	struct jump_entry *iter;
+	unsigned long tmp_addr;
+
+	if (entry >= __start___jump_table && entry < __stop___jump_table) {
+		// We should only modify the kernel text section
+		if (!kernel_text_address(addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				addr);
+			return FAIL;
+		}
+
+		if (!bhv_jump_label_validate_opcode(
+			    entry, bhv_jump_label_type(entry), opcode, len)) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Jump label opcode validation failed");
+			return FAIL;
+		}
+
+		// Search for overlapping jump labels.
+		for (iter = __start___jump_table; iter < __stop___jump_table;
+		     iter++) {
+			if (iter == entry)
+				continue;
+
+			tmp_addr = (unsigned long)jump_entry_code(iter);
+
+			if (addr <= tmp_addr && tmp_addr < addr + len) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label (0x%lx) overlaps with jump label (0x%lx)",
+					(unsigned long)(entry),
+					(unsigned long)(iter));
+				return FAIL;
+			}
+		}
+
+		return SUCCESS;
+	}
+
+	list_for_each_entry(i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			struct module *tmp;
+
+			if (i->mod->state != MODULE_STATE_COMING &&
+			    i->mod->state != MODULE_STATE_LIVE) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Invalid module state for jump label (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			if ((jump_entry_is_init(entry) &&
+			     i->mod->state != MODULE_STATE_COMING)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Trying to apply jump label in incorrect module state (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			// Module entries should only point to the text section
+			// of the module
+			// Note: We use the kernel API to check this. We could perform
+			// the check manually but using the kernel API seems more stable.
+			tmp = __module_text_address(addr);
+			if (tmp == NULL) {
+				// No module found
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label trying to write address outside of module (0x%lx)",
+					addr);
+				return FAIL;
+			}
+
+			if (tmp != i->mod) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label (0x%lx<->0x%lx)",
+					(unsigned long)tmp,
+					(unsigned long)i->mod);
+				return FAIL;
+			}
+
+			if (!bhv_jump_label_validate_opcode(
+				    entry, bhv_jump_label_type(entry), opcode,
+				    len)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label");
+				return FAIL;
+			}
+
+			// Search for overlapping jump labels.
+			for (iter = i->entries_start; iter < i->entries_stop;
+			     iter++) {
+				if (iter == entry)
+					continue;
+
+				tmp_addr = (unsigned long)jump_entry_code(iter);
+
+				if (addr <= tmp_addr && tmp_addr < addr + len) {
+					snprintf(
+						error_msg,
+						HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+						"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+						(unsigned long)entry,
+						(unsigned long)iter);
+					return FAIL;
+				}
+			}
+
+			return SUCCESS;
+		}
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+	return FAIL;
+}
+
+static int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+						 const void *opcode, size_t len)
+{
+	int rv = 0;
+	unsigned long r;
+	jump_label_validation_t validation_ok;
+	void *dest_virt_addr = (void *)jump_entry_code(entry);
+	char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return -E2BIG;
+	}
+
+	validation_ok = validate_jmp_labels(entry, opcode, len, message);
+	if (validation_ok == SKIP) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return 0;
+	}
+
+	if (validation_ok == FAIL) {
+		if (bhv_patch_violation_hypercall(dest_virt_addr, message)) {
+			// Block this patch
+			if (HypABI__Richard__Close__hypercall())
+				pr_err("%s: could not close vault",
+				       __FUNCTION__);
+			return -EACCES;
+		}
+
+		// The violation should only be logged. Thus we continue.
+	}
+
+	r = bhv_patch_hypercall(dest_virt_addr, opcode, (uint64_t)len, true);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+	return rv;
+}
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool has_overlapping_entries(struct jump_entry *entry,
+				    struct jump_entry *start,
+				    struct jump_entry *stop, struct module *mod,
+				    size_t op_size,
+				    char *error_msg)
+{
+	unsigned long addr = jump_entry_code(entry);
+	struct jump_entry *iter = NULL;
+	struct module *tmp_mod;
+
+	/*
+        * Module entries should only point to the text section of the module.
+        */
+	preempt_disable();
+	tmp_mod = __module_text_address(addr);
+	preempt_enable();
+
+	if (tmp_mod == NULL) {
+		snprintf(
+			error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			"Jump label trying to write address outside of module (0x%lx | entry @ 0x%px)",
+			addr, entry);
+		return false;
+	}
+
+	if (tmp_mod != mod) {
+		snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			 "Found invalid module for jump label (0x%lx<->0x%lx)",
+			 (unsigned long)tmp_mod, (unsigned long)mod);
+		return false;
+	}
+
+	/* Disallow overlapping entries. */
+	for (iter = start; iter < stop; iter++) {
+		unsigned long tmp_addr;
+		int tmp_size;
+
+		if (iter == entry)
+			continue;
+
+		tmp_addr = (unsigned long)jump_entry_code(iter);
+#ifdef JUMP_LABEL_NOP_SIZE
+		tmp_size = JUMP_LABEL_NOP_SIZE;
+#else
+		arch_jump_entry_size(entry);
+#endif
+
+		if ((addr <= tmp_addr && tmp_addr < addr + op_size) ||
+		    (addr >= tmp_addr && addr < tmp_addr + tmp_size)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+				(unsigned long)entry, (unsigned long)iter);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_mod(struct jump_entry *entry,
+					size_t op_size,
+					char *error_msg)
+{
+	struct static_key_mod *m;
+	struct static_key_mod *jlm;
+
+	struct jump_entry *iter_start;
+	struct jump_entry *iter_stop;
+
+	struct static_key *key = jump_entry_key(entry);
+
+	if (!static_key_linked(key)) {
+		struct module *mod;
+
+		preempt_disable();
+		mod = __module_address((unsigned long)key);
+		if (mod) {
+			iter_start = mod->jump_entries;
+			iter_stop = mod->jump_entries + mod->num_jump_entries;
+		}
+		preempt_enable();
+
+		if (!mod) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Unknown module jump label (%lx)",
+				 (unsigned long)entry);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	jlm = static_key_mod(jump_entry_key(entry));
+
+	//pr_info("%s:%d\n", __FUNCTION__, __LINE__);
+
+	for (m = jlm; m; m = m->next) {
+		struct module *mod = NULL;
+
+		if (!m->entries)
+			continue;
+
+		mod = m->mod;
+
+		if (!mod)
+			continue;
+
+		//pr_info("%s:%d | %s addr @ 0x%lx\n", __FUNCTION__, __LINE__, mod->name, addr);
+
+		iter_start = mod->jump_entries;
+		iter_stop = iter_start + mod->num_jump_entries;
+
+		/* Continue if we did not find the correct jump label table. */
+		if (entry < iter_start || entry >= iter_stop)
+			continue;
+
+		if (mod->state != MODULE_STATE_COMING &&
+		    mod->state != MODULE_STATE_LIVE) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Invalid module state for jump label (%u)",
+				 mod->state);
+			return false;
+		}
+
+		if (jump_entry_is_init(entry) &&
+		    mod->state != MODULE_STATE_COMING) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Trying to apply jump label in incorrect module state (%u)",
+				mod->state);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_kern(struct jump_entry *entry,
+					 char *error_msg)
+{
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (entry >= __start___jump_table && entry < __stop___jump_table) {
+		/*
+                 * Ensure that this jump label only modifies the kernel .text
+                 * segment.
+                 */
+		if (!__kernel_text_address((unsigned long)dest_addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				(unsigned long)dest_addr);
+			return false;
+		}
+
+		/*
+                 * Note: we do not search for overlapping target addresses of
+                 * jump labels that are part of the kernel image; the section
+                 * holding the jump labels cannot be overwritten from the
+                 * outside of the vault.
+                 */
+
+		//pr_info("%s:%d | kern addr @ 0x%px\n", __FUNCTION__, __LINE__, dest_addr);
+
+		return true;
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Jump label destination (0x%lx) outside of kernel text region",
+		 (unsigned long)dest_addr);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int bhv_validate_jump_label_entry(struct jump_entry *entry,
+					 const void *opcode, size_t op_size)
+{
+	bool is_valid = false;
+	HypABI__Patch__PatchViolation__arg__T violation_arg;
+	char *error_msg_ptr = violation_arg.message;
+
+	if (entry >= __start___jump_table && entry < __stop___jump_table) {
+		is_valid = bhv_validate_jump_label_kern(entry, error_msg_ptr);
+	} else {
+		is_valid = bhv_validate_jump_label_mod(entry, op_size, error_msg_ptr);
+	}
+
+	if (!is_valid) {
+		if (bhv_patch_violation_hypercall(
+			    (void *)jump_entry_code(entry), error_msg_ptr)) {
+			return -EACCES;
+		}
+
+		dump_stack();
+		BUG();
+
+		/* XXX: Shall we continue without reporting an issue? */
+		return -1;
+	}
+
+	return 0;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t op_size)
+{
+	int rc = 0;
+	unsigned long flags;
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	if (op_size > HypABI__Patch__PatchViolation__MAX_MSG_SZ) {
+		return -E2BIG;
+	}
+
+	rc = bhv_validate_jump_label_entry(entry, opcode, op_size);
+	if (rc) {
+		dump_stack();
+		BUG();
+
+		return rc;
+	}
+
+	//pr_info("[BHV] Patching addr @ 0x%px with %llx (size=%d)\n", dest_addr, *((uint64_t *)opcode), op_size);
+
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(dest_addr, opcode, (uint64_t)op_size);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git security/bhv/reg_protect.c security/bhv/reg_protect.c
new file mode 100644
index 0000000000..5cae2a0da5
--- /dev/null
+++ security/bhv/reg_protect.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield)
+{
+	return HypABI__RegisterProtection__Freeze__HYPERCALL(
+			.register_selector = reg_selector,
+			.freeze_bitfield = freeze_bitfield);
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void)
+{
+	bhv_start_reg_protect_arch();
+}
+/***************************************************/
diff --git security/bhv/reverse_shell_detection.c security/bhv/reverse_shell_detection.c
new file mode 100644
index 0000000000..35e885d2f7
--- /dev/null
+++ security/bhv/reverse_shell_detection.c
@@ -0,0 +1,633 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024-2025 - BlueRock Security Inc.
+ * Authors:  Sebastian Vogl <sebastian@bluerock.io>
+ * 
+ */
+
+#include <linux/binfmts.h>
+#include <linux/file.h>
+#include <linux/wait.h> //needs to be before pipe_fs_i on 6.12
+#include <linux/pipe_fs_i.h>
+#include <linux/sched/signal.h>
+#include <linux/fdtable.h>
+#include <net/sock.h>
+#include <net/af_unix.h>
+
+#include <bhv/reverse_shell_detection.h>
+#include <bhv/guestlog.h>
+#include <bhv/util.h>
+#include <bhv/config.h>
+#include <bhv/context.h>
+
+#define STDIN 0
+#define STDOUT 1
+#define STDERR 2
+
+/***********************************************************************
+ * Hypercalls
+ ***********************************************************************/
+static inline bool rsd_process_interpreter_bound(struct brs_policy *policy,
+						 pid_t interpreter_pid,
+						 const char *interpreter_path,
+						 uint32_t socket_fd,
+						 struct file *socket)
+{
+	int r = 0;
+
+	if (BRS_EVT_ENABLED(&policy->flast, interpreter_bound)) {
+		r = brs_guestlog_log_reverse_shell_detection_interpreter_bound(
+			policy, interpreter_pid, interpreter_path, socket_fd,
+			socket);
+	}
+
+	return r;
+}
+
+typedef struct {
+	struct brs_policy *policy;
+	struct task_struct *interpreter; // The interpreter with the pipe.
+	uint32_t interpreter_fd;
+	const char *interpreter_path; // Used when the path of the interpreter
+	// different then in the task struct.
+	struct task_struct *transitive; // The process containing the socket.
+	uint32_t transitive_sock_fd;
+	uint32_t transitive_pipe_fd;
+	struct sockaddr *address;
+} transitive_info;
+
+static inline bool rsd_process_interpreter_transitive(transitive_info *info)
+{
+	int r = 0;
+
+	if (BRS_EVT_ENABLED(&info->policy->flast,
+			    interpreter_bound_transitive)) {
+		r = brs_guestlog_log_reverse_shell_detection_interpreter_transitive(
+			info->policy, info->interpreter, info->interpreter_fd,
+			info->interpreter_path, info->transitive,
+			info->transitive_pipe_fd, info->transitive_sock_fd,
+			info->address);
+	}
+
+	return r;
+}
+/***********************************************************************/
+
+static inline bool is_interpreter(struct brs_policy *policy,
+				  struct task_struct *task)
+{
+	char *buf;
+	const char *path;
+	bool rv;
+
+	if (!task || !task->active_mm || !task->active_mm->exe_file)
+		return false;
+
+	// Check for interpreter
+	buf = brs_aa_kmalloc(PATH_MAX);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+		return false;
+	}
+
+	path = brs_get_file_path(task->active_mm->exe_file, buf, PATH_MAX);
+	rv = brs_policy_reverse_shell_is_interpreter(policy, path);
+
+	kfree(buf);
+	return rv;
+}
+
+static inline bool is_socket(struct file *f)
+{
+	if (f == NULL || f->f_path.dentry == NULL ||
+	    f->f_path.dentry->d_inode == NULL)
+		return false;
+
+	return S_ISSOCK(f->f_path.dentry->d_inode->i_mode);
+}
+
+static inline bool is_remote_socket(struct file *f)
+{
+	struct socket *sock;
+
+	if (!is_socket(f))
+		return false;
+
+	sock = sock_from_file(f);
+	if (unlikely(sock == NULL || sock->sk == NULL))
+		return false;
+
+	// Only consider INET sockets
+	if (sock->sk->sk_family != AF_INET && sock->sk->sk_family != AF_INET6) {
+		return false;
+	}
+
+	return true;
+}
+
+static inline bool is_domain_socket(struct file *f)
+{
+	struct socket *sock;
+	if (!is_socket(f))
+		return false;
+
+	sock = sock_from_file(f);
+	if (unlikely(sock == NULL || sock->sk == NULL))
+		return false;
+
+	// Only consider domain sockets
+	return sock->sk->sk_family == AF_UNIX;
+}
+
+static inline bool domain_sockets_connected(struct file *f, struct file *f2)
+{
+	struct socket *sock1;
+	struct socket *sock2;
+
+	if (!is_socket(f) || !is_socket(f2))
+		return false;
+
+	sock1 = sock_from_file(f);
+	sock2 = sock_from_file(f2);
+	if (unlikely(sock1 == NULL || sock2 == NULL || sock1->sk == NULL ||
+		     sock2->sk == NULL))
+		return false;
+
+	// Only consider domain sockets
+	if (unix_peer(sock1->sk) == sock2->sk) {
+		return true;
+	}
+
+	return false;
+}
+
+static inline bool is_standard_fd(uint32_t fd)
+{
+	if (fd >= STDIN && fd <= STDERR)
+		return true;
+
+	return false;
+}
+
+static bool _brs_reverse_shell_find_fd(struct task_struct *task,
+				       bool remote_socket, bool domain_socket,
+				       bool pipe, uint32_t start_fd,
+				       uint32_t end_fd, struct file *target,
+				       uint32_t *out)
+{
+	struct files_struct *files;
+	struct file *cur;
+	bool rv = false;
+	uint32_t i = start_fd;
+
+	task_lock(task);
+	files = task->files;
+	if (!files)
+		goto out;
+
+	while ((cur = files_lookup_fd_raw(files, i)) && i <= end_fd) {
+		if (target) {
+			if (pipe && brs_is_pipe(cur) &&
+			    cur->private_data == target->private_data) {
+				goto found;
+			}
+
+			if (domain_socket && is_domain_socket(target) &&
+			    is_domain_socket(cur) &&
+			    domain_sockets_connected(target, cur)) {
+				goto found;
+			}
+		} else {
+			if (remote_socket && is_remote_socket(cur)) {
+				goto found;
+			} else if (domain_socket && is_domain_socket(cur)) {
+				goto found;
+			} else if (pipe && brs_is_pipe(cur)) {
+				goto found;
+			}
+		}
+
+		i++;
+	}
+
+	// no match
+	goto out;
+
+found:
+	rv = true;
+	*out = i;
+
+out:
+	task_unlock(task);
+	return rv;
+}
+
+static bool _brs_reverse_shell_has_remote_socket(struct task_struct *task,
+						 uint32_t start_fd,
+						 uint32_t *out)
+{
+	return _brs_reverse_shell_find_fd(task, true, false, false, start_fd,
+					  0xffffffff, NULL, out);
+}
+
+static bool
+_brs_reverse_shell_has_pipe_or_domain_socket(struct task_struct *task,
+					     uint32_t start_fd, uint32_t end_fd,
+					     uint32_t *out)
+{
+	return _brs_reverse_shell_find_fd(task, false, true, true, start_fd,
+					  end_fd, NULL, out);
+}
+
+static bool _brs_reverse_shell_find_pipe_or_domain_socket(
+	struct task_struct *task, uint32_t start_fd, uint32_t end_fd,
+	struct file *f, uint32_t *out)
+{
+	return _brs_reverse_shell_find_fd(task, false, true, true, start_fd,
+					  end_fd, f, out);
+}
+
+typedef struct {
+	struct task_struct *task;
+	struct list_head list;
+} task_skip_list;
+
+static inline bool _brs_reverse_shell_in_skip_list(struct task_struct *task,
+						   struct list_head *skip_list)
+{
+	task_skip_list *iter;
+
+	list_for_each_entry (iter, skip_list, list) {
+		if (task == iter->task) {
+			return true;
+		}
+	}
+
+	return false;
+}
+
+static int brs_reverse_shell_find_transitive_helper(
+	transitive_info *info, uint32_t current_fd, struct file *current_file,
+	struct task_struct *skip, struct list_head *skip_list, bool *event)
+{
+	struct task_struct *cur;
+	uint32_t transitive_fd;
+	uint32_t tmp_fd;
+	int rv = 0;
+
+	task_skip_list *skip_entry;
+
+	if (!current_file)
+		return 0;
+
+	skip_entry = brs_aa_kmalloc(sizeof(task_skip_list));
+	if (skip_entry == NULL) {
+		pr_err("Could not allocate memory for skip list!\n");
+		return -EPERM;
+	}
+	skip_entry->task = skip;
+	list_add_tail(&skip_entry->list, skip_list);
+
+	for_each_process (cur) {
+		if (!pid_alive(cur))
+			continue;
+
+		// Ignore pid 1
+		if (cur->pid == 1)
+			continue;
+
+		// Skip kernel threads
+		if ((cur->flags & PF_KTHREAD))
+			continue;
+
+		// Did we already consider this process?
+		if (_brs_reverse_shell_in_skip_list(cur, skip_list))
+			continue;
+
+		// Find the current pipe we are looking for.
+		// If the current FD is stdin, we are looking for stdout and vice versa.
+		if (_brs_reverse_shell_find_pipe_or_domain_socket(
+			    cur, current_fd == STDIN ? STDOUT : STDIN,
+			    /*current_fd == STDIN ? STDERR : STDIN*/ 0xffffffff,
+			    current_file, &transitive_fd)) {
+			// We found a matching file.
+			// Do we already have an interpreter?
+			if (info->interpreter &&
+			    _brs_reverse_shell_has_remote_socket(cur, 0,
+								 &tmp_fd)) {
+				// This is a transitive shell!
+				info->transitive = cur;
+				info->transitive_pipe_fd = transitive_fd;
+				info->transitive_sock_fd = tmp_fd;
+				*event = true;
+				rv = 0;
+				goto out;
+			} else if (!info->interpreter &&
+				   is_standard_fd(transitive_fd) &&
+				   is_interpreter(info->policy, cur)) {
+				// An interpreter is the endpoint of the pipe. This is also a transitive shell!
+				info->interpreter = cur;
+				info->interpreter_path = NULL;
+				info->interpreter_fd = transitive_fd;
+				*event = true;
+				rv = 0;
+				goto out;
+			} else {
+				// This is not a transitive shell, lets look for other pipes to follow.
+				uint32_t i =
+					current_fd == STDIN ? STDOUT : STDIN;
+				struct file *tmp_file;
+				while (_brs_reverse_shell_has_pipe_or_domain_socket(
+					cur, i,
+					/*current_fd == STDIN ? STDERR : STDIN*/
+					0xffffffff, &tmp_fd)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+					tmp_file = task_lookup_fdget_rcu(
+						cur, tmp_fd);
+#else
+					tmp_file =
+						task_lookup_fd_rcu(cur, tmp_fd);
+#endif
+					rv = brs_reverse_shell_find_transitive_helper(
+						info, tmp_fd, tmp_file, cur,
+						skip_list, event);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+					if (tmp_file)
+						fput(tmp_file);
+#endif
+
+					// We found something in our extended search or there was an error
+					if (rv || *event) {
+						goto out;
+					}
+
+					// Search for the next
+					i = tmp_fd + 1;
+				}
+			}
+
+			// We have not found a reverse shell.
+			rv = 0;
+			goto out;
+		}
+	}
+
+out:
+	// Skip list will be cleaned by the caller.
+	return rv;
+}
+
+static int brs_reverse_shell_find_transitive(transitive_info *info,
+					     uint32_t current_fd,
+					     struct file *current_file,
+					     struct task_struct *skip)
+{
+	int rv = 0;
+	bool event = false;
+	LIST_HEAD(skip_list);
+	task_skip_list *iter;
+	task_skip_list *tmp;
+
+	rcu_read_lock();
+	read_lock(&tasklist_lock);
+	rv = brs_reverse_shell_find_transitive_helper(
+		info, current_fd, current_file, skip, &skip_list, &event);
+	read_unlock(&tasklist_lock);
+	rcu_read_unlock();
+
+	// Clear list
+	list_for_each_entry_safe (iter, tmp, &skip_list, list) {
+		list_del(&iter->list);
+		kfree(iter);
+	}
+
+	// Check for errors
+	if (rv) {
+		pr_warn("An error occurred during transitive shell detection. Blocking.\n");
+		return rv;
+	}
+
+	// Check for event
+	if (event) {
+		rv = rsd_process_interpreter_transitive(info);
+	}
+
+	return rv;
+}
+
+int brs_reverse_shell_fd_dup(struct brs_policy *policy, unsigned int fd,
+			     struct file *file)
+{
+	bool socket = false;
+	bool pipe = false;
+	const char *path;
+	char *buf;
+	int rv = 0;
+	bool container_host;
+
+	if (!brs_reverse_shell_detetection_is_enabled(policy))
+		return 0;
+
+	// We are only considering stdin, stdout, and stderr
+	if (!is_standard_fd(fd))
+		return 0;
+
+	container_host = brs__is__ReverseShellDetection__container_host();
+	// If host is false, we only consider container.
+	if (!container_host && !brs_task_in_container(current)) {
+		return 0;
+	}
+
+	// Check if this file is a socket or pipe.
+	if (BRS_EVT_ENABLED(&policy->flast, interpreter_bound)) {
+		socket = is_remote_socket(file);
+	}
+
+	if (BRS_EVT_ENABLED(&policy->flast, interpreter_bound_transitive)) {
+		pipe = brs_is_pipe(file) || is_domain_socket(file);
+	}
+
+	if (!socket && !pipe)
+		return 0;
+
+	if (!current->active_mm || !current->active_mm->exe_file) {
+		pr_warn("Unknown binary!\n");
+		return 0;
+	}
+
+	// Check for interpreter
+	buf = brs_aa_kmalloc(PATH_MAX);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+		return -ENOMEM;
+	}
+
+	path = brs_get_file_path(current->active_mm->exe_file, buf, PATH_MAX);
+	if (!brs_policy_reverse_shell_is_interpreter(policy, path)) {
+		rv = 0;
+		goto out;
+	}
+
+	if (socket) {
+		if ((rv = rsd_process_interpreter_bound(policy, current->tgid,
+							path, fd, file))) {
+			goto out;
+		}
+	} else if (pipe) {
+		// Check for transitive reverse shells
+		transitive_info info;
+		info.policy = policy;
+		info.interpreter = current;
+		info.interpreter_path = NULL;
+		info.interpreter_fd = fd;
+		info.address = NULL;
+		rv = brs_reverse_shell_find_transitive(&info, fd, file,
+						       current);
+	}
+
+out:
+	kfree(buf);
+	return rv;
+}
+
+int brs_reverse_shell_detection_socket_connect(struct brs_policy *policy,
+					       struct socket *sock,
+					       struct sockaddr *address,
+					       int addrlen)
+{
+	uint32_t tmp_fd;
+	uint32_t i;
+	int rv;
+	struct file *file;
+	transitive_info info;
+	bool container_host;
+
+	if (!BRS_EVT_ENABLED(&policy->flast, interpreter_bound_transitive))
+		return 0;
+
+	container_host = brs__is__ReverseShellDetection__container_host();
+	// If host is false, we only consider container.
+	if (!container_host && !brs_task_in_container(current)) {
+		return 0;
+	}
+
+	if (address->sa_family == AF_UNIX || address->sa_family == AF_LOCAL)
+		return 0;
+
+	info.policy = policy;
+	info.transitive = current;
+	info.address = address;
+	info.interpreter = NULL;
+
+	i = STDIN;
+	while (_brs_reverse_shell_has_remote_socket(current, i, &tmp_fd)) {
+		int err = 0;
+		struct socket *tmp_sock = sockfd_lookup(tmp_fd, &err);
+
+		if (err) {
+			pr_err("Could not find socket!\n");
+			return 0;
+		}
+
+		if (tmp_sock == sock) {
+			info.transitive_sock_fd = tmp_fd;
+			break;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	i = STDIN;
+	while (_brs_reverse_shell_has_pipe_or_domain_socket(current, i, STDERR,
+							    &tmp_fd)) {
+		info.transitive_pipe_fd = tmp_fd;
+		file = fget_task(current, tmp_fd);
+
+		rv = brs_reverse_shell_find_transitive(&info, tmp_fd, file,
+						       current);
+
+		fput(file);
+
+		if (rv) {
+			return rv;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	return 0;
+}
+
+int brs_reverse_shell_exec(struct brs_policy *policy, struct linux_binprm *bprm,
+			   const char *path)
+{
+	uint32_t tmp_fd;
+	uint32_t i;
+	int rv;
+	struct file *file;
+	transitive_info info;
+	bool container_host;
+
+	if (!brs_reverse_shell_detetection_is_enabled(policy))
+		return 0;
+
+	container_host = brs__is__ReverseShellDetection__container_host();
+	// If host is false, we only consider container.
+	if (!container_host && !brs_task_in_container(current)) {
+		return 0;
+	}
+
+	if (!brs_policy_reverse_shell_is_interpreter(policy, path)) {
+		return 0;
+	}
+
+	i = STDIN;
+	while (_brs_reverse_shell_has_remote_socket(current, i, &tmp_fd)) {
+		// Only consider standard FDs for interpreters
+		if (!is_standard_fd(tmp_fd))
+			break;
+
+		file = fget_task(current, tmp_fd);
+		rv = rsd_process_interpreter_bound(policy, current->tgid, path,
+						   tmp_fd, file);
+		fput(file);
+
+		if (rv) {
+			return rv;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	if (!BRS_EVT_ENABLED(&policy->flast, interpreter_bound_transitive))
+		return 0;
+
+	i = STDIN;
+	while (_brs_reverse_shell_has_pipe_or_domain_socket(current, i, STDERR,
+							    &tmp_fd)) {
+		// Only consider standard FDs for interpreters
+		if (!is_standard_fd(tmp_fd))
+			break;
+
+		info.policy = policy;
+		info.interpreter = current;
+		info.interpreter_path = path;
+		info.interpreter_fd = i;
+		info.address = NULL;
+		file = fget_task(current, tmp_fd);
+
+		rv = brs_reverse_shell_find_transitive(&info, tmp_fd, file,
+						       current);
+
+		fput(file);
+
+		if (rv) {
+			return rv;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	return 0;
+}
diff --git security/bhv/sysfs.c security/bhv/sysfs.c
new file mode 100644
index 0000000000..92a1cbbecc
--- /dev/null
+++ security/bhv/sysfs.c
@@ -0,0 +1,69 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef CONFIG_SYSFS
+#error CONFIG_SYSFS required!
+#endif
+
+#include <linux/kobject.h>
+
+#include <bhv/bhv.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/sysfs_reg_protect.h>
+#include <bhv/sysfs_version.h>
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_sysfs(void)
+{
+	struct kobject *bhv_kobj;
+	struct kobject *version_kobj;
+	struct kobject *integrity_kobj;
+	struct kobject *integrity_freeze_kobj;
+	struct kobject *register_kobj;
+	struct kobject *register_freeze_kobj;
+	struct kobject *fops_kobj;
+	struct kobject *fopsstatus_kobj;
+
+#define CREATE_KOBJ(kobj, name, parent)                                        \
+	kobj = kobject_create_and_add(name, parent);                           \
+	BUG_ON(!kobj);
+
+#ifndef CONFIG_SYS_HYPERVISOR
+	struct kobject *hypervisor_kobj;
+	CREATE_KOBJ(hypervisor_kobj, "hypervisor", NULL);
+#endif // CONFIG_SYS_HYPERVISOR
+
+	CREATE_KOBJ(bhv_kobj, "bhv", hypervisor_kobj);
+	
+	CREATE_KOBJ(version_kobj, "version", bhv_kobj);
+	bhv_start_sysfs_version(version_kobj);
+
+	CREATE_KOBJ(integrity_kobj, "integrity", bhv_kobj);
+	CREATE_KOBJ(integrity_freeze_kobj, "freeze", integrity_kobj);
+	bhv_start_sysfs_integrity_freeze(integrity_freeze_kobj);
+
+	if (bhv_reg_protect_is_enabled()) {
+		CREATE_KOBJ(register_kobj, "register", bhv_kobj);
+		CREATE_KOBJ(register_freeze_kobj, "freeze", register_kobj);
+
+		bhv_start_sysfs_reg_protect(register_freeze_kobj);
+	}
+
+	if (bhv_fileops_file_protection_is_enabled()) {
+		CREATE_KOBJ(fops_kobj, "fileops_protection", bhv_kobj);
+		CREATE_KOBJ(fopsstatus_kobj, "status", fops_kobj);
+
+		bhv_start_sysfs_fileops_protection(fops_kobj, fopsstatus_kobj);
+	}
+}
+/**********************************************************/
diff --git security/bhv/sysfs_fops.c security/bhv/sysfs_fops.c
new file mode 100644
index 0000000000..e8a9bb1a05
--- /dev/null
+++ security/bhv/sysfs_fops.c
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kobject.h>
+#include <linux/printk.h>
+#include <asm-generic/set_memory.h>
+
+#include <bhv/bhv.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/module.h>
+#include <bhv/sysfs_fops.h>
+
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf);
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf);
+
+static struct bhv_fopsstatus_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint8_t param;
+} const __section(".rodata") _bhv_fopsstatus_sysfs_data[] = {
+#define FOPS_MAP(name, idx, _, __)                                             \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected,     \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FOPS_MAP_DIRNULL(name, idx, _)                                         \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected_dn,  \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FILEOPS_INTERNAL_FOPSMAP_ALL
+#include <bhv/fileops_internal_fopsmap.h>
+};
+#define attr_to_bsd(ptr)                                                       \
+	container_of((ptr), struct bhv_fopsstatus_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c %c\n",
+			 fileops_map[data->param][0] ? '1' : '0',
+			 fileops_map[data->param][1] ? '1' : '0');
+}
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n",
+			 fileops_map[data->param][0] ? '1' : '0');
+}
+#undef attr_to_bsd
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_fileops_protection(struct kobject *fops,
+						  struct kobject *status)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsstatus_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsstatus_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(status, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_integrity_freeze.c security/bhv/sysfs_integrity_freeze.c
new file mode 100644
index 0000000000..388352ea4d
--- /dev/null
+++ security/bhv/sysfs_integrity_freeze.c
@@ -0,0 +1,103 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/sysfs_integrity_freeze.h>
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count);
+
+static struct bhv_intfr_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t param;
+	const bool *const flagp;
+} const __section(".rodata") _bhv_intfr_sysfs_data[] = {
+	{ .attr = __ATTR(create, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__CREATE,
+	  .flagp = &bhv_integrity_freeze_create_currently_frozen },
+	{ .attr = __ATTR(update, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__UPDATE,
+	  .flagp = &bhv_integrity_freeze_update_currently_frozen },
+	{ .attr = __ATTR(remove, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+	  .flagp = &bhv_integrity_freeze_remove_currently_frozen },
+	{ .attr = __ATTR(patch, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__PATCH,
+	  .flagp = &bhv_integrity_freeze_patch_currently_frozen },
+};
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_intfr_sysfs_data, attr)
+#define cur_flag_val(bisdp) (*((bisdp)->flagp) ? '1' : '0')
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n", cur_flag_val(data));
+}
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1' || buf[0] == '2')) {
+		struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+		if (cur_flag_val(data) == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((cur_flag_val(data) == '0') &&
+			   (buf[0] == '1' || buf[0] == '2')) {
+			int ret = bhv_enable_integrity_freeze_flag(
+				data->param, buf[0] == '2');
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_integrity_freeze(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_intfr_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_intfr_sysfs_data); ++i)
+		attr_array[i] = &_bhv_intfr_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_reg_protect.c security/bhv/sysfs_reg_protect.c
new file mode 100644
index 0000000000..2b9da7f03d
--- /dev/null
+++ security/bhv/sysfs_reg_protect.c
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs_reg_protect.h>
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count);
+
+static struct bhv_rp_sysfs_data {
+	const struct kobj_attribute attr;
+	const enum HypABI__RegisterProtection__Freeze__RegisterSelector reg;
+} const __section(".rodata") _bhv_rp_sysfs_data[] = {
+#define OP(regn)                                                               \
+	{ .attr = __ATTR(regn, 0640, _bhv_rp_sysfs_show, _bhv_rp_sysfs_store), \
+	  .reg = HypABI__RegisterProtection__Freeze__RegisterSelector__##regn },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+static struct bhv_rp_sysfs_data_var {
+	uint64_t curr_status;
+} _bhv_rp_sysfs_data_var[] = {
+#define OP(regn) { .curr_status = 0UL },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+static_assert(ARRAY_SIZE(_bhv_rp_sysfs_data) ==
+	      ARRAY_SIZE(_bhv_rp_sysfs_data_var));
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_rp_sysfs_data, attr)
+#define attr_to_bsd_var(ptr) \
+	_bhv_rp_sysfs_data_var + (attr_to_bsd(ptr) - _bhv_rp_sysfs_data)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static DEFINE_MUTEX(_bhv_rp_sysfs_lock);
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int b;
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	b = scnprintf(buf, PAGE_SIZE, "%016llx\n", data_var->curr_status);
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+	return b;
+}
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	uint64_t nval;
+	int rv = 0;
+	int i = sscanf(buf, "%llx", &nval);
+	if (i != 1) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	nval |= data_var->curr_status;
+
+	rv = bhv_reg_protect_freeze(data->reg, nval);
+
+	if (!rv)
+		data_var->curr_status = nval;
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+
+	if (rv < 0) {
+		return rv;
+	} else {
+		return count;
+	}
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_reg_protect(struct kobject *kobj)
+{
+	int i;
+	const struct attribute *attr_array
+		[1 +
+		 HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_rp_sysfs_data); ++i)
+		attr_array[i] = &_bhv_rp_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_version.c security/bhv/sysfs_version.c
new file mode 100644
index 0000000000..8129f4cabf
--- /dev/null
+++ security/bhv/sysfs_version.c
@@ -0,0 +1,70 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_version.h>
+
+#define __BHV_VERSION(a, b, c) a, b, c
+#define __BHV_VAS_ABI_VERSION(a, b, c, d) a, b, c, d
+#include <bhv/version.h>
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf);
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf);
+
+static struct bhv_version_sysfs_data {
+	const struct kobj_attribute attr;
+} const __section(".rodata") _bhv_version_sysfs_data[] = {
+	{
+		.attr = __ATTR(bhv_version, 0600,
+			       _bhv_version_sysfs_show_bhv_version, NULL),
+	},
+	{
+		.attr = __ATTR(bhv_abi_version, 0600,
+			       _bhv_version_sysfs_show_bhv_abi_version, NULL),
+	},
+};
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02d.%02d.%d\n", BHV_VERSION);
+}
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02u.%02u.%02u:%05u\n", HypABI__ABI_VERSION);
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_version(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_version_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_version_sysfs_data); ++i)
+		attr_array[i] = &_bhv_version_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/task.h security/bhv/task.h
new file mode 100644
index 0000000000..1acb24fcb8
--- /dev/null
+++ security/bhv/task.h
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bluerock.io>
+ */
+
+#ifndef __SECURITY_BHV_TASK_H__
+#define __SECURITY_BHV_TASK_H__
+
+#include <linux/lsm_hooks.h>
+#include <linux/sched.h>
+
+extern struct lsm_blob_sizes brs_blob_sizes;
+
+#ifdef CONFIG_BRS
+struct bhv_vas { 
+	volatile unsigned long cap_raised;	// bit mask of raised capabilities
+};
+#endif
+
+struct brs_task_security {
+#ifdef CONFIG_BRS
+	struct bhv_vas vas;
+#endif
+};
+
+static inline struct brs_task_security *
+brs_task(const struct task_struct *const task) {
+	return task->security + brs_blob_sizes.lbs_task;
+}
+
+#endif /* __SECURITY_BHV_TASK_H__ */
diff --git security/bhv/vmalloc_to_page.c security/bhv/vmalloc_to_page.c
new file mode 100644
index 0000000000..4237ee1114
--- /dev/null
+++ security/bhv/vmalloc_to_page.c
@@ -0,0 +1,78 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copied from kernel v6.1, mm/vmalloc.c
+/*
+ *  Copyright (C) 1993  Linus Torvalds
+ *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
+ *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
+ *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002
+ *  Numa awareness, Christoph Lameter, SGI, June 2005
+ *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <bhv/vault.h>
+
+
+/*
+ * Walk a vmap address to the struct page it maps. Huge vmap mappings will
+ * return the tail page that corresponds to the base page address, which
+ * matches small vmap mappings.
+ */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr)
+{
+	unsigned long addr = (unsigned long) vmalloc_addr;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	/*
+	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
+	 * architectures that do not vmalloc module space
+	 */
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (pgd_none(*pgd))
+		return NULL;
+	if (WARN_ON_ONCE(pgd_leaf(*pgd)))
+		return NULL; /* XXX: no allowance for huge pgd */
+	if (WARN_ON_ONCE(pgd_bad(*pgd)))
+		return NULL;
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return NULL;
+	if (p4d_leaf(*p4d))
+		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(p4d_bad(*p4d)))
+		return NULL;
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return NULL;
+	if (pud_leaf(*pud))
+		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return NULL;
+	if (pmd_leaf(*pmd))
+		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pmd_bad(*pmd)))
+		return NULL;
+
+	ptep = pte_offset_map(pmd, addr);
+	pte = *ptep;
+	if (pte_present(pte))
+		page = pte_page(pte);
+	pte_unmap(ptep);
+
+	return page;
+}
diff --git security/integrity/digsig.c security/integrity/digsig.c
index 45c3e5dda3..fad6f3dfdc 100644
--- security/integrity/digsig.c
+++ security/integrity/digsig.c
@@ -17,6 +17,8 @@
 #include <crypto/public_key.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
+
 #include "integrity.h"
 
 static struct key *keyring[INTEGRITY_KEYRING_MAX];
@@ -45,6 +47,7 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 		return ERR_PTR(-EINVAL);
 
 	if (!keyring[id]) {
+		int rc = 0;
 		keyring[id] =
 			request_key(&key_type_keyring, keyring_name[id], NULL);
 		if (IS_ERR(keyring[id])) {
@@ -53,6 +56,19 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 			keyring[id] = NULL;
 			return ERR_PTR(err);
 		}
+
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets registered
+		 * directly in set_platform_trusted_keys.
+		 */
+		rc = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
+
+	} else {
+		int rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
 	}
 
 	return keyring[id];
@@ -111,6 +127,21 @@ static int __init __integrity_init_keyring(const unsigned int id,
 			keyring_name[id], err);
 		keyring[id] = NULL;
 	} else {
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] and
+		 * keyring[INTEGRITY_KEYRING_MACHINE] get also registered in
+		 * set_platform_trusted_keys and set_machine_trusted_keys,
+		 * respectively, to allow passing the given keyring directly and
+		 * indirectly (through INTEGRITY_KEYRING_PLATFORM and
+		 * INTEGRITY_KEYRING_MACHINE) to verify_pkcs7_message_sig.
+		 */
+		err = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (err) {
+			key_put(keyring[id]);
+			keyring[id] = NULL;
+			return err;
+		}
+
 		if (id == INTEGRITY_KEYRING_PLATFORM)
 			set_platform_trusted_keys(keyring[id]);
 		if (id == INTEGRITY_KEYRING_MACHINE && imputed_trust_enabled())
@@ -174,6 +205,10 @@ static int __init integrity_add_key(const unsigned int id, const void *data,
 	if (!keyring[id])
 		return -EINVAL;
 
+	rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+	if (rc)
+		return rc;
+
 	key = key_create_or_update(make_key_ref(keyring[id], 1), "asymmetric",
 				   NULL, data, size, perm,
 				   KEY_ALLOC_NOT_IN_QUOTA);
diff --git security/integrity/ima/ima_mok.c security/integrity/ima/ima_mok.c
index 95cc31525c..87c7d97033 100644
--- security/integrity/ima/ima_mok.c
+++ security/integrity/ima/ima_mok.c
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
 
 struct key *ima_blacklist_keyring;
 
@@ -44,6 +45,10 @@ static __init int ima_mok_init(void)
 
 	if (IS_ERR(ima_blacklist_keyring))
 		panic("Can't allocate IMA blacklist keyring.");
+	else
+		if (bhv_keyring_register_system_keyring(&ima_blacklist_keyring))
+			panic("Can't register IMA blacklist keyring.");
+
 	return 0;
 }
 device_initcall(ima_mok_init);
diff --git security/security.c security/security.c
index c5981e558b..f0671c8405 100644
--- security/security.c
+++ security/security.c
@@ -5978,3 +5978,5 @@ void security_initramfs_populated(void)
 {
 	call_void_hook(initramfs_populated);
 }
+
+#include <bhv/lsm/security.c.inc.h>
diff --git security/selinux/hooks.c security/selinux/hooks.c
index fc926d3cac..3d21d5a20b 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -129,7 +129,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index e172f182b6..eb90406dc9 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -567,9 +571,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi;
 	struct selinux_load_state load_state;
@@ -625,6 +629,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* !CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* !CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2173,3 +2203,38 @@ static int __init init_sel_fs(void)
 }
 
 __initcall(init_sel_fs);
+
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count);
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinux_null.mnt->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&selinux_state.policy_mutex);
+
+	rv = security_load_policy(data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(&load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(&load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&selinux_state.policy_mutex);
+	return rv;
+}
+#endif /* CONFIG_BHV_VAS */
diff --git tools/objtool/Build tools/objtool/Build
index a3cdf8af66..fa4669d54f 100644
--- tools/objtool/Build
+++ tools/objtool/Build
@@ -4,6 +4,7 @@ objtool-y += weak.o
 
 objtool-y += check.o
 objtool-y += special.o
+objtool-y += vault.o
 objtool-y += builtin-check.o
 objtool-y += elf.o
 objtool-y += objtool.o
diff --git tools/objtool/builtin-check.c tools/objtool/builtin-check.c
index 387d56a7f5..67e4c8795d 100644
--- tools/objtool/builtin-check.c
+++ tools/objtool/builtin-check.c
@@ -80,6 +80,7 @@ static const struct option check_options[] = {
 	OPT_BOOLEAN('s', "stackval", &opts.stackval, "validate frame pointer rules"),
 	OPT_BOOLEAN('t', "static-call", &opts.static_call, "annotate static calls"),
 	OPT_BOOLEAN('u', "uaccess", &opts.uaccess, "validate uaccess rules for SMAP"),
+	OPT_BOOLEAN('v', "vault", &opts.vault, "generate vault metadata"),
 	OPT_BOOLEAN(0  , "cfi", &opts.cfi, "annotate kernel control flow integrity (kCFI) function preambles"),
 	OPT_CALLBACK_OPTARG(0, "dump", NULL, NULL, "orc", "dump metadata", parse_dump),
 
@@ -142,7 +143,8 @@ static bool opts_valid(void)
 	    opts.sls			||
 	    opts.stackval		||
 	    opts.static_call		||
-	    opts.uaccess) {
+	    opts.uaccess                ||
+	    opts.vault) {
 		if (opts.dump_orc) {
 			ERROR("--dump can't be combined with other actions");
 			return false;
diff --git tools/objtool/check.c tools/objtool/check.c
index d4d82bb9b5..34bb5f7eda 100644
--- tools/objtool/check.c
+++ tools/objtool/check.c
@@ -13,6 +13,7 @@
 #include <objtool/arch.h>
 #include <objtool/check.h>
 #include <objtool/special.h>
+#include <objtool/vault.h>
 #include <objtool/warn.h>
 #include <objtool/endianness.h>
 
@@ -61,8 +62,8 @@ struct instruction *next_insn_same_sec(struct objtool_file *file,
 	return insn;
 }
 
-static struct instruction *next_insn_same_func(struct objtool_file *file,
-					       struct instruction *insn)
+struct instruction *next_insn_same_func(struct objtool_file *file,
+					struct instruction *insn)
 {
 	struct instruction *next = next_insn_same_sec(file, insn);
 	struct symbol *func = insn_func(insn);
@@ -1034,6 +1035,154 @@ static int create_direct_call_sections(struct objtool_file *file)
 	return 0;
 }
 
+#define SEC_VAULT_TEXT ".bhv.vault.text.jump_label"
+#define SEC_VAULT_TEXT_SHARED ".bhv.vault.shared.text.jump_label"
+#define SEC_VAULT_TEXT_REF ".ref.text.bhv.vault.text.jump_label"
+
+static bool is_sec_in_vault(struct section *sec)
+{
+	if (!sec)
+		return false;
+
+	if (strncmp(sec->name, VAULT_TEXT_SECTION,
+		    strlen(VAULT_TEXT_SECTION)) &&
+	    strncmp(sec->name, VAULT_REF_TEXT_SECTION,
+		    strlen(VAULT_REF_TEXT_SECTION)))
+		return false;
+
+	return true;
+}
+
+static struct reloc *insn_reloc(struct objtool_file *file,
+				struct instruction *insn);
+
+static int add_vault_return_site(struct objtool_file *file,
+				 struct instruction *insn,
+				 struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_return_list,
+			    vault_return_node) {
+		if (insn_iter == next_insn)
+			return 0;
+	}
+
+	list_add_tail(&next_insn->vault_return_node, &file->vault_return_list);
+
+	return 0;
+}
+
+static int add_vault_rethunk_site(struct objtool_file *file,
+				  struct instruction *insn,
+				  struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+		if (insn_iter == insn)
+			return 0;
+	}
+
+	list_add_tail(&insn->vault_rethunk_node, &file->vault_rethunk_list);
+
+	return 0;
+}
+
+static int collect_vault_return_sites(struct objtool_file *file,
+				      struct section *sec)
+{
+	struct symbol *func;
+	struct instruction *insn;
+
+	list_for_each_entry(func, &sec->symbol_list, list) {
+		func_for_each_insn(file, func, insn) {
+			struct section *dest_sec = NULL;
+			struct symbol *sym = NULL;
+
+			/* We are interested only in jumps and calls */
+			if (!is_jump(insn) && !is_call(insn))
+				continue;
+
+			if (insn->jump_dest) {
+				sym = insn_func(insn->jump_dest);
+				dest_sec = insn->jump_dest->sec;
+			} else if (insn_call_dest(insn)) {
+				sym = insn_call_dest(insn);
+				dest_sec = sym->sec;
+			}
+
+			if (!dest_sec) {
+				struct reloc *reloc = insn_reloc(file, insn);
+				if (!reloc)
+					continue;
+
+				sym = reloc->sym;
+				dest_sec = sym->sec;
+			}
+
+			if (!is_sec_in_vault(dest_sec)) {
+				if (!sym)
+					continue;
+
+				if (sym->retpoline_thunk)
+					add_vault_rethunk_site(file, insn, sym);
+				else
+					add_vault_return_site(file, insn, sym);
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int inspect_vault(struct objtool_file *file)
+{
+	struct section *sec;
+
+	sec = find_section_by_name(file->elf, ".bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	sec = find_section_by_name(file->elf,
+				   ".ref.text.bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	return 0;
+}
+
 /*
  * Warnings shouldn't be reported for ignored functions.
  */
@@ -4897,6 +5046,23 @@ int check(struct objtool_file *file)
 		warnings += ret;
 	}
 
+	if (opts.vault) {
+		ret = inspect_vault(file);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		//ret = create_vault_entries_section(file);
+		ret = create_vault_section(file, VAULT_SECTION_RETURN_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		ret = create_vault_section(file, VAULT_SECTION_RETHUNK_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+	}
 	free_insns(file);
 
 	if (opts.verbose)
diff --git tools/objtool/elf.c tools/objtool/elf.c
index 3d27983dc9..499b299766 100644
--- tools/objtool/elf.c
+++ tools/objtool/elf.c
@@ -881,18 +881,18 @@ static struct reloc *elf_init_reloc(struct elf *elf, struct section *rsec,
 	return reloc;
 }
 
-struct reloc *elf_init_reloc_text_sym(struct elf *elf, struct section *sec,
-				      unsigned long offset,
-				      unsigned int reloc_idx,
-				      struct section *insn_sec,
-				      unsigned long insn_off)
+struct reloc *
+elf_init_reloc_text_sym_relo_type(struct elf *elf, struct section *sec,
+				  unsigned long offset, unsigned int reloc_idx,
+				  struct section *insn_sec,
+				  unsigned long insn_off, unsigned int type)
 {
 	struct symbol *sym = insn_sec->sym;
 	int addend = insn_off;
 
 	if (!(insn_sec->sh.sh_flags & SHF_EXECINSTR)) {
-		WARN("bad call to %s() for data symbol %s",
-		     __func__, sym->name);
+		WARN("bad call to %s() for data symbol %s", __func__,
+		     sym->name);
 		return NULL;
 	}
 
@@ -911,7 +911,18 @@ struct reloc *elf_init_reloc_text_sym(struct elf *elf, struct section *sec,
 	}
 
 	return elf_init_reloc(elf, sec->rsec, reloc_idx, offset, sym, addend,
-			      elf_text_rela_type(elf));
+			      type);
+}
+
+struct reloc *elf_init_reloc_text_sym(struct elf *elf, struct section *sec,
+				      unsigned long offset,
+				      unsigned int reloc_idx,
+				      struct section *insn_sec,
+				      unsigned long insn_off)
+{
+	return elf_init_reloc_text_sym_relo_type(elf, sec, offset, reloc_idx,
+						 insn_sec, insn_off,
+						 elf_text_rela_type(elf));
 }
 
 struct reloc *elf_init_reloc_data_sym(struct elf *elf, struct section *sec,
diff --git tools/objtool/include/objtool/builtin.h tools/objtool/include/objtool/builtin.h
index fcca6662c8..bf61e4df75 100644
--- tools/objtool/include/objtool/builtin.h
+++ tools/objtool/include/objtool/builtin.h
@@ -26,6 +26,7 @@ struct opts {
 	bool uaccess;
 	int prefix;
 	bool cfi;
+	bool vault;
 
 	/* options: */
 	bool backtrace;
diff --git tools/objtool/include/objtool/check.h tools/objtool/include/objtool/check.h
index daa46f1f09..7fb91c032a 100644
--- tools/objtool/include/objtool/check.h
+++ tools/objtool/include/objtool/check.h
@@ -43,6 +43,9 @@ struct alt_group {
 struct instruction {
 	struct hlist_node hash;
 	struct list_head call_node;
+	struct list_head vault_entry_node;
+	struct list_head vault_return_node;
+	struct list_head vault_rethunk_node;
 	struct section *sec;
 	unsigned long offset;
 	unsigned long immediate;
@@ -111,9 +114,27 @@ static inline bool is_jump(struct instruction *insn)
 	return is_static_jump(insn) || is_dynamic_jump(insn);
 }
 
+static inline bool is_static_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL;
+}
+
+static inline bool is_dynamic_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL_DYNAMIC;
+}
+
+static inline bool is_call(struct instruction *insn)
+{
+        return is_static_call(insn) || is_dynamic_call(insn);
+}
+
 struct instruction *find_insn(struct objtool_file *file,
 			      struct section *sec, unsigned long offset);
 
+struct instruction *next_insn_same_func(struct objtool_file *file,
+				        struct instruction *insn);
+
 struct instruction *next_insn_same_sec(struct objtool_file *file, struct instruction *insn);
 
 #define sec_for_each_insn(file, _sec, insn)				\
diff --git tools/objtool/include/objtool/elf.h tools/objtool/include/objtool/elf.h
index d7e815c2fd..857758108a 100644
--- tools/objtool/include/objtool/elf.h
+++ tools/objtool/include/objtool/elf.h
@@ -116,6 +116,12 @@ struct section *elf_create_section_pair(struct elf *elf, const char *name,
 
 struct symbol *elf_create_prefix_symbol(struct elf *elf, struct symbol *orig, long size);
 
+struct reloc *
+elf_init_reloc_text_sym_relo_type(struct elf *elf, struct section *sec,
+				  unsigned long offset, unsigned int reloc_idx,
+				  struct section *insn_sec,
+				  unsigned long insn_off, unsigned int type);
+
 struct reloc *elf_init_reloc_text_sym(struct elf *elf, struct section *sec,
 				      unsigned long offset,
 				      unsigned int reloc_idx,
diff --git tools/objtool/include/objtool/objtool.h tools/objtool/include/objtool/objtool.h
index 94a33ee7b3..c97a47cb04 100644
--- tools/objtool/include/objtool/objtool.h
+++ tools/objtool/include/objtool/objtool.h
@@ -28,6 +28,9 @@ struct objtool_file {
 	struct list_head mcount_loc_list;
 	struct list_head endbr_list;
 	struct list_head call_list;
+	struct list_head vault_entry_list;
+	struct list_head vault_return_list;
+	struct list_head vault_rethunk_list;
 	bool ignore_unreachables, hints, rodata;
 
 	unsigned int nr_endbr;
diff --git tools/objtool/include/objtool/vault.h tools/objtool/include/objtool/vault.h
new file mode 100644
index 0000000000..3c6e83cd4a
--- /dev/null
+++ tools/objtool/include/objtool/vault.h
@@ -0,0 +1,20 @@
+#ifndef _VAULT_H
+#define _VAULT_H
+
+#include <stdbool.h>
+#include <objtool/check.h>
+#include <objtool/elf.h>
+
+/* XXX: Change names! */
+#define VAULT_TEXT_SECTION 		".bhv.vault.text.jump_label"
+#define VAULT_REF_TEXT_SECTION 		".ref.text.bhv.vault.text.jump_label"
+
+enum vault_section_type {
+	VAULT_SECTION_ENTRY_POINTS,
+	VAULT_SECTION_RETURN_SITES,
+	VAULT_SECTION_RETHUNK_SITES,
+};
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t);
+
+#endif /* _VAULT_H */
diff --git tools/objtool/objtool.c tools/objtool/objtool.c
index f40febdd6e..caa32f5dba 100644
--- tools/objtool/objtool.c
+++ tools/objtool/objtool.c
@@ -106,6 +106,9 @@ struct objtool_file *objtool_open_read(const char *_objname)
 	INIT_LIST_HEAD(&file.mcount_loc_list);
 	INIT_LIST_HEAD(&file.endbr_list);
 	INIT_LIST_HEAD(&file.call_list);
+	INIT_LIST_HEAD(&file.vault_entry_list);
+	INIT_LIST_HEAD(&file.vault_return_list);
+	INIT_LIST_HEAD(&file.vault_rethunk_list);
 	file.ignore_unreachables = opts.no_unreachable;
 	file.hints = false;
 
diff --git tools/objtool/vault.c tools/objtool/vault.c
new file mode 100644
index 0000000000..0293b8812c
--- /dev/null
+++ tools/objtool/vault.c
@@ -0,0 +1,174 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) 2024 Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <assert.h>
+#include <objtool/arch.h>
+#include <objtool/builtin.h>
+#include <objtool/vault.h>
+#include <objtool/warn.h>
+#include <objtool/check.h>
+
+static const char *vault_section_names[] = { ".vault_entries", ".vault_sites",
+					     ".vault_rethunks" };
+
+static int create_entry_points(struct objtool_file *file)
+{
+	/* XXX: We do not identify vault entry points, yet. */
+	return 0;
+}
+
+static int create_return_sites(struct objtool_file *file)
+{
+	int idx = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETURN_SITES];
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_return_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_return_list))
+		return 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node)
+		idx++;
+
+	printf("Found %d returns to the vault\n", idx);
+
+	sec = elf_create_section_pair(file->elf, sec_name, sizeof(unsigned long),
+				      idx, idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node) {
+		unsigned long *loc = (unsigned long *)sec->data->d_buf + idx;
+		memset(loc, 0, sizeof(unsigned long));
+
+		if (!elf_init_reloc_text_sym_relo_type(
+			    file->elf, sec, idx * sizeof(unsigned long), idx,
+			    insn->sec, insn->offset, R_X86_64_64)) {
+			WARN("Cannot add entry[%d] to section %s\n", idx,
+			     sec->name);
+			return -1;
+		}
+
+		printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			idx, insn->offset, insn_func(insn)->name,
+			sec->name);
+
+		idx++;
+	}
+
+	return 0;
+}
+
+static int create_rethunk_sites(struct objtool_file *file)
+{
+	int idx = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETHUNK_SITES];
+
+	static const int NUM_POSSIBLE_RETURNS = 2;
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_rethunk_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_rethunk_list))
+		return 0;
+
+	/* Account for all possible return points to the patched instruction. */
+	list_for_each_entry(insn, &file->vault_rethunk_list, vault_rethunk_node) {
+		/*
+		 * Note: depending on the system's configuration, retpolines can
+		 * patch a jump/call instruction differently. Yet, there can be
+		 * only two possible return locations, to which the retpoline
+		 * can return to:
+		 *
+		 *   (i)  instruction, right after a short jump (2 bytes)
+		 *   (ii) next instruction after the original jmp/call
+		 */
+		idx += NUM_POSSIBLE_RETURNS;
+	}
+
+	printf("Found %d possible thunk returns to the vault\n", idx);
+
+	sec = elf_create_section_pair(file->elf, sec_name, sizeof(unsigned long),
+				      idx, idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+
+		unsigned long insn_offsets[NUM_POSSIBLE_RETURNS];
+		struct instruction *next_insn = next_insn_same_func(file, insn);
+		if (next_insn == NULL) {
+        		printf("Cannot find next instruction of %s:0x%lx\n",
+               			insn->sec->name, insn->offset);
+        		return -1;
+		}
+
+		insn_offsets[0] = insn->offset + 2;
+		insn_offsets[1] = next_insn->offset;
+
+		for (int i = 0; i < NUM_POSSIBLE_RETURNS; ++i) {
+			unsigned long *loc =
+				(unsigned long *)sec->data->d_buf + idx;
+			memset(loc, 0, sizeof(unsigned long));
+
+			if (!elf_init_reloc_text_sym_relo_type(
+				    file->elf, sec, idx * sizeof(unsigned long),
+				    idx, insn->sec, insn_offsets[i],
+				    R_X86_64_64)) {
+				WARN("Cannot add entry[%d] to section %s\n",
+				     idx, sec->name);
+				return -1;
+			}
+
+			printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			       idx, insn_offsets[i], insn_func(insn)->name,
+			       sec->name);
+
+			idx++;
+		}
+	}
+
+	return 0;
+}
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t)
+{
+	switch (t) {
+	case VAULT_SECTION_ENTRY_POINTS:
+		return create_entry_points(file);
+	case VAULT_SECTION_RETURN_SITES:
+		return create_return_sites(file);
+	case VAULT_SECTION_RETHUNK_SITES:
+		return create_rethunk_sites(file);
+	default:
+		WARN("Unknown vault section type");
+		return -1;
+	}
+
+	return 0;
+}
+
