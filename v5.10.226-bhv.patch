diff --git arch/Kconfig arch/Kconfig
index 72e4cef06..34a9e057b 100644
--- arch/Kconfig
+++ arch/Kconfig
@@ -77,6 +77,7 @@ config KPROBES
 	bool "Kprobes"
 	depends on MODULES
 	depends on HAVE_KPROBES
+	depends on !BHV_LOCKDOWN
 	select KALLSYMS
 	help
 	  Kprobes allows you to trap at almost any kernel address and
diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index d6465823b..f1ea15a79 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,4 +3,5 @@ obj-y			+= kernel/ mm/
 obj-$(CONFIG_NET)	+= net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 000000000..b812d00cf
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/arm64/bhv/init/init.c arch/arm64/bhv/init/init.c
new file mode 100644
index 000000000..b3281d1ca
--- /dev/null
+++ arch/arm64/bhv/init/init.c
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <bhv/integrity.h>
+#include <bhv/init/init.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
+
+void __init bhv_init_arch(void)
+{
+}
diff --git arch/arm64/bhv/init/start.c arch/arm64/bhv/init/start.c
new file mode 100644
index 000000000..531fe4df7
--- /dev/null
+++ arch/arm64/bhv/init/start.c
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/integrity.h>
+#include <bhv/init/start.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 000000000..23b3ab736
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,115 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+extern char vdso_start[], vdso_end[];
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif
+
+/************************************************************
+ * start
+ ************************************************************/
+int __init_km bhv_start_integrity_arch(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 3
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON(KLN_SYM(vdso_start) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_start) >= KLN_SYM(__end_rodata));
+	BUG_ON(KLN_SYM(vdso_end) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_end) >= KLN_SYM(__end_rodata));
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 */
+	bhv_mem_region_create_ctor(
+		&n[0]->region, NULL,
+		bhv_virt_to_phys_single(KLN_SYMBOL(void *, __start_rodata)),
+		KLN_SYM(vdso_start) - KLN_SYM(__start_rodata),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[1]->region, &n[0]->region,
+		bhv_virt_to_phys_single(KLN_SYMBOL(void *, vdso_end)),
+		KLN_SYM(__end_rodata) - KLN_SYM(vdso_end),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[2]->region, &n[1]->region,
+		bhv_virt_to_phys_single(KLN_SYMBOL(void *, vdso_start)),
+		KLN_SYM(vdso_end) - KLN_SYM(vdso_start),
+		HypABI__Integrity__MemType__VDSO,
+		HypABI__Integrity__MemFlags__NONE, "KERNEL VDSO SECTION");
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+}
+/************************************************************/
+
+/************************************************************
+ * start
+ ************************************************************/
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	*pgd_offset = 0;
+	*pgd_value = 0;
+}
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	init_ptpg_arg->init_pgd = 0;
+	init_ptpg_arg->pt_levels = 0;
+	init_ptpg_arg->num_ranges = 0;
+}
+/************************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	return true;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 000000000..fb8bfffb3
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,460 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+#include <asm/bhv/patch.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#if defined(BHV_KVERS_6_1)
+#include <asm/cacheflush.h>
+#endif
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_patch_hypercall((void *)&updptr[i], (uint8_t *)&insn,
+				    sizeof(insn), false);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+#if defined(BHV_KVERS_6_1)
+#define ALT_CAP(a) ((a)->cpufeature & ~ARM64_CB_BIT)
+#define ALT_HAS_CB(a) ((a)->cpufeature & ARM64_CB_BIT)
+
+extern DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(
+			 ctr_el0, CTR_EL0_DminLine_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+		int cap = ALT_CAP(alt);
+
+		if (!test_bit(cap, feature_mask))
+			continue;
+
+		if (!cpus_have_cap(cap))
+			continue;
+
+		if (ALT_HAS_CB(alt))
+			BUG_ON(alt->alt_len != 0);
+		else
+			BUG_ON(alt->alt_len != alt->orig_len);
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (ALT_HAS_CB(alt)) {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		} else {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	/*
+	 * The core module code takes care of cache maintenance in
+	 * flush_module_icache().
+	 */
+	if (!mod->arch.is_module) {
+		dsb(ish);
+		icache_inval_all_pou();
+		isb();
+
+		/* Ignore ARM64_CB bit from feature mask */
+		bitmap_or(applied_alternatives, applied_alternatives,
+			  feature_mask, ARM64_NCAPS);
+		bitmap_and(applied_alternatives, applied_alternatives,
+			   cpu_hwcaps, ARM64_NCAPS);
+	}
+
+	return 0;
+}
+#else /* BHV_KVERS_6_1 */
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(ctr_el0,
+							   CTR_DMINLINE_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+
+		if (!test_bit(alt->cpufeature, feature_mask))
+			continue;
+
+		/* Use ARM64_CB_PATCH as an unconditional patch */
+		if (alt->cpufeature < ARM64_CB_PATCH &&
+		    !cpus_have_cap(alt->cpufeature))
+			continue;
+
+		if (alt->cpufeature == ARM64_CB_PATCH) {
+			if (alt->alt_len != 0) {
+				return -EACCES;
+			}
+		} else {
+			if (alt->alt_len != alt->orig_len) {
+				return -EACCES;
+			}
+		}
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (alt->cpufeature < ARM64_CB_PATCH) {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		} else {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	return 0;
+}
+#endif /* BHV_KVERS_6_1 */
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 000000000..5c47935b5
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+u32 __always_inline bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/reg_protect.c arch/arm64/bhv/reg_protect.c
new file mode 100644
index 000000000..10233d22a
--- /dev/null
+++ arch/arm64/bhv/reg_protect.c
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+}
+/***************************************************/
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index 3cb3c4ab3..a0a1983d8 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -14,6 +14,8 @@
 #include <linux/stddef.h>
 #include <linux/stringify.h>
 
+#include <bhv/interface/abi_base_autogen.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
diff --git arch/arm64/include/asm/bhv/domain.h arch/arm64/include/asm/bhv/domain.h
new file mode 100644
index 000000000..70b96ab43
--- /dev/null
+++ arch/arm64/include/asm/bhv/domain.h
@@ -0,0 +1,76 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) pgd
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_RDONLY);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pte_read(pmd_pte(pmd));
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pte_read(pud_pte(pud));
+}
+
+static inline bool pte_exec(pte_t pte)
+{
+	return !!(pte_val(pte) & (PTE_PXN | PTE_UXN));
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return pte_exec(pmd_pte(pmd));
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return pte_exec(pud_pte(pud));
+}
+
+static inline bool pmd_large(pmd_t pmd)
+{
+	return pmd_thp_or_huge(pmd);
+}
+
+static inline bool pud_large(pud_t pud)
+{
+	return pud_huge(pud);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return bhv_domain_is_user_pte(pmd_pte(pmd));
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return bhv_domain_is_user_pte(pud_pte(pud));
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..7c9d75eb2
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 000000000..a4282166e
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+#ifndef VASKM // inside kernel tree
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+#endif // VASKM
+
+#else /* !CONFIG_BHV_VAS */
+#ifndef VASKM // inside kernel tree
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+#endif // VASKM
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 47dafd6ab..d7ff5ca2e 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -80,6 +80,10 @@ alternative_cb_end
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
 void kvm_compute_layout(void);
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 5f8e4c2df..bc9198619 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -17,6 +17,10 @@
 #include <asm/sections.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
 #define __ALT_PTR(a,f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -30,6 +34,16 @@ struct alt_region {
 	struct alt_instr *end;
 };
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+
+void bhv_init_alternatives(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+#endif
+
 bool alternative_is_applied(u16 cpufeature)
 {
 	if (WARN_ON(cpufeature >= ARM64_NCAPS))
@@ -41,6 +55,7 @@ bool alternative_is_applied(u16 cpufeature)
 /*
  * Check if the target PC is within an alternative block.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline bool branch_insn_requires_update(struct alt_instr *alt, unsigned long pc)
 {
 	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
@@ -49,6 +64,7 @@ static __always_inline bool branch_insn_requires_update(struct alt_instr *alt, u
 
 #define align_down(x, a)	((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr, __le32 *altinsnptr)
 {
 	u32 insn;
@@ -94,8 +110,9 @@ static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
 	return insn;
 }
 
-static noinstr void patch_alternative(struct alt_instr *alt,
-			      __le32 *origptr, __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static __bhv_noinstr void patch_alternative(struct alt_instr *alt, __le32 *origptr,
+				      __le32 *updptr, int nr_inst)
 {
 	__le32 *replptr;
 	int i;
@@ -105,7 +122,21 @@ static noinstr void patch_alternative(struct alt_instr *alt,
 		u32 insn;
 
 		insn = get_alt_insn(alt, origptr + i, replptr + i);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key)) {
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+			continue;
+		}
+
+		if (bhv_integrity_is_enabled()) {
+			bhv_apply_alternatives((void *)&updptr[i],
+					       (void *)&insn, sizeof(insn));
+		}
+#else
 		updptr[i] = cpu_to_le32(insn);
+#endif
 	}
 }
 
@@ -114,6 +145,7 @@ static noinstr void patch_alternative(struct alt_instr *alt,
  * accidentally call into the cache.S code, which is patched by us at
  * runtime.
  */
+#ifndef CONFIG_BHV_VAULT_SPACES
 static void clean_dcache_range_nopatch(u64 start, u64 end)
 {
 	u64 cur, d_size, ctr_el0;
@@ -131,15 +163,27 @@ static void clean_dcache_range_nopatch(u64 start, u64 end)
 		asm volatile("dc civac, %0" : : "r" (cur) : "memory");
 	} while (cur += d_size, cur < end);
 }
+#endif
 
-static void __apply_alternatives(void *alt_region,  bool is_module,
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __apply_alternatives(void *alt_region, bool is_module,
 				 unsigned long *feature_mask)
 {
-	struct alt_instr *alt;
 	struct alt_region *region = alt_region;
+	struct alt_instr *alt;
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* !CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 
@@ -169,10 +213,19 @@ static void __apply_alternatives(void *alt_region,  bool is_module,
 
 		alt_cb(alt, origptr, updptr, nr_inst);
 
+		/*
+		 * With spaces-based vaults, it is the task of BHV to take care
+		 * of d-cache flushing after patching. Given that the Linux
+		 * kernel's .text segment is write-protected, writing back any
+		 * dirty cache entries targeting the .text segment will result
+		 * in data abort violations.
+		 */
+#ifndef CONFIG_BHV_VAULT_SPACES
 		if (!is_module) {
 			clean_dcache_range_nopatch((u64)origptr,
 						   (u64)(origptr + nr_inst));
 		}
+#endif
 	}
 
 	/*
@@ -196,6 +249,7 @@ static void __apply_alternatives(void *alt_region,  bool is_module,
  * We might be patching the stop_machine state machine, so implement a
  * really simple polling protocol here.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __apply_alternatives_multi_stop(void *unused)
 {
 	struct alt_region region = {
@@ -222,6 +276,7 @@ static int __apply_alternatives_multi_stop(void *unused)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_alternatives_multi_stop);
 
 void __init apply_alternatives_all(void)
 {
@@ -234,30 +289,48 @@ void __init apply_alternatives_all(void)
  * a feature detect on the boot CPU). No need to worry about other CPUs
  * here.
  */
-void __init apply_boot_alternatives(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline void __apply_boot_alternatives(void)
 {
 	struct alt_region region = {
 		.begin	= (struct alt_instr *)__alt_instructions,
 		.end	= (struct alt_instr *)__alt_instructions_end,
 	};
 
+	__apply_alternatives(&region, false, &boot_capabilities[0]);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_boot_alternatives);
+
+void __init apply_boot_alternatives(void)
+{
 	/* If called on non-boot cpu things could go wrong */
 	WARN_ON(smp_processor_id() != 0);
-
-	__apply_alternatives(&region, false, &boot_capabilities[0]);
+	__apply_boot_alternatives();
 }
 
 #ifdef CONFIG_MODULES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void apply_alternatives_module(void *start, size_t length)
 {
 	struct alt_region region = {
 		.begin	= start,
 		.end	= start + length,
 	};
+
 	DECLARE_BITMAP(all_capabilities, ARM64_NPATCHABLE);
 
 	bitmap_fill(all_capabilities, ARM64_NPATCHABLE);
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
+#endif
+#endif
+
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives_module);
 #endif
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index 9a8a0ae1e..27f128ec3 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -9,10 +9,27 @@
 #include <linux/jump_label.h>
 #include <asm/insn.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+
+void bhv_init_jump_label(void)
+{
+        if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+#endif
+
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -23,9 +40,23 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_jump_label_transform_static(struct jump_entry *entry,
 				      enum jump_label_type type)
 {
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index 90337910c..2f58ead41 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -31,6 +31,14 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -593,9 +601,10 @@ static enum mitigation_state spectre_v4_enable_hw_mitigation(void)
  * Patch a branch over the Spectre-v4 mitigation code with a NOP so that
  * we fallthrough and check whether firmware needs to be called on this CPU.
  */
-void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
-						  __le32 *origptr,
-						  __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
+						      __le32 *origptr,
+						      __le32 *updptr, int nr_inst)
 {
 	BUG_ON(nr_inst != 1); /* Branch -> NOP */
 
@@ -605,17 +614,35 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_final_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+#ifdef CONFIG_BHV_VAULT_SPACES
+			bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+					       sizeof(insn));
+#else
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+#endif
+
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
  * Patch a NOP in the Spectre-v4 mitigation code with an SMC/HVC instruction
  * to call into firmware to adjust the mitigation state.
  */
-void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
+					          __le32 *origptr, __le32 *updptr,
+					          int nr_inst)
 {
 	u32 insn;
 
@@ -632,7 +659,22 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1109,8 +1151,9 @@ void spectre_bhb_enable_mitigation(const struct arm64_cpu_capabilities *entry)
 }
 
 /* Patched to correct the immediate */
-void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
+				               __le32 *origptr, __le32 *updptr, int nr_inst)
 {
 	u8 rd;
 	u32 insn;
@@ -1126,7 +1169,22 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, loop_count, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_apply_alternatives((void *)updptr, (uint8_t *)&insn,
+				       sizeof(insn));
+#else
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+#endif
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index eb4b24652..6d5815027 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -51,6 +51,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/init/init.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -323,6 +325,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT) && ((u64)_text % MIN_KIMG_ALIGN) != 0)
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index 71f4b5f24..728d7d135 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -121,7 +121,7 @@ SECTIONS
 		_text = .;
 		HEAD_TEXT
 	}
-	.text : {			/* Real text segment		*/
+	.text : ALIGN(SEGMENT_ALIGN) {	/* Real text segment		*/
 		_stext = .;		/* Text and read-only data	*/
 			IRQENTRY_TEXT
 			SOFTIRQENTRY_TEXT
@@ -135,8 +135,12 @@ SECTIONS
 			IDMAP_TEXT
 			HIBERNATE_TEXT
 			TRAMP_TEXT
+			BHV_TEXT
 			*(.fixup)
 			*(.gnu.warning)
+#ifdef CONFIG_BHV_VAULT_SPACES
+            BHV_VAULT_TEXT(jump_label)
+#endif
 		. = ALIGN(16);
 		*(.got)			/* Global offset table		*/
 	}
@@ -176,16 +180,36 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 		__alt_instructions = .;
 		*(.altinstructions)
+#ifdef CONFIG_BHV_VAULT_SPACES
+		. = ALIGN(PAGE_SIZE);
+#endif
 		__alt_instructions_end = .;
 	}
 
@@ -250,6 +274,36 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+    . = ALIGN(PAGE_SIZE);
+    .bhv.vault.comm : AT(ADDR(.bhv.vault.comm) - LOAD_OFFSET) {
+		__bhv_vault_comm_start = .;
+		. += PAGE_SIZE;
+		. = ALIGN(PAGE_SIZE);
+		__bhv_vault_comm_end = .;
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.data : AT(ADDR(.bhv.vault.data) - LOAD_OFFSET) {
+		BHV_VAULT_DATA(jump_label)
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.rodata : AT(ADDR(.bhv.vault.rodata) - LOAD_OFFSET) {
+		BHV_VAULT_RO_DATA(jump_label)
+	}
+#endif
+
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index e0404bcab..88494d756 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -12,6 +12,11 @@
 #include <asm/insn.h>
 #include <asm/kvm_mmu.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -97,8 +102,21 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn, bhv_patch_arg_t *bhv_arg)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+	bhv_patch_hypercall((void *)dest, &le32_insn, sizeof(le32_insn), false,
+			    bhv_arg);
+}
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst,
+			       bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	int i;
 
@@ -116,7 +134,17 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (has_vhe() || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop(),
+						  bhv_arg);
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -127,15 +155,29 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn, bhv_arg);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
 void *__kvm_bp_vect_base;
 int __kvm_harden_el2_vector_slot;
 
+#ifdef CONFIG_BHV_VAS
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst,
+			     bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
 void kvm_patch_vector_branch(struct alt_instr *alt,
 			     __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u64 addr;
 	u32 insn;
@@ -170,7 +212,15 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 						-16,
 						AARCH64_INSN_VARIANT_64BIT,
 						AARCH64_INSN_LDST_STORE_PAIR_PRE_INDEX);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movz x0, #(addr & 0xffff) */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -178,15 +228,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -194,10 +258,26 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
diff --git arch/arm64/mm/fault.c arch/arm64/mm/fault.c
index d8baedd16..9c7a03ffc 100644
--- arch/arm64/mm/fault.c
+++ arch/arm64/mm/fault.c
@@ -39,6 +39,10 @@
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr,
 		      struct pt_regs *regs);
@@ -313,12 +317,28 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 		return;
 
 	if (is_el1_permission_fault(addr, esr, regs)) {
+#ifdef CONFIG_BHV_VAS
+		uint8_t type;
+		if (esr & ESR_ELx_WNR) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+			msg = "write to read-only memory";
+		} else if (is_el1_instruction_abort(esr)) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+			msg = "execute from non-executable memory";
+		} else {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+			msg = "read from unreadable memory";
+		}
+		if (bhv_guestlog_log_kaccess_events())
+			bhv_guestlog_log_kaccess((uint64_t)addr, type);
+#else /* CONFIG_BHV_VAS */
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
 		else if (is_el1_instruction_abort(esr))
 			msg = "execute from non-executable memory";
 		else
 			msg = "read from unreadable memory";
+#endif /* CONFIG_BHV_VAS */
 	} else if (addr < PAGE_SIZE) {
 		msg = "NULL pointer dereference";
 	} else {
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 80cc79760..1dce37b43 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -43,6 +43,9 @@
 #include <asm/tlb.h>
 #include <asm/alternative.h>
 
+#include <bhv/init/start.h>
+#include <bhv/vault.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -87,6 +90,25 @@ phys_addr_t __ro_after_init arm64_dma_phys_limit;
 phys_addr_t __ro_after_init arm64_dma_phys_limit = PHYS_MASK + 1;
 #endif
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+static void __ref bhv_vault_release_memory(void)
+{
+	int rc;
+	HypABI__Wagner__Delete__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+	return;
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+}
+#endif
+
 #ifdef CONFIG_KEXEC_CORE
 /*
  * reserve_crashkernel() - reserves memory for crash kernel
@@ -592,6 +614,10 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_vault_release_memory();
+#endif
+	bhv_start();
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
 			   POISON_FREE_INITMEM, "unused kernel");
diff --git arch/x86/Kbuild arch/x86/Kbuild
index 30dec0197..3235bbc86 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -13,6 +13,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 000000000..11409f4fb
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,16 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/x86/bhv/init/init.c arch/x86/bhv/init/init.c
new file mode 100644
index 000000000..4447e0ecb
--- /dev/null
+++ arch/x86/bhv/init/init.c
@@ -0,0 +1,147 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/types.h> // This is here so that hypervisor.h knows bool
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+
+#include <vdso/datapage.h>
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+#include <asm/vvar.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_64.data),
+				   vdso_image_64.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 64");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_x32.data),
+				   vdso_image_x32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE X32");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys_single(vdso_image_32.data),
+				   vdso_image_32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 32");
+	(*region_counter)++;
+
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static __always_inline void
+bhv_init_add_vvar(bhv_mem_region_t *init_phys_mem_regions,
+		  unsigned int *region_counter)
+{
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   __pa_symbol(&__vvar_page), PAGE_SIZE,
+				   HypABI__Integrity__MemType__VVAR, HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL VVAR PAGE");
+	(*region_counter)++;
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vvar(init_phys_mem_regions, region_counter);
+}
+
+static uint32_t __init bhv_detect(void)
+{
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0; /* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return hypervisor_cpuid_base("BHV.VMM.VAS.", 0);
+
+	return 0;
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform
+};
+
+#else // out of tree
+
+#include <common.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	// We handle VDSOs in integrity.c, so no need to do anything here.
+}
+
+#endif // VASKM
+
+void __init bhv_init_arch(void)
+{
+#ifndef VASKM // inside kernel tree
+	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+#endif // VASKM
+}
diff --git arch/x86/bhv/init/start.c arch/x86/bhv/init/start.c
new file mode 100644
index 000000000..e008570c4
--- /dev/null
+++ arch/x86/bhv/init/start.c
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <bhv/init/start.h>
+#include <bhv/integrity.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 000000000..d110b4a39
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,590 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/pgtable.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_EFI
+#include <linux/efi.h>
+#endif /* CONFIG_EFI */
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <linux/pagewalk.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif //VASKM
+
+struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} typedef table_data_t;
+
+static table_data_t table_data __ro_after_init = { 0 };
+
+struct {
+	uint64_t start_addr;
+	uint64_t size;
+	const char *label;
+	uint32_t mem_type;
+} typedef ro_region_t;
+
+/**********************************************************
+ * start
+ **********************************************************/
+static int __init_km bhv_start_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = bhv_virt_to_phys_single((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(
+		head, addr, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE, "IDT");
+}
+
+static int __init_km bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_start_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region.create);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#ifndef VASKM // inside kernel tree
+
+static inline int __init_km bhv_start_rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(
+		head, bhv_virt_to_phys_single(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int __init_km bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = bhv_start_rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#endif // VASKM
+
+static void __init_km bhv_start_integrity_add_vdso_common(
+	uint64_t start_addr, uint64_t size, const char *label,
+	ro_region_t *range, unsigned int *cur_entry, size_t range_sz)
+{
+	unsigned int i;
+	uint64_t end = start_addr + size;
+	uint64_t cur_end = 0;
+
+	BUG_ON(size == 0);
+	BUG_ON((*cur_entry) >= range_sz);
+	BUG_ON(start_addr < range[0].start_addr);
+
+	// Check for overlaps
+	for (i = 0; i < (*cur_entry); i++) {
+		cur_end = range[i].start_addr + range[i].size;
+
+		// No overlap. Nothing to do.
+		if (end <= range[i].start_addr || start_addr >= cur_end)
+			continue;
+
+		// No range that we add should be exactly the same as an
+		// existing one.
+		if (start_addr == range[i].start_addr && end == cur_end) {
+			BUG(); // Range already exists
+		}
+
+		// Overlap. Split range.
+		// Case 1 (overlap left): New range starts before current range
+		//                        with/before the current range.
+		//                        Update the new range to end when cur starts.
+		//                        Thus creating range A and B.
+		//  -----------------------
+		// |      A      ##########B##########
+		// |     new     #         |   cur   #
+		// |             ##########|##########
+		// ------------------------
+		if (start_addr < range[i].start_addr && end <= cur_end) {
+			size = range[i].start_addr - start_addr;
+			continue;
+		}
+
+		// Case 2 (overlap right): New range starts within current range
+		//                         and ends with/after the current range.
+		//                         Update cur to end when new starts.
+		//                         Thus creating range A and B.
+		//                         -----------------------
+		//              #####A####|##########  B          |
+		//              #   cur   |         #     new     |
+		//              ##########|##########             |
+		//                        ------------------------
+		if (range[i].start_addr < start_addr && cur_end <= end) {
+			range[i].size = start_addr - range[i].start_addr;
+			continue;
+		}
+
+		// Case 3: New range fully encompasses current range.
+		//         Create three ranges A, B, C.
+		//  --------------------------------------------
+		// |     A     ##########B##########     C      |
+		// |    new    #        cur        #    new     |
+		// |           #####################            |
+		// ---------------------------------------------
+		if (start_addr <= range[i].start_addr && cur_end <= end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				// No. Add new Range A.
+				range[(*cur_entry)].start_addr = start_addr;
+				range[(*cur_entry)].size =
+					range[i].start_addr - start_addr;
+				range[(*cur_entry)].label = label;
+				range[(*cur_entry)].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Update new to be C. It will be added after the loop.
+				// B is already in our range list.
+				start_addr = cur_end;
+				size = end - cur_end;
+			} else {
+				// Remaining ranges are the same. Just update B with new label/type.
+				// Since a range can only have one label, it will take the
+				// label of the new range. This generally seems to make sense
+				// since we add the entire read-only section and then split it
+				// with smaller sections.
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+			continue;
+		}
+
+		// Case 4: Current range fully encompasses new range.
+		//         Create three ranges A, B, C.
+		// ##############################################
+		// #     A      ---------B---------      C      #
+		// #    cur    |        new        |    cur     #
+		// #            -------------------             #
+		// ##############################################
+		if (range[i].start_addr <= start_addr && end <= cur_end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				range[(*cur_entry)].start_addr =
+					range[i].start_addr;
+				range[(*cur_entry)].size =
+					start_addr - range[i].start_addr;
+				range[(*cur_entry)].label = range[i].label;
+				range[(*cur_entry)].mem_type =
+					range[i].mem_type;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Create C. B will be added after the loop.
+				range[i].start_addr = end;
+				range[i].size = cur_end - end;
+			} else {
+				// Remaining ranges are the same.
+				// Shrink the current range to B. We also update its label/type.
+				// This generally seems to make sense since we add the
+				// entire read-only section and then split it with smaller
+				// sections.
+				range[i].start_addr = start_addr;
+				range[i].size = size;
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+
+			continue;
+		}
+
+		BUG(); // "Unexpected case"
+	}
+
+	if (size != 0) {
+		range[(*cur_entry)].start_addr = start_addr;
+		range[(*cur_entry)].size = size;
+		range[(*cur_entry)].label = label;
+		range[(*cur_entry)].mem_type = HypABI__Integrity__MemType__VDSO;
+		(*cur_entry)++;
+	}
+}
+
+#define VDSO_KLN_SYM(sym) KLN_SYMBOL_P(const struct vdso_image *, sym)
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_64_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start =
+		bhv_virt_to_phys_single(VDSO_KLN_SYM(vdso_image_64)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_64)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 64",
+					    range, cur_entry, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_x32_to_range(ro_region_t *range,
+						unsigned int *cur_entry,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start =
+		bhv_virt_to_phys_single(VDSO_KLN_SYM(vdso_image_x32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_x32)->size;
+	bhv_start_integrity_add_vdso_common(start, size,
+					    "KERNEL VDSO IMAGE X32", range,
+					    cur_entry, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_32_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start =
+		bhv_virt_to_phys_single(VDSO_KLN_SYM(vdso_image_32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_32)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 32",
+					    range, cur_entry, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void __init_km bhv_start_integrity_get_ro_ranges(ro_region_t *range,
+							unsigned int *cur_entry,
+							size_t range_sz)
+{
+	BUG_ON(range_sz < 1);
+
+	range[(*cur_entry)].start_addr = bhv_virt_to_phys_single(
+		(void *)(KLN_SYM(__start_rodata) & PAGE_MASK));
+	range[(*cur_entry)].size = PAGE_ALIGN(KLN_SYM(__end_rodata)) -
+				   (KLN_SYM(__start_rodata) & PAGE_MASK);
+	range[(*cur_entry)].label = "KERNEL READ-ONLY DATA SECTION";
+	range[(*cur_entry)].mem_type =
+		HypABI__Integrity__MemType__DATA_READ_ONLY;
+	(*cur_entry)++;
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, cur_entry,
+						       range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, cur_entry,
+							range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, cur_entry,
+						       range_sz);
+}
+
+static int __init_km bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+	unsigned int i;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	unsigned int nr_ro_ranges = 0;
+	ro_region_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_MAX_RO_RANGES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, &nr_ro_ranges,
+					  BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   nr_ro_ranges, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < nr_ro_ranges; i++) {
+		bhv_mem_region_create_ctor(
+			&n[i]->region, (prev ? &prev->region : NULL),
+			ro_ranges[i].start_addr, ro_ranges[i].size,
+			ro_ranges[i].mem_type,
+			HypABI__Integrity__MemFlags__NONE, ro_ranges[i].label);
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, nr_ro_ranges, (void **)&n);
+
+	return rc;
+}
+
+#ifdef CONFIG_EFI
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		bhv_mem_region_create_ctor(&n->region, NULL, md->phys_addr,
+					   (md->num_pages) << PAGE_SHIFT,
+					   HypABI__Integrity__MemType__CODE,
+					   HypABI__Integrity__MemFlags__NONE,
+					   "EFI REGION");
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1,
+							 &(n->region.create));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+#else /* CONFIG_EFI */
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	return 0;
+}
+#endif /* CONFIG_EFI */
+
+int __init_km bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+#ifndef VASKM // inside kernel tree
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+#endif // VASKM
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+	return bhv_start_integrity_add_ro();
+}
+/**********************************************************/
+
+#define BHV_DIR_PTR_ENTRY_MASK 0x800FFFFFFFFFF89D
+
+/**********************************************************
+ * start
+ **********************************************************/
+struct bhv_pw_data {
+	bool set;
+	uint64_t pgd_offset;
+	uint64_t pgd_value;
+};
+
+static int __init_km bhv_start_pgd_entry(pgd_t *pgd, unsigned long addr,
+					 unsigned long next,
+					 struct mm_walk *walk)
+{
+	struct bhv_pw_data *data = (struct bhv_pw_data *)walk->private;
+	pr_info("%s: pgd=%llx (%llx) addr=%lx next=%lx\n", __FUNCTION__,
+		(uint64_t)pgd, bhv_virt_to_phys_single(pgd), addr, next);
+	BUG_ON(data->set);
+	data->pgd_offset = ((uint64_t)pgd & 0xFFF);
+	data->pgd_value = *((uint64_t *)pgd);
+	data->set = true;
+	return 1;
+}
+
+static const struct mm_walk_ops bhv_start_walk_ops = {
+	.pgd_entry = bhv_start_pgd_entry,
+	.p4d_entry = NULL,
+	.pud_entry = NULL,
+	.pmd_entry = NULL,
+	.pte_entry = NULL,
+	.pte_hole = NULL,
+	.hugetlb_entry = NULL,
+	.test_walk = NULL,
+	.pre_vma = NULL,
+	.post_vma = NULL
+};
+
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	struct bhv_pw_data text_pw_data = { 0 };
+	struct bhv_pw_data ro_pw_data = { 0 };
+
+	mmap_write_lock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(_stext), KLN_SYM(_etext),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &text_pw_data);
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(__start_rodata), KLN_SYM(__end_rodata),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &ro_pw_data);
+	mmap_write_unlock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+
+	BUG_ON(text_pw_data.pgd_offset != ro_pw_data.pgd_offset ||
+	       text_pw_data.pgd_value != ro_pw_data.pgd_value);
+
+	*pgd_offset = text_pw_data.pgd_offset;
+	*pgd_value = (text_pw_data.pgd_value & BHV_DIR_PTR_ENTRY_MASK);
+}
+/**********************************************************/
+
+/**********************************************************
+ * late_start
+ **********************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	BUG_ON(CONFIG_PGTABLE_LEVELS < 4 || CONFIG_PGTABLE_LEVELS > 5);
+
+	init_ptpg_arg->init_pgd = (uint64_t)bhv_virt_to_phys_single(
+		KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd);
+	init_ptpg_arg->pt_levels = pgtable_l5_enabled() ? 5 : 4;
+	init_ptpg_arg->num_ranges = 2;
+
+	init_ptpg_arg->ranges[0] = (uint64_t)KLN_SYM(_stext);
+	init_ptpg_arg->ranges[1] = (uint64_t)KLN_SYM(_etext);
+	init_ptpg_arg->ranges[2] = (uint64_t)KLN_SYM(__start_rodata);
+	init_ptpg_arg->ranges[3] = (uint64_t)KLN_SYM(__end_rodata);
+}
+/**********************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	uint64_t value = *((uint64_t *)(((uint8_t *)mm->pgd) + pgd_offset));
+	return ((value & BHV_DIR_PTR_ENTRY_MASK) == pgd_value);
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 000000000..d5a23494d
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,846 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/kversion.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+#define NOPS x86_nops
+#elif defined BHV_KVERS_5_10
+#define NOPS ideal_nops
+#endif // BHV_KVERS
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end,
+				      const s32 *locks_begin,
+				      const s32 *locks_end, u8 *text_begin,
+				      u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len, bool patch)
+{
+	size_t total_length = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) > sizeof(buf))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(buf + total_length, NOPS[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, NOPS[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_patch_hypercall(insns, buf, total_length, false);
+	}
+}
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off, bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(u8 *target,
+								   bool lock)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target,
+				    "Invalid altinst smp unlock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch.
+		}
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target, "Invalid altinst smp lock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch
+		}
+
+		opcode = unlock_opcode;
+	}
+
+	r = bhv_patch_hypercall((void *)target, &opcode, 1, false);
+
+	if (r) {
+		panic("BHV patch hypercall failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+		    ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+		bytes[i++] = BYTES_NOP1;
+#elif defined BHV_KVERS_5_10
+		bytes[i++] = GENERIC_NOP1;
+#endif // BHV_KVERS
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (invalid op)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+
+	// Retpolines may be disabled or there is another error
+	// this is not an attack.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr,
+			    "Invalid altinst retpoline (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_optimize_nops(bytes, len, true);
+	bhv_patch_hypercall((void *)addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+#ifdef CONFIG_RETHUNK
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		return;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+	    WARN_ONCE(dest != &__x86_return_thunk,
+		      "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+		      addr))
+		return;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	// Feature may be disabled.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_patch_hypercall(addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE)*/
+
+#ifdef CONFIG_PARAVIRT
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+
+#if defined(BHV_KVERS_5_10)
+	used = pv_ops.init.patch(p->type, insn_buff, (unsigned long)p->instr,
+				 p->len);
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+#endif // BHV_KVERS
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, false);
+
+	bhv_patch_hypercall((void *)(p->instr), insn_buff, p->len, false);
+
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_PARAVIRT */
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		if (bhv_patch_violation_hypercall(
+			    instr, "Invalid altinst patch (too big)")) {
+			// Block attempt.
+			return -EACCES;
+		}
+
+		// Allow patch.
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		// This can happen legitmately. We just return.
+		return -EINVAL;
+	}
+
+	if (!boot_cpu_has(a->cpuid & ~ALTINSTR_FLAG_INV) ==
+	    !(a->cpuid & ALTINSTR_FLAG_INV)) {
+		bhv_optimize_nops(instr, a->instrlen, true);
+		return 0;
+	}
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+#if defined(BHV_KVERS_5_10)
+	if (a->instrlen > a->replacementlen) {
+		bhv_add_nops(insn_buff + a->replacementlen,
+			     a->instrlen - a->replacementlen, false);
+		insn_buff_sz += a->instrlen - a->replacementlen;
+	}
+
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+#endif // BHV_KVERS
+
+	if (insn_buff_sz >= HypABI__Patch__MAX_PATCH_SZ)
+		panic("Instruction buffer size too small!");
+
+	rv = bhv_patch_hypercall((void *)instr, insn_buff, insn_buff_sz, false);
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	bhv_optimize_nops(instr, a->instrlen, true);
+#endif // BHV_KVERS_5_15
+
+	return rv;
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a);
+		else
+			bhv_alternatives_patch_vault(a);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+	    cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __bhv_text bhv_apply_ibt_endbr_vault(s32 *s)
+{
+	int ret;
+	u32 endbr, poison;
+	void *addr;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	poison = gen_endbr_poison();
+	addr = (void *)s + *s;
+
+	if (get_kernel_nofault(endbr, addr))
+		goto out;
+
+	if (!is_endbr(endbr))
+		goto out;
+
+	bhv_patch_hypercall(addr, (uint8_t *)&poison, 4, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __init_or_module bhv_apply_ibt_endbr(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_ibt_endbr_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 000000000..8aecb2e6c
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,101 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif // VASKM
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len == 5) {
+		DEF_CHECK_NOP(STATIC_KEY_INIT_NOP);
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *,
+				     ideal_nops)[NOP_ATOMIC5]);
+	}
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len == 2) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[2]);
+	} else if (len == 5) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[5]);
+	}
+#endif // LINUX_VERSION_CODE
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len != 5)
+		return false;
+
+	code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+
+
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+#endif // LINUX_VERSION_CODE
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
+#endif
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 000000000..be74be068
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <linux/version.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static void __always_inline bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static void __always_inline bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+extern void __static_call_return(void);
+
+static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
+{
+	u8 ret = 0;
+
+	if (insn[0] == 0x0f) {
+		u8 tmp = insn[1];
+		if ((tmp & 0xf0) == 0x80)
+			ret = tmp;
+	}
+
+	return ret;
+}
+#endif // LINUX_VERSION_CODE >= 6.1
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func)
+{
+	int ret;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	const void *emulate = NULL;
+#endif // LINUX_VERSION_CODE >= 5.12
+	int size = CALL_INSN_SIZE;
+	const void *code;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	u8 buf[6];
+#endif // LINUX_VERSION_CODE >= 6.1
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+#endif // LINUX_VERSION_CODE >= 5.12
+
+		break;
+
+	case NOP:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 13, 0)
+		code = KLN_SYMBOL(const uint8_t *const *, x86_nops)[5];
+#else // LINUX_VERSION_CODE <= 5.13
+		code = KLN_SYMBOL(const uint8_t *const *,
+				  ideal_nops)[NOP_ATOMIC5];
+#endif // LINUX_VERSION_CODE
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+#else // LINUX_VERSION_CODE <= 5.14
+		code = text_gen_insn(RET_INSN_OPCODE, insn, func);
+		size = RET_INSN_SIZE;
+#endif // LINUX_VERSION_CODE
+		break;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	case JCC:
+		if (!func) {
+			func = KLN_SYMBOL(void *, __static_call_return);
+			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+				func = __x86_return_thunk;
+		}
+
+		buf[0] = 0x0f;
+		__text_gen_insn(buf + 1, __is_Jcc(insn), insn + 1, func, 5);
+		code = buf;
+		size = 6;
+
+		break;
+#endif // LINUX_VERSION_CODE >= 6.1
+	}
+
+	if (memcmp(insn, code, size) == 0) {
+		if (bhv_patch_violation_hypercall(insn,
+						  "Invalid static call")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow patch
+	}
+
+	if (size > HypABI__Patch__MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_patch_hypercall((void *)insn, code, size, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func)
+{
+	unsigned long flags;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/reg_protect.c arch/x86/bhv/reg_protect.c
new file mode 100644
index 000000000..76b062fef
--- /dev/null
+++ arch/x86/bhv/reg_protect.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#include <asm/processor-flags.h>
+
+#define BHV_CR0_FREEZE X86_CR0_WP
+#define BHV_CR4_FREEZE 0x0ULL
+#define BHV_EFER_FREEZE 0x0ULL
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+	int r;
+
+	if (BHV_CR0_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR0,
+			BHV_CR0_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR0!\n");
+	}
+
+	if (BHV_CR4_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR4,
+			BHV_CR4_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR4!\n");
+	}
+
+	if (BHV_EFER_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__EFER,
+			BHV_EFER_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze EFER!\n");
+	}
+}
+/***************************************************/
diff --git arch/x86/entry/common.c arch/x86/entry/common.c
index 93a3122cd..e5b32e57d 100644
--- arch/x86/entry/common.c
+++ arch/x86/entry/common.c
@@ -35,11 +35,15 @@
 #include <asm/syscall.h>
 #include <asm/irq_stack.h>
 
+#include <bhv/integrity.h>
+
 #ifdef CONFIG_X86_64
 __visible noinstr void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 {
 	nr = syscall_enter_from_user_mode(regs, nr);
 
+	bhv_pt_protect_check_pgd(current->active_mm);
+
 	instrumentation_begin();
 	if (likely(nr < NR_syscalls)) {
 		nr = array_index_nospec(nr, NR_syscalls);
diff --git arch/x86/entry/entry_64.S arch/x86/entry/entry_64.S
index bd785386d..0bce28ad0 100644
--- arch/x86/entry/entry_64.S
+++ arch/x86/entry/entry_64.S
@@ -239,6 +239,11 @@ SYM_FUNC_START(__switch_to_asm)
 	pushq	%r14
 	pushq	%r15
 
+#ifdef CONFIG_MEM_NS
+	movq	PER_CPU_VAR(bhv_domain_current_domain), %r12
+	pushq	%r12
+#endif
+
 	/* switch stack */
 	movq	%rsp, TASK_threadsp(%rdi)
 	movq	TASK_threadsp(%rsi), %rsp
@@ -257,6 +262,17 @@ SYM_FUNC_START(__switch_to_asm)
 	 */
 	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
 
+#ifdef CONFIG_MEM_NS
+	// Save arguments in r12 and r13
+	movq	%rdi, %r12
+	movq	%rsi, %r13
+	popq	%rdi
+	callq	bhv_domain_switch
+	// Restore original arguments
+	movq	%r12, %rdi
+	movq	%r13, %rsi
+#endif
+
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
diff --git arch/x86/include/asm/bhv/domain.h arch/x86/include/asm/bhv/domain.h
new file mode 100644
index 000000000..bbfece7fa
--- /dev/null
+++ arch/x86/include/asm/bhv/domain.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) kernel_to_user_pgdp(pgd)
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return pte_present(pte);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pmd_present(pmd);
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pud_present(pud);
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return !(pgprot_val(pmd_pgprot(pmd)) & _PAGE_NX);
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return !(pgprot_val(pud_pgprot(pud)) & _PAGE_NX);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pgprot_val(pte_pgprot(pte)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return !!(pgprot_val(pmd_pgprot(pmd)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return !!(pgprot_val(pud_pgprot(pud)) & _PAGE_USER);
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..99a1e4955
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 000000000..786be1e45
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+#endif // VASKM
+#else /* CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 000000000..f2d2bd51f
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#include <bhv/vault.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL is used in 6.1
+#if defined(CONFIG_RETPOLINE) &&                                               \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __bhv_text bhv_apply_retpolines_vault(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __bhv_text bhv_apply_returns_vault(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#ifdef CONFIG_PARAVIRT
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+#endif // VASKM
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	JCC = 4,
+#endif // LINUX_VERSION_CODE >= 6.1
+};
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func);
+
+#if defined BHV_KVERS_5_15 || defined(BHV_KVERS_6_1)
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func, bool modinit)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+
+#elif defined BHV_KVERS_5_10
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+#endif // BHV_KVERS
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec..591e48b05 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/include/asm/pgtable.h arch/x86/include/asm/pgtable.h
index 9bacde3ff..6346e6641 100644
--- arch/x86/include/asm/pgtable.h
+++ arch/x86/include/asm/pgtable.h
@@ -27,6 +27,8 @@
 #include <asm/fpu/api.h>
 #include <asm-generic/pgtable_uffd.h>
 
+#include <bhv/domain_pt.h>
+
 extern pgd_t early_top_pgt[PTRS_PER_PGD];
 bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
@@ -1035,18 +1037,21 @@ static inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
+	bhv_domain_set_pte_at(mm, addr, ptep, pte);
 	set_pte(ptep, pte);
 }
 
 static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
 			      pmd_t *pmdp, pmd_t pmd)
 {
+	bhv_domain_set_pmd_at(mm, addr, pmdp, pmd);
 	set_pmd(pmdp, pmd);
 }
 
 static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
 			      pud_t *pudp, pud_t pud)
 {
+	bhv_domain_set_pud_at(mm, addr, pudp, pud);
 	native_set_pud(pudp, pud);
 }
 
@@ -1076,7 +1081,9 @@ extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
-	pte_t pte = native_ptep_get_and_clear(ptep);
+	pte_t pte;
+	bhv_domain_clear_pte(mm, addr, ptep, *ptep);
+	pte = native_ptep_get_and_clear(ptep);
 	return pte;
 }
 
@@ -1091,6 +1098,7 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
 		 * Full address destruction in progress; paravirt does not
 		 * care about updates and native needs no locking
 		 */
+		bhv_domain_clear_pte(mm, addr, ptep, pte);
 		pte = native_local_ptep_get_and_clear(ptep);
 	} else {
 		pte = ptep_get_and_clear(mm, addr, ptep);
@@ -1103,6 +1111,7 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pte_t *ptep)
 {
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+	bhv_domain_set_pte_at(mm, addr, ptep, *ptep);
 }
 
 #define flush_tlb_fix_spurious_fault(vma, address) do { } while (0)
@@ -1138,6 +1147,7 @@ static inline int pmd_write(pmd_t pmd)
 static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pmd_t *pmdp)
 {
+	bhv_domain_clear_pmd(mm, addr, pmdp, *pmdp);
 	return native_pmdp_get_and_clear(pmdp);
 }
 
@@ -1145,6 +1155,7 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long
 static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 					unsigned long addr, pud_t *pudp)
 {
+	bhv_domain_clear_pud(mm, addr, pudp, *pudp);
 	return native_pudp_get_and_clear(pudp);
 }
 
@@ -1152,6 +1163,7 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pmd_t *pmdp)
 {
+	bhv_domain_set_pmd_at(mm, addr, pmdp, *pmdp);
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
 }
 
diff --git arch/x86/include/asm/switch_to.h arch/x86/include/asm/switch_to.h
index 9f69cc497..f2b8695dc 100644
--- arch/x86/include/asm/switch_to.h
+++ arch/x86/include/asm/switch_to.h
@@ -19,6 +19,9 @@ asmlinkage void ret_from_fork(void);
  * order of the fields must match the code in __switch_to_asm().
  */
 struct inactive_task_frame {
+#ifdef CONFIG_MEM_NS
+	unsigned long domain;
+#endif
 #ifdef CONFIG_X86_64
 	unsigned long r15;
 	unsigned long r14;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 9ceef8515..d128b4e8c 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -30,12 +30,32 @@
 #include <asm/fixmap.h>
 #include <asm/asm-prototypes.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/domain.h>
+
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
 
 #define MAX_PATCH_LEN (255-1)
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, sync_core);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled, X86_FEATURE_RETHUNK);
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled, X86_FEATURE_RETPOLINE);
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled,
+			    X86_FEATURE_RETPOLINE_LFENCE);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
 static int __initdata_or_module debug_alternative;
 
 static int __init debug_alt(char *str)
@@ -256,7 +276,18 @@ void __init arch_init_ideal_nops(void)
 	}
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_alternatives(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_alternatives);
+#endif
+
 /* Use this to add nops to a buffer, then text_poke the whole buffer. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __init_or_module add_nops(void *insns, unsigned int len)
 {
 	while (len > 0) {
@@ -278,13 +309,15 @@ void text_poke_early(void *addr, const void *opcode, size_t len);
 /*
  * Are we looking at a near JMP with a 1 or 4-byte displacement.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool is_jmp(const u8 opcode)
 {
 	return opcode == 0xeb || opcode == 0xe9;
 }
 
-static void __init_or_module
-recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insn_buff)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __init_or_module recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					    u8 *repl_insn, u8 *insn_buff)
 {
 	u8 *next_rip, *tgt_rip;
 	s32 n_dspl, o_dspl;
@@ -349,6 +382,7 @@ recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insn_buff)
  *
  * Return: number of NOPs found (and replaced).
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline int optimize_nops_range(u8 *instr, u8 instrlen, int off)
 {
 	unsigned long flags;
@@ -379,7 +413,8 @@ static __always_inline int optimize_nops_range(u8 *instr, u8 instrlen, int off)
  * "noinline" to cause control flow change and thus invalidate I$ and
  * cause refetch after modification.
  */
-static void __init_or_module noinline optimize_nops(u8 *instr, size_t len)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __bhv_init_or_module noinline optimize_nops(u8 *instr, size_t len)
 {
 	struct insn insn;
 	int i = 0;
@@ -416,13 +451,23 @@ static void __init_or_module noinline optimize_nops(u8 *instr, size_t len)
  * Marked "noinline" to cause control flow change and thus insn cache
  * to refetch changed I$ lines.
  */
-void __init_or_module noinline apply_alternatives(struct alt_instr *start,
-						  struct alt_instr *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_alternatives(struct alt_instr *start,
+						      struct alt_instr *end)
 {
 	struct alt_instr *a;
 	u8 *instr, *replacement;
 	u8 insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif
+#endif
+
 	DPRINTK("alt table %px, -> %px", start, end);
 
 	/*
@@ -452,16 +497,26 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 		instr = (u8 *)&a->instr_offset + a->instr_offset;
 		replacement = (u8 *)&a->repl_offset + a->repl_offset;
 		BUG_ON(a->instrlen > sizeof(insn_buff));
+#ifdef CONFIG_BHV_VAULT_SPACES
+		/* XXX: Why is this failing in the VAS kernel? */
+		if (feature >= (NCAPINTS + NBUGINTS) * 32)
+			continue;
+#else
 		BUG_ON(feature >= (NCAPINTS + NBUGINTS) * 32);
-
+#endif
 		/*
 		 * Patch if either:
 		 * - feature is present
 		 * - feature not present but ALTINSTR_FLAG_INV is set to mean,
 		 *   patch if feature is *NOT* present.
 		 */
-		if (!boot_cpu_has(feature) == !(a->cpuid & ALTINSTR_FLAG_INV))
+		if (!boot_cpu_has(feature) == !(a->cpuid & ALTINSTR_FLAG_INV)) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			memcpy(insn_buff, instr, a->instrlen);
+			insn_buff_sz = a->instrlen;
+#endif
 			goto next;
+		}
 
 		DPRINTK("feat: %s%d*32+%d, old: (%pS (%px) len: %d), repl: (%px, len: %d)",
 			(a->cpuid & ALTINSTR_FLAG_INV) ? "!" : "",
@@ -497,20 +552,31 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 
 		DUMP_BYTES(insn_buff, insn_buff_sz, "%px: final_insn: ", instr);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+next:
+		optimize_nops(insn_buff, insn_buff_sz);
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(instr, insn_buff, insn_buff_sz);
+		else
+			text_poke_early(instr, insn_buff, insn_buff_sz);
+#else
 		text_poke_early(instr, insn_buff, insn_buff_sz);
 
 next:
 		optimize_nops(instr, a->instrlen);
+#endif
 	}
 
 	kasan_enable_current();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives);
 
 #if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
 
 /*
  * CALL/JMP *%\reg
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int emit_indirect(int op, int reg, u8 *bytes)
 {
 	int i = 0;
@@ -560,6 +626,7 @@ static int emit_indirect(int op, int reg, u8 *bytes)
  *
  * It also tries to inline spectre_v2=retpoline,amd when size permits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 {
 	retpoline_thunk_t *target;
@@ -575,8 +642,13 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE() &&
+	    !bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE())
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+#endif
 		return -1;
 
 	op = insn->opcode.bytes[0];
@@ -609,7 +681,11 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/*
 	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE()) {
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+#endif
 		bytes[i++] = 0x0f;
 		bytes[i++] = 0xae;
 		bytes[i++] = 0xe8; /* LFENCE */
@@ -629,10 +705,21 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 /*
  * Generated by 'objtool --retpoline'.
  */
-void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
+
+/* XXX: Consider moving module_finalze in module.c into the vault. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_retpolines(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS && !CONFIG_BHV_VAULT_SPACES */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -670,10 +757,18 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 			optimize_nops(bytes, len);
 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_retpolines);
 
 #ifdef CONFIG_RETHUNK
 
@@ -688,11 +783,16 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
  *
  *   RET
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 {
 	int i = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETHUNK()) {
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+#endif
 		if (x86_return_thunk == __x86_return_thunk)
 			return -1;
 
@@ -707,10 +807,19 @@ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 	return i;
 }
 
-void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module __apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_returns(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS && !CONFIG_BHV_VAULT_SPACES */
+
 	for (s = start; s < end; s++) {
 		void *dest = NULL, *addr = (void *)s + *s;
 		struct insn insn;
@@ -740,10 +849,23 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 		if (len == insn.length) {
 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_returns);
+
+void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+{
+	__apply_returns(start, end);
+}
 #else
 void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 #endif /* CONFIG_RETHUNK */
@@ -756,8 +878,9 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 #endif /* CONFIG_RETPOLINE && CONFIG_STACK_VALIDATION */
 
 #ifdef CONFIG_SMP
-static void alternatives_smp_lock(const s32 *start, const s32 *end,
-				  u8 *text, u8 *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void alternatives_smp_lock(const s32 *start, const s32 *end, u8 *text,
+				  u8 *text_end)
 {
 	const s32 *poff;
 
@@ -767,11 +890,21 @@ static void alternatives_smp_lock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn DS segment override prefix into lock prefix */
-		if (*ptr == 0x3e)
-			text_poke(ptr, ((unsigned char []){0xf0}), 1);
+		if (*ptr == 0x3e) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0xf0 }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#endif
+		}
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 				    u8 *text, u8 *text_end)
 {
@@ -783,11 +916,19 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn lock prefix into DS segment override prefix */
-		if (*ptr == 0xf0)
-			text_poke(ptr, ((unsigned char []){0x3E}), 1);
+		if (*ptr == 0xf0) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0x3E }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#endif
+		}
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -806,14 +947,27 @@ struct smp_alt_module {
 static LIST_HEAD(smp_alt_modules);
 static bool uniproc_patched = false;	/* protected by text_mutex */
 
-void __init_or_module alternatives_smp_module_add(struct module *mod,
-						  char *name,
-						  void *locks, void *locks_end,
-						  void *text,  void *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_add(struct module *mod,
+						      char *name, void *locks,
+						      void *locks_end,
+						      void *text,
+						      void *text_end)
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	if (!uniproc_patched)
 		goto unlock;
 
@@ -838,16 +992,44 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
-	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end,
+							 locks, locks_end, text,
+							 text_end);
+			// Apply
+			smp_lock = false;
+			p.locks_begin = locks;
+			p.locks_end = locks_end;
+			bhv_alternatives_apply_custom_filter(
+				&p, &smp_lock, bhv_alternatives_find_by_lock);
+		}
+	} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
 unlock:
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_add);
 
-void __init_or_module alternatives_smp_module_del(struct module *mod)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_del(struct module *mod)
 {
 	struct smp_alt_module *item;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	list_for_each_entry(item, &smp_alt_modules, next) {
 		if (mod != item->mod)
 			continue;
@@ -855,9 +1037,15 @@ void __init_or_module alternatives_smp_module_del(struct module *mod)
 		kfree(item);
 		break;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_del);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void alternatives_enable_smp(void)
 {
 	struct smp_alt_module *mod;
@@ -865,25 +1053,50 @@ void alternatives_enable_smp(void)
 	/* Why bother if there are no other CPUs? */
 	BUG_ON(num_possible_cpus() == 1);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (uniproc_patched) {
 		pr_info("switching to SMP code\n");
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
-			alternatives_smp_lock(mod->locks, mod->locks_end,
-					      mod->text, mod->text_end);
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+		}
 		uniproc_patched = false;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_enable_smp);
 
 /*
  * Return 1 if the address range is reserved for SMP-alternatives.
  * Must hold text_mutex.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int alternatives_text_reserved(void *start, void *end)
 {
 	struct smp_alt_module *mod;
@@ -906,15 +1119,27 @@ int alternatives_text_reserved(void *start, void *end)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_text_reserved);
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_PARAVIRT
-void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
-				     struct paravirt_patch_site *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module apply_paravirt(struct paravirt_patch_site *start,
+					 struct paravirt_patch_site *end)
 {
 	struct paravirt_patch_site *p;
 	char insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (p = start; p < end; p++)
+			bhv_apply_paravirt(p);
+		return;
+	}
+#endif
+#endif
+
 	for (p = start; p < end; p++) {
 		unsigned int used;
 
@@ -927,9 +1152,18 @@ void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
 
 		/* Pad the rest with nops */
 		add_nops(insn_buff + used, p->len - used);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(p->instr, insn_buff, p->len);
+		else
+			text_poke_early(p->instr, insn_buff, p->len);
+#else
 		text_poke_early(p->instr, insn_buff, p->len);
+#endif
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_paravirt);
+
 extern struct paravirt_patch_site __start_parainstructions[],
 	__stop_parainstructions[];
 #endif	/* CONFIG_PARAVIRT */
@@ -950,6 +1184,7 @@ extern struct paravirt_patch_site __start_parainstructions[],
  * convention such that we can 'call' it from assembly.
  */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 extern void int3_magic(unsigned int *ptr); /* defined in asm */
 
 asm (
@@ -983,7 +1218,7 @@ int3_exception_notify(struct notifier_block *self, unsigned long val, void *data
 	return NOTIFY_STOP;
 }
 
-static void __init int3_selftest(void)
+static noinline void __init int3_selftest(void)
 {
 	static __initdata struct notifier_block int3_exception_nb = {
 		.notifier_call	= int3_exception_notify,
@@ -1017,7 +1252,8 @@ static void __init int3_selftest(void)
 	unregister_die_notifier(&int3_exception_nb);
 }
 
-void __init alternative_instructions(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __alternative_instructions(void)
 {
 	int3_selftest();
 
@@ -1069,6 +1305,12 @@ void __init alternative_instructions(void)
 	restart_nmi();
 	alternatives_patched = 1;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __alternative_instructions);
+
+void __init alternative_instructions(void)
+{
+	__alternative_instructions();
+}
 
 /**
  * text_poke_early - Update instructions on a live kernel at boot time
@@ -1082,11 +1324,25 @@ void __init alternative_instructions(void)
  * instructions. And on the local CPU you need to be protected against NMI or
  * MCE handlers seeing an inconsistent instruction while you patch.
  */
-void __init_or_module text_poke_early(void *addr, const void *opcode,
-				      size_t len)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module text_poke_early(void *addr, const void *opcode,
+					  size_t len)
 {
 	unsigned long flags;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	if (boot_cpu_has(X86_FEATURE_NX) &&
 	    is_module_text_address((unsigned long)addr)) {
 		/*
@@ -1098,7 +1354,11 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 	} else {
 		local_irq_save(flags);
 		memcpy(addr, opcode, len);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_sync_core();
+#else
 		sync_core();
+#endif
 		local_irq_restore(flags);
 
 		/*
@@ -1107,6 +1367,7 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 		 */
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_early);
 
 typedef struct {
 	struct mm_struct *mm;
@@ -1125,6 +1386,7 @@ typedef struct {
  *          loaded, thereby preventing interrupt handler bugs from overriding
  *          the kernel memory protection.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 {
 	temp_mm_state_t temp_state;
@@ -1141,6 +1403,9 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 	switch_mm_irqs_off(NULL, mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -1159,10 +1424,14 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 	return temp_state;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(prev_state.mm == NULL ? NULL : prev_state.mm->owner);
+#endif
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
@@ -1175,6 +1444,7 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 __ro_after_init struct mm_struct *poking_mm;
 __ro_after_init unsigned long poking_addr;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *__text_poke(void *addr, const void *opcode, size_t len)
 {
 	bool cross_page_boundary = offset_in_page(addr) + len > PAGE_SIZE;
@@ -1240,7 +1510,22 @@ static void *__text_poke(void *addr, const void *opcode, size_t len)
 	prev = use_temporary_mm(poking_mm);
 
 	kasan_disable_current();
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives((u8 *)poking_addr + offset_in_page(addr),
+				       opcode, len);
+	} else {
+		if (bhv_integrity_is_enabled())
+			bhv_apply_alternatives((u8 *)poking_addr + offset_in_page(addr),
+					       opcode, len);
+		else
+			memcpy((u8 *)poking_addr + offset_in_page(addr), opcode, len);
+	}
+#else
 	memcpy((u8 *)poking_addr + offset_in_page(addr), opcode, len);
+#endif
+
 	kasan_enable_current();
 
 	/*
@@ -1278,6 +1563,7 @@ static void *__text_poke(void *addr, const void *opcode, size_t len)
 	pte_unmap_unlock(ptep, ptl);
 	return addr;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __text_poke);
 
 /**
  * text_poke - Update instructions on a live kernel
@@ -1295,6 +1581,7 @@ static void *__text_poke(void *addr, const void *opcode, size_t len)
  * by registering a module notifier, and ordering module removal and patching
  * trough a mutex.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *text_poke(void *addr, const void *opcode, size_t len)
 {
 	lockdep_assert_held(&text_mutex);
@@ -1321,11 +1608,17 @@ void *text_poke_kgdb(void *addr, const void *opcode, size_t len)
 	return __text_poke(addr, opcode, len);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_sync_core(void *info)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_sync_core();
+#else
 	sync_core();
+#endif
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void text_poke_sync(void)
 {
 	on_each_cpu(do_sync_core, NULL, 1);
@@ -1350,6 +1643,7 @@ struct bp_patching_desc {
 
 static struct bp_patching_desc bp_desc;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline
 struct bp_patching_desc *try_get_desc(void)
 {
@@ -1361,6 +1655,7 @@ struct bp_patching_desc *try_get_desc(void)
 	return desc;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void put_desc(void)
 {
 	struct bp_patching_desc *desc = &bp_desc;
@@ -1369,11 +1664,13 @@ static __always_inline void put_desc(void)
 	arch_atomic_dec(&desc->refs);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void *text_poke_addr(struct text_poke_loc *tp)
 {
 	return _stext + tp->rel_addr;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline int patch_cmp(const void *key, const void *elt)
 {
 	struct text_poke_loc *tp = (struct text_poke_loc *) elt;
@@ -1464,7 +1761,9 @@ noinstr int poke_int3_handler(struct pt_regs *regs)
 }
 
 #define TP_VEC_MAX (PAGE_SIZE / sizeof(struct text_poke_loc))
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct text_poke_loc tp_vec[TP_VEC_MAX];
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static int tp_vec_nr;
 
 /**
@@ -1488,6 +1787,7 @@ static int tp_vec_nr;
  *		  replacing opcode
  *	- sync cores
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries)
 {
 	unsigned char int3 = INT3_INSN_OPCODE;
@@ -1597,6 +1897,7 @@ static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries
 		atomic_cond_read_acquire(&bp_desc.refs, !VAL);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
 			       const void *opcode, size_t len, const void *emulate)
 {
@@ -1667,6 +1968,7 @@ static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
  * We hard rely on the tp_vec being ordered; ensure this is so by flushing
  * early if needed.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool tp_order_fail(void *addr)
 {
 	struct text_poke_loc *tp;
@@ -1684,6 +1986,7 @@ static bool tp_order_fail(void *addr)
 	return false;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_flush(void *addr)
 {
 	if (tp_vec_nr == TP_VEC_MAX || tp_order_fail(addr)) {
@@ -1692,12 +1995,24 @@ static void text_poke_flush(void *addr)
 	}
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void text_poke_finish(void)
+{
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		text_poke_flush(NULL);
+	}
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_finish);
+#else
 void text_poke_finish(void)
 {
 	text_poke_flush(NULL);
 }
+#endif
 
-void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc *tp;
 
@@ -1706,11 +2021,25 @@ void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const voi
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	text_poke_flush(addr);
 
 	tp = &tp_vec[tp_vec_nr++];
 	text_poke_loc_init(tp, addr, opcode, len, emulate);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_queue);
 
 /**
  * text_poke_bp() -- update instructions on live kernel on SMP
@@ -1723,10 +2052,24 @@ void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const voi
  * dynamically allocated memory. This function should be used when it is
  * not possible to allocate memory.
  */
-void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc tp;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	if (unlikely(system_state == SYSTEM_BOOTING)) {
 		text_poke_early(addr, opcode, len);
 		return;
@@ -1735,3 +2078,4 @@ void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *
 	text_poke_loc_init(&tp, addr, opcode, len, emulate);
 	text_poke_bp_batch(&tp, 1);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_bp);
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3..20bfa373b 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index ee1a283f8..93615b106 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -10,6 +10,7 @@
 #include <asm/proto.h>
 #include <asm/desc.h>
 #include <asm/hw_irq.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -310,6 +311,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index 5ba8477c2..53fecfda7 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -15,7 +15,29 @@
 #include <asm/kprobes.h>
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
 
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_jump_label(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_jump_label);
+#endif
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void bug_at(const void *ip, int line)
 {
 	/*
@@ -27,6 +49,7 @@ static void bug_at(const void *ip, int line)
 	BUG();
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static const void *
 __jump_label_set_jump_code(struct jump_entry *entry, enum jump_label_type type, int init)
 {
@@ -57,13 +80,33 @@ __jump_label_set_jump_code(struct jump_entry *entry, enum jump_label_type type,
 
 	return code;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_set_jump_code);
 
-static inline void __jump_label_transform(struct jump_entry *entry,
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
 					  enum jump_label_type type,
 					  int init)
 {
 	const void *opcode = __jump_label_set_jump_code(entry, type, init);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_patch_jump_label(entry, opcode, JUMP_LABEL_NOP_SIZE);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, opcode, JUMP_LABEL_NOP_SIZE);
+		return;
+	}
+#else
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, opcode, JUMP_LABEL_NOP_SIZE);
+		return;
+	}
+#endif
+
 	/*
 	 * As long as only a single processor is running and the code is still
 	 * not marked as RO, text_poke_early() can be used; Checking that
@@ -84,21 +127,32 @@ static inline void __jump_label_transform(struct jump_entry *entry,
 	text_poke_bp((void *)jump_entry_code(entry), opcode, JUMP_LABEL_NOP_SIZE, NULL);
 }
 
-static void __ref jump_label_transform(struct jump_entry *entry,
-				       enum jump_label_type type,
-				       int init)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void jump_label_transform(struct jump_entry *entry,
+				 enum jump_label_type type,
+				 int init)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	__jump_label_transform(entry, type, init);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
 	jump_label_transform(entry, type, 0);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool arch_jump_label_transform_queue(struct jump_entry *entry,
 				     enum jump_label_type type)
 {
@@ -112,20 +166,65 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		return true;
 	}
 
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+
+	/* We need this check to patch patching the above static key. */
+#else /* !CONFIG_BHV_VAULT_SPACES */
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_SPACES */
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	opcode = __jump_label_set_jump_code(entry, type, 0);
-	text_poke_queue((void *)jump_entry_code(entry),
-			opcode, JUMP_LABEL_NOP_SIZE, NULL);
+	text_poke_queue((void *)jump_entry_code(entry), opcode,
+			JUMP_LABEL_NOP_SIZE, NULL);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void arch_jump_label_transform_apply(void)
+{
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		bhv_wrapper_mutex_lock(&text_mutex);
+#else
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+#endif
+		text_poke_finish();
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_mutex_unlock(&text_mutex);
+#else
+		mutex_unlock(&text_mutex);
+#endif
+	}
+}
+#else /* !CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
 
 static enum {
 	JL_STATE_START,
@@ -133,8 +232,9 @@ static enum {
 	JL_STATE_UPDATE,
 } jlstate __initdata_or_module = JL_STATE_START;
 
-__init_or_module void arch_jump_label_transform_static(struct jump_entry *entry,
-				      enum jump_label_type type)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+__bhv_init_or_module void arch_jump_label_transform_static(struct jump_entry *entry,
+				      			   enum jump_label_type type)
 {
 	/*
 	 * This function is called at boot up and when modules are
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index 455e19584..1952cf18f 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -255,6 +258,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*retpolines = NULL, *returns = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".text", secstrings + s->sh_name))
 			text = s;
@@ -282,9 +291,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks && text) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = (void *)text->sh_addr;
+		arch.text_end = (void *)text->sh_addr + text->sh_size;
+	}
+#endif
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (locks && text) {
diff --git arch/x86/kernel/paravirt.c arch/x86/kernel/paravirt.c
index 9d91061b8..5cf818936 100644
--- arch/x86/kernel/paravirt.c
+++ arch/x86/kernel/paravirt.c
@@ -33,6 +33,8 @@
 #include <asm/io_bitmap.h>
 #include <asm/text-patching.h>
 
+#include <bhv/vault.h>
+
 /*
  * nop stub, which must not clobber anything *including the stack* to
  * avoid confusing the entry prologues.
@@ -60,6 +62,7 @@ struct branch {
        u32 delta;
 } __attribute__((packed));
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static unsigned paravirt_patch_call(void *insn_buff, const void *target,
 				    unsigned long addr, unsigned len)
 {
@@ -103,6 +106,7 @@ void __init native_pv_lock_init(void)
 		static_branch_disable(&virt_spin_lock_key);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 unsigned paravirt_patch_default(u8 type, void *insn_buff,
 				unsigned long addr, unsigned len)
 {
@@ -134,7 +138,9 @@ unsigned paravirt_patch_default(u8 type, void *insn_buff,
 
 	return ret;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, paravirt_patch_default);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 unsigned paravirt_patch_insns(void *insn_buff, unsigned len,
 			      const char *start, const char *end)
 {
@@ -147,6 +153,7 @@ unsigned paravirt_patch_insns(void *insn_buff, unsigned len,
 
 	return insn_len;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, paravirt_patch_insns);
 
 struct static_key paravirt_steal_enabled;
 struct static_key paravirt_steal_rq_enabled;
diff --git arch/x86/kernel/paravirt_patch.c arch/x86/kernel/paravirt_patch.c
index 2fada2c34..b4c4d95f8 100644
--- arch/x86/kernel/paravirt_patch.c
+++ arch/x86/kernel/paravirt_patch.c
@@ -1,6 +1,8 @@
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/stringify.h>
 
+#include <bhv/vault.h>
+
 #include <asm/paravirt.h>
 #include <asm/asm-offsets.h>
 
@@ -65,6 +67,7 @@ static const struct patch_lock patch_data_lock = {
 };
 #endif /* CONFIG_PARAVIRT_SPINLOCKS */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 unsigned int native_patch(u8 type, void *insn_buff, unsigned long addr,
 			  unsigned int len)
 {
diff --git arch/x86/kernel/process.c arch/x86/kernel/process.c
index 4f731981d..3ef959588 100644
--- arch/x86/kernel/process.c
+++ arch/x86/kernel/process.c
@@ -44,6 +44,8 @@
 #include <asm/proto.h>
 #include <asm/frame.h>
 
+#include <bhv/domain.h>
+
 #include "process.h"
 
 /*
@@ -161,6 +163,10 @@ int copy_thread(unsigned long clone_flags, unsigned long sp, unsigned long arg,
 	frame->flags = X86_EFLAGS_FIXED;
 #endif
 
+#ifdef CONFIG_MEM_NS
+	frame->domain = bhv_get_domain(p);
+#endif
+
 	/* Kernel thread ? */
 	if (unlikely(p->flags & PF_KTHREAD)) {
 		memset(childregs, 0, sizeof(struct pt_regs));
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index 273e9b77b..0bbf61197 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,28 +4,62 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(void, mutex_unlock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(bool, cpu_feature_enabled, u16, feature);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_VAULT_SPACES)
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
 	JMP = 2,  /* tramp / site tail-call */
 	RET = 3,  /* tramp / site cond-tail-call */
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
  * that there is no false-positive trampoline identification while also being a
  * speculation stop.
  */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 tramp_ud[] = { 0x0f, 0xb9, 0xcc };
 
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
 
-static void __ref __static_call_transform(void *insn, enum insn_type type,
-					  void *func, bool modinit)
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_static_call(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_static_call);
+#endif
+
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void __static_call_transform(void *insn, enum insn_type type,
+				    void *func, bool modinit)
 {
 	int size = CALL_INSN_SIZE;
 	const void *code;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func);
+		return;
+	}
+#endif
+
 	switch (type) {
 	case CALL:
 		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
@@ -40,7 +74,11 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 		break;
 
 	case RET:
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (bhv_wrapper_cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#else
 		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#endif
 			code = text_gen_insn(JMP32_INSN_OPCODE, insn, x86_return_thunk);
 		else
 			code = &retinsn;
@@ -50,12 +88,21 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	if (memcmp(insn, code, size) == 0)
 		return;
 
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(insn, code, size);
+		return;
+	}
+#endif
+
 	if (system_state == SYSTEM_BOOTING || modinit)
 		return text_poke_early(insn, code, size);
 
 	text_poke_bp(insn, code, size, NULL);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_call_validate(void *insn, bool tail)
 {
 	u8 opcode = *(u8 *)insn;
@@ -76,6 +123,7 @@ static void __static_call_validate(void *insn, bool tail)
 	WARN_ONCE(1, "unexpected static_call insn opcode 0x%x at %pS\n", opcode, insn);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline enum insn_type __sc_insn(bool null, bool tail)
 {
 	/*
@@ -91,9 +139,14 @@ static inline enum insn_type __sc_insn(bool null, bool tail)
 	return 2*tail + null;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (tramp) {
 		__static_call_validate(tramp, true);
@@ -105,8 +158,13 @@ void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 		__static_call_transform(site, __sc_insn(!func, tail), func, false);
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, arch_static_call_transform);
 EXPORT_SYMBOL_GPL(arch_static_call_transform);
 
 #ifdef CONFIG_RETHUNK
@@ -121,6 +179,7 @@ EXPORT_SYMBOL_GPL(arch_static_call_transform);
  * This means that __static_call_transform() above can have overwritten the
  * return trampoline and we now need to fix things up to be consistent.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool __static_call_fixup(void *tramp, u8 op, void *dest)
 {
 	unsigned long addr = (unsigned long)tramp;
@@ -141,11 +200,27 @@ bool __static_call_fixup(void *tramp, u8 op, void *dest)
 		return false;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
+
 	if (op == RET_INSN_OPCODE || dest == &__x86_return_thunk)
 		__static_call_transform(tramp, RET, NULL, true);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 
 	return true;
 }
+/*
+ * XXX: REMOVE ENTRY POINT AS SOON AS WE ADD ALTERNATIVE INSTRUCTIONS INTO THE
+ * VAULT.
+ */
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_fixup);
+
 #endif
diff --git arch/x86/kernel/traps.c arch/x86/kernel/traps.c
index 98838b784..71a27e64d 100644
--- arch/x86/kernel/traps.c
+++ arch/x86/kernel/traps.c
@@ -29,6 +29,7 @@
 #include <linux/errno.h>
 #include <linux/kexec.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/timer.h>
 #include <linux/init.h>
@@ -84,6 +85,16 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
+__always_inline static bool is_vault(unsigned long addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
+
 __always_inline int is_valid_bugaddr(unsigned long addr)
 {
 	if (addr < TASK_SIZE_MAX)
@@ -219,6 +230,11 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 {
 	bool handled = false;
 
+	if (is_vault(regs->ip)){
+		show_regs(regs);
+		panic("[%s] BUG in vault at %pS\n", __FUNCTION__, (void*)regs->ip);
+	}
+
 	if (!is_valid_bugaddr(regs->ip))
 		return handled;
 
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index 740f87d8a..88a2fbc6b 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -150,9 +150,14 @@ SECTIONS
 		ALIGN_ENTRY_TEXT_END
 		SOFTIRQENTRY_TEXT
 		STATIC_CALL_TEXT
+		BHV_TEXT
 		*(.fixup)
 		*(.gnu.warning)
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+		BHV_VAULT_TEXT(jump_label)
+#endif
+
 #ifdef CONFIG_RETPOLINE
 		__indirect_thunk_start = .;
 		*(.text..__x86.indirect_thunk)
@@ -253,8 +258,12 @@ SECTIONS
 	 *
 	 * See static_cpu_has() for an example.
 	 */
+	. = ALIGN(PAGE_SIZE);
 	.altinstr_aux : AT(ADDR(.altinstr_aux) - LOAD_OFFSET) {
+		__altinstr_aux_start = .;
 		*(.altinstr_aux)
+		. = ALIGN(PAGE_SIZE);
+		__altinstr_aux_end = .;
 	}
 
 	INIT_DATA_SECTION(16)
@@ -280,7 +289,7 @@ SECTIONS
 	 * baremetal native ones. Think page table operations.
 	 * Details in paravirt_types.h
 	 */
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.parainstructions : AT(ADDR(.parainstructions) - LOAD_OFFSET) {
 		__parainstructions = .;
 		*(.parainstructions)
@@ -293,18 +302,19 @@ SECTIONS
 	 * __x86_indirect_thunk_*(). These instructions can be patched along
 	 * with alternatives, after which the section can be freed.
 	 */
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {
 		__retpoline_sites = .;
 		*(.retpoline_sites)
 		__retpoline_sites_end = .;
 	}
 
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.return_sites : AT(ADDR(.return_sites) - LOAD_OFFSET) {
 		__return_sites = .;
 		*(.return_sites)
 		__return_sites_end = .;
+		. = ALIGN(PAGE_SIZE);
 	}
 #endif
 
@@ -315,8 +325,10 @@ SECTIONS
 	 */
 	. = ALIGN(8);
 	.altinstructions : AT(ADDR(.altinstructions) - LOAD_OFFSET) {
+		. = ALIGN(PAGE_SIZE);
 		__alt_instructions = .;
 		*(.altinstructions)
+		. = ALIGN(PAGE_SIZE);
 		__alt_instructions_end = .;
 	}
 
@@ -348,15 +360,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -390,6 +416,34 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.comm : AT(ADDR(.bhv.vault.comm) - LOAD_OFFSET) {
+		__bhv_vault_comm_start = .;
+		. += PAGE_SIZE;
+		. = ALIGN(PAGE_SIZE);
+		__bhv_vault_comm_end = .;
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.data : AT(ADDR(.bhv.vault.data) - LOAD_OFFSET) {
+		BHV_VAULT_DATA(jump_label)
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.rodata : AT(ADDR(.bhv.vault.rodata) - LOAD_OFFSET) {
+		BHV_VAULT_RO_DATA(jump_label)
+	}
+#endif
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/lib/inat.c arch/x86/lib/inat.c
index b0f3b2a62..7d5351f4f 100644
--- arch/x86/lib/inat.c
+++ arch/x86/lib/inat.c
@@ -6,15 +6,19 @@
  */
 #include <asm/insn.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 /* Attribute tables are generated from opcode map */
 #include "inat-tables.c"
 
 /* Attribute search APIs */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
 {
 	return inat_primary_table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int inat_get_last_prefix_id(insn_byte_t last_pfx)
 {
 	insn_attr_t lpfx_attr;
@@ -23,6 +27,7 @@ int inat_get_last_prefix_id(insn_byte_t last_pfx)
 	return inat_last_prefix_id(lpfx_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 				      insn_attr_t esc_attr)
 {
@@ -42,6 +47,7 @@ insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 	return table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 				     insn_attr_t grp_attr)
 {
@@ -62,6 +68,7 @@ insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 	       inat_group_common_attribute(grp_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
 				   insn_byte_t vex_p)
 {
diff --git arch/x86/lib/insn.c arch/x86/lib/insn.c
index 24e89239d..e79762926 100644
--- arch/x86/lib/insn.c
+++ arch/x86/lib/insn.c
@@ -18,6 +18,8 @@
 
 #include <asm/emulate_prefix.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 /* Verify next sizeof(t) bytes can be on the same instruction */
 #define validate_next(t, insn, n)	\
 	((insn)->next_byte + sizeof(t) + n <= (insn)->end_kaddr)
@@ -42,6 +44,7 @@
  * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
  * @x86_64:	!0 for 64-bit kernel or 64-bit app
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 {
 	/*
@@ -66,6 +69,7 @@ void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 static const insn_byte_t xen_prefix[] = { __XEN_EMULATE_PREFIX };
 static const insn_byte_t kvm_prefix[] = { __KVM_EMULATE_PREFIX };
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __insn_get_emulate_prefix(struct insn *insn,
 				     const insn_byte_t *prefix, size_t len)
 {
@@ -85,6 +89,7 @@ static int __insn_get_emulate_prefix(struct insn *insn,
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void insn_get_emulate_prefix(struct insn *insn)
 {
 	if (__insn_get_emulate_prefix(insn, xen_prefix, sizeof(xen_prefix)))
@@ -105,6 +110,7 @@ static void insn_get_emulate_prefix(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_prefixes(struct insn *insn)
 {
 	struct insn_field *prefixes = &insn->prefixes;
@@ -244,6 +250,7 @@ int insn_get_prefixes(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_opcode(struct insn *insn)
 {
 	struct insn_field *opcode = &insn->opcode;
@@ -315,6 +322,7 @@ int insn_get_opcode(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_modrm(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -364,6 +372,7 @@ int insn_get_modrm(struct insn *insn)
  * If necessary, first collects the instruction up to and including the
  * ModRM byte.  No effect if @insn->x86_64 is 0.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_rip_relative(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -395,6 +404,7 @@ int insn_rip_relative(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_sib(struct insn *insn)
 {
 	insn_byte_t modrm;
@@ -438,6 +448,7 @@ int insn_get_sib(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_displacement(struct insn *insn)
 {
 	insn_byte_t mod, rm, base;
@@ -501,6 +512,7 @@ int insn_get_displacement(struct insn *insn)
 }
 
 /* Decode moffset16/32/64. Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_moffset(struct insn *insn)
 {
 	switch (insn->addr_bytes) {
@@ -530,6 +542,7 @@ static int __get_moffset(struct insn *insn)
 }
 
 /* Decode imm v32(Iz). Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv32(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -553,6 +566,7 @@ static int __get_immv32(struct insn *insn)
 }
 
 /* Decode imm v64(Iv/Ov), Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -581,6 +595,7 @@ static int __get_immv(struct insn *insn)
 }
 
 /* Decode ptr16:16/32(Ap) */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immptr(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -620,6 +635,7 @@ static int __get_immptr(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_immediate(struct insn *insn)
 {
 	int ret;
@@ -701,6 +717,7 @@ int insn_get_immediate(struct insn *insn)
  *  - 0 on success
  *  - < 0 on error
 */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_length(struct insn *insn)
 {
 	int ret;
@@ -731,6 +748,7 @@ int insn_get_length(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_decode(struct insn *insn, const void *kaddr, int buf_len, enum insn_mode m)
 {
 	int ret;
diff --git arch/x86/lib/memcpy_64.S arch/x86/lib/memcpy_64.S
index 59cf2343f..2f2f0905f 100644
--- arch/x86/lib/memcpy_64.S
+++ arch/x86/lib/memcpy_64.S
@@ -1,13 +1,18 @@
 /* SPDX-License-Identifier: GPL-2.0-only */
 /* Copyright 2002 Andi Kleen */
 
+#include <bhv/vault.h>
 #include <linux/linkage.h>
 #include <asm/errno.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
 #include <asm/export.h>
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE jump_label
+#else
 .pushsection .noinstr.text, "ax"
+#endif
 
 /*
  * We build a jump to memcpy_orig by default which gets NOPped out on
@@ -183,4 +188,8 @@ SYM_FUNC_START_LOCAL(memcpy_orig)
 	RET
 SYM_FUNC_END(memcpy_orig)
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_ASM_POP_SECTION
+#else
 .popsection
+#endif
diff --git arch/x86/lib/memset_64.S arch/x86/lib/memset_64.S
index d624f2bc4..097f8df3a 100644
--- arch/x86/lib/memset_64.S
+++ arch/x86/lib/memset_64.S
@@ -1,11 +1,14 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 /* Copyright 2002 Andi Kleen, SuSE Labs */
 
+#include <bhv/vault.h>
 #include <linux/linkage.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h>
 #include <asm/export.h>
 
+BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE jump_label
+
 /*
  * ISO C memset - set a memory block to a byte value. This function uses fast
  * string to get better performance than the original function. The code is
@@ -138,3 +141,5 @@ SYM_FUNC_START_LOCAL(memset_orig)
 	jmp .Lafter_bad_alignment
 .Lfinal:
 SYM_FUNC_END(memset_orig)
+
+BHV_ASM_POP_SECTION
diff --git arch/x86/mm/fault.c arch/x86/mm/fault.c
index 98a5924d9..79c42a8d5 100644
--- arch/x86/mm/fault.c
+++ arch/x86/mm/fault.c
@@ -31,6 +31,10 @@
 #include <asm/pgtable_areas.h>		/* VMALLOC_START, ...		*/
 #include <asm/kvm_para.h>		/* kvm_handle_async_pf		*/
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
@@ -513,6 +517,31 @@ static void show_ldttss(const struct desc_ptr *gdt, const char *name, u16 index)
 static void
 show_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kaccess_events()) {
+		// is kernel?
+		if (!(error_code & X86_PF_USER)) {
+			// is perm violation?
+			if ((error_code & X86_PF_PROT) &&
+			    !(error_code & X86_PF_RSVD) &&
+			    !(error_code & X86_PF_PK)) {
+				uint8_t type;
+				if (error_code &
+				    X86_PF_INSTR) { // is instr fetch?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+				} else if (error_code &
+					   X86_PF_WRITE) { // is write?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+				} else {
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+				}
+				bhv_guestlog_log_kaccess((uint64_t)address,
+							 type);
+			}
+		}
+	}
+#endif
+
 	if (!oops_may_print())
 		return;
 
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index dd15fdee4..d3a2e7ff4 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -39,6 +39,9 @@
 
 #include "mm_internal.h"
 
+#include <bhv/init/start.h>
+#include <bhv/vault.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -953,12 +956,84 @@ void free_kernel_image_pages(const char *what, void *begin, void *end)
 		set_memory_np_noalias(begin_ul, len_pages);
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __alt_instructions[];
+extern char __alt_instructions_end[];
+extern char __altinstr_aux_start[];
+extern char __altinstr_aux_end[];
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+static void __ref bhv_vault_release_memory(void)
+{
+	int rc;
+	HypABI__Wagner__Delete__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return;
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+#ifdef CONFIG_PARAVIRT
+	vault.mem.gpa = bhv_virt_to_phys_single(__parainstructions);
+	vault.mem.size = (unsigned long)__parainstructions_end - (unsigned long)__parainstructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+#endif
+
+#ifdef CONFIG_RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end - (unsigned long)__retpoline_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end - (unsigned long)__return_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+#endif
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__altinstr_aux_start);
+	vault.mem.size = (unsigned long)__altinstr_aux_end - (unsigned long)__altinstr_aux_start;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+}
+
+#endif
+
 void __ref free_initmem(void)
 {
 	e820__reallocate_tables();
 
 	mem_encrypt_free_decrypted_mem();
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_vault_release_memory();
+#endif
+
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git arch/x86/mm/pat/memtype.c arch/x86/mm/pat/memtype.c
index adc76b413..462feb81b 100644
--- arch/x86/mm/pat/memtype.c
+++ arch/x86/mm/pat/memtype.c
@@ -312,7 +312,7 @@ void init_cache_modes(void)
 		 * NOTE: When WC or WP is used, it is redirected to UC- per
 		 * the default setup in __cachemode2pte_tbl[].
 		 */
-		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, UC_MINUS) | PAT(3, UC) |
+		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, WC) | PAT(3, UC) |
 		      PAT(4, WB) | PAT(5, WT) | PAT(6, UC_MINUS) | PAT(7, UC);
 	}
 
diff --git arch/x86/mm/pgtable.c arch/x86/mm/pgtable.c
index 204b25ee2..1271e8d71 100644
--- arch/x86/mm/pgtable.c
+++ arch/x86/mm/pgtable.c
@@ -7,6 +7,10 @@
 #include <asm/fixmap.h>
 #include <asm/mtrr.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 phys_addr_t physical_mask __ro_after_init = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;
 EXPORT_SYMBOL(physical_mask);
@@ -489,8 +493,12 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 {
 	int changed = !pte_same(*ptep, entry);
 
-	if (changed && dirty)
+	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pte_at(vma->vm_mm, address, ptep, entry);
+#endif
 		set_pte(ptep, entry);
+	}
 
 	return changed;
 }
@@ -505,6 +513,9 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pmd_at(vma->vm_mm, address, pmdp, entry);
+#endif
 		set_pmd(pmdp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pmd
@@ -525,6 +536,9 @@ int pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,
 	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pud_at(vma->vm_mm, address, pudp, entry);
+#endif
 		set_pud(pudp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pud
diff --git arch/x86/mm/tlb.c arch/x86/mm/tlb.c
index 569ac1d57..c892cbd91 100644
--- arch/x86/mm/tlb.c
+++ arch/x86/mm/tlb.c
@@ -15,6 +15,8 @@
 #include <asm/cache.h>
 #include <asm/apic.h>
 
+#include <bhv/domain.h>
+
 #include "mm_internal.h"
 
 #ifdef CONFIG_PARAVIRT
@@ -314,6 +316,9 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	local_irq_save(flags);
 	switch_mm_irqs_off(prev, next, tsk);
 	local_irq_restore(flags);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(next == NULL ? NULL : next->owner);
+#endif
 }
 
 static inline unsigned long mm_mangle_tif_spec_ib(struct task_struct *next)
@@ -513,7 +518,7 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 		smp_mb();
 		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
 		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
-				next_tlb_gen)
+		    next_tlb_gen)
 			return;
 
 		/*
diff --git arch/x86/tools/Makefile arch/x86/tools/Makefile
index 55b1ab378..37fa1dd3c 100644
--- arch/x86/tools/Makefile
+++ arch/x86/tools/Makefile
@@ -29,9 +29,9 @@ posttest: $(obj)/insn_decoder_test vmlinux $(obj)/insn_sanity
 hostprogs += insn_decoder_test insn_sanity
 
 # -I needed for generated C source and C source which in the kernel tree.
-HOSTCFLAGS_insn_decoder_test.o := -Wall -I$(objtree)/arch/x86/lib/ -I$(srctree)/arch/x86/include/uapi/ -I$(srctree)/arch/x86/include/ -I$(srctree)/arch/x86/lib/ -I$(srctree)/include/uapi/
+HOSTCFLAGS_insn_decoder_test.o := -Wall -I$(objtree)/tools/arch/x86/lib/ -I$(srctree)/arch/x86/include/uapi/ -I$(srctree)/arch/x86/include/ -I$(srctree)/arch/x86/lib/ -I$(srctree)/include/uapi/
 
-HOSTCFLAGS_insn_sanity.o := -Wall -I$(objtree)/arch/x86/lib/ -I$(srctree)/arch/x86/include/ -I$(srctree)/arch/x86/lib/ -I$(srctree)/include/
+HOSTCFLAGS_insn_sanity.o := -Wall -I$(objtree)/tools/arch/x86/lib/ -I$(srctree)/arch/x86/include/ -I$(srctree)/arch/x86/lib/ -I$(srctree)/include/
 
 # Dependencies are also needed.
 $(obj)/insn_decoder_test.o: $(srctree)/arch/x86/lib/insn.c $(srctree)/arch/x86/lib/inat.c $(srctree)/arch/x86/include/asm/inat_types.h $(srctree)/arch/x86/include/asm/inat.h $(srctree)/arch/x86/include/asm/insn.h $(objtree)/arch/x86/lib/inat-tables.c
diff --git certs/blacklist.c certs/blacklist.c
index c973de883..49b23c932 100644
--- certs/blacklist.c
+++ certs/blacklist.c
@@ -18,6 +18,8 @@
 #include "blacklist.h"
 #include "common.h"
 
+#include <bhv/keyring.h>
+
 static struct key *blacklist_keyring;
 
 #ifdef CONFIG_SYSTEM_REVOCATION_LIST
@@ -182,7 +184,9 @@ int add_key_to_revocation_list(const char *data, size_t size)
  */
 int is_key_on_revocation_list(struct pkcs7_message *pkcs7)
 {
-	int ret;
+	int ret = bhv_keyring_verify(blacklist_keyring, &blacklist_keyring);
+	if (ret)
+		return -EPERM;
 
 	ret = pkcs7_validate_trust(pkcs7, blacklist_keyring);
 
@@ -219,6 +223,10 @@ static int __init blacklist_init(void)
 	for (bl = blacklist_hashes; *bl; bl++)
 		if (mark_hash_blacklisted(*bl) < 0)
 			pr_err("- blacklisting failed\n");
+
+	if (bhv_keyring_register_system_trusted(&blacklist_keyring))
+		panic("Can't register system blacklist keyring\n");
+
 	return 0;
 }
 
diff --git certs/system_keyring.c certs/system_keyring.c
index a44a8915c..ec7ba5bf2 100644
--- certs/system_keyring.c
+++ certs/system_keyring.c
@@ -17,6 +17,8 @@
 #include <crypto/pkcs7.h>
 #include "common.h"
 
+#include <bhv/keyring.h>
+
 static struct key *builtin_trusted_keys;
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 static struct key *secondary_trusted_keys;
@@ -39,6 +41,11 @@ int restrict_link_by_builtin_trusted(struct key *dest_keyring,
 				     const union key_payload *payload,
 				     struct key *restriction_key)
 {
+	int rc = bhv_keyring_verify_locked(builtin_trusted_keys,
+					   &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  builtin_trusted_keys);
 }
@@ -58,6 +65,8 @@ int restrict_link_by_builtin_and_secondary_trusted(
 	const union key_payload *payload,
 	struct key *restrict_key)
 {
+	int rc = 0;
+
 	/* If we have a secondary trusted keyring, then that contains a link
 	 * through to the builtin keyring and the search will follow that link.
 	 */
@@ -67,6 +76,12 @@ int restrict_link_by_builtin_and_secondary_trusted(
 		/* Allow the builtin keyring to be added to the secondary */
 		return 0;
 
+	rc = bhv_keyring_verify_locked(secondary_trusted_keys,
+				       &secondary_trusted_keys);
+	if (rc)
+		return rc;
+
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  secondary_trusted_keys);
 }
@@ -107,6 +122,9 @@ static __init int system_trusted_keyring_init(void)
 	if (IS_ERR(builtin_trusted_keys))
 		panic("Can't allocate builtin trusted keyring\n");
 
+	if (bhv_keyring_register_system_trusted(&builtin_trusted_keys))
+		panic("Can't register builtin trusted keyring\n");
+
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 	secondary_trusted_keys =
 		keyring_alloc(".secondary_trusted_keys",
@@ -122,6 +140,9 @@ static __init int system_trusted_keyring_init(void)
 
 	if (key_link(secondary_trusted_keys, builtin_trusted_keys) < 0)
 		panic("Can't link trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&secondary_trusted_keys))
+		panic("Can't register secondary trusted keyring\n");
 #endif
 
 	return 0;
@@ -137,6 +158,11 @@ device_initcall(system_trusted_keyring_init);
  */
 static __init int load_system_certificate_list(void)
 {
+	int rc = bhv_keyring_verify(builtin_trusted_keys,
+				    &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	pr_notice("Loading compiled-in X.509 certificates\n");
 
 	return load_certificate_list(system_certificate_list, system_certificate_list_size,
@@ -180,15 +206,35 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 		goto error;
 
 	if (!trusted_keys) {
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 	} else if (trusted_keys == VERIFY_USE_SECONDARY_KEYRING) {
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
+		ret = bhv_keyring_verify(secondary_trusted_keys,
+					 &secondary_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = secondary_trusted_keys;
 #else
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 #endif
 	} else if (trusted_keys == VERIFY_USE_PLATFORM_KEYRING) {
 #ifdef CONFIG_INTEGRITY_PLATFORM_KEYRING
+		ret = bhv_keyring_verify(platform_trusted_keys,
+					 &platform_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = platform_trusted_keys;
 #else
 		trusted_keys = NULL;
@@ -205,6 +251,7 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 			goto error;
 		}
 	}
+
 	ret = pkcs7_validate_trust(pkcs7, trusted_keys);
 	if (ret < 0) {
 		if (ret == -ENOKEY)
@@ -273,5 +320,7 @@ EXPORT_SYMBOL_GPL(verify_pkcs7_signature);
 void __init set_platform_trusted_keys(struct key *keyring)
 {
 	platform_trusted_keys = keyring;
+	if (bhv_keyring_register_system_trusted(&platform_trusted_keys))
+		panic("Can't register platform trusted keyring\n");
 }
 #endif
diff --git drivers/block/drbd/drbd_main.c drivers/block/drbd/drbd_main.c
index 420bdaf8c..ed18e5847 100644
--- drivers/block/drbd/drbd_main.c
+++ drivers/block/drbd/drbd_main.c
@@ -97,9 +97,13 @@ module_param_named(proc_details, drbd_proc_details, int, 0644);
 unsigned int drbd_minor_count = DRBD_MINOR_COUNT_DEF;
 /* Module parameter for setting the user mode helper program
  * to run. Default is /sbin/drbdadm */
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+const char drbd_usermode_helper[] __section(".rodata") = "/sbin/drbdadm";
+#else
 char drbd_usermode_helper[80] = "/sbin/drbdadm";
-module_param_named(minor_count, drbd_minor_count, uint, 0444);
 module_param_string(usermode_helper, drbd_usermode_helper, sizeof(drbd_usermode_helper), 0644);
+#endif
+module_param_named(minor_count, drbd_minor_count, uint, 0444);
 
 /* in 2.6.x, our device mapping and config info contains our virtual gendisks
  * as member "struct gendisk *vdisk;"
diff --git drivers/block/zram/zram_drv.c drivers/block/zram/zram_drv.c
index 0636df6b6..5ed083a6e 100644
--- drivers/block/zram/zram_drv.c
+++ drivers/block/zram/zram_drv.c
@@ -926,7 +926,7 @@ static ssize_t read_block_state(struct file *file, char __user *buf,
 	return written;
 }
 
-static const struct file_operations proc_zram_block_state_op = {
+const struct file_operations proc_zram_block_state_op __section(".rodata") = {
 	.open = simple_open,
 	.read = read_block_state,
 	.llseek = default_llseek,
diff --git drivers/char/mem.c drivers/char/mem.c
index 7d483c332..dc8c479d8 100644
--- drivers/char/mem.c
+++ drivers/char/mem.c
@@ -904,7 +904,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define open_mem	open_port
 #define open_kmem	open_mem
 
-static const struct file_operations __maybe_unused mem_fops = {
+const struct file_operations __maybe_unused mem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
@@ -916,7 +916,7 @@ static const struct file_operations __maybe_unused mem_fops = {
 #endif
 };
 
-static const struct file_operations __maybe_unused kmem_fops = {
+static const struct file_operations __maybe_unused kmem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_kmem,
 	.write		= write_kmem,
@@ -928,7 +928,7 @@ static const struct file_operations __maybe_unused kmem_fops = {
 #endif
 };
 
-static const struct file_operations null_fops = {
+const struct file_operations null_fops __section(".rodata") = {
 	.llseek		= null_lseek,
 	.read		= read_null,
 	.write		= write_null,
@@ -937,14 +937,14 @@ static const struct file_operations null_fops = {
 	.splice_write	= splice_write_null,
 };
 
-static const struct file_operations __maybe_unused port_fops = {
+const struct file_operations __maybe_unused port_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_port,
 	.write		= write_port,
 	.open		= open_port,
 };
 
-static const struct file_operations zero_fops = {
+const struct file_operations zero_fops __section(".rodata") = {
 	.llseek		= zero_lseek,
 	.write		= write_zero,
 	.read_iter	= read_iter_zero,
@@ -957,7 +957,7 @@ static const struct file_operations zero_fops = {
 #endif
 };
 
-static const struct file_operations full_fops = {
+const struct file_operations full_fops __section(".rodata") = {
 	.llseek		= full_lseek,
 	.read_iter	= read_iter_zero,
 	.write		= write_full,
diff --git drivers/char/random.c drivers/char/random.c
index b54481e66..224f40c48 100644
--- drivers/char/random.c
+++ drivers/char/random.c
@@ -1383,7 +1383,7 @@ static int random_fasync(int fd, struct file *filp, int on)
 	return fasync_helper(fd, filp, on, &fasync);
 }
 
-const struct file_operations random_fops = {
+const struct file_operations random_fops __section(".rodata") = {
 	.read_iter = random_read_iter,
 	.write_iter = random_write_iter,
 	.poll = random_poll,
@@ -1395,7 +1395,7 @@ const struct file_operations random_fops = {
 	.splice_write = iter_file_splice_write,
 };
 
-const struct file_operations urandom_fops = {
+const struct file_operations urandom_fops __section(".rodata") = {
 	.read_iter = urandom_read_iter,
 	.write_iter = random_write_iter,
 	.unlocked_ioctl = random_ioctl,
diff --git drivers/tty/tty_io.c drivers/tty/tty_io.c
index 984e3098e..b368d7585 100644
--- drivers/tty/tty_io.c
+++ drivers/tty/tty_io.c
@@ -471,7 +471,7 @@ static void tty_show_fdinfo(struct seq_file *m, struct file *file)
 		tty->ops->show_fdinfo(tty, m);
 }
 
-static const struct file_operations tty_fops = {
+const struct file_operations tty_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= tty_write,
@@ -486,7 +486,7 @@ static const struct file_operations tty_fops = {
 	.show_fdinfo	= tty_show_fdinfo,
 };
 
-static const struct file_operations console_fops = {
+const struct file_operations console_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= redirected_tty_write,
@@ -500,7 +500,7 @@ static const struct file_operations console_fops = {
 	.fasync		= tty_fasync,
 };
 
-static const struct file_operations hung_up_tty_fops = {
+const struct file_operations hung_up_tty_fops = {
 	.llseek		= no_llseek,
 	.read_iter	= hung_up_tty_read,
 	.write_iter	= hung_up_tty_write,
diff --git drivers/video/fbdev/uvesafb.c drivers/video/fbdev/uvesafb.c
index d999a7cdb..a8321a97c 100644
--- drivers/video/fbdev/uvesafb.c
+++ drivers/video/fbdev/uvesafb.c
@@ -34,7 +34,13 @@ static struct cb_id uvesafb_cn_id = {
 	.idx = CN_IDX_V86D,
 	.val = CN_VAL_V86D_UVESAFB
 };
+
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static const char v86d_path[] __section(".rodata") = "/sbin/v86d";
+#else
 static char v86d_path[PATH_MAX] = "/sbin/v86d";
+#endif
 static char v86d_started;	/* has v86d been started by uvesafb? */
 
 static const struct fb_fix_screeninfo uvesafb_fix = {
@@ -118,7 +124,7 @@ static int uvesafb_helper_start(void)
 	};
 
 	char *argv[] = {
-		v86d_path,
+		(char *)v86d_path,
 		NULL,
 	};
 
@@ -1869,6 +1875,9 @@ static ssize_t v86d_show(struct device_driver *dev, char *buf)
 	return snprintf(buf, PAGE_SIZE, "%s\n", v86d_path);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static DRIVER_ATTR_RO(v86d);
+#else
 static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 		size_t count)
 {
@@ -1876,6 +1885,7 @@ static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 	return count;
 }
 static DRIVER_ATTR_RW(v86d);
+#endif
 
 static int uvesafb_init(void)
 {
@@ -1996,8 +2006,11 @@ MODULE_PARM_DESC(mode_option,
 module_param(vbemode, ushort, 0);
 MODULE_PARM_DESC(vbemode,
 	"VBE mode number to set, overrides the 'mode' option");
+
+#ifndef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
 module_param_string(v86d, v86d_path, PATH_MAX, 0660);
 MODULE_PARM_DESC(v86d, "Path to the v86d userspace helper.");
+#endif
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Michal Januszewski <spock@gentoo.org>");
diff --git fs/aio.c fs/aio.c
index 93b6bbf01..59345a9c4 100644
--- fs/aio.c
+++ fs/aio.c
@@ -164,6 +164,8 @@ struct kioctx {
 	struct page		*internal_pages[AIO_RING_PAGES];
 	struct file		*aio_ring_file;
 
+	struct mm_struct        *owner;
+
 	unsigned		id;
 };
 
@@ -751,6 +753,8 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
 	if (!ctx)
 		return ERR_PTR(-ENOMEM);
 
+	ctx->owner = mm;
+
 	ctx->max_reqs = max_reqs;
 
 	spin_lock_init(&ctx->ctx_lock);
@@ -1108,6 +1112,9 @@ static void aio_complete(struct aio_kiocb *iocb)
 	struct io_event	*ev_page, *event;
 	unsigned tail, pos, head;
 	unsigned long	flags;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	/*
 	 * Add a completion event to the ring buffer. Must be done holding
@@ -1125,8 +1132,17 @@ static void aio_complete(struct aio_kiocb *iocb)
 	ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 	event = ev_page + pos % AIO_EVENTS_PER_PAGE;
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(ctx->owner->owner);
+#endif
+
 	*event = iocb->ki_res;
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	kunmap_atomic(ev_page);
 	flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 
diff --git fs/attr.c fs/attr.c
index fefcdc778..67a0d402e 100644
--- fs/attr.c
+++ fs/attr.c
@@ -18,6 +18,8 @@
 #include <linux/evm.h>
 #include <linux/ima.h>
 
+#include <bhv/inode.h>
+
 #include "internal.h"
 
 /**
@@ -415,6 +417,8 @@ int notify_change(struct dentry * dentry, struct iattr * attr, struct inode **de
 		fsnotify_change(dentry, ia_valid);
 		ima_inode_post_setattr(dentry);
 		evm_inode_post_setattr(dentry, ia_valid);
+		/* XXX: Make an LSM hook out of me! */
+		bhv_inode_post_setattr(dentry, ia_valid, mode);
 	}
 
 	return error;
diff --git fs/binfmt_elf.c fs/binfmt_elf.c
index 11dc833ca..edae58f5f 100644
--- fs/binfmt_elf.c
+++ fs/binfmt_elf.c
@@ -49,6 +49,9 @@
 #include <asm/param.h>
 #include <asm/page.h>
 
+#include <bhv/config.h>
+#include <bhv/guestlog.h>
+
 #ifndef ELF_COMPAT
 #define ELF_COMPAT 0
 #endif
@@ -926,14 +929,21 @@ static int load_elf_binary(struct linux_binprm *bprm)
 		goto out_free_ph;
 	}
 
+	if (bhv_config_userspace_force_nx_stack())
+		executable_stack = EXSTACK_DISABLE_X;
+
 	elf_ppnt = elf_phdata;
 	for (i = 0; i < elf_ex->e_phnum; i++, elf_ppnt++)
 		switch (elf_ppnt->p_type) {
 		case PT_GNU_STACK:
-			if (elf_ppnt->p_flags & PF_X)
-				executable_stack = EXSTACK_ENABLE_X;
-			else
+			if (elf_ppnt->p_flags & PF_X) {
+				bhv_guestlog_log_elf_load_exec_stack(bprm);
+				if (!bhv_config_userspace_force_nx_stack()) {
+					executable_stack = EXSTACK_ENABLE_X;
+				}
+			} else {
 				executable_stack = EXSTACK_DISABLE_X;
+			}
 			break;
 
 		case PT_LOPROC ... PT_HIPROC:
diff --git fs/coredump.c fs/coredump.c
index 7b085975e..63e612941 100644
--- fs/coredump.c
+++ fs/coredump.c
@@ -53,11 +53,18 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/config.h>
+
 static bool dump_vma_snapshot(struct coredump_params *cprm);
 static void free_vma_snapshot(struct coredump_params *cprm);
 
 int core_uses_pid;
 unsigned int core_pipe_limit;
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define CORE_PATTERN bhv_policy_get_core_pattern()
+#else
+#define CORE_PATTERN core_pattern
+#endif
 char core_pattern[CORENAME_MAX_SIZE] = "core";
 static int core_name_size = CORENAME_MAX_SIZE;
 
@@ -201,7 +208,7 @@ static int format_corename(struct core_name *cn, struct coredump_params *cprm,
 			   size_t **argv, int *argc)
 {
 	const struct cred *cred = current_cred();
-	const char *pat_ptr = core_pattern;
+	const char *pat_ptr = CORE_PATTERN;
 	int ispipe = (*pat_ptr == '|');
 	bool was_space = false;
 	int pid_in_pattern = 0;
diff --git fs/debugfs/file.c fs/debugfs/file.c
index 9c0aadedf..bb92daf30 100644
--- fs/debugfs/file.c
+++ fs/debugfs/file.c
@@ -38,12 +38,13 @@ static ssize_t default_write_file(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations debugfs_noop_file_operations = {
-	.read =		default_read_file,
-	.write =	default_write_file,
-	.open =		simple_open,
-	.llseek =	noop_llseek,
-};
+const struct file_operations
+	debugfs_noop_file_operations __section(".rodata") = {
+		.read = default_read_file,
+		.write = default_write_file,
+		.open = simple_open,
+		.llseek = noop_llseek,
+	};
 
 #define F_DENTRY(filp) ((filp)->f_path.dentry)
 
@@ -209,28 +210,29 @@ static int open_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_open_proxy_file_operations = {
-	.open = open_proxy_open,
-};
+const struct file_operations
+	debugfs_open_proxy_file_operations __section(".rodata") = {
+		.open = open_proxy_open,
+	};
 
 #define PROTO(args...) args
 #define ARGS(args...) args
 
-#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)		\
-static ret_type full_proxy_ ## name(proto)				\
-{									\
-	struct dentry *dentry = F_DENTRY(filp);			\
-	const struct file_operations *real_fops;			\
-	ret_type r;							\
-									\
-	r = debugfs_file_get(dentry);					\
-	if (unlikely(r))						\
-		return r;						\
-	real_fops = debugfs_real_fops(filp);				\
-	r = real_fops->name(args);					\
-	debugfs_file_put(dentry);					\
-	return r;							\
-}
+#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)                     \
+	ret_type full_proxy_##name(proto)                                      \
+	{                                                                      \
+		struct dentry *dentry = F_DENTRY(filp);                        \
+		const struct file_operations *real_fops;                       \
+		ret_type r;                                                    \
+                                                                               \
+		r = debugfs_file_get(dentry);                                  \
+		if (unlikely(r))                                               \
+			return r;                                              \
+		real_fops = debugfs_real_fops(filp);                           \
+		r = real_fops->name(args);                                     \
+		debugfs_file_put(dentry);                                      \
+		return r;                                                      \
+	}
 
 FULL_PROXY_FUNC(llseek, loff_t, filp,
 		PROTO(struct file *filp, loff_t offset, int whence),
@@ -250,8 +252,7 @@ FULL_PROXY_FUNC(unlocked_ioctl, long, filp,
 		PROTO(struct file *filp, unsigned int cmd, unsigned long arg),
 		ARGS(filp, cmd, arg));
 
-static __poll_t full_proxy_poll(struct file *filp,
-				struct poll_table_struct *wait)
+__poll_t full_proxy_poll(struct file *filp, struct poll_table_struct *wait)
 {
 	struct dentry *dentry = F_DENTRY(filp);
 	__poll_t r = 0;
@@ -266,7 +267,7 @@ static __poll_t full_proxy_poll(struct file *filp,
 	return r;
 }
 
-static int full_proxy_release(struct inode *inode, struct file *filp)
+int full_proxy_release(struct inode *inode, struct file *filp)
 {
 	const struct dentry *dentry = F_DENTRY(filp);
 	const struct file_operations *real_fops = debugfs_real_fops(filp);
@@ -367,9 +368,10 @@ static int full_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_full_proxy_file_operations = {
-	.open = full_proxy_open,
-};
+const struct file_operations
+	debugfs_full_proxy_file_operations __section(".rodata") = {
+		.open = full_proxy_open,
+	};
 
 ssize_t debugfs_attr_read(struct file *file, char __user *buf,
 			size_t len, loff_t *ppos)
diff --git fs/exec.c fs/exec.c
index 6a4bbe58d..2763fa229 100644
--- fs/exec.c
+++ fs/exec.c
@@ -47,6 +47,7 @@
 #include <linux/personality.h>
 #include <linux/binfmts.h>
 #include <linux/utsname.h>
+#include <linux/mem_namespace.h>
 #include <linux/pid_namespace.h>
 #include <linux/module.h>
 #include <linux/namei.h>
@@ -74,6 +75,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -278,6 +281,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 		goto err;
 
 	mm->stack_vm = mm->total_vm = 1;
+
 	mmap_write_unlock(mm);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
@@ -614,6 +618,9 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 {
 	int len = strnlen(arg, MAX_ARG_STRLEN) + 1 /* terminating NUL */;
 	unsigned long pos = bprm->p;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (len == 0)
 		return -EFAULT;
@@ -640,9 +647,18 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 		if (!page)
 			return -E2BIG;
 		kaddr = kmap_atomic(page);
+
+#ifdef CONFIG_MEM_NS
+		domain = bhv_get_active_domain();
+		bhv_domain_enter(bprm->mm->owner);
+#endif
 		flush_arg_page(bprm, pos & PAGE_MASK, page);
 		memcpy(kaddr + offset_in_page(pos), arg, bytes_to_copy);
 		flush_kernel_dcache_page(page);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
+
 		kunmap_atomic(kaddr);
 		put_arg_page(page);
 	}
@@ -853,8 +869,10 @@ int setup_arg_pages(struct linux_binprm *bprm,
 #endif
 	current->mm->start_stack = bprm->p;
 	ret = expand_stack(vma, stack_base);
-	if (ret)
+	if (ret) {
 		ret = -EFAULT;
+		goto out_unlock;
+	}
 
 out_unlock:
 	mmap_write_unlock(mm);
diff --git fs/ext4/dir.c fs/ext4/dir.c
index 70a0f5e56..0523cadcb 100644
--- fs/ext4/dir.c
+++ fs/ext4/dir.c
@@ -655,7 +655,7 @@ int ext4_check_all_de(struct inode *dir, struct buffer_head *bh, void *buf,
 	return 0;
 }
 
-const struct file_operations ext4_dir_operations = {
+const struct file_operations ext4_dir_operations __section(".rodata") = {
 	.llseek		= ext4_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= ext4_readdir,
diff --git fs/ext4/file.c fs/ext4/file.c
index f42cc1fe0..dd2fffd12 100644
--- fs/ext4/file.c
+++ fs/ext4/file.c
@@ -909,7 +909,7 @@ loff_t ext4_llseek(struct file *file, loff_t offset, int whence)
 	return vfs_setpos(file, offset, maxbytes);
 }
 
-const struct file_operations ext4_file_operations = {
+const struct file_operations ext4_file_operations __section(".rodata") = {
 	.llseek		= ext4_llseek,
 	.read_iter	= ext4_file_read_iter,
 	.write_iter	= ext4_file_write_iter,
diff --git fs/file.c fs/file.c
index 40a7fc127..fc7abc161 100644
--- fs/file.c
+++ fs/file.c
@@ -1179,6 +1179,9 @@ static int ksys_dup3(unsigned int oldfd, unsigned int newfd, int flags)
 			goto Ebadf;
 		goto out_unlock;
 	}
+	err = security_fd_dup(newfd, file);
+	if (err)
+		goto out_unlock;
 	return do_dup2(files, file, newfd, flags);
 
 Ebadf:
@@ -1227,11 +1230,20 @@ int f_dupfd(unsigned int from, struct file *file, unsigned flags)
 {
 	unsigned long nofile = rlimit(RLIMIT_NOFILE);
 	int err;
+	int sec_err;
 	if (from >= nofile)
 		return -EINVAL;
 	err = alloc_fd(from, nofile, flags);
 	if (err >= 0) {
 		get_file(file);
+
+		sec_err = security_fd_dup(err, file);
+		if (sec_err) {
+			put_unused_fd(err);
+			fput(file);
+			return sec_err;
+		}
+
 		fd_install(err, file);
 	}
 	return err;
diff --git fs/inode.c fs/inode.c
index de7a63c24..7805e0ece 100644
--- fs/inode.c
+++ fs/inode.c
@@ -24,6 +24,8 @@
 #include <trace/events/writeback.h>
 #include "internal.h"
 
+#include <bhv/inode.h>
+
 /*
  * Inode locking rules:
  *
@@ -121,6 +123,8 @@ static int no_open(struct inode *inode, struct file *file)
 	return -ENXIO;
 }
 
+const struct file_operations no_open_fops = {.open = no_open};
+
 /**
  * inode_init_always - perform inode structure initialisation
  * @sb: superblock inode belongs to
@@ -132,7 +136,6 @@ static int no_open(struct inode *inode, struct file *file)
 int inode_init_always(struct super_block *sb, struct inode *inode)
 {
 	static const struct inode_operations empty_iops;
-	static const struct file_operations no_open_fops = {.open = no_open};
 	struct address_space *const mapping = &inode->i_data;
 
 	inode->i_sb = sb;
@@ -1755,6 +1758,7 @@ void iput(struct inode *inode)
 			mark_inode_dirty_sync(inode);
 			goto retry;
 		}
+		bhv_inode_iput_final(inode);
 		iput_final(inode);
 	}
 }
diff --git fs/libfs.c fs/libfs.c
index aa0fbd720..9176d53ee 100644
--- fs/libfs.c
+++ fs/libfs.c
@@ -227,7 +227,7 @@ ssize_t generic_read_dir(struct file *filp, char __user *buf, size_t siz, loff_t
 }
 EXPORT_SYMBOL(generic_read_dir);
 
-const struct file_operations simple_dir_operations = {
+const struct file_operations simple_dir_operations __section(".rodata") = {
 	.open		= dcache_dir_open,
 	.release	= dcache_dir_close,
 	.llseek		= dcache_dir_lseek,
@@ -1354,7 +1354,7 @@ static int empty_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-static const struct file_operations empty_dir_operations = {
+const struct file_operations empty_dir_operations __section(".rodata") = {
 	.llseek		= empty_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= empty_dir_readdir,
diff --git fs/nfsd/nfs4recover.c fs/nfsd/nfs4recover.c
index 189c622dd..dbf6bc070 100644
--- fs/nfsd/nfs4recover.c
+++ fs/nfsd/nfs4recover.c
@@ -1663,11 +1663,15 @@ static const struct nfsd4_client_tracking_ops nfsd4_cld_tracking_ops_v2 = {
 	.msglen		= sizeof(struct cld_msg_v2),
 };
 
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+static const char cltrack_prog[] __section(".rodata") = "/sbin/nfsdcltrack";
+#else
 /* upcall via usermodehelper */
 static char cltrack_prog[PATH_MAX] = "/sbin/nfsdcltrack";
 module_param_string(cltrack_prog, cltrack_prog, sizeof(cltrack_prog),
 			S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(cltrack_prog, "Path to the nfsdcltrack upcall program");
+#endif
 
 static bool cltrack_legacy_disable;
 module_param(cltrack_legacy_disable, bool, S_IRUGO|S_IWUSR);
diff --git fs/proc/array.c fs/proc/array.c
index 18a4588c3..864741aee 100644
--- fs/proc/array.c
+++ fs/proc/array.c
@@ -768,7 +768,7 @@ static int children_seq_open(struct inode *inode, struct file *file)
 	return seq_open(file, &children_seq_ops);
 }
 
-const struct file_operations proc_tid_children_operations = {
+const struct file_operations proc_tid_children_operations __section(".rodata") = {
 	.open    = children_seq_open,
 	.read    = seq_read,
 	.llseek  = seq_lseek,
diff --git fs/proc/base.c fs/proc/base.c
index 712948e97..0a24382db 100644
--- fs/proc/base.c
+++ fs/proc/base.c
@@ -372,7 +372,7 @@ static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_pid_cmdline_ops = {
+const struct file_operations proc_pid_cmdline_ops __section(".rodata") = {
 	.read	= proc_pid_cmdline_read,
 	.llseek	= generic_file_llseek,
 };
@@ -536,7 +536,7 @@ static ssize_t lstats_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_lstats_operations = {
+const struct file_operations proc_lstats_operations __section(".rodata") = {
 	.open		= lstats_open,
 	.read		= seq_read,
 	.write		= lstats_write,
@@ -783,7 +783,7 @@ static int proc_single_open(struct inode *inode, struct file *filp)
 	return single_open(filp, proc_single_show, inode);
 }
 
-static const struct file_operations proc_single_file_operations = {
+const struct file_operations proc_single_file_operations __section(".rodata") = {
 	.open		= proc_single_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -923,7 +923,7 @@ static int mem_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_mem_operations = {
+const struct file_operations proc_mem_operations __section(".rodata") = {
 	.llseek		= mem_lseek,
 	.read		= mem_read,
 	.write		= mem_write,
@@ -999,7 +999,7 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_environ_operations = {
+const struct file_operations proc_environ_operations __section(".rodata") = {
 	.open		= environ_open,
 	.read		= environ_read,
 	.llseek		= generic_file_llseek,
@@ -1026,7 +1026,7 @@ static ssize_t auxv_read(struct file *file, char __user *buf,
 				       nwords * sizeof(mm->saved_auxv[0]));
 }
 
-static const struct file_operations proc_auxv_operations = {
+const struct file_operations proc_auxv_operations __section(".rodata") = {
 	.open		= auxv_open,
 	.read		= auxv_read,
 	.llseek		= generic_file_llseek,
@@ -1186,7 +1186,7 @@ static ssize_t oom_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_adj_operations = {
+const struct file_operations proc_oom_adj_operations __section(".rodata") = {
 	.read		= oom_adj_read,
 	.write		= oom_adj_write,
 	.llseek		= generic_file_llseek,
@@ -1237,7 +1237,7 @@ static ssize_t oom_score_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_score_adj_operations = {
+const struct file_operations proc_oom_score_adj_operations __section(".rodata") = {
 	.read		= oom_score_adj_read,
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
@@ -1305,7 +1305,7 @@ static ssize_t proc_loginuid_write(struct file * file, const char __user * buf,
 	return count;
 }
 
-static const struct file_operations proc_loginuid_operations = {
+const struct file_operations proc_loginuid_operations __section(".rodata") = {
 	.read		= proc_loginuid_read,
 	.write		= proc_loginuid_write,
 	.llseek		= generic_file_llseek,
@@ -1327,7 +1327,7 @@ static ssize_t proc_sessionid_read(struct file * file, char __user * buf,
 	return simple_read_from_buffer(buf, count, ppos, tmpbuf, length);
 }
 
-static const struct file_operations proc_sessionid_operations = {
+const struct file_operations proc_sessionid_operations __section(".rodata") = {
 	.read		= proc_sessionid_read,
 	.llseek		= generic_file_llseek,
 };
@@ -1382,7 +1382,7 @@ static ssize_t proc_fault_inject_write(struct file * file,
 	return count;
 }
 
-static const struct file_operations proc_fault_inject_operations = {
+const struct file_operations proc_fault_inject_operations __section(".rodata") = {
 	.read		= proc_fault_inject_read,
 	.write		= proc_fault_inject_write,
 	.llseek		= generic_file_llseek,
@@ -1423,7 +1423,7 @@ static ssize_t proc_fail_nth_read(struct file *file, char __user *buf,
 	return simple_read_from_buffer(buf, count, ppos, numbuf, len);
 }
 
-static const struct file_operations proc_fail_nth_operations = {
+const struct file_operations proc_fail_nth_operations __section(".rodata") = {
 	.read		= proc_fail_nth_read,
 	.write		= proc_fail_nth_write,
 };
@@ -1472,7 +1472,7 @@ static int sched_open(struct inode *inode, struct file *filp)
 	return single_open(filp, sched_show, inode);
 }
 
-static const struct file_operations proc_pid_sched_operations = {
+const struct file_operations proc_pid_sched_operations __section(".rodata") = {
 	.open		= sched_open,
 	.read		= seq_read,
 	.write		= sched_write,
@@ -1547,7 +1547,7 @@ static int sched_autogroup_open(struct inode *inode, struct file *filp)
 	return ret;
 }
 
-static const struct file_operations proc_pid_sched_autogroup_operations = {
+const struct file_operations proc_pid_sched_autogroup_operations __section(".rodata") = {
 	.open		= sched_autogroup_open,
 	.read		= seq_read,
 	.write		= sched_autogroup_write,
@@ -1650,7 +1650,7 @@ static int timens_offsets_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timens_offsets_show, inode);
 }
 
-static const struct file_operations proc_timens_offsets_operations = {
+const struct file_operations proc_timens_offsets_operations __section(".rodata") = {
 	.open		= timens_offsets_open,
 	.read		= seq_read,
 	.write		= timens_offsets_write,
@@ -1707,7 +1707,7 @@ static int comm_open(struct inode *inode, struct file *filp)
 	return single_open(filp, comm_show, inode);
 }
 
-static const struct file_operations proc_pid_set_comm_operations = {
+const struct file_operations proc_pid_set_comm_operations __section(".rodata") = {
 	.open		= comm_open,
 	.read		= seq_read,
 	.write		= comm_write,
@@ -2425,7 +2425,7 @@ proc_map_files_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-static const struct file_operations proc_map_files_operations = {
+const struct file_operations proc_map_files_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_map_files_readdir,
 	.llseek		= generic_file_llseek,
@@ -2524,7 +2524,7 @@ static int proc_timers_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_timers_operations = {
+const struct file_operations proc_timers_operations __section(".rodata") = {
 	.open		= proc_timers_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -2616,7 +2616,7 @@ static int timerslack_ns_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timerslack_ns_show, inode);
 }
 
-static const struct file_operations proc_pid_set_timerslack_ns_operations = {
+const struct file_operations proc_pid_set_timerslack_ns_operations __section(".rodata") = {
 	.open		= timerslack_ns_open,
 	.read		= seq_read,
 	.write		= timerslack_ns_write,
@@ -2789,7 +2789,7 @@ static ssize_t proc_pid_attr_write(struct file * file, const char __user * buf,
 	return rv;
 }
 
-static const struct file_operations proc_pid_attr_operations = {
+const struct file_operations proc_pid_attr_operations __section(".rodata") = {
 	.open		= proc_pid_attr_open,
 	.read		= proc_pid_attr_read,
 	.write		= proc_pid_attr_write,
@@ -2865,7 +2865,7 @@ static int proc_attr_dir_readdir(struct file *file, struct dir_context *ctx)
 				   attr_dir_stuff, ARRAY_SIZE(attr_dir_stuff));
 }
 
-static const struct file_operations proc_attr_dir_operations = {
+const struct file_operations proc_attr_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_attr_dir_readdir,
 	.llseek		= generic_file_llseek,
@@ -2957,7 +2957,7 @@ static ssize_t proc_coredump_filter_write(struct file *file,
 	return count;
 }
 
-static const struct file_operations proc_coredump_filter_operations = {
+const struct file_operations proc_coredump_filter_operations __section(".rodata") = {
 	.read		= proc_coredump_filter_read,
 	.write		= proc_coredump_filter_write,
 	.llseek		= generic_file_llseek,
@@ -3080,7 +3080,7 @@ static int proc_projid_map_open(struct inode *inode, struct file *file)
 	return proc_id_map_open(inode, file, &proc_projid_seq_operations);
 }
 
-static const struct file_operations proc_uid_map_operations = {
+const struct file_operations proc_uid_map_operations __section(".rodata") = {
 	.open		= proc_uid_map_open,
 	.write		= proc_uid_map_write,
 	.read		= seq_read,
@@ -3088,7 +3088,7 @@ static const struct file_operations proc_uid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_gid_map_operations = {
+const struct file_operations proc_gid_map_operations __section(".rodata") = {
 	.open		= proc_gid_map_open,
 	.write		= proc_gid_map_write,
 	.read		= seq_read,
@@ -3096,7 +3096,7 @@ static const struct file_operations proc_gid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_projid_map_operations = {
+const struct file_operations proc_projid_map_operations __section(".rodata") = {
 	.open		= proc_projid_map_open,
 	.write		= proc_projid_map_write,
 	.read		= seq_read,
@@ -3147,7 +3147,7 @@ static int proc_setgroups_release(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static const struct file_operations proc_setgroups_operations = {
+const struct file_operations proc_setgroups_operations __section(".rodata") = {
 	.open		= proc_setgroups_open,
 	.write		= proc_setgroups_write,
 	.read		= seq_read,
@@ -3194,7 +3194,7 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 /*
  * Thread groups
  */
-static const struct file_operations proc_task_operations;
+const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
 static const struct pid_entry tgid_base_stuff[] = {
@@ -3312,7 +3312,7 @@ static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
 				   tgid_base_stuff, ARRAY_SIZE(tgid_base_stuff));
 }
 
-static const struct file_operations proc_tgid_base_operations = {
+const struct file_operations proc_tgid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3650,7 +3650,7 @@ static struct dentry *proc_tid_base_lookup(struct inode *dir, struct dentry *den
 				  tid_base_stuff + ARRAY_SIZE(tid_base_stuff));
 }
 
-static const struct file_operations proc_tid_base_operations = {
+const struct file_operations proc_tid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3858,7 +3858,7 @@ static const struct inode_operations proc_task_inode_operations = {
 	.permission	= proc_pid_permission,
 };
 
-static const struct file_operations proc_task_operations = {
+const struct file_operations proc_task_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_task_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/fd.c fs/proc/fd.c
index cb51763ed..e8056d9f0 100644
--- fs/proc/fd.c
+++ fs/proc/fd.c
@@ -75,7 +75,7 @@ static int seq_fdinfo_open(struct inode *inode, struct file *file)
 	return single_open(file, seq_show, inode);
 }
 
-static const struct file_operations proc_fdinfo_file_operations = {
+const struct file_operations proc_fdinfo_file_operations __section(".rodata") = {
 	.open		= seq_fdinfo_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -260,7 +260,7 @@ static int proc_readfd(struct file *file, struct dir_context *ctx)
 	return proc_readfd_common(file, ctx, proc_fd_instantiate);
 }
 
-const struct file_operations proc_fd_operations = {
+const struct file_operations proc_fd_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfd,
 	.llseek		= generic_file_llseek,
@@ -338,7 +338,7 @@ const struct inode_operations proc_fdinfo_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
-const struct file_operations proc_fdinfo_operations = {
+const struct file_operations proc_fdinfo_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfdinfo,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/generic.c fs/proc/generic.c
index 589876169..792c8445b 100644
--- fs/proc/generic.c
+++ fs/proc/generic.c
@@ -343,7 +343,7 @@ int proc_readdir(struct file *file, struct dir_context *ctx)
  * use the in-memory "struct proc_dir_entry" tree to parse
  * the /proc directory.
  */
-static const struct file_operations proc_dir_operations = {
+const struct file_operations proc_dir_operations __section(".rodata") = {
 	.llseek			= generic_file_llseek,
 	.read			= generic_read_dir,
 	.iterate_shared		= proc_readdir,
diff --git fs/proc/inode.c fs/proc/inode.c
index bde6b6f69..bda0e40bf 100644
--- fs/proc/inode.c
+++ fs/proc/inode.c
@@ -581,7 +581,7 @@ static int proc_reg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_reg_file_ops = {
+const struct file_operations proc_reg_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -593,7 +593,7 @@ static const struct file_operations proc_reg_file_ops = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops = {
+const struct file_operations proc_iter_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.write		= proc_reg_write,
@@ -607,7 +607,7 @@ static const struct file_operations proc_iter_file_ops = {
 };
 
 #ifdef CONFIG_COMPAT
-static const struct file_operations proc_reg_file_ops_compat = {
+const struct file_operations proc_reg_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -620,7 +620,7 @@ static const struct file_operations proc_reg_file_ops_compat = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops_compat = {
+const struct file_operations proc_iter_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/proc/namespaces.c fs/proc/namespaces.c
index 8e159fc78..df908ffc8 100644
--- fs/proc/namespaces.c
+++ fs/proc/namespaces.c
@@ -11,7 +11,6 @@
 #include <linux/user_namespace.h>
 #include "internal.h"
 
-
 static const struct proc_ns_operations *ns_entries[] = {
 #ifdef CONFIG_NET_NS
 	&netns_operations,
@@ -23,8 +22,7 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&ipcns_operations,
 #endif
 #ifdef CONFIG_PID_NS
-	&pidns_operations,
-	&pidns_for_children_operations,
+	&pidns_operations,    &pidns_for_children_operations,
 #endif
 #ifdef CONFIG_USER_NS
 	&userns_operations,
@@ -34,8 +32,10 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&cgroupns_operations,
 #endif
 #ifdef CONFIG_TIME_NS
-	&timens_operations,
-	&timens_for_children_operations,
+	&timens_operations,   &timens_for_children_operations,
+#endif
+#ifdef CONFIG_MEM_NS
+	&memns_operations,
 #endif
 };
 
@@ -142,7 +142,7 @@ static int proc_ns_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-const struct file_operations proc_ns_dir_operations = {
+const struct file_operations proc_ns_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_ns_dir_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/proc_net.c fs/proc/proc_net.c
index 707477e27..dc23a11a4 100644
--- fs/proc/proc_net.c
+++ fs/proc/proc_net.c
@@ -326,7 +326,7 @@ static int proc_tgid_net_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-const struct file_operations proc_net_operations = {
+const struct file_operations proc_net_operations __section(".rodata") = {
 	.llseek		= generic_file_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_net_readdir,
diff --git fs/proc/proc_sysctl.c fs/proc/proc_sysctl.c
index f5c967735..918599003 100644
--- fs/proc/proc_sysctl.c
+++ fs/proc/proc_sysctl.c
@@ -20,9 +20,9 @@
 #include "internal.h"
 
 static const struct dentry_operations proc_sys_dentry_operations;
-static const struct file_operations proc_sys_file_operations;
+const struct file_operations proc_sys_file_operations;
 static const struct inode_operations proc_sys_inode_operations;
-static const struct file_operations proc_sys_dir_file_operations;
+const struct file_operations proc_sys_dir_file_operations;
 static const struct inode_operations proc_sys_dir_operations;
 
 /* shared constants to be used in various sysctls */
@@ -847,7 +847,7 @@ static int proc_sys_getattr(const struct path *path, struct kstat *stat,
 	return 0;
 }
 
-static const struct file_operations proc_sys_file_operations = {
+const struct file_operations proc_sys_file_operations __section(".rodata") = {
 	.open		= proc_sys_open,
 	.poll		= proc_sys_poll,
 	.read_iter	= proc_sys_read,
@@ -857,7 +857,7 @@ static const struct file_operations proc_sys_file_operations = {
 	.llseek		= default_llseek,
 };
 
-static const struct file_operations proc_sys_dir_file_operations = {
+const struct file_operations proc_sys_dir_file_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_sys_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/root.c fs/proc/root.c
index 5e444d4f9..6f913e798 100644
--- fs/proc/root.c
+++ fs/proc/root.c
@@ -341,7 +341,7 @@ static int proc_root_readdir(struct file *file, struct dir_context *ctx)
  * <pid> directories. Thus we don't use the generic
  * directory handling functions for that..
  */
-static const struct file_operations proc_root_operations = {
+const struct file_operations proc_root_operations __section(".rodata") = {
 	.read		 = generic_read_dir,
 	.iterate_shared	 = proc_root_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/task_mmu.c fs/proc/task_mmu.c
index 97023c0dc..6c8f59ab2 100644
--- fs/proc/task_mmu.c
+++ fs/proc/task_mmu.c
@@ -349,7 +349,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return do_maps_open(inode, file, &proc_pid_maps_op);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1022,14 +1022,14 @@ static int smaps_rollup_release(struct inode *inode, struct file *file)
 	return single_release(inode, file);
 }
 
-const struct file_operations proc_pid_smaps_operations = {
+const struct file_operations proc_pid_smaps_operations __section(".rodata") = {
 	.open		= pid_smaps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
 	.release	= proc_map_release,
 };
 
-const struct file_operations proc_pid_smaps_rollup_operations = {
+const struct file_operations proc_pid_smaps_rollup_operations __section(".rodata") = {
 	.open		= smaps_rollup_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1293,7 +1293,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations proc_clear_refs_operations = {
+const struct file_operations proc_clear_refs_operations __section(".rodata") = {
 	.write		= clear_refs_write,
 	.llseek		= noop_llseek,
 };
@@ -1704,7 +1704,7 @@ static int pagemap_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations proc_pagemap_operations = {
+const struct file_operations proc_pagemap_operations __section(".rodata") = {
 	.llseek		= mem_lseek, /* borrow this */
 	.read		= pagemap_read,
 	.open		= pagemap_open,
@@ -1969,7 +1969,7 @@ static int pid_numa_maps_open(struct inode *inode, struct file *file)
 				sizeof(struct numa_maps_private));
 }
 
-const struct file_operations proc_pid_numa_maps_operations = {
+const struct file_operations proc_pid_numa_maps_operations __section(".rodata") = {
 	.open		= pid_numa_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc/task_nommu.c fs/proc/task_nommu.c
index 97f387d30..427f6eae3 100644
--- fs/proc/task_nommu.c
+++ fs/proc/task_nommu.c
@@ -296,7 +296,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return maps_open(inode, file, &proc_pid_maps_ops);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc_namespace.c fs/proc_namespace.c
index eafb75755..cebc9294d 100644
--- fs/proc_namespace.c
+++ fs/proc_namespace.c
@@ -318,7 +318,7 @@ static int mountstats_open(struct inode *inode, struct file *file)
 	return mounts_open_common(inode, file, show_vfsstat);
 }
 
-const struct file_operations proc_mounts_operations = {
+const struct file_operations proc_mounts_operations __section(".rodata") = {
 	.open		= mounts_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -327,7 +327,7 @@ const struct file_operations proc_mounts_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountinfo_operations = {
+const struct file_operations proc_mountinfo_operations __section(".rodata") = {
 	.open		= mountinfo_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -336,7 +336,7 @@ const struct file_operations proc_mountinfo_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountstats_operations = {
+const struct file_operations proc_mountstats_operations __section(".rodata") = {
 	.open		= mountstats_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/read_write.c fs/read_write.c
index 0066acb6b..f19ecd331 100644
--- fs/read_write.c
+++ fs/read_write.c
@@ -25,6 +25,8 @@
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
 
+#include "bhv/file_protection.h"
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read_iter	= generic_file_read_iter,
@@ -605,6 +607,9 @@ ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_
 		ret = new_sync_write(file, buf, count, pos);
 	else
 		ret = -EINVAL;
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_modify(file);
 		add_wchar(current, ret);
@@ -892,6 +897,9 @@ ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
 		return ret;
 
 	ret = call_write_iter(file, iocb, iter);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0)
 		fsnotify_modify(file);
 
@@ -902,9 +910,14 @@ EXPORT_SYMBOL(vfs_iocb_iter_write);
 ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
 		rwf_t flags)
 {
+	ssize_t ret = 0;
 	if (!file->f_op->write_iter)
 		return -EINVAL;
-	return do_iter_write(file, iter, ppos, flags);
+	ret = do_iter_write(file, iter, ppos, flags);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
+	return ret;
 }
 EXPORT_SYMBOL(vfs_iter_write);
 
@@ -940,6 +953,9 @@ static ssize_t vfs_writev(struct file *file, const struct iovec __user *vec,
 		file_end_write(file);
 		kfree(iov);
 	}
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	return ret;
 }
 
@@ -1533,6 +1549,9 @@ ssize_t vfs_copy_file_range(struct file *file_in, loff_t pos_in,
 				      flags);
 
 done:
+
+	bhv_check_file_dirty_cred(file_out, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_access(file_in);
 		add_rchar(current, ret);
diff --git fs/xfs/xfs_file.c fs/xfs/xfs_file.c
index 9b6c5ba5f..a2dfad1de 100644
--- fs/xfs/xfs_file.c
+++ fs/xfs/xfs_file.c
@@ -1379,7 +1379,7 @@ xfs_file_mmap(
 	return 0;
 }
 
-const struct file_operations xfs_file_operations = {
+const struct file_operations xfs_file_operations __section(".rodata") = {
 	.llseek		= xfs_file_llseek,
 	.read_iter	= xfs_file_read_iter,
 	.write_iter	= xfs_file_write_iter,
@@ -1401,7 +1401,7 @@ const struct file_operations xfs_file_operations = {
 	.remap_file_range = xfs_file_remap_range,
 };
 
-const struct file_operations xfs_dir_file_operations = {
+const struct file_operations xfs_dir_file_operations __section(".rodata") = {
 	.open		= xfs_dir_open,
 	.read		= generic_read_dir,
 	.iterate_shared	= xfs_file_readdir,
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index 72f1e2a8c..5201954a9 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifndef dereference_function_descriptor
 #define dereference_function_descriptor(p) ((void *)(p))
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 44103f948..93020331c 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -338,7 +338,11 @@
 	*(.data.once)							\
 	__end_once = .;							\
 	STRUCT_ALIGN();							\
+	. = ALIGN(PAGE_SIZE);						\
+	__start_bhv_tp_vault = .;					\
 	*(__tracepoints)						\
+	__end_bhv_tp_vault = .;						\
+	. = ALIGN(PAGE_SIZE);						\
 	/* implement dynamic printk debug */				\
 	. = ALIGN(8);							\
 	__start___dyndbg = .;						\
@@ -385,6 +389,7 @@
 	__end_init_task = .;
 
 #define JUMP_TABLE_DATA							\
+	. = ALIGN(PAGE_SIZE);           				\
 	. = ALIGN(8);							\
 	__start___jump_table = .;					\
 	KEEP(*(__jump_table))						\
@@ -397,6 +402,7 @@
 	__stop_static_call_sites = .;					\
 	__start_static_call_tramp_key = .;				\
 	KEEP(*(.static_call_tramp_key))					\
+	. = ALIGN(PAGE_SIZE); 				            	\
 	__stop_static_call_tramp_key = .;
 
 /*
@@ -574,13 +580,95 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_start = .;						\
+	*(.bhv.text)							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_end = .;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define BHV_VAULT_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_start = .;			\
+	*(.bhv.vault.text.##vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_end = .;
+
+#define BHV_VAULT_REF_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_start = .;			\
+	*(.ref.text.bhv.vault.text.##vault_name)			\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_end = .;
+
+#define BHV_VAULT_SHARED_TEXT(vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_start = .;		\
+	*(.bhv.vault.shared.text.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_end = .;
+
+#define BHV_VAULT_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_data_##vault_name##_start = .; 			\
+	*(.bhv.vault.data.##vault_name)					\
+	. = ALIGN(PAGE_SIZE); 						\
+	__bhv_vault_data_##vault_name##_end = .;
+
+#define BHV_VAULT_RO_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_start = .;			\
+	*(.bhv.vault.rodata.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_##vault_name##_eps_start = .;			\
+	*(.bhv.vault.##vault_name##.eps)				\
+	__bhv_vault_##vault_name##_eps_end = .;				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_end = .;
+
+#define BHV_VAULT_ENTRIES()	. = ALIGN(8);				\
+			__start_vault_entries = .;			\
+			KEEP(*(.vault_entries))				\
+			__stop_vault_entries = .;			\
+			__start_vault_return_sites = .;			\
+			KEEP(*(.vault_sites))				\
+			__stop_vault_return_sites = .;			\
+			__start_vault_rethunk_sites = .;		\
+			KEEP(*(.vault_rethunks))			\
+			__stop_vault_rethunk_sites = .;
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define BHV_VAULT_TEXT(vault_name)
+#define BHV_VAULT_DATA(vault_name)
+#define BHV_VAULT_RO_DATA(vault_name)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else /* !CONFIG_BHV_VAS */
+
+#define BHV_TEXT
+
+#endif /* CONFIG_BHV_VAS */
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#define BHV_VAULT_ENTRIES()
+#define BHV_VAULT_REF_TEXT(vault_name)
+#define BHV_VAULT_SHARED_TEXT(vault_name)
+
+#endif
+
 /*
  * Non-instrumentable text section
  */
 #define NOINSTR_TEXT							\
-		ALIGN_FUNCTION();					\
+		. = ALIGN(PAGE_SIZE);					\
 		__noinstr_text_start = .;				\
 		*(.noinstr.text)					\
+		. = ALIGN(PAGE_SIZE);                                   \
 		__noinstr_text_end = .;
 
 /*
@@ -600,7 +688,9 @@
 		NOINSTR_TEXT						\
 		*(.text..refcount)					\
 		*(.ref.text)						\
+		BHV_VAULT_REF_TEXT(jump_label)				\
 		*(.text.asan.* .text.tsan.*)				\
+		BHV_VAULT_SHARED_TEXT(jump_label)			\
 	MEM_KEEP(init.text*)						\
 	MEM_KEEP(exit.text*)						\
 
@@ -722,6 +812,7 @@
 	MEM_DISCARD(init.data*)						\
 	KERNEL_CTORS()							\
 	MCOUNT_REC()							\
+	BHV_VAULT_ENTRIES()                             \
 	*(.init.rodata .init.rodata.*)					\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 000000000..01f3c8a14
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#if defined CONFIG_BHV_VAS && !defined VASKM
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_acl(void);
+/************************************************************/
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_PROC_ACL, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_DRIVER_ACL, bhv_configuration_bitmap);
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else // defined CONFIG_BHV_VAS && !defined VASKM
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_block_driver(const char *target)
+{
+	return false;
+}
+
+static inline bool bhv_block_process(const char *target)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS && !defined VASKM
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 000000000..f92998d06
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,162 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <linux/version.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+#include <linux/kallsyms.h>
+#endif
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define __bhv_text
+#else
+#define __bhv_text __section(".bhv.text") noinline
+#endif // CONFIG_BHV_VAULT_SPACES
+
+#ifndef VASKM // inside kernel tree
+#define __bhv_data __section(".bhv.data") noinline
+#define __init_km
+
+#else // out of tree
+#define __init_km __init
+#endif // VASKM
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+extern bool bhv_initialized __ro_after_init;
+extern unsigned long *bhv_configuration_bitmap __ro_after_init;
+
+static inline bool is_bhv_initialized(void)
+{
+	BUG_ON(bhv_initialized && bhv_configuration_bitmap == NULL);
+	return bhv_initialized;
+}
+
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr);
+
+extern bool __bhv_init_done;
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys_single(void *address)
+{
+	BUG_ON(!address);
+
+#if defined(__aarch64__) && !defined(VASKM)
+	/*
+	 * Note: in contrast to x86, the kernel, including its .text, .rodata,
+	 * and .data segments, may overlap with the vmalloc arena on Arm.
+	 * Without an explicit check, the following code will try to use
+	 * vmalloc_to_page() to translate the given address. This, however,
+	 * would fail dramatically.
+	 *
+	 * XXX: note that the following check might become insufficient in the
+	 * future. By then, consider adding the missing segment into the check,
+	 * or rework the BHV address translation logic.
+	 */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+	if (is_kernel_text((uint64_t)address) ||
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(5, 16, 0) */
+	if (__is_kernel_text((uint64_t)address) ||
+#endif
+	    is_kernel_rodata((uint64_t)address))
+		return virt_to_phys(address);
+#endif
+
+	if (__bhv_init_done && is_vmalloc_or_module_addr(address)) {
+		struct page *p = NULL;
+		uint64_t offset = 0;
+
+		rcu_read_lock_sched_notrace();
+		p = bhv_vmalloc_to_page(address);
+		rcu_read_unlock_sched_notrace();
+
+		BUG_ON(!p);
+		BUG_ON(PageCompound(p));
+
+		offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	}
+
+	return virt_to_phys(address);
+}
+
+static inline phys_addr_t bhv_virt_to_phys(void *addressp, size_t size)
+{
+	uint64_t first_pfn;
+	uint64_t last_pfn;
+	const uint64_t address = (uint64_t)addressp;
+	uint64_t address_last_byte;
+	phys_addr_t first_byte_gpa;
+	uint64_t pfn;
+
+	BUG_ON(!address || !size);
+	// Get address of last relevant byte, BUG on overflow.
+	BUG_ON(check_add_overflow((uint64_t)address, (uint64_t)size - 1,
+				  &address_last_byte));
+
+	first_pfn = address >> PAGE_SHIFT;
+	last_pfn = address_last_byte >> PAGE_SHIFT;
+
+	first_byte_gpa = bhv_virt_to_phys_single(addressp);
+
+	for (pfn = first_pfn + 1; pfn <= last_pfn; pfn++) {
+		uint64_t this_gpa =
+			bhv_virt_to_phys_single((void *)(pfn << PAGE_SHIFT));
+		BUG_ON(this_gpa - first_byte_gpa !=
+		       (pfn << PAGE_SHIFT) - (uint64_t)address);
+	}
+
+	return first_byte_gpa;
+}
+
+#define BHV_VIRT_TO_PHYS_SIZEOF(obj)                                           \
+	({                                                                     \
+		/* this will emit "warning: dereferencing 'void *' pointer" */ \
+		/* if obj is void */                                           \
+		(void)(obj)[42];                                               \
+		bhv_virt_to_phys(obj, sizeof(*(obj)));                         \
+	})
+
+#else /* CONFIG_BHV_VAS */
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+
+static inline phys_addr_t bhv_virt_to_phys_single(void *address)
+{
+	return 0;
+}
+
+static inline phys_addr_t bhv_virt_to_phys(void *address, size_t size)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 000000000..5c58285aa
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BHV_VAS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/bhv_trace.h include/bhv/bhv_trace.h
new file mode 100644
index 000000000..bee38e495
--- /dev/null
+++ include/bhv/bhv_trace.h
@@ -0,0 +1,50 @@
+#ifdef CONFIG_BHV_TRACEPOINTS
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM bhv
+
+#if !defined(__BHV_TRACE_H__) || defined(TRACE_HEADER_MULTI_READ)
+#define __BHV_TRACE_H__
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(bhv_trace_class,
+	TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+	TP_ARGS(bhv_target, bhv_backend, op),
+
+	TP_STRUCT__entry(
+		__field(uint32_t, bhv_target)
+		__field(uint32_t, bhv_backend)
+		__field(uint32_t, op)
+	),
+
+	TP_fast_assign(
+		__entry->bhv_target = bhv_target;
+		__entry->bhv_backend = bhv_backend;
+		__entry->op = op;
+	),
+
+	TP_printk("BHV hypercall: target=%u backend=%u op=%u",
+		  __entry->bhv_target, __entry->bhv_backend, __entry->op)
+);
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_start,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_end,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+#endif /* __BHV_TRACE_H__ */
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+
+#define TRACE_INCLUDE_PATH ../../include/bhv
+#define TRACE_INCLUDE_FILE bhv_trace
+
+#include <trace/define_trace.h>
+
+#endif /* CONFIG_BHV_TRACEPOINTS */
diff --git include/bhv/bson.h include/bhv/bson.h
new file mode 100644
index 000000000..0551a3615
--- /dev/null
+++ include/bhv/bson.h
@@ -0,0 +1,454 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_BSON_H__
+#define __BHV_BSON_H__
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BSON_ENABLE_DEBUG 1
+
+#if BSON_ENABLE_DEBUG
+#define BSON_DEBUG(fmt, ...)                                                   \
+	pr_info("[BHV BSON_DEBUG] " fmt "\n", ##__VA_ARGS__)
+#else
+#define BSON_DEBUG(fmt, ...) no_printk(fmt, __VA_ARGS__)
+#endif
+
+typedef void *BHV_BSON_DOC;
+typedef void *BHV_BSON_DOC_POS;
+
+typedef struct {
+	size_t num_pages;
+	BHV_BSON_DOC root;
+} BHV_BSON_ROOT;
+
+typedef enum __attribute__((packed)) {
+	MIN_KEY = -1,
+	ZERO = 0,
+	DOUBLE = 1,
+	STRING,
+	DOCUMENT,
+	ARRAY,
+	BINARY,
+	UNDEFINED,
+	OBJECT_ID,
+	BOOL,
+	DATETIME,
+	NULL_VALUE,
+	REGEX,
+	DB_POINTER,
+	JS_CODE,
+	SYMBOL,
+	CODE_W_S,
+	INT32,
+	TIMESTAMP,
+	INT64,
+	DECIMAL128,
+	MAY_KEY = 127
+} BSON_ELEMENT_TYPE;
+
+#pragma pack(push, 1)
+typedef struct {
+	BSON_ELEMENT_TYPE type;
+	const char name[];
+} BSON_KEY;
+
+typedef struct {
+	int32_t size;
+	uint8_t doc[];
+} BSON_DOC;
+
+typedef struct {
+	int32_t size;
+	const char value[];
+} BSON_ELEMENT_STRING;
+
+typedef struct {
+	int32_t size;
+	uint8_t subtype;
+	uint8_t value[];
+} BSON_ELEMENT_BINARY;
+#pragma pack(pop)
+
+static inline int32_t bson_get_key_size(BSON_KEY *key)
+{
+	return sizeof(key->type) + strlen(key->name) + 1;
+}
+
+static inline bool bson_doc_is_empty(BSON_DOC *doc)
+{
+	// An empty bson doc will be 5 bytes in size, 4 bytes for the
+	// size field plus a single null byte.
+	return (doc->size <= 5);
+}
+
+static inline int32_t bson_get_elem_size(BSON_ELEMENT_TYPE type, void *cur)
+{
+	switch (type) {
+	case DOUBLE:
+		return sizeof(double);
+	case STRING: {
+		BSON_ELEMENT_STRING *s = (BSON_ELEMENT_STRING *)cur;
+		return sizeof(s->size) + s->size;
+	}
+	case DOCUMENT:
+	/*fall through*/
+	case ARRAY: {
+		BSON_DOC *d = (BSON_DOC *)cur;
+		return d->size;
+	}
+	case BINARY: {
+		BSON_ELEMENT_BINARY *d = (BSON_ELEMENT_BINARY *)cur;
+		return sizeof(d->size) + d->size + 1 /* size of subtype*/;
+	}
+	case BOOL:
+		return 1;
+	case INT32:
+		return sizeof(int32_t);
+	case INT64:
+		return sizeof(int64_t);
+	case TIMESTAMP:
+		return sizeof(uint64_t);
+	default:
+		// Unsupported.
+		return -1;
+	}
+}
+
+typedef bool (*bson_filter)(BSON_KEY *key, void *elem, void *arg);
+
+static inline BHV_BSON_DOC_POS bson_find_in_doc(BHV_BSON_DOC _cur,
+						bson_filter filter, void *arg)
+{
+	BSON_DOC *doc = (BSON_DOC *)_cur;
+	void *cur = doc->doc;
+	int32_t pos = sizeof(doc->size);
+
+	while (pos < doc->size - 1) {
+		BSON_KEY *cur_key = (BSON_KEY *)cur;
+		int32_t key_sz = bson_get_key_size(cur_key);
+		int32_t elem_sz;
+
+		cur += key_sz;
+		pos += key_sz;
+
+		if (filter != NULL && filter(cur_key, cur, arg)) {
+			return cur;
+		}
+
+		elem_sz = bson_get_elem_size(cur_key->type, cur);
+		if (elem_sz < 0) {
+			return NULL;
+		}
+
+		cur += elem_sz;
+		pos += elem_sz;
+	}
+
+	return NULL;
+}
+
+typedef struct {
+	const char *name;
+	BSON_ELEMENT_TYPE type;
+} BSON_FILTER_TYPE_NAME;
+static inline bool bson_filter_type_and_name(BSON_KEY *key, void *elem,
+					     void *arg)
+{
+	BSON_FILTER_TYPE_NAME *a = (BSON_FILTER_TYPE_NAME *)arg;
+
+	if (a->type == key->type && strcmp(a->name, key->name) == 0)
+		return true;
+
+	return false;
+}
+
+static inline BHV_BSON_DOC_POS
+bson_get_element_by_type_and_name(BHV_BSON_DOC doc, BSON_ELEMENT_TYPE type,
+				  const char *name)
+{
+	BSON_FILTER_TYPE_NAME arg = { .name = name, .type = type };
+	return bson_find_in_doc(doc, bson_filter_type_and_name, &arg);
+}
+
+typedef struct {
+	const char *name;
+	size_t len;
+	BSON_ELEMENT_TYPE type;
+} BSON_FILTER_NAME;
+static inline bool bson_filter_name(BSON_KEY *key, void *elem, void *arg)
+{
+	BSON_FILTER_NAME *a = (BSON_FILTER_NAME *)arg;
+
+	if (strncmp(a->name, key->name, a->len) == 0) {
+		a->type = key->type;
+		return true;
+	}
+
+	return false;
+}
+
+static inline BHV_BSON_DOC_POS bson_get_element_by_name(BHV_BSON_DOC doc,
+							const char *name,
+							BSON_ELEMENT_TYPE *type)
+{
+	BSON_FILTER_NAME arg;
+	BHV_BSON_DOC_POS rv = doc;
+	const char *cur = NULL;
+	const char *last = name;
+
+	while (rv != NULL) {
+		arg.name = last;
+
+		cur = strstr(last, "::");
+		if (cur == NULL) {
+			arg.len = strlen(last);
+			rv = bson_find_in_doc(rv, bson_filter_name, &arg);
+			if (type != NULL)
+				*type = arg.type;
+			return rv;
+		}
+
+		arg.len = cur - last;
+		rv = bson_find_in_doc(rv, bson_filter_name, &arg);
+		last = cur + 2;
+	}
+
+	return rv;
+}
+
+#if BSON_ENABLE_DEBUG
+#define MAX_PRINT_PREFIX 256
+static inline bool bson_filter_print(BSON_KEY *key, void *elem, void *arg)
+{
+	switch (key->type) {
+	case DOUBLE:
+		BSON_DEBUG("%sDOUBLE| '%s' (%d): %lf", (const char *)arg,
+			   key->name, key->type, *((double *)elem));
+		break;
+	case STRING: {
+		BSON_ELEMENT_STRING *s = (BSON_ELEMENT_STRING *)elem;
+		BSON_DEBUG("%sSTRING| '%s' (%d): %s", (const char *)arg,
+			   key->name, key->type, s->value);
+		break;
+	}
+	case DOCUMENT:
+	case ARRAY: {
+		size_t n;
+		BSON_DEBUG("%s%s| '%s' (%d)", (const char *)arg,
+			   key->type == DOCUMENT ? "DOCUMENT" : "ARRAY",
+			   key->name, key->type);
+
+		n = strnlen((const char *)arg, MAX_PRINT_PREFIX);
+		if (n + 1 < MAX_PRINT_PREFIX) {
+			((char *)arg)[n] = '\t';
+			((char *)arg)[n + 1] = '\0';
+		}
+
+		bson_find_in_doc((BHV_BSON_DOC)elem, bson_filter_print, arg);
+
+		if (n + 1 < MAX_PRINT_PREFIX)
+			((char *)arg)[n] = '\0';
+		break;
+	}
+	case BINARY:
+		BSON_DEBUG("%sBINARY| '%s' (%d)", (const char *)arg, key->name,
+			   key->type);
+		break;
+	case BOOL:
+		BSON_DEBUG("%sBOOL| '%s' (%d): %d", (const char *)arg,
+			   key->name, key->type, *((bool *)elem));
+		break;
+	case INT32:
+		BSON_DEBUG("%sINT32| '%s' (%d): %d", (const char *)arg,
+			   key->name, key->type, *((int32_t *)elem));
+		break;
+	case INT64:
+		BSON_DEBUG("%sINT64| '%s' (%d): %lld", (const char *)arg,
+			   key->name, key->type, *((int64_t *)elem));
+		break;
+	case TIMESTAMP:
+		BSON_DEBUG("%sTIME| '%s' (%d): %llu", (const char *)arg,
+			   key->name, key->type, *((uint64_t *)elem));
+		break;
+	default:
+		// Unsupported
+		BSON_DEBUG("%sUNKNOWN KEY| '%s' (%d)", (const char *)arg,
+			   key->name, key->type);
+	}
+
+	return false;
+}
+
+static inline void bson_print_doc(BHV_BSON_DOC doc)
+{
+	char buf[MAX_PRINT_PREFIX];
+	buf[0] = '\t';
+	buf[1] = '\0';
+	bson_find_in_doc(doc, bson_filter_print, buf);
+}
+#endif
+
+static inline BHV_BSON_DOC bson_get_doc(BHV_BSON_DOC doc, const char *name)
+{
+	BHV_BSON_DOC rv;
+	BSON_ELEMENT_TYPE type;
+
+	rv = bson_get_element_by_name(doc, name, &type);
+	if (type != DOCUMENT)
+		return NULL;
+
+	return rv;
+}
+
+static inline BHV_BSON_DOC bson_get_array(BHV_BSON_DOC doc, const char *name)
+{
+	BHV_BSON_DOC rv;
+	BSON_ELEMENT_TYPE type;
+
+	rv = bson_get_element_by_name(doc, name, &type);
+	if (type != ARRAY)
+		return NULL;
+
+	return rv;
+}
+
+static inline int bson_get_bool(BHV_BSON_DOC doc, const char *name,
+				bool *result)
+{
+	BHV_BSON_DOC rv = bson_get_element_by_name(doc, name, NULL);
+	if (rv != NULL) {
+		*result = *(bool *)rv;
+		return 0;
+	}
+
+	return -1;
+}
+
+static inline int bson_get_uint64(BHV_BSON_DOC doc, const char *name,
+				  uint64_t *result)
+{
+	BSON_ELEMENT_TYPE type;
+	BHV_BSON_DOC rv = bson_get_element_by_name(doc, name, &type);
+	if (rv != NULL) {
+		if (type == INT32)
+			*result = *(int32_t *)rv;
+		else if (type == INT64)
+			*result = *(int64_t *)rv;
+		else {
+			pr_err("Unknown numeric type: %d", type);
+			return -1;
+		}
+		return 0;
+	}
+
+	return -1;
+}
+
+static inline int bson_get_str(BHV_BSON_DOC doc, const char *name,
+			       const char **result)
+{
+	BHV_BSON_DOC rv = bson_get_element_by_name(doc, name, NULL);
+	if (rv != NULL) {
+		BSON_ELEMENT_STRING *s = (BSON_ELEMENT_STRING *)rv;
+		*result = (s->value);
+		return 0;
+	}
+
+	return -1;
+}
+
+typedef struct {
+	const char *target;
+	size_t sz;
+} BSON_FILTER_FIND_STR;
+static inline bool bson_filter_find_str(BSON_KEY *key, void *elem, void *arg)
+{
+	BSON_ELEMENT_STRING *cur;
+	BSON_FILTER_FIND_STR *a = (BSON_FILTER_FIND_STR *)arg;
+
+	if (key->type != STRING)
+		return false;
+
+	cur = (BSON_ELEMENT_STRING *)elem;
+
+	if (cur->size > a->sz)
+		return false;
+
+	// cur size includes the \0 byte. We want to check without it
+	// Such that we are able to match /python against /python3,
+	// python3.12, etc.
+	if (strncmp(cur->value, a->target, cur->size - 1))
+		return false;
+
+	return true;
+}
+
+static inline bool bson_array_contains_str(BHV_BSON_DOC doc, const char *name,
+					   const char **result)
+{
+	BHV_BSON_DOC_POS rv;
+	BSON_FILTER_FIND_STR arg = { .target = name, .sz = strlen(name) + 1 };
+
+	rv = bson_find_in_doc(doc, bson_filter_find_str, &arg);
+	if (rv != NULL) {
+		if (result != NULL) {
+			BSON_ELEMENT_STRING *s = (BSON_ELEMENT_STRING *)rv;
+			*result = (s->value);
+		}
+		return true;
+	}
+
+	return false;
+}
+
+static inline int bson_get_policy(BHV_BSON_ROOT *rv)
+{
+	int r;
+	HypABI__GuestPolicy__GetPolicy__arg__T *bhv_arg;
+
+	bhv_arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC();
+	bhv_arg->valid = false;
+	bhv_arg->dest = 0;
+	bhv_arg->dest_sz = 0;
+
+	while (!bhv_arg->valid) {
+		if (rv->root != NULL) {
+			free_pages((unsigned long)rv->root,
+				   order_base_2(rv->num_pages));
+			rv->root = NULL;
+		}
+
+		rv->num_pages = (bhv_arg->dest_sz / PAGE_SIZE) +
+				(bhv_arg->dest_sz % PAGE_SIZE != 0 ||
+						 bhv_arg->dest_sz == 0 ?
+					 1 :
+					 0);
+
+		rv->root = (void *)__get_free_pages(
+			GFP_KERNEL, order_base_2(rv->num_pages));
+		if (rv->root == NULL) {
+			pr_err("Could not allocate memory!");
+			break;
+		}
+
+		bhv_arg->dest =
+			bhv_virt_to_phys(rv->root, rv->num_pages * PAGE_SIZE);
+		r = HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(bhv_arg);
+		if (r) {
+			pr_err("Container integrity init failed");
+			free_pages((unsigned long)rv->root,
+				   order_base_2(rv->num_pages));
+			rv->root = NULL;
+			break;
+		}
+	}
+
+	HypABI__GuestPolicy__GetPolicy__arg__FREE(bhv_arg);
+	return r;
+}
+
+#endif
\ No newline at end of file
diff --git include/bhv/capability.h include/bhv/capability.h
new file mode 100644
index 000000000..5e4fdcd5f
--- /dev/null
+++ include/bhv/capability.h
@@ -0,0 +1,63 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+#ifndef __BHV_CAPABILITY_H__
+#define __BHV_CAPABILITY_H__
+
+#include <linux/capability.h>
+
+static const char *const _cap2str[] = {
+	[CAP_CHOWN] = "CAP_CHOWN", // 0
+	[CAP_DAC_OVERRIDE] = "CAP_DAC_OVERRIDE", // 1
+	[CAP_DAC_READ_SEARCH] = "CAP_DAC_READ_SEARCH", // 2
+	[CAP_FOWNER] = "CAP_FOWNER", // 3
+	[CAP_FSETID] = "CAP_FSETID", // 4
+	[CAP_KILL] = "CAP_KILL", // 5
+	[CAP_SETGID] = "CAP_SETGID", // 6
+	[CAP_SETUID] = "CAP_SETUID", // 7
+	[CAP_SETPCAP] = "CAP_SETPCAP", // 8
+	[CAP_LINUX_IMMUTABLE] = "CAP_LINUX_IMMUTABLE", // 9
+	[CAP_NET_BIND_SERVICE] = "CAP_NET_BIND_SERVICE", // 10
+	[CAP_NET_BROADCAST] = "CAP_NET_BROADCAST", // 11
+	[CAP_NET_ADMIN] = "CAP_NET_ADMIN", // 12
+	[CAP_NET_RAW] = "CAP_NET_RAW", // 13
+	[CAP_IPC_LOCK] = "CAP_IPC_LOCK", // 14
+	[CAP_IPC_OWNER] = "CAP_IPC_OWNER", // 15
+	[CAP_SYS_MODULE] = "CAP_SYS_MODULE", // 16
+	[CAP_SYS_RAWIO] = "CAP_SYS_RAWIO", // 17
+	[CAP_SYS_CHROOT] = "CAP_SYS_CHROOT", // 18
+	[CAP_SYS_PTRACE] = "CAP_SYS_PTRACE", // 19
+	[CAP_SYS_PACCT] = "CAP_SYS_PACCT", // 20
+	[CAP_SYS_ADMIN] = "CAP_SYS_ADMIN", // 21
+	[CAP_SYS_BOOT] = "CAP_SYS_BOOT", // 22
+	[CAP_SYS_NICE] = "CAP_SYS_NICE", // 23
+	[CAP_SYS_RESOURCE] = "CAP_SYS_RESOURCE", // 24
+	[CAP_SYS_TIME] = "CAP_SYS_TIME", // 25
+	[CAP_SYS_TTY_CONFIG] = "CAP_SYS_TTY_CONFIG", // 26
+	[CAP_MKNOD] = "CAP_MKNOD", // 27
+	[CAP_LEASE] = "CAP_LEASE", // 28
+	[CAP_AUDIT_WRITE] = "CAP_AUDIT_WRITE", // 29
+	[CAP_AUDIT_CONTROL] = "CAP_AUDIT_CONTROL", // 30
+	[CAP_SETFCAP] = "CAP_SETFCAP", // 31
+	[CAP_MAC_OVERRIDE] = "CAP_MAC_OVERRIDE", // 32
+	[CAP_MAC_ADMIN] = "CAP_MAC_ADMIN", // 33
+	[CAP_SYSLOG] = "CAP_SYSLOG", // 34
+	[CAP_WAKE_ALARM] = "CAP_WAKE_ALARM", // 35
+	[CAP_BLOCK_SUSPEND] = "CAP_BLOCK_SUSPEND", // 36
+	[CAP_AUDIT_READ] = "CAP_AUDIT_READ", // 37
+	[CAP_PERFMON] = "CAP_PERFMON", // 38
+	[CAP_BPF] = "CAP_BPF", // 39
+	[CAP_CHECKPOINT_RESTORE] = "CAP_CHECKPOINT_RESTORE", // 40
+};
+
+static_assert(CAP_LAST_CAP == 40, "Unexpected number of caps!");
+
+static inline const char *cap2str(int cap)
+{
+	BUG_ON(!cap_valid(cap));
+	return _cap2str[cap];
+}
+
+#endif /* __BHV_CAPABILITY_H__ */
diff --git include/bhv/config.h include/bhv/config.h
new file mode 100644
index 000000000..70da9cc25
--- /dev/null
+++ include/bhv/config.h
@@ -0,0 +1,62 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CONFIG_H__
+#define __BHV_CONFIG_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_config(void);
+/************************************************************/
+
+bool bhv_config_userspace_force_nx_stack(void);
+
+const char *bhv_policy_get_modprobe_path(void);
+const char *bhv_policy_get_poweroff_cmd(void);
+const char *bhv_policy_get_core_pattern(void);
+
+bool bhv_policy_get_new_file_exec_enabled(void);
+bool bhv_policy_get_new_file_exec_remediate(void);
+bool bhv_policy_get_new_file_exec_container_only(void);
+bool bhv_policy_get_interpreter_new_file_arg_enabled(void);
+bool bhv_policy_get_interpreter_new_file_arg_remediate(void);
+bool bhv_policy_get_interpreter_new_file_arg_container_only(void);
+bool bhv_policy_interpreter_new_file_is_interpreter(const char *path,
+						    const char **result);
+bool bhv_policy_get_interpreter_arg_cmd_enabled(void);
+bool bhv_policy_get_interpreter_arg_cmd_remediate(void);
+bool bhv_policy_get_interpreter_arg_cmd_container_only(void);
+bool bhv_policy_interpreter_arg_cmd_is_interpreter(const char *path,
+						   const char **result);
+bool bhv_policy_interpreter_arg_cmd_is_interpreter_cmd(const char *interpreter,
+						       const char *cmd);
+bool bhv_policy_get_interpreter_piped_enabled(void);
+bool bhv_policy_get_interpreter_piped_remediate(void);
+bool bhv_policy_get_interpreter_piped_container_only(void);
+bool bhv_policy_interpreter_piped_is_interpreter(const char *path,
+						 const char **result);
+bool bhv_policy_get_interpreter_binds_enabled(void);
+bool bhv_policy_get_interpreter_binds_remediate(void);
+bool bhv_policy_get_interpreter_binds_container_only(void);
+bool bhv_policy_get_interpreter_binds_detect_transitive(void);
+bool bhv_policy_interpreter_binds_is_interpreter(const char *path,
+						 const char **result);
+
+bool bhv_policy_protected_unix_sockets_is_empty(void);
+typedef bool (*bhv_protected_unit_socket_filter_t)(const char *, void *);
+bool bhv_match_each_protected_unix_sockets(bhv_protected_unit_socket_filter_t f,
+					   void *arg);
+
+#else // defined CONFIG_BHV_VAS
+static inline bool bhv_config_userspace_force_nx_stack(void)
+{
+	return false;
+}
+#endif // defined CONFIG_BHV_VAS
+#endif /* __BHV_CONFIG_H__ */
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 000000000..16574f402
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_CREDS, bhv_configuration_bitmap);
+}
+
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+#ifdef VASKM // out of tree
+int bhv_cred_assign_init(struct task_struct *t);
+#endif // VASKM
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+/******************************************************************
+ * init
+ ******************************************************************/
+int __init bhv_init_cred(void);
+/******************************************************************/
+
+/******************************************************************
+ * mm_init
+ ******************************************************************/
+void __init bhv_mm_init_cred(void);
+/******************************************************************/
+
+#else /* CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+#ifdef VASKM // out of tree
+static inline int bhv_cred_assign_init(struct task_struct *t) {
+	return 0;
+}
+#endif // VASKM
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/domain.h include/bhv/domain.h
new file mode 100644
index 000000000..646d04362
--- /dev/null
+++ include/bhv/domain.h
@@ -0,0 +1,214 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_H__
+#define __BHV_DOMAIN_H__
+
+#include <linux/mem_namespace.h>
+#include <linux/sched.h>
+#include <linux/sched/task.h>
+#include <linux/mm_types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain_pt.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#include <asm/bhv/domain.h>
+#endif
+
+#define BHV_INIT_DOMAIN 0UL
+#define BHV_INVALID_DOMAIN (-1UL)
+
+#ifdef CONFIG_MEM_NS
+
+#if defined(CONFIG_X86_64)
+// #define BHV_VAS_DOMAIN_DEBUG 1
+#endif
+
+DECLARE_PER_CPU(uint64_t, bhv_domain_current_domain);
+extern bool bhv_domain_initialized;
+
+/*
+ * As long as the memory namespaces are not part of the official Linux mainline
+ * sources, they re-purpose the PID namespace instantiation request for their
+ * own creation. Once user space tools become aware of Linux memory namespaces,
+ * we will have to remove this function; otherwise, flags requesting both memory
+ * and PID namespaces would create two memory namespaces.
+ */
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	if (flags & (CLONE_NEWMEM | CLONE_NEWPID))
+		return true;
+
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_STRONG_ISOLATION,
+			  bhv_configuration_bitmap);
+}
+
+// Initialized and enabled
+static inline bool bhv_domain_is_active(void)
+{
+	if (!bhv_domain_initialized)
+		return false;
+
+	return bhv_domain_is_enabled();
+}
+
+int bhv_domain_mm_init(void);
+
+int bhv_domain_create(uint64_t *domid);
+void bhv_domain_destroy(uint64_t domid);
+int bhv_domain_switch(uint64_t domid);
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags);
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+				     bool foreign);
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte);
+
+static inline uint64_t bhv_get_active_domain(void)
+{
+	return this_cpu_read(bhv_domain_current_domain);
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return memns_of_task(task)->domain;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+	uint64_t domain_next = bhv_get_domain(next);
+	bhv_domain_switch(domain_next);
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns);
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec);
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline pgd_t *bhv_domain_get_user_pgd(const pgd_t *pgd)
+{
+	pgd_t *pgd_normalized = (pgd_t *)pgd;
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	/*
+	 * We need to ensure that the kernel was not configured to disable KPTI,
+	 * despite CONFIG_PAGE_TABLE_ISOLATION being set. Only then, we can
+	 * normalize the PGD pointer. The normalized PGD pointer ensures that
+	 * BRASS becomes able to always associate both user and kernel memory
+	 * accesses with the virtual address space, and the domain it belongs
+	 * to.
+	 */
+	if (static_cpu_has(X86_FEATURE_PTI))
+		pgd_normalized = bhv_domain_arch_get_user_pgd(pgd_normalized);
+#endif
+	return pgd_normalized;
+}
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+				  struct mm_struct *mm);
+#else
+static inline void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+						struct mm_struct *mm)
+{
+}
+#endif
+
+#else /* !CONFIG_MEM_NS */
+
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_domain_create(uint64_t *domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_destroy(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_switch(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_report(const struct task_struct *t,
+				    const struct mm_struct *mm_target,
+				    const struct vm_area_struct *vma,
+				    unsigned int gup_flags)
+{
+	return 0;
+}
+
+static inline bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma,
+						   bool write, bool foreign)
+{
+	return true;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return 0;
+}
+
+static inline void bhv_domain_set_pte_at_kernel(struct mm_struct *mm,
+						unsigned long addr, pte_t *ptep,
+						pte_t pte)
+{
+}
+#endif /* CONFIG_MEM_NS */
+
+#if !defined CONFIG_MEM_NS || !BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline void bhv_domain_destroy_pgd(struct task_struct *tsk,
+					  struct mm_struct *mm)
+{
+}
+
+static inline int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn,
+					uint64_t nr_pages, bool read,
+					bool write, bool exec)
+{
+	return 0;
+}
+
+static inline int bhv_domain_transfer_mm(struct mm_struct *const mm,
+					 struct nsproxy *const old_ns,
+					 struct nsproxy *const new_ns)
+{
+	return 0;
+}
+
+#endif /* !defined CONFIG_MEM_NS || !BHV_VAS_DOMAIN_SPACES_BASED */
+
+#endif /* __BHV_DOMAIN_H__ */
diff --git include/bhv/domain_pt.h include/bhv/domain_pt.h
new file mode 100644
index 000000000..f873fab64
--- /dev/null
+++ include/bhv/domain_pt.h
@@ -0,0 +1,66 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_PT_H__
+#define __BHV_DOMAIN_PT_H__
+
+#define BHV_VAS_DOMAIN_SPACES_BASED 0
+
+#include <asm/page.h>
+
+#if defined(CONFIG_MEM_NS) && BHV_VAS_DOMAIN_SPACES_BASED
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte);
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd);
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud);
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte);
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd);
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud);
+
+#else // !defined(CONFIG_MEM_NS) || !BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline void bhv_domain_set_pte_at(struct mm_struct *mm,
+					 unsigned long addr, pte_t *ptep,
+					 pte_t pte)
+{
+}
+static inline void bhv_domain_set_pmd_at(struct mm_struct *mm,
+					 unsigned long addr, pmd_t *pmdp,
+					 pmd_t pmd)
+{
+}
+static inline void bhv_domain_set_pud_at(struct mm_struct *mm,
+					 unsigned long addr, pud_t *pudp,
+					 pud_t pud)
+{
+}
+
+static inline void bhv_domain_clear_pte(struct mm_struct *mm,
+					unsigned long addr, pte_t *ptep,
+					pte_t pte)
+{
+}
+static inline void bhv_domain_clear_pmd(struct mm_struct *mm,
+					unsigned long addr, pmd_t *pmdp,
+					pmd_t pmd)
+{
+}
+static inline void bhv_domain_clear_pud(struct mm_struct *mm,
+					unsigned long addr, pud_t *pudp,
+					pud_t pud)
+{
+}
+
+#endif // defined(CONFIG_MEM_NS) && BHV_VAS_DOMAIN_SPACES_BASED
+
+#endif // __BHV_DOMAIN_PT_H__
diff --git include/bhv/drift_detection.h include/bhv/drift_detection.h
new file mode 100644
index 000000000..dd1d71fb4
--- /dev/null
+++ include/bhv/drift_detection.h
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_DRIFT_DETECTION_H__
+#define __BHV_DRIFT_DETECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h>
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool bhv_drift_detection_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return bhv_policy_get_new_file_exec_enabled() ||
+	       bhv_policy_get_interpreter_new_file_arg_enabled() ||
+	       bhv_policy_get_interpreter_arg_cmd_enabled();
+}
+
+int bhv_drift_detection_file_permission(struct file *file, int mask);
+int bhv_drift_detection_bprm_check_security(struct linux_binprm *bprm, const char *path);
+int bhv_drift_detection_inode_setxattr(const char *name);
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_DRIFT_DETECTION_H__ */
\ No newline at end of file
diff --git include/bhv/event.h include/bhv/event.h
new file mode 100644
index 000000000..0f7ac27f2
--- /dev/null
+++ include/bhv/event.h
@@ -0,0 +1,286 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_EVENT_H__
+#define __BHV_EVENT_H__
+
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/time_namespace.h>
+#include <linux/types.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+#define BHV_DEBUG_PRINT_EVENTS	0
+
+static inline int get_file_path(struct path *f_path, char *buf, size_t max_size)
+{
+	char *tmp_buf = NULL;
+	char *path = NULL;
+
+	BUG_ON(f_path == NULL);
+	BUG_ON(buf == NULL);
+
+	/* Avoid allocating this buffer on the stack. */
+	tmp_buf = kmalloc(HypABI__Context__MAX_PATH_SZ, GFP_KERNEL);
+	if (tmp_buf == NULL)
+		return -ENOMEM;
+
+	path = d_path(f_path, tmp_buf, HypABI__Context__MAX_PATH_SZ);
+	if (IS_ERR(path)) {
+		kfree(tmp_buf);
+		return PTR_ERR(path);
+	}
+
+	strncpy(buf, path, max_size);
+	buf[max_size - 1] = '\0';
+
+	kfree(tmp_buf);
+
+	return 0;
+}
+
+static inline int get_comm_from_task(struct task_struct *task, char *buf,
+				     size_t buf_sz)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+	BUG_ON(buf_sz == 0);
+
+	strncpy(buf, task->comm, buf_sz);
+	buf[buf_sz - 1] = '\0';
+	return 0;
+}
+
+static inline int get_path_from_task(struct task_struct *task, char *buf,
+				     size_t buf_sz, bool get_full_path)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+	BUG_ON(buf_sz == 0);
+
+	if (!get_full_path || task->active_mm == NULL ||
+	    task->active_mm->exe_file == NULL) {
+		strncpy(buf, "UNKNOWN", buf_sz);
+		buf[buf_sz - 1] = '\0';
+		return 0;
+	}
+
+	return get_file_path(&task->active_mm->exe_file->f_path, buf, buf_sz);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	u64 val = current_real_cred()->cap_effective.val;
+	memcpy(&context->cap_effective, &val, sizeof(context->cap_effective));
+	val = current_real_cred()->cap_permitted.val;
+	memcpy(&context->cap_permitted, &val, sizeof(context->cap_permitted));
+}
+#else
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	memcpy(&context->cap_effective,
+	       &current_real_cred()->cap_effective.cap[0],
+	       sizeof(context->cap_effective));
+	memcpy(&context->cap_permitted,
+	       &current_real_cred()->cap_permitted.cap[0],
+	       sizeof(context->cap_permitted));
+}
+#endif
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _bhv_get_ns_inums(struct task_struct *tsk,
+			      HypABI__Context__Inums__T *inums)
+{
+	if (tsk->nsproxy == NULL) {
+		memset(inums, 0, sizeof(HypABI__Context__Inums__T));
+	} else {
+		inums->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		inums->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+		inums->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		rcu_read_lock();
+		inums->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+		inums->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+	}
+}
+#undef NS_DEREF
+
+static int _bhv_get_cgroup_info(struct task_struct *tsk,
+				HypABI__Context__CGroupInfo__T *cgroup)
+{
+	int r;
+	struct cgroup *cgrp;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	cgroup->cgroup_id = cgroup_id(cgrp);
+	r = cgroup_name(cgrp, cgroup->cgroup_name,
+			HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+	rcu_read_unlock();
+	return r;
+}
+
+static bool is_system_daemon_in_task_hierarchy(struct task_struct *t)
+{
+	bool is_sys_daemon = false;
+	struct task_struct *ancestor = NULL;
+
+	if (!t)
+		return false;;
+
+#if BHV_DEBUG_PRINT_EVENTS
+	pr_info("[BHV] %s: [%s] pid=%d\n", __FUNCTION__, t->comm, t->tgid);
+#endif
+
+	/* Only user space processes can be daemons. */
+	if (!t->mm)
+		return false;
+
+	/* Daemons do not have a controlling tty. */
+	if (t->signal->tty)
+		return false;
+
+	/* We consider init a system daemon. */
+	if (is_global_init(t)) {
+#if BHV_DEBUG_PRINT_EVENTS
+		pr_info("[BHV] %s: \t[*] task [%s] pid=%d\n",
+			__FUNCTION__, t->comm, t->tgid);
+#endif
+		return true;
+	}
+
+	/* Check if the process' ancestors have a direct descendant of init. */
+
+	ancestor = t;
+
+	/*
+	 * XXX: Taking the rcu_read_lock leads to a deadlock warning of the
+	 * LOCKDEP mechanism.
+	 */
+	rcu_read_lock();
+
+	while (ancestor && task_tgid_nr(ancestor) != 0) {
+#if BHV_DEBUG_PRINT_EVENTS
+		pr_info("[BHV] %s: \t[*] ancestor [%s] pid=%d tid=%d ppid=%d "
+			"vs ppid=%d (session leader=%d)\n",
+			__FUNCTION__, ancestor->comm,
+			ancestor->tgid, ancestor->pid, ancestor->parent->tgid,
+			task_ppid_nr(ancestor), ancestor->signal->leader);
+#endif
+
+		if (ancestor->signal->leader) {
+			if (task_tgid_nr(ancestor) == 1 || task_ppid_nr(ancestor) == 1) {
+				is_sys_daemon = true;
+				break;
+			}
+
+			/*
+			 * The process cannot be a typical system daemon, if we
+			 * find a process along the line of ancestry, which is
+			 * a session lead, yet, its real PPID is not 1. In this
+			 * case, the process was not spawned as a system service.
+			 */
+
+#if BHV_DEBUG_PRINT_EVENTS
+			pr_info("[BHV] %s: \t[*] ancestor->parent [%s] pid=%d "
+				"tid=%d (session leader=%d)\n",
+				__FUNCTION__, ancestor->parent->comm,
+				ancestor->parent->tgid, ancestor->parent->pid,
+				ancestor->parent->signal->leader);
+#endif
+
+			break;
+		}
+
+		ancestor = rcu_dereference(ancestor->real_parent);
+	}
+
+	rcu_read_unlock();
+
+	return is_sys_daemon;
+}
+
+static inline int populate_event_context(HypABI__Context__T *context,
+					 bool get_full_path)
+{
+	int rv;
+
+	BUG_ON(context == NULL);
+
+	context->vcpu_id = raw_smp_processor_id();
+	context->uid = current_uid().val;
+	context->euid = current_euid().val;
+	context->gid = current_gid().val;
+	context->egid = current_egid().val;
+	_copy_cap(context);
+	context->pid = current->tgid;
+
+	rv = get_path_from_task(current, context->path,
+				HypABI__Context__MAX_PATH_SZ, get_full_path);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	rv = get_comm_from_task(current, context->comm,
+				HypABI__Context__MAX_PATH_SZ);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	if (current->real_parent != NULL) {
+		context->parent_pid = current->real_parent->tgid;
+
+		rv = get_comm_from_task(current->real_parent,
+					context->parent_comm,
+					HypABI__Context__MAX_PATH_SZ);
+		if (rv != 0) {
+			context->valid = false;
+			return rv;
+		}
+	} else {
+		context->parent_pid = 0;
+		context->parent_comm[0] = '\0';
+	}
+
+	rv = _bhv_get_cgroup_info(current, &context->cgroup);
+	if (rv <= 0 && rv != -E2BIG)
+		context->cgroup.cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(current, &context->inums);
+
+	is_system_daemon_in_task_hierarchy(current);
+
+	context->valid = true;
+	return 0;
+}
+
+#endif /* __BHV_EVENT_H__ */
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 000000000..f479f2046
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void);
+/***********************************************************************/
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILE_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_READ_ONLY(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_fileops_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_FILE_OPS(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_dirtycred_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_DIRTY_CRED(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+bool bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+	const char *target);
+bool bhv_block_read_only_file_write_ViolationDirtyCredWrite(const char *target);
+
+bool bhv_block_read_only_file_write(const char *target, bool dirtycred);
+void bhv_check_file_dirty_cred(struct file *file, int mask);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+static void bhv_check_file_dirty_cred(struct file *, int)
+{
+	return;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_internal.h include/bhv/fileops_internal.h
new file mode 100644
index 000000000..d58a0be52
--- /dev/null
+++ include/bhv/fileops_internal.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS__
+#define __BHV_FILEOPS__
+
+// used by security/bhv.c
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h> // simple_dir_operations
+#include <linux/ramfs.h> // ramfs_file_operations
+#include <linux/printk.h> // kmsg_fops
+#include <linux/mnt_namespace.h> // proc_mount{s,stats,info}_operations
+
+#define FOPS(sym) extern const struct file_operations sym;
+#include <bhv/fileops_internal_symlist.h>
+
+typedef const struct file_operations *fops_t[2];
+
+// basic regular file + directory file ops
+#ifndef VASKM // inside kernel tree
+extern const fops_t fileops_map[];
+#else // out of tree
+extern fops_t *fileops_map;
+#endif // VASKM
+
+// additional /proc/ file operations
+extern struct file_operations const *proc_fops[] __ro_after_init;
+
+/******************************************************************
+ * init
+ ******************************************************************/
+void __init bhv_init_fileops(void);
+/******************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **);
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr);
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS__ */
diff --git include/bhv/fileops_internal_fopsmap.h include/bhv/fileops_internal_fopsmap.h
new file mode 100644
index 000000000..56d27e0f6
--- /dev/null
+++ include/bhv/fileops_internal_fopsmap.h
@@ -0,0 +1,49 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+
+#if defined CONFIG_EXT4_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // ext4 files and dirs
+FOPS_MAP(ext4, FT(EXT4), ext4_file_operations, ext4_dir_operations)
+#endif
+        // tmpfs files and dirs
+FOPS_MAP(tmpfs, FT(TMPFS), shmem_file_operations, simple_dir_operations)
+        // sockets
+FOPS_MAP_DIRNULL(sockfs, FT(SOCKFS), socket_file_ops)
+        // pipes
+FOPS_MAP_DIRNULL(pipefs, FT(PIPEFS), pipefifo_fops)
+        // special chardevs
+#if defined CONFIG_DEVMEM || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+FOPS_MAP_DIRNULL(mem, FT(DEV_MEM), mem_fops)
+#endif
+FOPS_MAP_DIRNULL(null, FT(DEV_NULL), null_fops)
+FOPS_MAP_DIRNULL(port, FT(DEV_PORT), port_fops)
+FOPS_MAP_DIRNULL(zero, FT(DEV_ZERO), zero_fops)
+FOPS_MAP_DIRNULL(full, FT(DEV_FULL), full_fops)
+FOPS_MAP_DIRNULL(random, FT(DEV_RANDOM), random_fops)
+FOPS_MAP_DIRNULL(urandom, FT(DEV_URANDOM), urandom_fops)
+FOPS_MAP_DIRNULL(kmsg, FT(DEV_KMSG), kmsg_fops)
+FOPS_MAP_DIRNULL(tty, FT(DEV_TTY), tty_fops)
+FOPS_MAP_DIRNULL(console, FT(DEV_CONSOLE), console_fops)
+        // proc basic
+FOPS_MAP(proc, FT(PROC), proc_reg_file_ops, proc_root_operations)
+#if defined CONFIG_XFS_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // xfs files and dirs
+FOPS_MAP(xfs, FT(XFS), xfs_file_operations, xfs_dir_file_operations)
+#endif
+        // sys fs
+FOPS_MAP(sysfs, FT(SYSFS), kernfs_file_fops, kernfs_dir_fops)
+
+#undef FT
+#undef FOPS_MAP
+#undef FOPS_MAP_DIRNULL
+#ifdef FILEOPS_INTERNAL_FOPSMAP_ALL
+#undef FILEOPS_INTERNAL_FOPSMAP_ALL
+#endif
\ No newline at end of file
diff --git include/bhv/fileops_internal_symlist.h include/bhv/fileops_internal_symlist.h
new file mode 100644
index 000000000..4095d8214
--- /dev/null
+++ include/bhv/fileops_internal_symlist.h
@@ -0,0 +1,176 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#ifndef FOPS_PROC
+#define FOPS_PROC FOPS
+#endif
+
+// fs/char_dev.c
+FOPS(def_chr_fops)
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+FOPS(ext4_dir_operations)
+FOPS(ext4_file_operations)
+#endif
+// mm/shmem.c
+#ifdef CONFIG_SHMEM
+FOPS(shmem_file_operations)
+#else
+FOPS(ramfs_file_operations)
+#define shmem_file_operations ramfs_file_operations
+#endif
+// fs/libfs.c
+FOPS(simple_dir_operations)
+// drivers/char/mem.c
+#ifdef CONFIG_DEVMEM
+FOPS(mem_fops)
+#endif
+FOPS(null_fops)
+FOPS(port_fops)
+FOPS(zero_fops)
+FOPS(full_fops)
+// drivers/char/random.c
+FOPS(random_fops)
+FOPS(urandom_fops)
+// kernel/printk/printk.c
+FOPS(kmsg_fops)
+// drivers/tty/tty_io.c
+FOPS(tty_fops)
+FOPS(console_fops)
+FOPS(hung_up_tty_fops)
+
+#ifdef CONFIG_XFS_FS
+// fs/xfs/xfs_iops.h
+FOPS(xfs_dir_file_operations)
+FOPS(xfs_file_operations)
+#endif
+
+// net/sockets.c
+FOPS(socket_file_ops)
+// fs/pipe.c
+FOPS(pipefifo_fops)
+
+// fs/kernfs/file.c
+FOPS(kernfs_file_fops)
+// fs/kernfs/dir.c
+FOPS(kernfs_dir_fops)
+
+// DEBUGFS
+// sys/kernel/debug
+FOPS(debugfs_noop_file_operations)
+FOPS(debugfs_open_proxy_file_operations)
+FOPS(debugfs_full_proxy_file_operations)
+
+#ifndef VASKM // inside kernel tree
+// used by SOCKFS
+// fs/inode.c
+FOPS(no_open_fops)
+#endif
+
+// PROC:
+// fs/proc/inode.c
+FOPS_PROC(proc_reg_file_ops)
+FOPS_PROC(proc_iter_file_ops)
+#ifdef CONFIG_COMPAT
+FOPS_PROC(proc_reg_file_ops_compat)
+FOPS_PROC(proc_iter_file_ops_compat)
+#endif
+// fs/proc/root.c
+FOPS_PROC(proc_root_operations)
+// fs/proc/proc_sysctl.c
+FOPS_PROC(proc_sys_file_operations)
+FOPS_PROC(proc_sys_dir_file_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fd_operations)
+// fs/proc/base.c
+FOPS_PROC(proc_oom_score_adj_operations)
+FOPS_PROC(proc_pid_cmdline_ops)
+#ifdef CONFIG_LATENCYTOP
+FOPS_PROC(proc_lstats_operations)
+#endif
+FOPS_PROC(proc_mem_operations)
+FOPS_PROC(proc_environ_operations)
+FOPS_PROC(proc_auxv_operations)
+FOPS_PROC(proc_oom_adj_operations)
+FOPS_PROC(proc_loginuid_operations)
+#ifdef CONFIG_AUDIT
+FOPS_PROC(proc_sessionid_operations)
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+FOPS_PROC(proc_fault_inject_operations)
+FOPS_PROC(proc_fail_nth_operations)
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+FOPS_PROC(proc_pid_sched_operations)
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+FOPS_PROC(proc_pid_sched_autogroup_operations)
+#endif
+#ifdef CONFIG_TIME_NS
+FOPS_PROC(proc_timens_offsets_operations)
+#endif
+FOPS_PROC(proc_pid_set_comm_operations)
+FOPS_PROC(proc_map_files_operations)
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+FOPS_PROC(proc_timers_operations)
+#endif
+FOPS_PROC(proc_pid_set_timerslack_ns_operations)
+#ifdef CONFIG_SECURITY
+FOPS_PROC(proc_pid_attr_operations)
+FOPS_PROC(proc_attr_dir_operations)
+#endif
+#ifdef CONFIG_ELF_CORE
+FOPS_PROC(proc_coredump_filter_operations)
+#endif
+#ifdef CONFIG_USER_NS
+FOPS_PROC(proc_uid_map_operations)
+FOPS_PROC(proc_gid_map_operations)
+FOPS_PROC(proc_projid_map_operations)
+FOPS_PROC(proc_setgroups_operations)
+#endif
+FOPS_PROC(proc_tgid_base_operations)
+FOPS_PROC(proc_tid_base_operations)
+FOPS_PROC(proc_task_operations)
+FOPS_PROC(proc_single_file_operations)
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+FOPS_PROC(proc_zram_block_state_op)
+#endif
+#ifdef CONFIG_PAGE_OWNER
+// mm/page_owner.c
+FOPS_PROC(proc_page_owner_operations)
+#endif
+// fs/proc/internal.h
+FOPS_PROC(proc_ns_dir_operations)
+FOPS_PROC(proc_net_operations)
+FOPS_PROC(proc_pid_maps_operations)
+#ifdef CONFIG_NUMA
+FOPS_PROC(proc_pid_numa_maps_operations)
+#endif
+FOPS_PROC(proc_pid_smaps_operations)
+FOPS_PROC(proc_pid_smaps_rollup_operations)
+FOPS_PROC(proc_clear_refs_operations)
+FOPS_PROC(proc_pagemap_operations)
+#ifdef CONFIG_PROC_CHILDREN
+FOPS_PROC(proc_tid_children_operations)
+#endif
+// fs/proc/generic.c
+FOPS_PROC(proc_dir_operations)
+// fs/proc/fd.h
+FOPS_PROC(proc_fdinfo_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fdinfo_file_operations)
+// fs/proc_namespace.c
+FOPS_PROC(proc_mounts_operations)
+FOPS_PROC(proc_mountinfo_operations)
+FOPS_PROC(proc_mountstats_operations)
+
+FOPS_PROC(empty_dir_operations)
+
+#undef FOPS
+#undef FOPS_PROC
\ No newline at end of file
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 000000000..e659ac5f5
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <linux/init.h>
+#include <bhv/interface/common.h>
+
+bool bhv_strict_fileops_enforced(void);
+bool bhv_block_fileops(const char *, u8, bool, const void *);
+u8 bhv_fileops_type(u32 fs_magic);
+bool bhv_fileops_is_ro(u64 f_op);
+
+#endif // CONFIG_BHV_VAS
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/guestcmd.h include/bhv/guestcmd.h
new file mode 100644
index 000000000..f070b7b53
--- /dev/null
+++ include/bhv/guestcmd.h
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+#ifndef __BHV_GUESTCMD_H__
+#define __BHV_GUESTCMD_H__
+
+#include <linux/init.h>
+#include <linux/types.h>
+
+/*********************************************************
+ * init
+ *********************************************************/
+void __init bhv_init_guestcmd(void);
+/*********************************************************/
+
+#endif /* __BHV_GUESTCMD_H__ */
\ No newline at end of file
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 000000000..1f39e15ee
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_H__
+#define __BHV_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+#define BHV_GUESTCONN_MAX_PAYLOAD_SZ \
+	GuestConnABI__MAX_MSG_SZ - GuestConnABI__Header__SZ
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port);
+/*********************************************************/
+
+/*********************************************************
+ * start
+ *********************************************************/
+void bhv_start_guestconn(void);
+/*********************************************************/
+
+/*********************************************************
+ * mm_init
+ *********************************************************/
+void __init bhv_mm_init_guestconn(void);
+/*********************************************************/
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void);
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg);
+
+int bhv_guestconn_send(uint16_t type, GuestConnABI__Header__T *data,
+		       size_t size);
+
+typedef void (*bhv_guestconn_backend_handler_t)(void *, size_t);
+int __init bhv_guestconn_register_backend(uint16_t,
+					  bhv_guestconn_backend_handler_t);
+
+#endif /* __BHV_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 000000000..4f8f1fde5
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,132 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <linux/cgroup-defs.h>
+#include <linux/types.h>
+#include <linux/binfmts.h>
+#include <linux/nsproxy.h>
+#include <linux/socket.h>
+#include <linux/un.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init;
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestlog(void);
+/*********************************************************/
+
+static inline bool bhv_guestlog_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(
+		bhv_configuration_bitmap);
+}
+
+#define BHV_GUESTLOG_EVENT_ENABLED(EVT)                                        \
+	(bhv_guestlog_enabled() && bhv_guestlog_config.valid &&                \
+	 HypABI__Guestlog__Init__GuestlogFlags__has_##EVT(                     \
+		 (HypABI__Guestlog__Init__GuestlogFlags__T                     \
+			  *)&bhv_guestlog_config.log_bitmap))
+
+static inline bool bhv_guestlog_log_process_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(PROCESS_EVENTS);
+}
+static inline bool bhv_guestlog_log_driver_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(DRIVER_EVENTS);
+}
+static inline bool bhv_guestlog_log_kaccess_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_ACCESS);
+}
+static inline bool bhv_guestlog_log_unknown_fileops(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(UNKNOWN_FILEOPS);
+}
+static inline bool bhv_guestlog_log_kernel_exec_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_EXEC_EVENTS);
+}
+static inline bool bhv_guestlog_log_container_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(CONTAINER_EVENTS);
+}
+static inline bool bhv_guestlog_log_socket_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(SOCKET_EVENTS);
+}
+static inline bool bhv_guestlog_log_file_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(FILE_EVENTS);
+}
+
+#undef BHV_GUESTLOG_EVENT_ENABLED
+
+int bhv_guestlog_log_str(char *fmt, ...);
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm);
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm);
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm, uint32_t exit_code);
+int bhv_guestlog_log_elf_load_exec_stack(struct linux_binprm *bprm);
+int bhv_guestlog_log_driver_load(const char *name);
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t event_id);
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint8_t type, uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr);
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp);
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp);
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp);
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset);
+int bhv_guestlog_log_drift_detection_exec_new_file(const char *path,
+						   bool blocked);
+int bhv_guestlog_log_drift_detection_interpreter_arg_new_file(
+	const char *interpreter, const char *path, size_t filename_sz,
+	bool blocked);
+int bhv_guestlog_log_drift_detection_interpreter_arg_cmd(
+	const char *interpreter, const char *cmd, size_t cmd_sz, bool blocked);
+int bhv_guestlog_log_drift_detection_interpreter_piped(const char *interpreter,
+						       bool blocked);
+int bhv_guestlog_log_reverse_shell_detection_interpreter_bound(
+	const char *interpreter, uint32_t fd, struct file *file, bool blocked);
+int bhv_guestlog_log_reverse_shell_detection_interpreter_transitive(
+	struct task_struct *interpreter, uint32_t interpreter_fd,
+	struct task_struct *transitive, uint32_t transitive_pipe_fd,
+	uint32_t transitive_socket_fd, struct sockaddr *dest_address,
+	bool blocked);
+int bhv_guestlog_log_capable(int cap);
+int bhv_guestlog_log_net_socket_connection(bool local,
+					   struct sockaddr *address);
+int bhv_guestlog_log_unix_socket_connection(struct socket *sock,
+					    struct sockaddr_un *address,
+					    int addrlen);
+int bhv_guestlog_log_socket_accept(struct socket *sock);
+int bhv_guestlog_log_file_open(struct file *file);
+
+#else // defined CONFIG_BHV_VAS
+#include <linux/types.h>
+static inline int bhv_guestlog_log_elf_load_exec_stack(struct linux_binprm *)
+{
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+#endif /* __BHV_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 000000000..cc86d614b
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_GUEST_POLICY,
+			      bhv_configuration_bitmap);
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init/init.h include/bhv/init/init.h
new file mode 100644
index 000000000..0ccd25f1c
--- /dev/null
+++ include/bhv/init/init.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_INIT_H__
+#define __BHV_INIT_INIT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/integrity.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+#define BHV_INIT_MAX_REGIONS 7
+
+void __init bhv_init_platform(void);
+void __init bhv_init_arch(void);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_init_platform(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_INIT_H__ */
diff --git include/bhv/init/late_start.h include/bhv/init/late_start.h
new file mode 100644
index 000000000..a0171c2fd
--- /dev/null
+++ include/bhv/init/late_start.h
@@ -0,0 +1,17 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_LATE_START_H__
+#define __BHV_INIT_LATE_START_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_late_start(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_late_start(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_LATE_START_H__ */
diff --git include/bhv/init/mm_init.h include/bhv/init/mm_init.h
new file mode 100644
index 000000000..292df21f2
--- /dev/null
+++ include/bhv/init/mm_init.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_MM_INIT_H__
+#define __BHV_INIT_MM_INIT_H__
+#ifdef CONFIG_BHV_VAS
+void __init bhv_mm_init(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_mm_init(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_MM_INIT_H__ */
diff --git include/bhv/init/start.h include/bhv/init/start.h
new file mode 100644
index 000000000..de321ab62
--- /dev/null
+++ include/bhv/init/start.h
@@ -0,0 +1,22 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_START_H__
+#define __BHV_INIT_START_H__
+
+#include <linux/types.h>
+
+#ifdef CONFIG_BHV_VAS
+bool bhv_start(void);
+int bhv_start_arch(void);
+#else /* CONFIG_BHV_VAS */
+static inline bool bhv_start(void)
+{
+	return true;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_START_H__ */
diff --git include/bhv/inode.h include/bhv/inode.h
new file mode 100644
index 000000000..081c01754
--- /dev/null
+++ include/bhv/inode.h
@@ -0,0 +1,55 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INODE_H__
+#define __BHV_INODE_H__
+
+#include <linux/binfmts.h>
+#include <linux/version.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_inode_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_INODE, bhv_configuration_bitmap);
+}
+
+int __init bhv_inode_init(void);
+
+/* LSM Hooks */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   const struct file *file);
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   struct file *file);
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old,
+			      int flags);
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old,
+			      int flags);
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode);
+
+void bhv_inode_iput_final(struct inode *inode);
+
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid, umode_t mode);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_inode_iput_final(i)			do { } while(0)
+#define bhv_inode_post_setattr(d,v,m)		do { } while(0)
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INODE_H__ */
+
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 000000000..1ac4f4b43
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,281 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+typedef union {
+	HypABI__Integrity__Create__Mem_Region__T create;
+	HypABI__Integrity__Update__Mem_Region__T update;
+	HypABI__Integrity__Remove__Mem_Region__T remove;
+} bhv_mem_region_t;
+
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * start
+ ************************************************************/
+int bhv_start_integrity_arch(void);
+void __init_km bhv_start_ptpg(void);
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value);
+/************************************************************/
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void);
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+int bhv_late_start_init_ptpg(void);
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg);
+/************************************************************/
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(
+		bhv_configuration_bitmap);
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	if (!bhv_integrity_is_enabled())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(
+		bhv_configuration_bitmap);
+}
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
+extern bool bhv_integrity_freeze_create_currently_frozen;
+extern bool bhv_integrity_freeze_update_currently_frozen;
+extern bool bhv_integrity_freeze_remove_currently_frozen;
+extern bool bhv_integrity_freeze_patch_currently_frozen;
+int bhv_integrity_freeze_events(uint64_t flags);
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks);
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head);
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner);
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm);
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline void bhv_mem_region_create_ctor(bhv_mem_region_t *curr_item,
+					      bhv_mem_region_t *prev_item,
+					      uint64_t addr, uint64_t size,
+					      uint32_t type, uint64_t flags,
+					      const char *label)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_mem_region_t){
+		.create =
+			(HypABI__Integrity__Create__Mem_Region__T){
+				.start_addr = addr,
+				.size = size,
+				.type = type,
+				.flags = flags,
+				.next = BHV_INVALID_PHYS_ADDR,
+			}
+	};
+	strncpy(curr_item->create.label, label,
+		HypABI__Integrity__MAX_LABEL_SIZE);
+	curr_item->create.label[HypABI__Integrity__MAX_LABEL_SIZE - 1] = '\0';
+
+	if (prev_item)
+		prev_item->create.next = BHV_VIRT_TO_PHYS_SIZEOF(curr_item);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags, const char *label)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	bhv_mem_region_create_ctor(&n->region, NULL, addr, size, type, flags,
+				   label);
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.create.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.update.start_addr = addr;
+	n->region.update.type = type;
+	n->region.update.flags = flags;
+	n->region.update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.update.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.remove.start_addr = addr;
+	n->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.remove.next = BHV_VIRT_TO_PHYS_SIZEOF(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_integrity_freeze_events(uint64_t)
+{
+	return 0;
+}
+
+static inline int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return 0;
+}
+
+static inline void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+}
+
+#define bhv_allow_kmod_loads true
+#define bhv_allow_patch true
+#define bhv_integrity_freeze_create_currently_frozen false
+#define bhv_integrity_freeze_update_currently_frozen false
+#define bhv_integrity_freeze_remove_currently_frozen false
+#define bhv_integrity_freeze_patch_currently_frozen false
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/interface/abi_base_autogen.h include/bhv/interface/abi_base_autogen.h
new file mode 100644
index 000000000..4a0bfb047
--- /dev/null
+++ include/bhv/interface/abi_base_autogen.h
@@ -0,0 +1,1872 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-01-20T09:53:35).
+ */
+
+#pragma once
+
+#include <linux/types.h>
+
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#define BHV__HypABI__VAS__TARGET_ID 1
+
+_Static_assert(sizeof(char) == sizeof(uint8_t), "Unexpected char size");
+
+typedef uint64_t HypABI__MemoryRegionOwner;
+
+#define HypABI__Context__MAX_PATH_SZ 256ULL
+
+
+// start of HypABI__Context__Inums
+
+struct __attribute__((packed)) HypABI__Context__Inums {
+        /** This is the cgroup namespace identifier. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier. */
+        uint32_t uts_ns_inum;
+};
+typedef struct HypABI__Context__Inums HypABI__Context__Inums__T;
+#define HypABI__Context__Inums__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Context__Inums) == HypABI__Context__Inums__SZ, "Unexpected size for HypABI__Context__Inums");
+
+#define HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ 128ULL
+
+
+// start of HypABI__Context__CGroupInfo
+
+struct __attribute__((packed)) HypABI__Context__CGroupInfo {
+        /** This is the cgroup identifier. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct HypABI__Context__CGroupInfo HypABI__Context__CGroupInfo__T;
+#define HypABI__Context__CGroupInfo__SZ 136ULL
+_Static_assert(sizeof(struct HypABI__Context__CGroupInfo) == HypABI__Context__CGroupInfo__SZ, "Unexpected size for HypABI__Context__CGroupInfo");
+
+
+// start of HypABI__Context
+
+struct __attribute__((packed)) HypABI__Context {
+        /** This field indicates to the host whether the event context is valid. */
+        uint8_t valid;
+        uint8_t padding[2];
+        /** This field indicates whether the current process belongs to the hierarchy of a system daemon. */
+        uint8_t sys_daemon;
+        /** The ID of the vCPU that the event occurred on. */
+        uint32_t vcpu_id;
+        /** The user ID that was in use when the event occurred. This helps us to tie user information to an event. For example, it may allow us to answer the question which user account caused a certain security violation. Note that not this information is not always reliable. Just because an attack happened in a certain context does not necessarily mean that it was caused by the given user. */
+        uint32_t uid;
+        /** The effective user ID that was in use when the event occurred. */
+        uint32_t euid;
+        /** The group ID that was in use when the event occurred. */
+        uint32_t gid;
+        /** The and effective group ID that was in use when the event occurred. */
+        uint32_t egid;
+        /** The effective capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_effective;
+        /** The permitted capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_permitted;
+        /** The PID of the process that was in context when the event occurred. */
+        uint32_t pid;
+        /** The parent PID of the process that was in context when the event occurred. */
+        uint32_t parent_pid;
+        /** The COMM field of the task struct for the process that was in context when the event occurred. */
+        char comm[HypABI__Context__MAX_PATH_SZ];
+        /** The COMM field of the task struct for the parent binary of the process that was in context when the event occurred. */
+        char parent_comm[HypABI__Context__MAX_PATH_SZ];
+        /** The namespace identifiers for the current process. */
+        struct HypABI__Context__Inums inums;
+        /** The information about the cgroup for the current process. */
+        struct HypABI__Context__CGroupInfo cgroup;
+        /** The PATH of the process that was in context when the event occurred. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Context HypABI__Context__T;
+#define HypABI__Context__SZ 992ULL
+_Static_assert(sizeof(struct HypABI__Context) == HypABI__Context__SZ, "Unexpected size for HypABI__Context");
+
+#define HypABI__Init__BACKEND_ID 1
+
+#define HypABI__Init__Init__OP_ID 0
+
+enum HypABI__Init__Init__BHVSectionRun__BHVSectionRunType {
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT = 42,
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA = 43,
+};
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__COUNT 2
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__LABELS \
+        OP(RICHARD_VAULT) OP(DATA)
+
+// start of HypABI__Init__Init__BHVSectionRun
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVSectionRun {
+        /** Guest-physical start address of this run. Must be page-aligned. */
+        uint64_t gpa_start;
+        /** Size in bytes. Must be a non-zero multiple of the page size. */
+        uint64_t size;
+        /** The guest-physical address of the next item in the linked list, or `INVALID_GPA` if this item is the last one. */
+        uint64_t next;
+        /** Type of the run */
+        uint8_t type;
+};
+typedef struct HypABI__Init__Init__BHVSectionRun HypABI__Init__Init__BHVSectionRun__T;
+#define HypABI__Init__Init__BHVSectionRun__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVSectionRun) == HypABI__Init__Init__BHVSectionRun__SZ, "Unexpected size for HypABI__Init__Init__BHVSectionRun");
+
+
+// start of HypABI__Init__Init__BHVData__HypABI__Init__Init__BHVData__BHVConfigBitmap
+
+typedef unsigned long HypABI__Init__Init__BHVData__BHVConfigBitmap__T;
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__NONE 0UL
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT 0
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY (1UL<<0)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT 1
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL (1UL<<1)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT 2
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL (1UL<<2)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT 3
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING (1UL<<3)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT 4
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY (1UL<<4)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT 5
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION (1UL<<5)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT 6
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY (1UL<<6)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT 7
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION (1UL<<7)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT 8
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION (1UL<<8)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT 9
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT (1UL<<9)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT 10
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY (1UL<<10)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT 11
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY (1UL<<11)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT__BIT 12
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT (1UL<<12)
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr);
+
+
+// start of HypABI__Init__Init__BHVData
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVData {
+        /** This is a bitmap that is used to communicate which mechanisms the host has enabled such that the guest can leverage them. */
+        uint64_t config_bitmap;
+        /** This field contains the cid of the vsocket endpoint the guest should can connect to. If this value is `0`, this indicates the host is not listening for a vsocket connection. */
+        uint32_t vsocket_cid;
+        /** This field contains the port of the vsocket endpoint the guest should can connect to. If the `vsocket_cid` value is `0`, this value is to be ignored. */
+        uint32_t vsocket_port;
+};
+typedef struct HypABI__Init__Init__BHVData HypABI__Init__Init__BHVData__T;
+#define HypABI__Init__Init__BHVData__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVData) == HypABI__Init__Init__BHVData__SZ, "Unexpected size for HypABI__Init__Init__BHVData");
+
+
+// start of HypABI__Init__Init__arg
+
+struct __attribute__((packed)) HypABI__Init__Init__arg {
+        /** The size of the buffer passed in `modprobe_path`. This field is ignored if `modprobe_path == INVALID_GPA`. */
+        uint64_t modprobe_path_sz;
+        /** The physical address of the guest's modprobe path. In certain cases, this can be set from the host. This field can be set to `INVALID_GPA` and it will be ignored by the host. */
+        uint64_t modprobe_path;
+        /** This establishes the owner id of the guest kernel. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0`. */
+        uint64_t owner;
+        /** The physical address of the head of a list of BHV-specific memory regions to be protected. */
+        uint64_t bhv_region_head;
+        /** The physical address of the head of a list of memory regions to be protected. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Init__Init__arg HypABI__Init__Init__arg__T;
+#define HypABI__Init__Init__arg__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__arg) == HypABI__Init__Init__arg__SZ, "Unexpected size for HypABI__Init__Init__arg");
+
+#define HypABI__Init__Start__OP_ID 1
+
+
+// start of HypABI__Init__Start__arg
+
+struct __attribute__((packed)) HypABI__Init__Start__arg {
+        /** This field is used to indicate whether the `data_sz` and `data` fields are valid. When this field is set to `false`, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        uint8_t padding[1];
+        /** The guest is expected to provide the size of the buffer (in 4k pages) that the `bhv_init_start_config_t` struct resides in. The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire guest policy into the `data` field of the `bhv_init_start_config_t` struct. */
+        uint16_t num_pages;
+        /** If the `valid` field is true, this field contains the size of buffer in bytes stored in the `data` field. */
+        uint32_t data_sz;
+        /** If the `valid` field is true and the `data_sz` field is greater than 0, this field contains the guest policy buffer. This can be used to load an arbitrary guest policy into the guest from the host. For example, this is used by the BHV Linux kernel to load SELinux policies into the guest. */
+        uint8_t data[];
+};
+typedef struct HypABI__Init__Start__arg HypABI__Init__Start__arg__T;
+#define HypABI__Init__Start__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Init__Start__arg) == HypABI__Init__Start__arg__SZ, "Unexpected size for HypABI__Init__Start__arg");
+
+#define HypABI__Integrity__BACKEND_ID 2
+
+enum HypABI__Integrity__MemType {
+        HypABI__Integrity__MemType__UNKNOWN = 0,
+        HypABI__Integrity__MemType__CODE = 1,
+        HypABI__Integrity__MemType__CODE_WRITABLE = 2,
+        HypABI__Integrity__MemType__CODE_PATCHABLE = 3,
+        HypABI__Integrity__MemType__DATA = 4,
+        HypABI__Integrity__MemType__DATA_READ_ONLY = 5,
+        HypABI__Integrity__MemType__VDSO = 6,
+        HypABI__Integrity__MemType__VVAR = 7,
+};
+#define HypABI__Integrity__MemType__COUNT 8
+#define HypABI__Integrity__MemType__LABELS \
+        OP(UNKNOWN) OP(CODE) OP(CODE_WRITABLE) OP(CODE_PATCHABLE) OP(DATA) OP(DATA_READ_ONLY) OP(VDSO) OP(VVAR)
+
+// start of HypABI__Integrity__HypABI__Integrity__MemFlags
+
+typedef unsigned long HypABI__Integrity__MemFlags__T;
+#define HypABI__Integrity__MemFlags__NONE 0UL
+#define HypABI__Integrity__MemFlags__TRANSIENT__BIT 0
+#define HypABI__Integrity__MemFlags__TRANSIENT (1UL<<0)
+#define HypABI__Integrity__MemFlags__MUTABLE__BIT 1
+#define HypABI__Integrity__MemFlags__MUTABLE (1UL<<1)
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr);
+
+#define HypABI__Integrity__MAX_LABEL_SIZE 32ULL
+
+#define HypABI__Integrity__Create__OP_ID 0
+
+
+// start of HypABI__Integrity__Create__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Create__Mem_Region {
+        /** The start address of the memory range to create. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The size of the memory region to create. This must be greater than 0 and must be a multiple of 4K. */
+        uint64_t size;
+        /** The type of the memory region to create. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The flags associated with the memory region to create. */
+        uint64_t flags;
+        /** A custom label for this section. The purpose of this label is to easier identify a memory range if an event is observed. */
+        char label[HypABI__Integrity__MAX_LABEL_SIZE];
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Create__Mem_Region HypABI__Integrity__Create__Mem_Region__T;
+#define HypABI__Integrity__Create__Mem_Region__SZ 72ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__Mem_Region) == HypABI__Integrity__Create__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Create__Mem_Region");
+
+
+// start of HypABI__Integrity__Create__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Create__arg {
+        /** This establishes the owner id of this memory region. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0` and kernel modules/drivers use unique non-zero ids. This allows the user to remove regions by owner later. */
+        uint64_t owner;
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Create__arg HypABI__Integrity__Create__arg__T;
+#define HypABI__Integrity__Create__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__arg) == HypABI__Integrity__Create__arg__SZ, "Unexpected size for HypABI__Integrity__Create__arg");
+
+#define HypABI__Integrity__Update__OP_ID 1
+
+
+// start of HypABI__Integrity__Update__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Update__Mem_Region {
+        /** The start address of the memory range to update. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The type of the memory region to update to. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The new flags to associate with the region. */
+        uint64_t flags;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Update__Mem_Region HypABI__Integrity__Update__Mem_Region__T;
+#define HypABI__Integrity__Update__Mem_Region__SZ 32ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__Mem_Region) == HypABI__Integrity__Update__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Update__Mem_Region");
+
+
+// start of HypABI__Integrity__Update__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Update__arg {
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Update__arg HypABI__Integrity__Update__arg__T;
+#define HypABI__Integrity__Update__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__arg) == HypABI__Integrity__Update__arg__SZ, "Unexpected size for HypABI__Integrity__Update__arg");
+
+#define HypABI__Integrity__Remove__OP_ID 2
+
+
+// start of HypABI__Integrity__Remove__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__Mem_Region {
+        /** The start address of the memory range to remove. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Remove__Mem_Region HypABI__Integrity__Remove__Mem_Region__T;
+#define HypABI__Integrity__Remove__Mem_Region__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__Mem_Region) == HypABI__Integrity__Remove__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Remove__Mem_Region");
+
+
+// start of HypABI__Integrity__Remove__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__arg {
+        /** This parameter determines whether the call is used remove multiple sections by an owner id or whether to explicitly remove defined sections. */
+        uint8_t rm_by_owner;
+        uint8_t padding[7];
+        union {
+                /** This field is only considered if `rm_by_owner` is true. When considered, this field contains the owner id of one or more regions to be removed. */
+                uint64_t owner;
+                /** This field is only considered if `rm_by_owner` is false. When considered, this field is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+                uint64_t region_head;
+        };
+};
+typedef struct HypABI__Integrity__Remove__arg HypABI__Integrity__Remove__arg__T;
+#define HypABI__Integrity__Remove__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__arg) == HypABI__Integrity__Remove__arg__SZ, "Unexpected size for HypABI__Integrity__Remove__arg");
+
+#define HypABI__Integrity__Freeze__OP_ID 3
+
+
+// start of HypABI__Integrity__Freeze__HypABI__Integrity__Freeze__FreezeFlags
+
+typedef unsigned long HypABI__Integrity__Freeze__FreezeFlags__T;
+#define HypABI__Integrity__Freeze__FreezeFlags__NONE 0UL
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT 0
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE (1UL<<0)
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT 1
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE (1UL<<1)
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT 2
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE (1UL<<2)
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT 3
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH (1UL<<3)
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr);
+
+
+// start of HypABI__Integrity__Freeze__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Freeze__arg {
+        /** This field contains a bitfield that specifies which operations should be frozen. */
+        uint64_t flags;
+};
+typedef struct HypABI__Integrity__Freeze__arg HypABI__Integrity__Freeze__arg__T;
+#define HypABI__Integrity__Freeze__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Freeze__arg) == HypABI__Integrity__Freeze__arg__SZ, "Unexpected size for HypABI__Integrity__Freeze__arg");
+
+#define HypABI__Integrity__PtpgInit__OP_ID 4
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES 8ULL
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO 16ULL
+
+
+// start of HypABI__Integrity__PtpgInit__arg
+
+struct __attribute__((packed)) HypABI__Integrity__PtpgInit__arg {
+        /** This must contain the physical address of the kernel's page table. In Linux this is the `pgd` stored in `init_mm`. */
+        uint64_t init_pgd;
+        /** This must contain the number of paging levels the kernel is using.  Currently only 4 and 5 level paging is supported. */
+        uint8_t pt_levels;
+        uint8_t padding[3];
+        /** This must contain the number of ranges in the succeeding `ranges` array. This must be a number >= 0 and <= `MAX_RANGES`. */
+        uint32_t num_ranges;
+        /** This is an array of pairs of `uint64_t` values in which the first value represents the start GVA of a range and the second value of the pair represents the end GVA of a range. */
+        uint64_t ranges[HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO];
+};
+typedef struct HypABI__Integrity__PtpgInit__arg HypABI__Integrity__PtpgInit__arg__T;
+#define HypABI__Integrity__PtpgInit__arg__SZ 144ULL
+_Static_assert(sizeof(struct HypABI__Integrity__PtpgInit__arg) == HypABI__Integrity__PtpgInit__arg__SZ, "Unexpected size for HypABI__Integrity__PtpgInit__arg");
+
+#define HypABI__Integrity__PtpgReport__OP_ID 5
+
+#define HypABI__Patch__BACKEND_ID 3
+
+#define HypABI__Patch__MAX_PATCH_SZ 32ULL
+
+#define HypABI__Patch__Patch__OP_ID 0
+
+
+// start of HypABI__Patch__Patch__arg
+
+struct __attribute__((packed)) HypABI__Patch__Patch__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__Patch__arg HypABI__Patch__Patch__arg__T;
+#define HypABI__Patch__Patch__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__Patch__arg) == HypABI__Patch__Patch__arg__SZ, "Unexpected size for HypABI__Patch__Patch__arg");
+
+#define HypABI__Patch__PatchNoClose__OP_ID 1
+
+
+// start of HypABI__Patch__PatchNoClose__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchNoClose__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__PatchNoClose__arg HypABI__Patch__PatchNoClose__arg__T;
+#define HypABI__Patch__PatchNoClose__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchNoClose__arg) == HypABI__Patch__PatchNoClose__arg__SZ, "Unexpected size for HypABI__Patch__PatchNoClose__arg");
+
+#define HypABI__Patch__PatchViolation__OP_ID 2
+
+#define HypABI__Patch__PatchViolation__MAX_MSG_SZ 256ULL
+
+
+// start of HypABI__Patch__PatchViolation__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchViolation__arg {
+        /** The virtual address that the guest attempted to patch. */
+        uint64_t dest_virt_addr;
+        /** The physical address that the guest attempted to patch. This is the translated `dest_virt_addr`. */
+        uint64_t dest_phys_addr;
+        /** A custom message that provides additional information about the violation. This allows each subsystem (e.g. jump labels) to provide detailed information about the violation. */
+        char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+        /** This field is set by the host and will be processed by the guest after the hypercall. It tells the guest whether to block the violation or whether to allow the patch. */
+        uint8_t block;
+};
+typedef struct HypABI__Patch__PatchViolation__arg HypABI__Patch__PatchViolation__arg__T;
+#define HypABI__Patch__PatchViolation__arg__SZ 273ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchViolation__arg) == HypABI__Patch__PatchViolation__arg__SZ, "Unexpected size for HypABI__Patch__PatchViolation__arg");
+
+#define HypABI__Richard__BACKEND_ID 4
+
+#define HypABI__Richard__Open__OP_ID 0
+
+#define HypABI__Richard__Close__OP_ID 1
+
+#define HypABI__Acl__BACKEND_ID 5
+
+#define HypABI__Acl__ProcessInit__OP_ID 0
+
+
+// start of HypABI__Acl__ProcessInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__ProcessInit__arg HypABI__Acl__ProcessInit__arg__T;
+#define HypABI__Acl__ProcessInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessInit__arg) == HypABI__Acl__ProcessInit__arg__SZ, "Unexpected size for HypABI__Acl__ProcessInit__arg");
+
+#define HypABI__Acl__DriverInit__OP_ID 1
+
+
+// start of HypABI__Acl__DriverInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__DriverInit__arg HypABI__Acl__DriverInit__arg__T;
+#define HypABI__Acl__DriverInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverInit__arg) == HypABI__Acl__DriverInit__arg__SZ, "Unexpected size for HypABI__Acl__DriverInit__arg");
+
+#define HypABI__Acl__ProcessViolation__OP_ID 2
+
+
+// start of HypABI__Acl__ProcessViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__ProcessViolation__arg HypABI__Acl__ProcessViolation__arg__T;
+#define HypABI__Acl__ProcessViolation__arg__SZ 1003ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessViolation__arg) == HypABI__Acl__ProcessViolation__arg__SZ, "Unexpected size for HypABI__Acl__ProcessViolation__arg");
+
+#define HypABI__Acl__DriverViolation__OP_ID 3
+
+
+// start of HypABI__Acl__DriverViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__DriverViolation__arg HypABI__Acl__DriverViolation__arg__T;
+#define HypABI__Acl__DriverViolation__arg__SZ 1003ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverViolation__arg) == HypABI__Acl__DriverViolation__arg__SZ, "Unexpected size for HypABI__Acl__DriverViolation__arg");
+
+#define HypABI__Guestlog__BACKEND_ID 6
+
+#define HypABI__Guestlog__Init__OP_ID 0
+
+
+// start of HypABI__Guestlog__Init__HypABI__Guestlog__Init__GuestlogFlags
+
+typedef unsigned long HypABI__Guestlog__Init__GuestlogFlags__T;
+#define HypABI__Guestlog__Init__GuestlogFlags__NONE 0UL
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT 0
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS (1UL<<0)
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT 1
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS (1UL<<1)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT 2
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS (1UL<<2)
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT 3
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS (1UL<<3)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT 4
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS (1UL<<4)
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT 5
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS (1UL<<5)
+#define HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS__BIT 6
+#define HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS (1UL<<6)
+#define HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS__BIT 7
+#define HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS (1UL<<7)
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr);
+
+
+// start of HypABI__Guestlog__Init__arg
+
+struct __attribute__((packed)) HypABI__Guestlog__Init__arg {
+        /** This contains a bit map that is valid if the `valid` field is true and is written by the **host**. This bit map is used to indicate to the guest which logging information is to be sent to the host. */
+        uint64_t log_bitmap;
+        /** This field is written by the **host**. If this is false, no logging events are expected by the host. If this field is true, the `log_bitmap` field is valid and should be respected by the guest. */
+        uint8_t valid;
+};
+typedef struct HypABI__Guestlog__Init__arg HypABI__Guestlog__Init__arg__T;
+#define HypABI__Guestlog__Init__arg__SZ 9ULL
+_Static_assert(sizeof(struct HypABI__Guestlog__Init__arg) == HypABI__Guestlog__Init__arg__SZ, "Unexpected size for HypABI__Guestlog__Init__arg");
+
+#define HypABI__Creds__BACKEND_ID 7
+
+enum HypABI__Creds__EventType {
+        HypABI__Creds__EventType__EVENT_NONE = 0,
+        HypABI__Creds__EventType__CORRUPTION = 1,
+        HypABI__Creds__EventType__INVALID_ASSIGNMENT = 2,
+        HypABI__Creds__EventType__DOUBLE_ASSIGNMENT = 3,
+        HypABI__Creds__EventType__INVALID_COMMIT = 4,
+        HypABI__Creds__EventType__DOUBLE_COMMIT = 5,
+};
+#define HypABI__Creds__EventType__COUNT 6
+#define HypABI__Creds__EventType__LABELS \
+        OP(EVENT_NONE) OP(CORRUPTION) OP(INVALID_ASSIGNMENT) OP(DOUBLE_ASSIGNMENT) OP(INVALID_COMMIT) OP(DOUBLE_COMMIT)
+
+// start of HypABI__Creds__TaskCred
+
+struct __attribute__((packed)) HypABI__Creds__TaskCred {
+        /** This is the guest virtual address of the `task_struct` associated with this cThis is the guest virtual address of the `task_struct` associated with this call. */
+        uint64_t addr;
+        /** This is the guest virtual address of the `cred` struct associated with task whose `task_struct` address is passed in the `addr` parameter. */
+        uint64_t cred;
+        /** This is the computed HMAC for the `task_struct` and `cred` structures passed in the previous two values. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Creds__TaskCred HypABI__Creds__TaskCred__T;
+#define HypABI__Creds__TaskCred__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__TaskCred) == HypABI__Creds__TaskCred__SZ, "Unexpected size for HypABI__Creds__TaskCred");
+
+#define HypABI__Creds__Configure__OP_ID 0
+
+
+// start of HypABI__Creds__Configure__arg
+
+struct __attribute__((packed)) HypABI__Creds__Configure__arg {
+        /** The SipHash key. */
+        uint64_t key[2];
+};
+typedef struct HypABI__Creds__Configure__arg HypABI__Creds__Configure__arg__T;
+#define HypABI__Creds__Configure__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Creds__Configure__arg) == HypABI__Creds__Configure__arg__SZ, "Unexpected size for HypABI__Creds__Configure__arg");
+
+#define HypABI__Creds__RegisterInitTask__OP_ID 1
+
+
+// start of HypABI__Creds__RegisterInitTask__arg
+
+struct __attribute__((packed)) HypABI__Creds__RegisterInitTask__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the first kernel thread (`swapper`). */
+        struct HypABI__Creds__TaskCred init_task;
+};
+typedef struct HypABI__Creds__RegisterInitTask__arg HypABI__Creds__RegisterInitTask__arg__T;
+#define HypABI__Creds__RegisterInitTask__arg__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__RegisterInitTask__arg) == HypABI__Creds__RegisterInitTask__arg__SZ, "Unexpected size for HypABI__Creds__RegisterInitTask__arg");
+
+#define HypABI__Creds__Assign__OP_ID 2
+
+
+// start of HypABI__Creds__Assign__arg
+
+struct __attribute__((packed)) HypABI__Creds__Assign__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the new task for which credentials are being allocated. */
+        struct HypABI__Creds__TaskCred new_task;
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the parent task of the task specified in the `new_task` parameter. The parent task is the task from which the privileges will be copied. */
+        struct HypABI__Creds__TaskCred parent;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Assign__arg HypABI__Creds__Assign__arg__T;
+#define HypABI__Creds__Assign__arg__SZ 49ULL
+_Static_assert(sizeof(struct HypABI__Creds__Assign__arg) == HypABI__Creds__Assign__arg__SZ, "Unexpected size for HypABI__Creds__Assign__arg");
+
+#define HypABI__Creds__AssignPriv__OP_ID 3
+
+
+// start of HypABI__Creds__AssignPriv__arg
+
+struct __attribute__((packed)) HypABI__Creds__AssignPriv__arg {
+        /** This is the guest virtual address of the `cred` struct that is being copied into. */
+        uint64_t cred;
+        /** This parameter expects the guest virtual address of the `task_struct` for the process from which credentials are being copied. If this parameter is `NULL`, it is assumed that the `swapper` process is the process from which credentials are being copied. */
+        uint64_t daemon;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__AssignPriv__arg HypABI__Creds__AssignPriv__arg__T;
+#define HypABI__Creds__AssignPriv__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Creds__AssignPriv__arg) == HypABI__Creds__AssignPriv__arg__SZ, "Unexpected size for HypABI__Creds__AssignPriv__arg");
+
+#define HypABI__Creds__Commit__OP_ID 4
+
+
+// start of HypABI__Creds__Commit__arg
+
+struct __attribute__((packed)) HypABI__Creds__Commit__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the current task that is committing its credentials. */
+        struct HypABI__Creds__TaskCred currnt;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Commit__arg HypABI__Creds__Commit__arg__T;
+#define HypABI__Creds__Commit__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Commit__arg) == HypABI__Creds__Commit__arg__SZ, "Unexpected size for HypABI__Creds__Commit__arg");
+
+#define HypABI__Creds__Release__OP_ID 5
+
+
+// start of HypABI__Creds__Release__arg
+
+struct __attribute__((packed)) HypABI__Creds__Release__arg {
+        /** This is the guest virtual address of the `cred` struct that is being released. */
+        uint64_t cred;
+};
+typedef struct HypABI__Creds__Release__arg HypABI__Creds__Release__arg__T;
+#define HypABI__Creds__Release__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Creds__Release__arg) == HypABI__Creds__Release__arg__SZ, "Unexpected size for HypABI__Creds__Release__arg");
+
+#define HypABI__Creds__Verification__OP_ID 6
+
+
+// start of HypABI__Creds__Verification__arg
+
+struct __attribute__((packed)) HypABI__Creds__Verification__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the task whose credentials must be verified. */
+        struct HypABI__Creds__TaskCred task;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Verification__arg HypABI__Creds__Verification__arg__T;
+#define HypABI__Creds__Verification__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Verification__arg) == HypABI__Creds__Verification__arg__SZ, "Unexpected size for HypABI__Creds__Verification__arg");
+
+#define HypABI__Creds__Log__OP_ID 7
+
+#define HypABI__Creds__Log__MAX_TASK_NAME_LEN 16ULL
+
+
+// start of HypABI__Creds__Log__arg
+
+struct __attribute__((packed)) HypABI__Creds__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter interpreted as a boolean. It is written by the **host** and indicates whether the guest to block the event or not. */
+        uint8_t block;
+        uint8_t padding[2];
+        /** This parameter contains the process ID of the process for which the event is being sent. */
+        uint32_t task_pid;
+        /** This parameter contains the guest virtual address of the `task_struct` of the process for which the event is being sent. */
+        uint64_t task_addr;
+        /** This parameter contains the guest virtual address of the `cred` struct of the process for which the event is being sent. */
+        uint64_t task_cred;
+        /** This parameter contains a NUL- terminated ASCII string that contains the name of the process for which the event is being sent. */
+        char task_name[HypABI__Creds__Log__MAX_TASK_NAME_LEN];
+};
+typedef struct HypABI__Creds__Log__arg HypABI__Creds__Log__arg__T;
+#define HypABI__Creds__Log__arg__SZ 1032ULL
+_Static_assert(sizeof(struct HypABI__Creds__Log__arg) == HypABI__Creds__Log__arg__SZ, "Unexpected size for HypABI__Creds__Log__arg");
+
+#define HypABI__FileProtection__BACKEND_ID 8
+
+#define HypABI__FileProtection__Init__OP_ID 0
+
+
+// start of HypABI__FileProtection__Init__HypABI__FileProtection__Init__Config
+
+typedef unsigned long HypABI__FileProtection__Init__Config__T;
+#define HypABI__FileProtection__Init__Config__NONE 0UL
+#define HypABI__FileProtection__Init__Config__READ_ONLY__BIT 0
+#define HypABI__FileProtection__Init__Config__READ_ONLY (1UL<<0)
+#define HypABI__FileProtection__Init__Config__FILE_OPS__BIT 1
+#define HypABI__FileProtection__Init__Config__FILE_OPS (1UL<<1)
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT 2
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED (1UL<<2)
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr);
+
+
+// start of HypABI__FileProtection__Init__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__Init__arg {
+        /** This bitmap is written by the **host** and is updated to indicate which file protection violations the host is expecting from the guest. */
+        uint64_t feature_bitmap;
+};
+typedef struct HypABI__FileProtection__Init__arg HypABI__FileProtection__Init__arg__T;
+#define HypABI__FileProtection__Init__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__Init__arg) == HypABI__FileProtection__Init__arg__SZ, "Unexpected size for HypABI__FileProtection__Init__arg");
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID 1
+
+
+// start of HypABI__FileProtection__ViolationWriteReadOnlyFile__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationWriteReadOnlyFile__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ 1003ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg) == HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationWriteReadOnlyFile__arg");
+
+#define HypABI__FileProtection__ViolationFileOps__OP_ID 2
+
+enum HypABI__FileProtection__ViolationFileOps__FopsType {
+        HypABI__FileProtection__ViolationFileOps__FopsType__EXT4 = 0,
+        HypABI__FileProtection__ViolationFileOps__FopsType__TMPFS = 1,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_MEM = 2,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_NULL = 3,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_PORT = 4,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_ZERO = 5,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_FULL = 6,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_RANDOM = 7,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_URANDOM = 8,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_KMSG = 9,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_TTY = 10,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_CONSOLE = 11,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PROC = 12,
+        HypABI__FileProtection__ViolationFileOps__FopsType__XFS = 13,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SOCKFS = 14,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PIPEFS = 15,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SYSFS = 16,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEBUGFS = 17,
+        HypABI__FileProtection__ViolationFileOps__FopsType__UNSUPPORTED = 255,
+};
+#define HypABI__FileProtection__ViolationFileOps__FopsType__COUNT 19
+#define HypABI__FileProtection__ViolationFileOps__FopsType__LABELS \
+        OP(EXT4) OP(TMPFS) OP(DEV_MEM) OP(DEV_NULL) OP(DEV_PORT) OP(DEV_ZERO) OP(DEV_FULL) OP(DEV_RANDOM) OP(DEV_URANDOM) OP(DEV_KMSG) OP(DEV_TTY) OP(DEV_CONSOLE) OP(PROC) OP(XFS) OP(SOCKFS) OP(PIPEFS) OP(SYSFS) OP(DEBUGFS) OP(UNSUPPORTED)
+#define HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ 1024ULL
+
+
+// start of HypABI__FileProtection__ViolationFileOps__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationFileOps__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a byte value set by the guest to indicate the file system type on which a violation occurred. */
+        uint8_t fops_type;
+        /** This parameter denotes whether the violating access was on a file (`false`) or directory (`true`). */
+        uint8_t is_dir;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+        uint8_t padding[5];
+        /** This parameter contains the address of the `file_ops` struct that was corrupted. */
+        uint64_t fops_ptr;
+        /** The path whose access generated the violation. This char array is at most 1024 bytes long. */
+        char path_name[HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ];
+};
+typedef struct HypABI__FileProtection__ViolationFileOps__arg HypABI__FileProtection__ViolationFileOps__arg__T;
+#define HypABI__FileProtection__ViolationFileOps__arg__SZ 2032ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationFileOps__arg) == HypABI__FileProtection__ViolationFileOps__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationFileOps__arg");
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID 3
+
+
+// start of HypABI__FileProtection__ViolationDirtyCredWrite__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationDirtyCredWrite__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationDirtyCredWrite__arg HypABI__FileProtection__ViolationDirtyCredWrite__arg__T;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ 1003ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationDirtyCredWrite__arg) == HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationDirtyCredWrite__arg");
+
+#define HypABI__RegisterProtection__BACKEND_ID 9
+
+#define HypABI__RegisterProtection__Freeze__OP_ID 0
+
+#ifdef CONFIG_X86_64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR3 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR4 = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__EFER = 8,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 4
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(CR0) OP(CR3) OP(CR4) OP(EFER)
+
+#elif defined CONFIG_ARM64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR1 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TCR = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__SCTLR = 8,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__VBAR = 16,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 5
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(TTBR0) OP(TTBR1) OP(TCR) OP(SCTLR) OP(VBAR)
+
+#else
+#error Unsupported architecture
+#endif
+
+// start of HypABI__RegisterProtection__Freeze__arg
+
+struct __attribute__((packed)) HypABI__RegisterProtection__Freeze__arg {
+        /** This parameter selects which register this freeze request applies to. */
+        uint64_t register_selector;
+        /** Any bit which is set in this bitfield will be frozen. As an example, a value of `ffffffffffffffff` will completely freeze a register. */
+        uint64_t freeze_bitfield;
+};
+typedef struct HypABI__RegisterProtection__Freeze__arg HypABI__RegisterProtection__Freeze__arg__T;
+#define HypABI__RegisterProtection__Freeze__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__RegisterProtection__Freeze__arg) == HypABI__RegisterProtection__Freeze__arg__SZ, "Unexpected size for HypABI__RegisterProtection__Freeze__arg");
+
+#define HypABI__Domain__BACKEND_ID 10
+
+
+// start of HypABI__Domain__Domain
+
+struct __attribute__((packed)) HypABI__Domain__Domain {
+        /** The ID of the domain that the operation refers to. */
+        uint64_t id;
+        /** The page global directory (process) that the operation refers to. */
+        uint64_t pgd;
+};
+typedef struct HypABI__Domain__Domain HypABI__Domain__Domain__T;
+#define HypABI__Domain__Domain__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Domain__Domain) == HypABI__Domain__Domain__SZ, "Unexpected size for HypABI__Domain__Domain");
+
+#define HypABI__Domain__Configure__OP_ID 0
+
+
+// start of HypABI__Domain__Configure__arg
+
+struct __attribute__((packed)) HypABI__Domain__Configure__arg {
+        uint8_t padding0[16];
+        /** Whether or not the guest is using page table isolation. */
+        uint8_t pti;
+        uint8_t padding1[7];
+        /** The GPA of the physical region that is used for the indirect communication channel. */
+        uint64_t batched_region;
+        /** Differentiates between the heavy- and light-weight strong isolation version. If `true`, the heavy-weight strong isolation version is in use. */
+        uint8_t isolate;
+        /** If `true`, forced memory access (e.g., through debuggers) is allowed. */
+        uint8_t allow_forced_mem_access;
+};
+typedef struct HypABI__Domain__Configure__arg HypABI__Domain__Configure__arg__T;
+#define HypABI__Domain__Configure__arg__SZ 34ULL
+_Static_assert(sizeof(struct HypABI__Domain__Configure__arg) == HypABI__Domain__Configure__arg__SZ, "Unexpected size for HypABI__Domain__Configure__arg");
+
+#define HypABI__Domain__Report__OP_ID 10
+
+
+// start of HypABI__Domain__Report__arg
+
+struct __attribute__((packed)) HypABI__Domain__Report__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__Report__arg HypABI__Domain__Report__arg__T;
+#define HypABI__Domain__Report__arg__SZ 1058ULL
+_Static_assert(sizeof(struct HypABI__Domain__Report__arg) == HypABI__Domain__Report__arg__SZ, "Unexpected size for HypABI__Domain__Report__arg");
+
+#define HypABI__Domain__ReportForcedMemAccess__OP_ID 11
+
+
+// start of HypABI__Domain__ReportForcedMemAccess__arg
+
+struct __attribute__((packed)) HypABI__Domain__ReportForcedMemAccess__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__ReportForcedMemAccess__arg HypABI__Domain__ReportForcedMemAccess__arg__T;
+#define HypABI__Domain__ReportForcedMemAccess__arg__SZ 1058ULL
+_Static_assert(sizeof(struct HypABI__Domain__ReportForcedMemAccess__arg) == HypABI__Domain__ReportForcedMemAccess__arg__SZ, "Unexpected size for HypABI__Domain__ReportForcedMemAccess__arg");
+
+#define HypABI__Inode__BACKEND_ID 11
+
+enum HypABI__Inode__EventType {
+        HypABI__Inode__EventType__EVENT_NONE = 0,
+        HypABI__Inode__EventType__NO_INODE = 1,
+        HypABI__Inode__EventType__CORRUPTION = 2,
+};
+#define HypABI__Inode__EventType__COUNT 3
+#define HypABI__Inode__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_INODE) OP(CORRUPTION)
+
+// start of HypABI__Inode__InodeInfo
+
+struct __attribute__((packed)) HypABI__Inode__InodeInfo {
+        /** This is the guest virtual address of the `struct inode` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct inode` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Inode__InodeInfo HypABI__Inode__InodeInfo__T;
+#define HypABI__Inode__InodeInfo__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__InodeInfo) == HypABI__Inode__InodeInfo__SZ, "Unexpected size for HypABI__Inode__InodeInfo");
+
+#define HypABI__Inode__Register__OP_ID 0
+
+
+// start of HypABI__Inode__Register__arg
+
+struct __attribute__((packed)) HypABI__Inode__Register__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Register__arg HypABI__Inode__Register__arg__T;
+#define HypABI__Inode__Register__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Register__arg) == HypABI__Inode__Register__arg__SZ, "Unexpected size for HypABI__Inode__Register__arg");
+
+#define HypABI__Inode__Update__OP_ID 1
+
+
+// start of HypABI__Inode__Update__arg
+
+struct __attribute__((packed)) HypABI__Inode__Update__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be updated by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Update__arg HypABI__Inode__Update__arg__T;
+#define HypABI__Inode__Update__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Update__arg) == HypABI__Inode__Update__arg__SZ, "Unexpected size for HypABI__Inode__Update__arg");
+
+#define HypABI__Inode__Release__OP_ID 2
+
+
+// start of HypABI__Inode__Release__arg
+
+struct __attribute__((packed)) HypABI__Inode__Release__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` that is to be released by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+};
+typedef struct HypABI__Inode__Release__arg HypABI__Inode__Release__arg__T;
+#define HypABI__Inode__Release__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Release__arg) == HypABI__Inode__Release__arg__SZ, "Unexpected size for HypABI__Inode__Release__arg");
+
+#define HypABI__Inode__Verify__OP_ID 3
+
+
+// start of HypABI__Inode__Verify__arg
+
+struct __attribute__((packed)) HypABI__Inode__Verify__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be verified by BRASS. */
+        struct HypABI__Inode__InodeInfo inode;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Inode__Verify__arg HypABI__Inode__Verify__arg__T;
+#define HypABI__Inode__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Inode__Verify__arg) == HypABI__Inode__Verify__arg__SZ, "Unexpected size for HypABI__Inode__Verify__arg");
+
+#define HypABI__Inode__Log__OP_ID 4
+
+
+// start of HypABI__Inode__Log__arg
+
+struct __attribute__((packed)) HypABI__Inode__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the inode in question. */
+        uint64_t inode_addr;
+        /** This parameter contains the uid field of the inode in question. */
+        uint32_t inode_uid;
+        /** This parameter contains the gid field of the inode in question. */
+        uint32_t inode_gid;
+        /** This parameter contains the mode field of the inode in question. */
+        uint16_t inode_mode;
+        uint8_t padding[4];
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /**  */
+        uint8_t block;
+        /** This parameter contains the file path to the binary that is represented by the inode in question. */
+        char file_path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Inode__Log__arg HypABI__Inode__Log__arg__T;
+#define HypABI__Inode__Log__arg__SZ 1272ULL
+_Static_assert(sizeof(struct HypABI__Inode__Log__arg) == HypABI__Inode__Log__arg__SZ, "Unexpected size for HypABI__Inode__Log__arg");
+
+#define HypABI__Keyring__BACKEND_ID 12
+
+enum HypABI__Keyring__EventType {
+        HypABI__Keyring__EventType__EVENT_NONE = 0,
+        HypABI__Keyring__EventType__NO_KEYRING = 1,
+        HypABI__Keyring__EventType__CORRUPTION = 2,
+};
+#define HypABI__Keyring__EventType__COUNT 3
+#define HypABI__Keyring__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_KEYRING) OP(CORRUPTION)
+
+// start of HypABI__Keyring__Keyring
+
+struct __attribute__((packed)) HypABI__Keyring__Keyring {
+        /** This is the guest virtual address of the `struct key` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct key` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Keyring__Keyring HypABI__Keyring__Keyring__T;
+#define HypABI__Keyring__Keyring__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Keyring) == HypABI__Keyring__Keyring__SZ, "Unexpected size for HypABI__Keyring__Keyring");
+
+#define HypABI__Keyring__Register__OP_ID 0
+
+
+// start of HypABI__Keyring__Register__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Register__arg {
+        /** This is the keyring structure containing the GVA of the `struct key` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        uint8_t block;
+};
+typedef struct HypABI__Keyring__Register__arg HypABI__Keyring__Register__arg__T;
+#define HypABI__Keyring__Register__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Register__arg) == HypABI__Keyring__Register__arg__SZ, "Unexpected size for HypABI__Keyring__Register__arg");
+
+#define HypABI__Keyring__Verify__OP_ID 2
+
+
+// start of HypABI__Keyring__Verify__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Verify__arg {
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Keyring__Verify__arg HypABI__Keyring__Verify__arg__T;
+#define HypABI__Keyring__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Verify__arg) == HypABI__Keyring__Verify__arg__SZ, "Unexpected size for HypABI__Keyring__Verify__arg");
+
+#define HypABI__Keyring__Log__OP_ID 3
+
+
+// start of HypABI__Keyring__Log__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the keyring in question. */
+        uint64_t keyring_addr;
+        /** This parameter contains the uid field of the keyring in question. */
+        uint32_t keyring_uid;
+        /** This parameter contains the gid field of the keyring in question. */
+        uint32_t keyring_gid;
+        /** This parameter contains the permissions of the keyring in question. */
+        uint32_t keyring_perm;
+        /** This parameter contains the ID of the keyring in question. */
+        uint32_t keyring_serial;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter tells the guest whether to block the event. */
+        uint8_t block;
+        /** This parameter contains the keyrings name/description. */
+        char keyring_desc[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Keyring__Log__arg HypABI__Keyring__Log__arg__T;
+#define HypABI__Keyring__Log__arg__SZ 1274ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Log__arg) == HypABI__Keyring__Log__arg__SZ, "Unexpected size for HypABI__Keyring__Log__arg");
+
+#define HypABI__Confserver__BACKEND_ID 13
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID 0
+
+
+// start of HypABI__Confserver__FreezeMemoryAfterBoot__arg
+
+struct __attribute__((packed)) HypABI__Confserver__FreezeMemoryAfterBoot__arg {
+        /** This field will be set to `true` if the host wants the guest to freeze its memory protections after boot. */
+        uint8_t freeze_memory_after_boot;
+};
+typedef struct HypABI__Confserver__FreezeMemoryAfterBoot__arg HypABI__Confserver__FreezeMemoryAfterBoot__arg__T;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__FreezeMemoryAfterBoot__arg) == HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ, "Unexpected size for HypABI__Confserver__FreezeMemoryAfterBoot__arg");
+
+#define HypABI__Confserver__StrictFileops__OP_ID 1
+
+
+// start of HypABI__Confserver__StrictFileops__arg
+
+struct __attribute__((packed)) HypABI__Confserver__StrictFileops__arg {
+        /** This field will be set to `true` if the host wants the guest to enforce strict file operation checks. */
+        uint8_t strict_fileops;
+};
+typedef struct HypABI__Confserver__StrictFileops__arg HypABI__Confserver__StrictFileops__arg__T;
+#define HypABI__Confserver__StrictFileops__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__StrictFileops__arg) == HypABI__Confserver__StrictFileops__arg__SZ, "Unexpected size for HypABI__Confserver__StrictFileops__arg");
+
+#define HypABI__Confserver__KernelConfig__OP_ID 2
+
+
+// start of HypABI__Confserver__KernelConfig__arg
+
+struct __attribute__((packed)) HypABI__Confserver__KernelConfig__arg {
+        /** This field will be set to `true` if the host wants the guest to enforce that all userspace stacks are initialized to be non-executable. */
+        uint8_t userspace_force_nx_stack;
+};
+typedef struct HypABI__Confserver__KernelConfig__arg HypABI__Confserver__KernelConfig__arg__T;
+#define HypABI__Confserver__KernelConfig__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__KernelConfig__arg) == HypABI__Confserver__KernelConfig__arg__SZ, "Unexpected size for HypABI__Confserver__KernelConfig__arg");
+
+#define HypABI__GuestPolicy__BACKEND_ID 14
+
+#define HypABI__GuestPolicy__GetPolicy__OP_ID 0
+
+
+// start of HypABI__GuestPolicy__GetPolicy__arg
+
+struct __attribute__((packed)) HypABI__GuestPolicy__GetPolicy__arg {
+        /** The guest physical address that the policy should be written to. The area needs to be consecutive in physical memory. */
+        uint64_t dest;
+        /** The guest is expected to provide the size of `dest` buffer. The host updates this value to the size of the written data. If the `dest buffer` should be too small, the host will set the field to the required size and will set `valid` to false. */
+        uint32_t dest_sz;
+        /** - */
+        uint8_t padding;
+        /** This parameter is written by the **host** and determines whether the policy could be written. */
+        uint8_t valid;
+};
+typedef struct HypABI__GuestPolicy__GetPolicy__arg HypABI__GuestPolicy__GetPolicy__arg__T;
+#define HypABI__GuestPolicy__GetPolicy__arg__SZ 14ULL
+_Static_assert(sizeof(struct HypABI__GuestPolicy__GetPolicy__arg) == HypABI__GuestPolicy__GetPolicy__arg__SZ, "Unexpected size for HypABI__GuestPolicy__GetPolicy__arg");
+
+#define HypABI__Wagner__BACKEND_ID 15
+
+
+// start of HypABI__Wagner__MemRegion
+
+struct __attribute__((packed)) HypABI__Wagner__MemRegion {
+        /** This is the guest-physical start address of the memory region. */
+        uint64_t gpa;
+        /** This is the size of the memory region (in Bytes). */
+        uint64_t size;
+};
+typedef struct HypABI__Wagner__MemRegion HypABI__Wagner__MemRegion__T;
+#define HypABI__Wagner__MemRegion__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemRegion) == HypABI__Wagner__MemRegion__SZ, "Unexpected size for HypABI__Wagner__MemRegion");
+
+
+// start of HypABI__Wagner__MemSegment
+
+struct __attribute__((packed)) HypABI__Wagner__MemSegment {
+        /** - */
+        uint64_t start;
+        /** - */
+        uint64_t end;
+};
+typedef struct HypABI__Wagner__MemSegment HypABI__Wagner__MemSegment__T;
+#define HypABI__Wagner__MemSegment__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemSegment) == HypABI__Wagner__MemSegment__SZ, "Unexpected size for HypABI__Wagner__MemSegment");
+
+
+// start of HypABI__Wagner__TransitPoint
+
+struct __attribute__((packed)) HypABI__Wagner__TransitPoint {
+        /** This is the guest-physical address of a transit point, that can be an entry point or a return point, to the vault. An entry point is a guest-physical address, the execution of which allows to enter the vault; a return point is a guest-physical address, to which the vault can safely return, e.g., after a function call. */
+        uint64_t gpa;
+};
+typedef struct HypABI__Wagner__TransitPoint HypABI__Wagner__TransitPoint__T;
+#define HypABI__Wagner__TransitPoint__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Wagner__TransitPoint) == HypABI__Wagner__TransitPoint__SZ, "Unexpected size for HypABI__Wagner__TransitPoint");
+
+#define HypABI__Wagner__Create__OP_ID 0
+
+
+// start of HypABI__Wagner__Create__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Create__arg {
+        /** The vault code region. */
+        struct HypABI__Wagner__MemRegion code;
+        /** The vault ref code region. */
+        struct HypABI__Wagner__MemRegion ref_code;
+        /** The shared code region. */
+        struct HypABI__Wagner__MemRegion shared_code;
+        /** The vault alternative instruction region. */
+        struct HypABI__Wagner__MemRegion altinstr_aux;
+        /** The vault noinstr region. */
+        struct HypABI__Wagner__MemRegion noinstr_text;
+        /** The vault thunk region. */
+        struct HypABI__Wagner__MemRegion thunks;
+        /** The vault data region. */
+        struct HypABI__Wagner__MemRegion data;
+        /** The vault read-only data region. */
+        struct HypABI__Wagner__MemRegion ro_data;
+        /** The vault entry text region. */
+        struct HypABI__Wagner__MemSegment entry_text;
+        /** This is the total number of entry points into the vault. */
+        uint32_t nr_entry_points;
+        /** This is the total number of return points into the vault. */
+        uint32_t nr_return_points;
+        /** This buffer contains `nr_entry_points` + `nr_return_points` entries with transit points. */
+        uint64_t transit_points;
+};
+typedef struct HypABI__Wagner__Create__arg HypABI__Wagner__Create__arg__T;
+#define HypABI__Wagner__Create__arg__SZ 160ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Create__arg) == HypABI__Wagner__Create__arg__SZ, "Unexpected size for HypABI__Wagner__Create__arg");
+
+#define HypABI__Wagner__Extend__OP_ID 1
+
+
+// start of HypABI__Wagner__Extend__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Extend__arg {
+        /** This entry contains the memory holding data that should be added to the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Extend__arg HypABI__Wagner__Extend__arg__T;
+#define HypABI__Wagner__Extend__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Extend__arg) == HypABI__Wagner__Extend__arg__SZ, "Unexpected size for HypABI__Wagner__Extend__arg");
+
+#define HypABI__Wagner__Delete__OP_ID 2
+
+
+// start of HypABI__Wagner__Delete__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Delete__arg {
+        /** This entry contains the memory holding data that should be removed from the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Delete__arg HypABI__Wagner__Delete__arg__T;
+#define HypABI__Wagner__Delete__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Delete__arg) == HypABI__Wagner__Delete__arg__SZ, "Unexpected size for HypABI__Wagner__Delete__arg");
+
+
+
+// GuestConnABI
+
+#define GuestConnABI__MAX_MSG_SZ 4096ULL
+
+
+// start of GuestConnABI__Header
+
+struct __attribute__((packed)) GuestConnABI__Header {
+        uint16_t backend;
+        uint16_t sz;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__Header GuestConnABI__Header__T;
+#define GuestConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__Header) == GuestConnABI__Header__SZ, "Unexpected size for GuestConnABI__Header");
+
+#define GuestConnABI__GuestLog__BACKEND_ID 0
+
+#define GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ 32ULL
+
+#define GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ 1024ULL
+
+#define GuestConnABI__GuestLog__PROC_COMM_SZ 16ULL
+
+#define GuestConnABI__GuestLog__PROC_CGROUP_NAME_SZ 128ULL
+
+
+// start of GuestConnABI__GuestLog__Header
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Header {
+        uint16_t evt;
+        uint16_t sz;
+};
+typedef struct GuestConnABI__GuestLog__Header GuestConnABI__GuestLog__Header__T;
+#define GuestConnABI__GuestLog__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Header) == GuestConnABI__GuestLog__Header__SZ, "Unexpected size for GuestConnABI__GuestLog__Header");
+
+
+// start of GuestConnABI__GuestLog__Message
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Message {
+        struct GuestConnABI__GuestLog__Header header;
+        struct HypABI__Context context;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__GuestLog__Message GuestConnABI__GuestLog__Message__T;
+#define GuestConnABI__GuestLog__Message__SZ 996ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Message) == GuestConnABI__GuestLog__Message__SZ, "Unexpected size for GuestConnABI__GuestLog__Message");
+
+#define GuestConnABI__GuestLog__StringMsg__EVT_ID 0
+
+
+// start of GuestConnABI__GuestLog__StringMsg
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__StringMsg {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated string that will be printed to the host logs. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__StringMsg GuestConnABI__GuestLog__StringMsg__T;
+#define GuestConnABI__GuestLog__StringMsg__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__StringMsg) == GuestConnABI__GuestLog__StringMsg__SZ, "Unexpected size for GuestConnABI__GuestLog__StringMsg");
+
+#define GuestConnABI__GuestLog__ProcessFork__EVT_ID 1
+
+
+// start of GuestConnABI__GuestLog__ProcessFork
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessFork {
+        /** The process ID of the newly forked process. */
+        uint32_t child_pid;
+        /** The process ID of the parent that forked the process. */
+        uint32_t parent_pid;
+        /** The offset from the start of the `buf` field where the command string for the child process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t child_comm_offset;
+        /** The offset from the start of the `buf` field where the command string for the parent process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t parent_comm_offset;
+        /** A buffer to store `child_comm_offset` and `parent_comm_offset`. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessFork GuestConnABI__GuestLog__ProcessFork__T;
+#define GuestConnABI__GuestLog__ProcessFork__SZ 16ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessFork) == GuestConnABI__GuestLog__ProcessFork__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessFork");
+
+#define GuestConnABI__GuestLog__ProcessExec__EVT_ID 2
+
+
+// start of GuestConnABI__GuestLog__ProcessExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExec {
+        /** The process ID of the process that executed the binary. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** A NUL-terminated ASCII string that contains the path of the binary to be executed. */
+        char name[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExec GuestConnABI__GuestLog__ProcessExec__T;
+#define GuestConnABI__GuestLog__ProcessExec__SZ 2088ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExec) == GuestConnABI__GuestLog__ProcessExec__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExec");
+
+#define GuestConnABI__GuestLog__ProcessExit__EVT_ID 3
+
+
+// start of GuestConnABI__GuestLog__ProcessExit
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExit {
+        /** The process ID of the process that terminated. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** The exit code of the process. */
+        uint32_t exit_code;
+        /** A NUL-terminated ASCII string that contains the path of the binary that was terminated. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExit GuestConnABI__GuestLog__ProcessExit__T;
+#define GuestConnABI__GuestLog__ProcessExit__SZ 12ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExit) == GuestConnABI__GuestLog__ProcessExit__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExit");
+
+#define GuestConnABI__GuestLog__DriverLoad__EVT_ID 4
+
+
+// start of GuestConnABI__GuestLog__DriverLoad
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriverLoad {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated ASCII string that contains the path of the binary that was loaded as a driver. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__DriverLoad GuestConnABI__GuestLog__DriverLoad__T;
+#define GuestConnABI__GuestLog__DriverLoad__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriverLoad) == GuestConnABI__GuestLog__DriverLoad__SZ, "Unexpected size for GuestConnABI__GuestLog__DriverLoad");
+
+#define GuestConnABI__GuestLog__KernelAccess__EVT_ID 5
+
+enum GuestConnABI__GuestLog__KernelAccess__AccessType {
+        GuestConnABI__GuestLog__KernelAccess__AccessType__READ = 0,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE = 1,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE = 2,
+};
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__COUNT 3
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__LABELS \
+        OP(READ) OP(WRITE) OP(EXECUTE)
+
+// start of GuestConnABI__GuestLog__KernelAccess
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelAccess {
+        uint64_t address;
+        uint8_t type;
+};
+typedef struct GuestConnABI__GuestLog__KernelAccess GuestConnABI__GuestLog__KernelAccess__T;
+#define GuestConnABI__GuestLog__KernelAccess__SZ 9ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelAccess) == GuestConnABI__GuestLog__KernelAccess__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelAccess");
+
+#define GuestConnABI__GuestLog__FopsUnknown__EVT_ID 6
+
+enum GuestConnABI__GuestLog__FopsUnknown__FileStructType {
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__UNKNOWN = 0,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE = 1,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY = 2,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL = 3,
+};
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__COUNT 4
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__LABELS \
+        OP(UNKNOWN) OP(FILE) OP(DIRECTORY) OP(SPECIAL)
+
+// start of GuestConnABI__GuestLog__FopsUnknown
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__FopsUnknown {
+        /** The file-system magic number as defined in [Linux](https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/magic.h). */
+        uint64_t magic;
+        /** The type of the file struct using the unknown file operation instance. */
+        uint8_t struct_type;
+        uint8_t padding[3];
+        /** Major device number for the unknown file operation instance (only relevant for special files). */
+        uint32_t special_major;
+        /** Minor device number for the unknown file operation instance (only relevant for special files). */
+        uint64_t special_minor;
+        /** This is the address of the unknown operations instance used. */
+        uint64_t address;
+        /** The name of the accessed file or directory which used the unknown file operations instance. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__FopsUnknown GuestConnABI__GuestLog__FopsUnknown__T;
+#define GuestConnABI__GuestLog__FopsUnknown__SZ 32ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__FopsUnknown) == GuestConnABI__GuestLog__FopsUnknown__SZ, "Unexpected size for GuestConnABI__GuestLog__FopsUnknown");
+
+#define GuestConnABI__GuestLog__KernelExec__EVT_ID 7
+
+
+// start of GuestConnABI__GuestLog__KernelExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelExec {
+        /** The path to the program that is being executed. */
+        char path[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        /** The arguments passed to the program that is being executed. */
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        /** The environment passed to the program that is being executed. */
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__KernelExec GuestConnABI__GuestLog__KernelExec__T;
+#define GuestConnABI__GuestLog__KernelExec__SZ 2080ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelExec) == GuestConnABI__GuestLog__KernelExec__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelExec");
+
+#define GuestConnABI__GuestLog__CgroupCreate__EVT_ID 8
+
+
+// start of GuestConnABI__GuestLog__CgroupCreate
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupCreate {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupCreate GuestConnABI__GuestLog__CgroupCreate__T;
+#define GuestConnABI__GuestLog__CgroupCreate__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupCreate) == GuestConnABI__GuestLog__CgroupCreate__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupCreate");
+
+#define GuestConnABI__GuestLog__CgroupDestroy__EVT_ID 9
+
+
+// start of GuestConnABI__GuestLog__CgroupDestroy
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupDestroy {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupDestroy GuestConnABI__GuestLog__CgroupDestroy__T;
+#define GuestConnABI__GuestLog__CgroupDestroy__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupDestroy) == GuestConnABI__GuestLog__CgroupDestroy__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupDestroy");
+
+#define GuestConnABI__GuestLog__NamespaceChange__EVT_ID 10
+
+
+// start of GuestConnABI__GuestLog__NamespaceChange
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__NamespaceChange {
+        /** The process ID of the process that updated. */
+        uint32_t target_task_pid;
+        uint8_t padding[4];
+        /** The process name of the process that updated. */
+        char target_task_name[GuestConnABI__GuestLog__PROC_COMM_SZ];
+        /** The incoming namespace identifiers for the target process. */
+        struct HypABI__Context__Inums incoming_inums;
+};
+typedef struct GuestConnABI__GuestLog__NamespaceChange GuestConnABI__GuestLog__NamespaceChange__T;
+#define GuestConnABI__GuestLog__NamespaceChange__SZ 64ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__NamespaceChange) == GuestConnABI__GuestLog__NamespaceChange__SZ, "Unexpected size for GuestConnABI__GuestLog__NamespaceChange");
+
+#define GuestConnABI__GuestLog__DriftDetectionNewFileExecution__EVT_ID 11
+
+
+// start of GuestConnABI__GuestLog__DriftDetectionNewFileExecution
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriftDetectionNewFileExecution {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        /** The path of the new file being executed. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct GuestConnABI__GuestLog__DriftDetectionNewFileExecution GuestConnABI__GuestLog__DriftDetectionNewFileExecution__T;
+#define GuestConnABI__GuestLog__DriftDetectionNewFileExecution__SZ 257ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriftDetectionNewFileExecution) == GuestConnABI__GuestLog__DriftDetectionNewFileExecution__SZ, "Unexpected size for GuestConnABI__GuestLog__DriftDetectionNewFileExecution");
+
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__EVT_ID 12
+
+
+// start of GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The path of the new file being executed. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__T;
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__SZ 513ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile) == GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__SZ, "Unexpected size for GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile");
+
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__EVT_ID 13
+
+
+// start of GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The command being executed. */
+        char command[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__T;
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__SZ 513ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd) == GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__SZ, "Unexpected size for GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd");
+
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__EVT_ID 14
+
+
+// start of GuestConnABI__GuestLog__DriftDetectionInterpreterPiped
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriftDetectionInterpreterPiped {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct GuestConnABI__GuestLog__DriftDetectionInterpreterPiped GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__T;
+#define GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__SZ 257ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriftDetectionInterpreterPiped) == GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__SZ, "Unexpected size for GuestConnABI__GuestLog__DriftDetectionInterpreterPiped");
+
+#define GuestConnABI__GuestLog__Capable__EVT_ID 15
+
+#define GuestConnABI__GuestLog__Capable__MAX_CAP_STR_SZ 32ULL
+
+
+// start of GuestConnABI__GuestLog__Capable
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Capable {
+        /** The Linux capability number of the capability being requested by the current task. */
+        uint64_t cap;
+        /** The Linux capability name of the capability being requested by the current task. */
+        char cap_str[GuestConnABI__GuestLog__Capable__MAX_CAP_STR_SZ];
+};
+typedef struct GuestConnABI__GuestLog__Capable GuestConnABI__GuestLog__Capable__T;
+#define GuestConnABI__GuestLog__Capable__SZ 40ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Capable) == GuestConnABI__GuestLog__Capable__SZ, "Unexpected size for GuestConnABI__GuestLog__Capable");
+
+#define GuestConnABI__GuestLog__ElfExecStack__EVT_ID 16
+
+
+// start of GuestConnABI__GuestLog__ElfExecStack
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ElfExecStack {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated ASCII string that contains the path of the elf binary with executable stack that was loaded. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ElfExecStack GuestConnABI__GuestLog__ElfExecStack__T;
+#define GuestConnABI__GuestLog__ElfExecStack__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ElfExecStack) == GuestConnABI__GuestLog__ElfExecStack__SZ, "Unexpected size for GuestConnABI__GuestLog__ElfExecStack");
+
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__EVT_ID 17
+
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__MAX_ADDR_STR_SZ 64ULL
+
+
+// start of GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        uint8_t padding[3];
+        /** Which file descriptor was used for the socket connection. */
+        uint32_t fd;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The destination address of the socket. */
+        char destination_addr[GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__MAX_ADDR_STR_SZ];
+        /** The source address of the socket. */
+        char source_addr[GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__MAX_ADDR_STR_SZ];
+};
+typedef struct GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__T;
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__SZ 392ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound) == GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__SZ, "Unexpected size for GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound");
+
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__EVT_ID 18
+
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ 64ULL
+
+
+// start of GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive {
+        /** Whether the event has been blocked. */
+        uint8_t blocked;
+        uint8_t padding[3];
+        /** Which file descriptor was used for the pipe. */
+        uint32_t interpreter_fd;
+        /** The pid of the interpreter process. */
+        uint32_t interpreter_pid;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The path of the process with the piping into interpreter and having a socket connection. */
+        char transitive_path[HypABI__Context__MAX_PATH_SZ];
+        /** The pid of the transitive process. */
+        uint32_t transitive_pid;
+        /** The pipe file desciptor that led to the transitive process. */
+        uint32_t transitive_pipe_fd;
+        /** The socket file descriptor in the transitive process. */
+        uint32_t transitive_socket_fd;
+        /** The destination address of the socket. */
+        char destination_addr[GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ];
+        /** The source address of the socket. */
+        char source_addr[GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ];
+};
+typedef struct GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__T;
+#define GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__SZ 664ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive) == GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__SZ, "Unexpected size for GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive");
+
+#define GuestConnABI__GuestLog__SocketConnection__EVT_ID 19
+
+#define GuestConnABI__GuestLog__SocketConnection__MAX_ADDR_STR_SZ 108ULL
+
+
+// start of GuestConnABI__GuestLog__SocketConnection
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__SocketConnection {
+        /** Whether this event is about a socket bind (true) or a remote connection (false). */
+        uint8_t local;
+        /** Whether this is a domain or network socket. */
+        uint8_t domain;
+        /** Whether this is a protected domain socket. */
+        uint8_t protected_sock;
+        /** The address the socket was bound to/connected to. */
+        char address[GuestConnABI__GuestLog__SocketConnection__MAX_ADDR_STR_SZ];
+};
+typedef struct GuestConnABI__GuestLog__SocketConnection GuestConnABI__GuestLog__SocketConnection__T;
+#define GuestConnABI__GuestLog__SocketConnection__SZ 111ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__SocketConnection) == GuestConnABI__GuestLog__SocketConnection__SZ, "Unexpected size for GuestConnABI__GuestLog__SocketConnection");
+
+#define GuestConnABI__GuestLog__SocketAccept__EVT_ID 20
+
+#define GuestConnABI__GuestLog__SocketAccept__MAX_ADDR_STR_SZ 64ULL
+
+
+// start of GuestConnABI__GuestLog__SocketAccept
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__SocketAccept {
+        /** The local address of the connection. */
+        char src_address[GuestConnABI__GuestLog__SocketAccept__MAX_ADDR_STR_SZ];
+        /** The remote address of the connection. */
+        char dest_address[GuestConnABI__GuestLog__SocketAccept__MAX_ADDR_STR_SZ];
+};
+typedef struct GuestConnABI__GuestLog__SocketAccept GuestConnABI__GuestLog__SocketAccept__T;
+#define GuestConnABI__GuestLog__SocketAccept__SZ 128ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__SocketAccept) == GuestConnABI__GuestLog__SocketAccept__SZ, "Unexpected size for GuestConnABI__GuestLog__SocketAccept");
+
+#define GuestConnABI__GuestLog__FileOpen__EVT_ID 21
+
+
+// start of GuestConnABI__GuestLog__FileOpen
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__FileOpen {
+        /** The path to the file being opened. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+        /** Whether the file opened is a link. */
+        uint8_t is_link;
+        /** Whether the file opened is a directory. */
+        uint8_t is_dir;
+        /** Whether the file opened is a pipe. */
+        uint8_t is_fifo;
+        /** Whether the file opened is a socket. */
+        uint8_t is_socket;
+        /** Whether the file is opened in a write-able manner. */
+        uint8_t mode_write;
+};
+typedef struct GuestConnABI__GuestLog__FileOpen GuestConnABI__GuestLog__FileOpen__T;
+#define GuestConnABI__GuestLog__FileOpen__SZ 261ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__FileOpen) == GuestConnABI__GuestLog__FileOpen__SZ, "Unexpected size for GuestConnABI__GuestLog__FileOpen");
+
+#define SendConnABI__CMD_BACKEND 0ULL
+
+#define SendConnABI__NUM_BACKENDS 1ULL
+
+#define SendConnABI__CMD_KILL_PROC 0ULL
+
+#define SendConnABI__CMD_KILL_CONTAINER 1ULL
+
+#define SendConnABI__NUM_CMDS 2ULL
+
+
+// start of SendConnABI__Header
+
+struct __attribute__((packed)) SendConnABI__Header {
+        /** The id of the backend that this message is destined for. */
+        uint16_t backend;
+        /** The size of the message in bytes including the header. */
+        uint16_t sz;
+};
+typedef struct SendConnABI__Header SendConnABI__Header__T;
+#define SendConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct SendConnABI__Header) == SendConnABI__Header__SZ, "Unexpected size for SendConnABI__Header");
+
+
+// start of SendConnABI__CmdKillProc
+
+struct __attribute__((packed)) SendConnABI__CmdKillProc {
+        /** The id of the kill proc cmd. */
+        uint64_t cmd_id;
+        /** The process id of the process to kill */
+        uint64_t pid;
+};
+typedef struct SendConnABI__CmdKillProc SendConnABI__CmdKillProc__T;
+#define SendConnABI__CmdKillProc__SZ 16ULL
+_Static_assert(sizeof(struct SendConnABI__CmdKillProc) == SendConnABI__CmdKillProc__SZ, "Unexpected size for SendConnABI__CmdKillProc");
+
+
+// start of SendConnABI__CmdKillContainer
+
+struct __attribute__((packed)) SendConnABI__CmdKillContainer {
+        /** The id of the kill proc cmd. */
+        uint64_t cmd_id;
+        /** The ID of the container to kill. */
+        char container_id[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct SendConnABI__CmdKillContainer SendConnABI__CmdKillContainer__T;
+#define SendConnABI__CmdKillContainer__SZ 264ULL
+_Static_assert(sizeof(struct SendConnABI__CmdKillContainer) == SendConnABI__CmdKillContainer__SZ, "Unexpected size for SendConnABI__CmdKillContainer");
+
diff --git include/bhv/interface/abi_hl_autogen.h include/bhv/interface/abi_hl_autogen.h
new file mode 100644
index 000000000..b8b6402f3
--- /dev/null
+++ include/bhv/interface/abi_hl_autogen.h
@@ -0,0 +1,1602 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-01-20T09:53:35).
+ */
+
+#pragma once
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+void HypABI__init_slabs(void);
+
+extern struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab;
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__Mem_Region__T *)kmem_cache_alloc(HypABI__Integrity__Create__Mem_Region__slab, gfp)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__Mem_Region__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__Mem_Region__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = &static; \
+        if (HypABI__Integrity__Create__Mem_Region__slab) \
+                __arg = HypABI__Integrity__Create__Mem_Region__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__Mem_Region__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Integrity__Create__arg__slab;
+#define HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__arg__T *)kmem_cache_alloc(HypABI__Integrity__Create__arg__slab, gfp)
+#define HypABI__Integrity__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__ALLOC() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Create__arg__slab) \
+                __arg = HypABI__Integrity__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall(const HypABI__Integrity__Create__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Create__arg__T *bhv_arg = HypABI__Integrity__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Create__arg__T));
+        rc = HypABI__Integrity__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Create__HYPERCALL(...) HypABI__Integrity__Create__hypercall((HypABI__Integrity__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Update__arg__slab;
+#define HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Update__arg__T *)kmem_cache_alloc(HypABI__Integrity__Update__arg__slab, gfp)
+#define HypABI__Integrity__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__ALLOC() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Update__arg__slab) \
+                __arg = HypABI__Integrity__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall(const HypABI__Integrity__Update__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Update__arg__T *bhv_arg = HypABI__Integrity__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Update__arg__T));
+        rc = HypABI__Integrity__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Update__HYPERCALL(...) HypABI__Integrity__Update__hypercall((HypABI__Integrity__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Remove__arg__slab;
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Remove__arg__T *)kmem_cache_alloc(HypABI__Integrity__Remove__arg__slab, gfp)
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Remove__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__ALLOC() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Remove__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Remove__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Remove__arg__slab) \
+                __arg = HypABI__Integrity__Remove__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Remove__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall(const HypABI__Integrity__Remove__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Remove__arg__T *bhv_arg = HypABI__Integrity__Remove__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Remove__arg__T));
+        rc = HypABI__Integrity__Remove__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Remove__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Remove__HYPERCALL(...) HypABI__Integrity__Remove__hypercall((HypABI__Integrity__Remove__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Freeze__arg__slab;
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Freeze__arg__T *)kmem_cache_alloc(HypABI__Integrity__Freeze__arg__slab, gfp)
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__ALLOC() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Freeze__arg__slab) \
+                __arg = HypABI__Integrity__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall(const HypABI__Integrity__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Freeze__arg__T *bhv_arg = HypABI__Integrity__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Freeze__arg__T));
+        rc = HypABI__Integrity__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Freeze__HYPERCALL(...) HypABI__Integrity__Freeze__hypercall((HypABI__Integrity__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab;
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__PtpgInit__arg__T *)kmem_cache_alloc(HypABI__Integrity__PtpgInit__arg__slab, gfp)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__PtpgInit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__PtpgInit__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = &static; \
+        if (HypABI__Integrity__PtpgInit__arg__slab) \
+                __arg = HypABI__Integrity__PtpgInit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__PtpgInit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall(const HypABI__Integrity__PtpgInit__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__PtpgInit__arg__T *bhv_arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__PtpgInit__arg__T));
+        rc = HypABI__Integrity__PtpgInit__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__PtpgInit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__PtpgInit__HYPERCALL(...) HypABI__Integrity__PtpgInit__hypercall((HypABI__Integrity__PtpgInit__arg__T){__VA_ARGS__})
+
+#define HypABI__Integrity__PtpgReport__hypercall() \
+        HypABI__Integrity__PtpgReport__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Patch__Patch__arg__slab;
+#define HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__Patch__arg__T *)kmem_cache_alloc(HypABI__Patch__Patch__arg__slab, gfp)
+#define HypABI__Patch__Patch__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__Patch__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__ALLOC() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__Patch__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__Patch__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = &static; \
+        if (HypABI__Patch__Patch__arg__slab) \
+                __arg = HypABI__Patch__Patch__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__Patch__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall(const HypABI__Patch__Patch__arg__T s)
+{
+        int rc;
+        HypABI__Patch__Patch__arg__T *bhv_arg = HypABI__Patch__Patch__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__Patch__arg__T));
+        rc = HypABI__Patch__Patch__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__Patch__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__Patch__HYPERCALL(...) HypABI__Patch__Patch__hypercall((HypABI__Patch__Patch__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab;
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchNoClose__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchNoClose__arg__slab, gfp)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchNoClose__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchNoClose__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchNoClose__arg__slab) \
+                __arg = HypABI__Patch__PatchNoClose__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchNoClose__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall(const HypABI__Patch__PatchNoClose__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchNoClose__arg__T *bhv_arg = HypABI__Patch__PatchNoClose__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchNoClose__arg__T));
+        rc = HypABI__Patch__PatchNoClose__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__PatchNoClose__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchNoClose__HYPERCALL(...) HypABI__Patch__PatchNoClose__hypercall((HypABI__Patch__PatchNoClose__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab;
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchViolation__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchViolation__arg__slab, gfp)
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__ALLOC() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchViolation__arg__slab) \
+                __arg = HypABI__Patch__PatchViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchViolation__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall(uint8_t *block_out, const HypABI__Patch__PatchViolation__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchViolation__arg__T *bhv_arg = HypABI__Patch__PatchViolation__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchViolation__arg__T));
+        rc = HypABI__Patch__PatchViolation__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Patch__PatchViolation__arg__T*)bhv_arg)->block;
+        HypABI__Patch__PatchViolation__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchViolation__HYPERCALL(BLOCK_OUT, ...) HypABI__Patch__PatchViolation__hypercall(BLOCK_OUT, (HypABI__Patch__PatchViolation__arg__T){__VA_ARGS__})
+
+#define HypABI__Richard__Open__hypercall() \
+        HypABI__Richard__Open__hypercall_noalloc()
+
+#define HypABI__Richard__Close__hypercall() \
+        HypABI__Richard__Close__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab;
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__ProcessViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__ProcessViolation__arg__slab, gfp)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__ProcessViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__ProcessViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__ProcessViolation__arg__slab) \
+                __arg = HypABI__Acl__ProcessViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__ProcessViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab;
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__DriverViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__DriverViolation__arg__slab, gfp)
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__DriverViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__ALLOC() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__DriverViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__DriverViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__DriverViolation__arg__slab) \
+                __arg = HypABI__Acl__DriverViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__DriverViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Guestlog__Init__arg__slab;
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Guestlog__Init__arg__T *)kmem_cache_alloc(HypABI__Guestlog__Init__arg__slab, gfp)
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Guestlog__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__ALLOC_NOCHECK() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__ALLOC() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__Guestlog__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__Guestlog__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = &static; \
+        if (HypABI__Guestlog__Init__arg__slab) \
+                __arg = HypABI__Guestlog__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Guestlog__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall(uint64_t *log_bitmap_out, uint8_t *valid_out, const HypABI__Guestlog__Init__arg__T s)
+{
+        int rc;
+        HypABI__Guestlog__Init__arg__T *bhv_arg = HypABI__Guestlog__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Guestlog__Init__arg__T));
+        rc = HypABI__Guestlog__Init__hypercall_noalloc(bhv_arg);
+        *log_bitmap_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->log_bitmap;
+        *valid_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->valid;
+        HypABI__Guestlog__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Guestlog__Init__HYPERCALL(LOG_BITMAP_OUT, VALID_OUT, ...) HypABI__Guestlog__Init__hypercall(LOG_BITMAP_OUT, VALID_OUT, (HypABI__Guestlog__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Configure__arg__slab;
+#define HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Configure__arg__T *)kmem_cache_alloc(HypABI__Creds__Configure__arg__slab, gfp)
+#define HypABI__Creds__Configure__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Configure__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__ALLOC() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Configure__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = &static; \
+        if (HypABI__Creds__Configure__arg__slab) \
+                __arg = HypABI__Creds__Configure__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Configure__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab;
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__RegisterInitTask__arg__T *)kmem_cache_alloc(HypABI__Creds__RegisterInitTask__arg__slab, gfp)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__RegisterInitTask__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__RegisterInitTask__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = &static; \
+        if (HypABI__Creds__RegisterInitTask__arg__slab) \
+                __arg = HypABI__Creds__RegisterInitTask__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__RegisterInitTask__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall(const HypABI__Creds__RegisterInitTask__arg__T s)
+{
+        int rc;
+        HypABI__Creds__RegisterInitTask__arg__T *bhv_arg = HypABI__Creds__RegisterInitTask__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__RegisterInitTask__arg__T));
+        rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__RegisterInitTask__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__RegisterInitTask__HYPERCALL(...) HypABI__Creds__RegisterInitTask__hypercall((HypABI__Creds__RegisterInitTask__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Assign__arg__slab;
+#define HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Assign__arg__T *)kmem_cache_alloc(HypABI__Creds__Assign__arg__slab, gfp)
+#define HypABI__Creds__Assign__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Assign__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__ALLOC() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Assign__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Assign__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = &static; \
+        if (HypABI__Creds__Assign__arg__slab) \
+                __arg = HypABI__Creds__Assign__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Assign__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall(uint8_t *ret_out, const HypABI__Creds__Assign__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Assign__arg__T *bhv_arg = HypABI__Creds__Assign__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Assign__arg__T));
+        rc = HypABI__Creds__Assign__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Assign__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Assign__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Assign__HYPERCALL(RET_OUT, ...) HypABI__Creds__Assign__hypercall(RET_OUT, (HypABI__Creds__Assign__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab;
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__AssignPriv__arg__T *)kmem_cache_alloc(HypABI__Creds__AssignPriv__arg__slab, gfp)
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__AssignPriv__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__ALLOC() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__AssignPriv__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__AssignPriv__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = &static; \
+        if (HypABI__Creds__AssignPriv__arg__slab) \
+                __arg = HypABI__Creds__AssignPriv__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__AssignPriv__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall(uint8_t *ret_out, const HypABI__Creds__AssignPriv__arg__T s)
+{
+        int rc;
+        HypABI__Creds__AssignPriv__arg__T *bhv_arg = HypABI__Creds__AssignPriv__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__AssignPriv__arg__T));
+        rc = HypABI__Creds__AssignPriv__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__AssignPriv__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__AssignPriv__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__AssignPriv__HYPERCALL(RET_OUT, ...) HypABI__Creds__AssignPriv__hypercall(RET_OUT, (HypABI__Creds__AssignPriv__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Commit__arg__slab;
+#define HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Commit__arg__T *)kmem_cache_alloc(HypABI__Creds__Commit__arg__slab, gfp)
+#define HypABI__Creds__Commit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Commit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__ALLOC() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Commit__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Commit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = &static; \
+        if (HypABI__Creds__Commit__arg__slab) \
+                __arg = HypABI__Creds__Commit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Commit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall(uint8_t *ret_out, const HypABI__Creds__Commit__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Commit__arg__T *bhv_arg = HypABI__Creds__Commit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Commit__arg__T));
+        rc = HypABI__Creds__Commit__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Commit__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Commit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Commit__HYPERCALL(RET_OUT, ...) HypABI__Creds__Commit__hypercall(RET_OUT, (HypABI__Creds__Commit__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Release__arg__slab;
+#define HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Release__arg__T *)kmem_cache_alloc(HypABI__Creds__Release__arg__slab, gfp)
+#define HypABI__Creds__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Release__arg__T *__arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__ALLOC() \
+        HypABI__Creds__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Release__arg__T *__arg = &static; \
+        if (HypABI__Creds__Release__arg__slab) \
+                __arg = HypABI__Creds__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall(const HypABI__Creds__Release__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Release__arg__T *bhv_arg = HypABI__Creds__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Release__arg__T));
+        rc = HypABI__Creds__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Release__HYPERCALL(...) HypABI__Creds__Release__hypercall((HypABI__Creds__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Verification__arg__slab;
+#define HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Verification__arg__T *)kmem_cache_alloc(HypABI__Creds__Verification__arg__slab, gfp)
+#define HypABI__Creds__Verification__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Verification__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__ALLOC() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Verification__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Verification__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = &static; \
+        if (HypABI__Creds__Verification__arg__slab) \
+                __arg = HypABI__Creds__Verification__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Verification__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall(uint8_t *ret_out, const HypABI__Creds__Verification__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Verification__arg__T *bhv_arg = HypABI__Creds__Verification__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Verification__arg__T));
+        rc = HypABI__Creds__Verification__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Verification__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Verification__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Verification__HYPERCALL(RET_OUT, ...) HypABI__Creds__Verification__hypercall(RET_OUT, (HypABI__Creds__Verification__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Log__arg__slab;
+#define HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Log__arg__T *)kmem_cache_alloc(HypABI__Creds__Log__arg__slab, gfp)
+#define HypABI__Creds__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Log__arg__T *__arg = HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__ALLOC() \
+        HypABI__Creds__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Log__arg__T *__arg = &static; \
+        if (HypABI__Creds__Log__arg__slab) \
+                __arg = HypABI__Creds__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall(uint8_t *block_out, const HypABI__Creds__Log__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Log__arg__T *bhv_arg = HypABI__Creds__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Log__arg__T));
+        rc = HypABI__Creds__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Creds__Log__arg__T*)bhv_arg)->block;
+        HypABI__Creds__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Creds__Log__hypercall(BLOCK_OUT, (HypABI__Creds__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__Init__arg__slab;
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__Init__arg__T *)kmem_cache_alloc(HypABI__FileProtection__Init__arg__slab, gfp)
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__ALLOC() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__Init__arg__slab) \
+                __arg = HypABI__FileProtection__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall(uint64_t *feature_bitmap_out, const HypABI__FileProtection__Init__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__Init__arg__T *bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__Init__arg__T));
+        rc = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+        *feature_bitmap_out = ((volatile HypABI__FileProtection__Init__arg__T*)bhv_arg)->feature_bitmap;
+        HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__Init__HYPERCALL(FEATURE_BITMAP_OUT, ...) HypABI__FileProtection__Init__hypercall(FEATURE_BITMAP_OUT, (HypABI__FileProtection__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *bhv_arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T));
+        rc = HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab;
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationFileOps__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationFileOps__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationFileOps__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationFileOps__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationFileOps__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationFileOps__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationFileOps__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationFileOps__arg__T *bhv_arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationFileOps__arg__T));
+        rc = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationFileOps__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationFileOps__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationFileOps__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationFileOps__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationFileOps__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationDirtyCredWrite__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationDirtyCredWrite__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *bhv_arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T));
+        rc = HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationDirtyCredWrite__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab;
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__RegisterProtection__Freeze__arg__T *)kmem_cache_alloc(HypABI__RegisterProtection__Freeze__arg__slab, gfp)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__RegisterProtection__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__RegisterProtection__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = &static; \
+        if (HypABI__RegisterProtection__Freeze__arg__slab) \
+                __arg = HypABI__RegisterProtection__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__RegisterProtection__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall(const HypABI__RegisterProtection__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__RegisterProtection__Freeze__arg__T *bhv_arg = HypABI__RegisterProtection__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__RegisterProtection__Freeze__arg__T));
+        rc = HypABI__RegisterProtection__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__RegisterProtection__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__RegisterProtection__Freeze__HYPERCALL(...) HypABI__RegisterProtection__Freeze__hypercall((HypABI__RegisterProtection__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Register__arg__slab;
+#define HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Register__arg__T *)kmem_cache_alloc(HypABI__Inode__Register__arg__slab, gfp)
+#define HypABI__Inode__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Register__arg__T *__arg = HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__ALLOC() \
+        HypABI__Inode__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Register__arg__T *__arg = &static; \
+        if (HypABI__Inode__Register__arg__slab) \
+                __arg = HypABI__Inode__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall(const HypABI__Inode__Register__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Register__arg__T *bhv_arg = HypABI__Inode__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Register__arg__T));
+        rc = HypABI__Inode__Register__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Register__HYPERCALL(...) HypABI__Inode__Register__hypercall((HypABI__Inode__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Update__arg__slab;
+#define HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Update__arg__T *)kmem_cache_alloc(HypABI__Inode__Update__arg__slab, gfp)
+#define HypABI__Inode__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Update__arg__T *__arg = HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__ALLOC() \
+        HypABI__Inode__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Update__arg__T *__arg = &static; \
+        if (HypABI__Inode__Update__arg__slab) \
+                __arg = HypABI__Inode__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall(const HypABI__Inode__Update__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Update__arg__T *bhv_arg = HypABI__Inode__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Update__arg__T));
+        rc = HypABI__Inode__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Update__HYPERCALL(...) HypABI__Inode__Update__hypercall((HypABI__Inode__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Release__arg__slab;
+#define HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Release__arg__T *)kmem_cache_alloc(HypABI__Inode__Release__arg__slab, gfp)
+#define HypABI__Inode__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Release__arg__T *__arg = HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__ALLOC() \
+        HypABI__Inode__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Release__arg__T *__arg = &static; \
+        if (HypABI__Inode__Release__arg__slab) \
+                __arg = HypABI__Inode__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall(const HypABI__Inode__Release__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Release__arg__T *bhv_arg = HypABI__Inode__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Release__arg__T));
+        rc = HypABI__Inode__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Release__HYPERCALL(...) HypABI__Inode__Release__hypercall((HypABI__Inode__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Verify__arg__slab;
+#define HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Verify__arg__T *)kmem_cache_alloc(HypABI__Inode__Verify__arg__slab, gfp)
+#define HypABI__Inode__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__ALLOC() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = &static; \
+        if (HypABI__Inode__Verify__arg__slab) \
+                __arg = HypABI__Inode__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall(uint8_t *ret_out, const HypABI__Inode__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Verify__arg__T *bhv_arg = HypABI__Inode__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Verify__arg__T));
+        rc = HypABI__Inode__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Inode__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Inode__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Verify__HYPERCALL(RET_OUT, ...) HypABI__Inode__Verify__hypercall(RET_OUT, (HypABI__Inode__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Log__arg__slab;
+#define HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Log__arg__T *)kmem_cache_alloc(HypABI__Inode__Log__arg__slab, gfp)
+#define HypABI__Inode__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Log__arg__T *__arg = HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__ALLOC() \
+        HypABI__Inode__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Log__arg__T *__arg = &static; \
+        if (HypABI__Inode__Log__arg__slab) \
+                __arg = HypABI__Inode__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall(uint8_t *block_out, const HypABI__Inode__Log__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Log__arg__T *bhv_arg = HypABI__Inode__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Log__arg__T));
+        rc = HypABI__Inode__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Inode__Log__arg__T*)bhv_arg)->block;
+        HypABI__Inode__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Inode__Log__hypercall(BLOCK_OUT, (HypABI__Inode__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Register__arg__slab;
+#define HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Register__arg__T *)kmem_cache_alloc(HypABI__Keyring__Register__arg__slab, gfp)
+#define HypABI__Keyring__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__ALLOC() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Register__arg__slab) \
+                __arg = HypABI__Keyring__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall(uint8_t *block_out, const HypABI__Keyring__Register__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Register__arg__T *bhv_arg = HypABI__Keyring__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Register__arg__T));
+        rc = HypABI__Keyring__Register__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Register__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Register__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Register__hypercall(BLOCK_OUT, (HypABI__Keyring__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Verify__arg__slab;
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Verify__arg__T *)kmem_cache_alloc(HypABI__Keyring__Verify__arg__slab, gfp)
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__ALLOC() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Verify__arg__slab) \
+                __arg = HypABI__Keyring__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall(uint8_t *ret_out, const HypABI__Keyring__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Verify__arg__T *bhv_arg = HypABI__Keyring__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Verify__arg__T));
+        rc = HypABI__Keyring__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Keyring__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Keyring__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Verify__HYPERCALL(RET_OUT, ...) HypABI__Keyring__Verify__hypercall(RET_OUT, (HypABI__Keyring__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Log__arg__slab;
+#define HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Log__arg__T *)kmem_cache_alloc(HypABI__Keyring__Log__arg__slab, gfp)
+#define HypABI__Keyring__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__ALLOC() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Log__arg__slab) \
+                __arg = HypABI__Keyring__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall(uint8_t *block_out, const HypABI__Keyring__Log__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Log__arg__T *bhv_arg = HypABI__Keyring__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Log__arg__T));
+        rc = HypABI__Keyring__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Log__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Log__hypercall(BLOCK_OUT, (HypABI__Keyring__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *)kmem_cache_alloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, gfp)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__FreezeMemoryAfterBoot__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = &static; \
+        if (HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab) \
+                __arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(uint8_t *freeze_memory_after_boot_out, const HypABI__Confserver__FreezeMemoryAfterBoot__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *bhv_arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T));
+        rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(bhv_arg);
+        *freeze_memory_after_boot_out = ((volatile HypABI__Confserver__FreezeMemoryAfterBoot__arg__T*)bhv_arg)->freeze_memory_after_boot;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__HYPERCALL(FREEZE_MEMORY_AFTER_BOOT_OUT, ...) HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(FREEZE_MEMORY_AFTER_BOOT_OUT, (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab;
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__StrictFileops__arg__T *)kmem_cache_alloc(HypABI__Confserver__StrictFileops__arg__slab, gfp)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__StrictFileops__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__StrictFileops__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = &static; \
+        if (HypABI__Confserver__StrictFileops__arg__slab) \
+                __arg = HypABI__Confserver__StrictFileops__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__StrictFileops__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall(uint8_t *strict_fileops_out, const HypABI__Confserver__StrictFileops__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__StrictFileops__arg__T *bhv_arg = HypABI__Confserver__StrictFileops__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__StrictFileops__arg__T));
+        rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+        *strict_fileops_out = ((volatile HypABI__Confserver__StrictFileops__arg__T*)bhv_arg)->strict_fileops;
+        HypABI__Confserver__StrictFileops__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__StrictFileops__HYPERCALL(STRICT_FILEOPS_OUT, ...) HypABI__Confserver__StrictFileops__hypercall(STRICT_FILEOPS_OUT, (HypABI__Confserver__StrictFileops__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__KernelConfig__arg__slab;
+#define HypABI__Confserver__KernelConfig__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__KernelConfig__arg__T *)kmem_cache_alloc(HypABI__Confserver__KernelConfig__arg__slab, gfp)
+#define HypABI__Confserver__KernelConfig__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__KernelConfig__arg__T *__arg = HypABI__Confserver__KernelConfig__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__KernelConfig__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__KernelConfig__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__KernelConfig__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__KernelConfig__arg__ALLOC() \
+        HypABI__Confserver__KernelConfig__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__KernelConfig__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__KernelConfig__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__KernelConfig__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__KernelConfig__arg__T *__arg = &static; \
+        if (HypABI__Confserver__KernelConfig__arg__slab) \
+                __arg = HypABI__Confserver__KernelConfig__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__KernelConfig__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__KernelConfig__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__KernelConfig__hypercall(uint8_t *userspace_force_nx_stack_out, const HypABI__Confserver__KernelConfig__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__KernelConfig__arg__T *bhv_arg = HypABI__Confserver__KernelConfig__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__KernelConfig__arg__T));
+        rc = HypABI__Confserver__KernelConfig__hypercall_noalloc(bhv_arg);
+        *userspace_force_nx_stack_out = ((volatile HypABI__Confserver__KernelConfig__arg__T*)bhv_arg)->userspace_force_nx_stack;
+        HypABI__Confserver__KernelConfig__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__KernelConfig__HYPERCALL(USERSPACE_FORCE_NX_STACK_OUT, ...) HypABI__Confserver__KernelConfig__hypercall(USERSPACE_FORCE_NX_STACK_OUT, (HypABI__Confserver__KernelConfig__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__GuestPolicy__GetPolicy__arg__slab;
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__GuestPolicy__GetPolicy__arg__T *)kmem_cache_alloc(HypABI__GuestPolicy__GetPolicy__arg__slab, gfp)
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__GuestPolicy__GetPolicy__arg__T *__arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__GuestPolicy__GetPolicy__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_NOCHECK() \
+        HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC() \
+        HypABI__GuestPolicy__GetPolicy__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__GuestPolicy__GetPolicy__arg__FREE(name) \
+        kmem_cache_free(HypABI__GuestPolicy__GetPolicy__arg__slab, name); \
+        name = NULL
+#define HypABI__GuestPolicy__GetPolicy__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__GuestPolicy__GetPolicy__arg__T *__arg = &static; \
+        if (HypABI__GuestPolicy__GetPolicy__arg__slab) \
+                __arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__GuestPolicy__GetPolicy__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__GuestPolicy__GetPolicy__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__GuestPolicy__GetPolicy__hypercall(uint32_t *dest_sz_out, uint8_t *valid_out, const HypABI__GuestPolicy__GetPolicy__arg__T s)
+{
+        int rc;
+        HypABI__GuestPolicy__GetPolicy__arg__T *bhv_arg = HypABI__GuestPolicy__GetPolicy__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__GuestPolicy__GetPolicy__arg__T));
+        rc = HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(bhv_arg);
+        *dest_sz_out = ((volatile HypABI__GuestPolicy__GetPolicy__arg__T*)bhv_arg)->dest_sz;
+        *valid_out = ((volatile HypABI__GuestPolicy__GetPolicy__arg__T*)bhv_arg)->valid;
+        HypABI__GuestPolicy__GetPolicy__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__GuestPolicy__GetPolicy__HYPERCALL(DEST_SZ_OUT, VALID_OUT, ...) HypABI__GuestPolicy__GetPolicy__hypercall(DEST_SZ_OUT, VALID_OUT, (HypABI__GuestPolicy__GetPolicy__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Create__arg__slab;
+#define HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Create__arg__T *)kmem_cache_alloc(HypABI__Wagner__Create__arg__slab, gfp)
+#define HypABI__Wagner__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__ALLOC() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Create__arg__slab) \
+                __arg = HypABI__Wagner__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall(const HypABI__Wagner__Create__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Create__arg__T *bhv_arg = HypABI__Wagner__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Create__arg__T));
+        rc = HypABI__Wagner__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Create__HYPERCALL(...) HypABI__Wagner__Create__hypercall((HypABI__Wagner__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Extend__arg__slab;
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Extend__arg__T *)kmem_cache_alloc(HypABI__Wagner__Extend__arg__slab, gfp)
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Extend__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__ALLOC() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Extend__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Extend__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Extend__arg__slab) \
+                __arg = HypABI__Wagner__Extend__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Extend__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall(const HypABI__Wagner__Extend__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Extend__arg__T *bhv_arg = HypABI__Wagner__Extend__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Extend__arg__T));
+        rc = HypABI__Wagner__Extend__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Extend__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Extend__HYPERCALL(...) HypABI__Wagner__Extend__hypercall((HypABI__Wagner__Extend__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Delete__arg__slab;
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Delete__arg__T *)kmem_cache_alloc(HypABI__Wagner__Delete__arg__slab, gfp)
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Delete__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__ALLOC() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Delete__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Delete__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Delete__arg__slab) \
+                __arg = HypABI__Wagner__Delete__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Delete__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall(const HypABI__Wagner__Delete__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Delete__arg__T *bhv_arg = HypABI__Wagner__Delete__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Delete__arg__T));
+        rc = HypABI__Wagner__Delete__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Delete__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Delete__HYPERCALL(...) HypABI__Wagner__Delete__hypercall((HypABI__Wagner__Delete__arg__T){__VA_ARGS__})
+
diff --git include/bhv/interface/abi_ml_autogen.h include/bhv/interface/abi_ml_autogen.h
new file mode 100644
index 000000000..c9d52f009
--- /dev/null
+++ include/bhv/interface/abi_ml_autogen.h
@@ -0,0 +1,380 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-01-20T09:53:35).
+ */
+
+#pragma once
+
+#include <linux/bitops.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Init__Init__hypercall_noalloc(HypABI__Init__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Init__OP_ID, arg, HypABI__Init__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Init__Start__hypercall_noalloc(HypABI__Init__Start__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Start__OP_ID, arg, HypABI__Init__Start__arg__SZ + extra_size);
+}
+
+static inline bool HypABI__Integrity__MemFlags__has_TRANSIENT(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__TRANSIENT__BIT, addr);
+}
+static inline bool HypABI__Integrity__MemFlags__has_MUTABLE(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__MUTABLE__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall_noalloc(HypABI__Integrity__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Create__OP_ID, arg, HypABI__Integrity__Create__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall_noalloc(HypABI__Integrity__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Update__OP_ID, arg, HypABI__Integrity__Update__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall_noalloc(HypABI__Integrity__Remove__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Remove__OP_ID, arg, HypABI__Integrity__Remove__arg__SZ);
+}
+
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall_noalloc(HypABI__Integrity__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Freeze__OP_ID, arg, HypABI__Integrity__Freeze__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall_noalloc(HypABI__Integrity__PtpgInit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgInit__OP_ID, arg, HypABI__Integrity__PtpgInit__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgReport__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgReport__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall_noalloc(HypABI__Patch__Patch__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__Patch__OP_ID, arg, HypABI__Patch__Patch__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall_noalloc(HypABI__Patch__PatchNoClose__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchNoClose__OP_ID, arg, HypABI__Patch__PatchNoClose__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall_noalloc(HypABI__Patch__PatchViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchViolation__OP_ID, arg, HypABI__Patch__PatchViolation__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Richard__Open__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Open__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Richard__Close__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Close__OP_ID, NULL, 0);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessInit__hypercall_noalloc(HypABI__Acl__ProcessInit__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessInit__OP_ID, arg, HypABI__Acl__ProcessInit__arg__SZ + extra_size);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverInit__hypercall_noalloc(HypABI__Acl__DriverInit__arg__T *arg, size_t extra_size)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverInit__OP_ID, arg, HypABI__Acl__DriverInit__arg__SZ + extra_size);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessViolation__hypercall_noalloc(HypABI__Acl__ProcessViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessViolation__OP_ID, arg, HypABI__Acl__ProcessViolation__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverViolation__hypercall_noalloc(HypABI__Acl__DriverViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverViolation__OP_ID, arg, HypABI__Acl__DriverViolation__arg__SZ);
+}
+
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_SOCKET_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__SOCKET_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_FILE_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__FILE_EVENTS__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall_noalloc(HypABI__Guestlog__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Guestlog__BACKEND_ID, HypABI__Guestlog__Init__OP_ID, arg, HypABI__Guestlog__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Configure__hypercall_noalloc(HypABI__Creds__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Configure__OP_ID, arg, HypABI__Creds__Configure__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall_noalloc(HypABI__Creds__RegisterInitTask__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__RegisterInitTask__OP_ID, arg, HypABI__Creds__RegisterInitTask__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall_noalloc(HypABI__Creds__Assign__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Assign__OP_ID, arg, HypABI__Creds__Assign__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall_noalloc(HypABI__Creds__AssignPriv__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__AssignPriv__OP_ID, arg, HypABI__Creds__AssignPriv__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall_noalloc(HypABI__Creds__Commit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Commit__OP_ID, arg, HypABI__Creds__Commit__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall_noalloc(HypABI__Creds__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Release__OP_ID, arg, HypABI__Creds__Release__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall_noalloc(HypABI__Creds__Verification__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Verification__OP_ID, arg, HypABI__Creds__Verification__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall_noalloc(HypABI__Creds__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Log__OP_ID, arg, HypABI__Creds__Log__arg__SZ);
+}
+
+static inline bool HypABI__FileProtection__Init__Config__has_READ_ONLY(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__READ_ONLY__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_FILE_OPS(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__FILE_OPS__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_DIRTY_CRED(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT, addr);
+}
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall_noalloc(HypABI__FileProtection__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__Init__OP_ID, arg, HypABI__FileProtection__Init__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID, arg, HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(HypABI__FileProtection__ViolationFileOps__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationFileOps__OP_ID, arg, HypABI__FileProtection__ViolationFileOps__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID, arg, HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall_noalloc(HypABI__RegisterProtection__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__RegisterProtection__BACKEND_ID, HypABI__RegisterProtection__Freeze__OP_ID, arg, HypABI__RegisterProtection__Freeze__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__Configure__hypercall_noalloc(HypABI__Domain__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Configure__OP_ID, arg, HypABI__Domain__Configure__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__Report__hypercall_noalloc(HypABI__Domain__Report__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Report__OP_ID, arg, HypABI__Domain__Report__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Domain__ReportForcedMemAccess__hypercall_noalloc(HypABI__Domain__ReportForcedMemAccess__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__ReportForcedMemAccess__OP_ID, arg, HypABI__Domain__ReportForcedMemAccess__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall_noalloc(HypABI__Inode__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Register__OP_ID, arg, HypABI__Inode__Register__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall_noalloc(HypABI__Inode__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Update__OP_ID, arg, HypABI__Inode__Update__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall_noalloc(HypABI__Inode__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Release__OP_ID, arg, HypABI__Inode__Release__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall_noalloc(HypABI__Inode__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Verify__OP_ID, arg, HypABI__Inode__Verify__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall_noalloc(HypABI__Inode__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Log__OP_ID, arg, HypABI__Inode__Log__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall_noalloc(HypABI__Keyring__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Register__OP_ID, arg, HypABI__Keyring__Register__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall_noalloc(HypABI__Keyring__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Verify__OP_ID, arg, HypABI__Keyring__Verify__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall_noalloc(HypABI__Keyring__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Log__OP_ID, arg, HypABI__Keyring__Log__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID, arg, HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall_noalloc(HypABI__Confserver__StrictFileops__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__StrictFileops__OP_ID, arg, HypABI__Confserver__StrictFileops__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Confserver__KernelConfig__hypercall_noalloc(HypABI__Confserver__KernelConfig__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__KernelConfig__OP_ID, arg, HypABI__Confserver__KernelConfig__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__GuestPolicy__GetPolicy__hypercall_noalloc(HypABI__GuestPolicy__GetPolicy__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__GuestPolicy__BACKEND_ID, HypABI__GuestPolicy__GetPolicy__OP_ID, arg, HypABI__GuestPolicy__GetPolicy__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall_noalloc(HypABI__Wagner__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Create__OP_ID, arg, HypABI__Wagner__Create__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall_noalloc(HypABI__Wagner__Extend__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Extend__OP_ID, arg, HypABI__Wagner__Extend__arg__SZ);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall_noalloc(HypABI__Wagner__Delete__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Delete__OP_ID, arg, HypABI__Wagner__Delete__arg__SZ);
+}
+
diff --git include/bhv/interface/abi_version_autogen.h include/bhv/interface/abi_version_autogen.h
new file mode 100644
index 000000000..2c1a130c9
--- /dev/null
+++ include/bhv/interface/abi_version_autogen.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-01-20T09:53:35).
+ */
+
+#pragma once
+
+#define HypABI__ABI_VERSION __BHV_VAS_ABI_VERSION(24, 52, 0, 1625)
+
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 000000000..eb89156a9
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,64 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define __BHV_VAS_ABI_VERSION(rel_year, rel_week, rel_extra, internal_info)    \
+	({                                                                     \
+		static_assert((unsigned long)(rel_year) <= 99);                \
+		static_assert((unsigned long)(rel_year) >= 23);                \
+		static_assert((unsigned long)(rel_week) < 53);                 \
+		static_assert((unsigned long)(rel_extra) < 0xff);              \
+		static_assert((unsigned long)(internal_info) > 0x00000);       \
+		static_assert((unsigned long)(internal_info) < 0xfffff);       \
+		(unsigned long)0xbedUL << (13 * 4) |                           \
+			(unsigned long)(rel_year) << (11 * 4) |                \
+			(unsigned long)(rel_week) << (9 * 4) |                 \
+			(unsigned long)(rel_extra) << (7 * 4) |                \
+			(unsigned long)(0x00UL) << (5 * 4) |                   \
+			(unsigned long)(internal_info) << (0 * 4);             \
+	})
+
+#include <bhv/version.h>
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_REGISTER_PROTECTION 9
+#define BHV_VAS_BACKEND_DOMAIN 10
+#define BHV_VAS_BACKEND_INODE 11
+#define BHV_VAS_BACKEND_KEYRING 12
+#define BHV_VAS_BACKEND_CONFSERVER 13
+#define BHV_VAS_BACKEND_CONTAINER_INTEGRITY 14
+
+/* BHV CONFIGURATION BITS */
+#define BHV_CONFIG_PROC_ACL 1
+#define BHV_CONFIG_DRIVER_ACL 2
+#define BHV_CONFIG_LOGGING 3
+#define BHV_CONFIG_CREDS 4
+#define BHV_CONFIG_FILE_PROTECTION 5
+#define BHV_CONFIG_GUEST_POLICY 6
+#define BHV_CONFIG_REGISTER_PROTECTION 7
+#define BHV_CONFIG_STRONG_ISOLATION 8
+#define BHV_CONFIG_INODE 10
+#define BHV_CONFIG_KEYRING 11
+#define BHV_CONFIG_CONTAINER_INTEGRITY 12
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 000000000..862c0a1de
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static __always_inline int bhv_hypercall_vas(uint32_t backend, uint32_t op,
+					     void *arg, size_t arg_len)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	BUG_ON(!!arg != !!arg_len);
+
+	if (arg != NULL)
+		phys_addr = bhv_virt_to_phys(arg, arg_len);
+
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_start(TARGET_BHV_VAS, backend, op);
+#endif
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			   phys_addr);
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_end(TARGET_BHV_VAS, backend, op);
+#endif
+
+	if (rv) {
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION, arg,
+		      phys_addr);
+#else
+		pr_warn("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+			rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			arg, phys_addr);
+		dump_stack();
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 000000000..696feea4b
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,207 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_PATCH_H
+#define _ASM_INTERFACE_BHV_PATCH_H
+
+#include <linux/kernel.h>
+
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define PATCH_BODY()                                                     \
+	int rc;                                                          \
+	HypABI__Patch__Patch__arg__T bhv_arg;                            \
+                                                                         \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                      \
+                                                                         \
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys(dest_virt_addr, size); \
+	memcpy(bhv_arg.src_value, src, size);                            \
+	bhv_arg.size = size;                                             \
+                                                                         \
+	rc = HypABI__Patch__Patch__hypercall_noalloc(&bhv_arg);          \
+                                                                         \
+	return rc;
+
+static __always_inline int __bhv_patch_hypercall_single(void *dest_virt_addr,
+							const uint8_t *src,
+							uint64_t size)
+{
+	PATCH_BODY();
+}
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define PATCH_BODY(TYP)                                                       \
+	int rc;                                                               \
+	static HypABI__Patch__##TYP##__arg__T early_arg;                      \
+	HypABI__Patch__##TYP##__arg__T *bhv_arg =                             \
+		HypABI__Patch__##TYP##__arg__ALLOC_STATICFALLBACK(early_arg); \
+                                                                              \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                           \
+                                                                              \
+	bhv_arg->dest_phys_addr = bhv_virt_to_phys(dest_virt_addr, size);     \
+	memcpy(bhv_arg->src_value, src, size);                                \
+	bhv_arg->size = size;                                                 \
+                                                                              \
+	rc = HypABI__Patch__##TYP##__hypercall_noalloc(bhv_arg);              \
+                                                                              \
+	HypABI__Patch__##TYP##__arg__FREE_STATICFALLBACK(bhv_arg, early_arg); \
+                                                                              \
+	return rc;
+
+static __always_inline int
+__bhv_patchnoclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				    uint64_t size)
+{
+	PATCH_BODY(PatchNoClose);
+}
+
+static __always_inline int
+__bhv_patchclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+                                 uint64_t size)
+{
+       PATCH_BODY(Patch);
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#undef PATCH_BODY
+
+static __always_inline uint64_t __bhv_patch_round(void **const dest_virt_addr,
+						  const uint8_t *const src,
+						  uint64_t *const size,
+#ifndef CONFIG_BHV_VAULT_SPACES
+						  const bool close_vault,
+#endif
+						  unsigned long *const rc)
+{
+	unsigned long r;
+	uint64_t bytes_until_page_boundary =
+		PAGE_SIZE - ((uint64_t)*dest_virt_addr % PAGE_SIZE);
+	uint64_t this_patch_size = min3(*size, bytes_until_page_boundary,
+					(uint64_t)HypABI__Patch__MAX_PATCH_SZ);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	r = __bhv_patch_hypercall_single(*dest_virt_addr, src, this_patch_size);
+#else
+	if (close_vault && *size == this_patch_size) {
+		r = __bhv_patchclose_hypercall_single(*dest_virt_addr, src,
+						      this_patch_size);
+	} else {
+		r = __bhv_patchnoclose_hypercall_single(*dest_virt_addr, src,
+							this_patch_size);
+	}
+#endif
+	if (r)
+		*rc = r;
+
+	*dest_virt_addr += this_patch_size;
+	*size -= this_patch_size;
+	return this_patch_size;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ */
+static __always_inline int bhv_patch_hypercall(void *dest_virt_addr,
+					       const uint8_t *src,
+					       uint64_t size
+#ifndef CONFIG_BHV_VAULT_SPACES
+					       , const bool close_vault
+#endif
+					       )
+{
+	unsigned long rc = 0;
+
+	BUG_ON(!src);
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, &rc);
+#else
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ * Sets memory using a one-byte pattern rather than just copying (like memset)
+ */
+static __always_inline int bhv_patch_hypercall_memset(void *dest_virt_addr,
+						      uint64_t size,
+                             uint8_t pattern
+#ifndef CONFIG_BHV_VAULT_SPACES
+                             , bool close_vault
+#endif
+					       	      )
+{
+	unsigned long rc = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(__arch64__)
+	size_t i = 0;
+
+	for (i = 0; i < HypABI__Patch__MAX_PATCH_SZ; i++)
+		buf[i] = pattern;
+#else
+	memset(buf, pattern, HypABI__Patch__MAX_PATCH_SZ);
+#endif
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		__bhv_patch_round(&dest_virt_addr, buf, &size, &rc);
+#else
+		__bhv_patch_round(&dest_virt_addr, buf, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+* Handle patch violation hypercalls
+*
+* This function sends a patch violation hypercall and determines whether the
+* patch should be blocked.
+*
+* \returns True if the patch should be blocked, false otherwise.
+*/
+static __always_inline bool bhv_patch_violation_hypercall(void *dest_virt_addr,
+							  const char *message)
+{
+	unsigned long r;
+	bool rc;
+
+	HypABI__Patch__PatchViolation__arg__T bhv_arg;
+
+	// Setup arguments. We block by default
+	bhv_arg.dest_virt_addr = (uint64_t)dest_virt_addr;
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys_single(dest_virt_addr);
+	bhv_arg.block = true;
+
+	if (message != NULL)
+		strncpy(bhv_arg.message, message,
+			HypABI__Patch__PatchViolation__MAX_MSG_SZ);
+	bhv_arg.message[HypABI__Patch__PatchViolation__MAX_MSG_SZ - 1] = '\0';
+
+	r = HypABI__Patch__PatchViolation__hypercall_noalloc(&bhv_arg);
+	rc = r || bhv_arg.block;
+
+	// Block in case of error or if block is set
+	return rc;
+}
+
+#endif /* _ASM_INTERFACE_BHV_PATCH_H */
diff --git include/bhv/kernel-kln.h include/bhv/kernel-kln.h
new file mode 100644
index 000000000..d73de9ce6
--- /dev/null
+++ include/bhv/kernel-kln.h
@@ -0,0 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#define KLN_SYM(sym) ((unsigned long) sym)
+#define KLN_SYMBOL(ty, sym) ((ty)sym)
+#define KLN_SYMBOL_P(ty, sym) ((ty)&sym)
diff --git include/bhv/keyring.h include/bhv/keyring.h
new file mode 100644
index 000000000..6eba70a7c
--- /dev/null
+++ include/bhv/keyring.h
@@ -0,0 +1,40 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_KEYRING_H__
+#define __BHV_KEYRING_H__
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_KEYS
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_keyring_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_KEYRING, bhv_configuration_bitmap);
+}
+
+int __init bhv_init_keyring(void);
+
+int bhv_keyring_register_system_trusted(struct key **k);
+int bhv_keyring_verify(struct key *keyring, void *anchor);
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_keyring_register_system_trusted(k) 0
+#define bhv_keyring_verify(k, a) 0
+#define bhv_keyring_verify_locked(k, a) 0
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* CONFIG_KEYS */
+
+#endif /* __BHV_KEYRING_H__ */
+
diff --git include/bhv/kversion.h include/bhv/kversion.h
new file mode 100644
index 000000000..d109fd40c
--- /dev/null
+++ include/bhv/kversion.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/version.h>
+
+#ifndef VASKM // inside kernel tree
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 186) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+#define BHV_KVERS_5_10
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 72) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+#define BHV_KVERS_5_15
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 31) && LINUX_VERSION_CODE < KERNEL_VERSION(6, 2, 0)
+#define BHV_KVERS_6_1
+#else
+#error Unsupported linux version
+#endif
+
+#endif // VASKM
+
+#undef LINUX_VERSION_CODE
+#undef KERNEL_VERSION
+#undef LINUX_VERSION_MAJOR
+#undef LINUX_VERSION_PATCHLEVEL
+#undef LINUX_VERSION_SUBLEVEL
diff --git include/bhv/memory_freeze.h include/bhv/memory_freeze.h
new file mode 100644
index 000000000..fbea9c9e0
--- /dev/null
+++ include/bhv/memory_freeze.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#ifdef CONFIG_BHV_VAS
+
+void bhv_memory_freeze_init(void);
+
+#else // !CONFIG_BHV_VAS
+
+static inline void bhv_memory_freeze_init(void)
+{
+}
+
+#endif // CONFIG_BHV_VAS
\ No newline at end of file
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 000000000..1835ae267
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description);
+#endif //VASKM
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+#if 0
+static inline void bhv_protect_generic_memory(uint64_t owner, const void *base,
+					      uint64_t size, uint32_t type,
+					      uint64_t flags, char *description)
+{
+}
+#endif
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 000000000..4c74007c1
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,153 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+extern struct mutex bhv_alternatives_mutex;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_init_alternatives(void);
+#ifdef CONFIG_JUMP_LABEL
+void bhv_init_jump_label(void);
+#else
+static inline void bhv_init_jump_label(void) {}
+#endif
+#ifdef CONFIG_HAVE_STATIC_CALL
+void bhv_init_static_call(void);
+#else
+static inline void bhv_init_static_call(void) {}
+#endif
+#endif
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void);
+/**************************************************/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+int bhv_bpf_write(void *dst, void *src, size_t sz);
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz);
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages);
+void bhv_rm_bpf_code_range(uint64_t pfn);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+#ifndef CONFIG_BHV_VAULT_SPACES
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_JUMP_LABEL */
+
+static void __always_inline bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static void __always_inline bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len);
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __init_or_module bhv_apply_retpolines(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#if defined CONFIG_PARAVIRT && defined CONFIG_X86
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* defined CONFIG_PARAVIRT && defined CONFIG_X86 */
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __init_or_module bhv_apply_ibt_endbr(s32 *s);
+#endif
+#endif // VASKM
+
+#else // CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif // CONFIG_BHV_VAS
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/reg_protect.h include/bhv/reg_protect.h
new file mode 100644
index 000000000..9d19bf3a1
--- /dev/null
+++ include/bhv/reg_protect.h
@@ -0,0 +1,42 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void);
+void bhv_start_reg_protect_arch(void);
+/***************************************************/
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield);
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/reverse_shell_detection.h include/bhv/reverse_shell_detection.h
new file mode 100644
index 000000000..949cc28e8
--- /dev/null
+++ include/bhv/reverse_shell_detection.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_REVERSE_SHELL_DETECTION_H__
+#define __BHV_REVERSE_SHELL_DETECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h>
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool bhv_reverse_shell_detetection_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return bhv_policy_get_interpreter_binds_enabled();
+}
+
+int bhv_reverse_shell_detection_socket_connect(struct socket *sock,
+					       struct sockaddr *address,
+					       int addrlen);
+int bhv_reverse_shell_fd_dup(unsigned int fd, struct file *file);
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_REVERSE_SHELL_DETECTION_H__ */
\ No newline at end of file
diff --git include/bhv/sysfs.h include/bhv/sysfs.h
new file mode 100644
index 000000000..8231447a7
--- /dev/null
+++ include/bhv/sysfs.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_sysfs(void);
+/**********************************************************/
diff --git include/bhv/sysfs_fops.h include/bhv/sysfs_fops.h
new file mode 100644
index 000000000..3c48c20da
--- /dev/null
+++ include/bhv/sysfs_fops.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_fileops_protection(struct kobject *fops,
+					struct kobject *status);
+/***************************************************/
+
+#ifdef VASKM // out of tree
+int bhv_freeze_fops_map(void);
+extern bool bhv_allow_update_fileops_map;
+#endif // VASKM
+#endif /* CONFIG_BHV_VAS */
+
+
+
diff --git include/bhv/sysfs_integrity_freeze.h include/bhv/sysfs_integrity_freeze.h
new file mode 100644
index 000000000..62ab213f1
--- /dev/null
+++ include/bhv/sysfs_integrity_freeze.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_integrity_freeze(struct kobject *kobj);
+/***************************************************/
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
\ No newline at end of file
diff --git include/bhv/sysfs_reg_protect.h include/bhv/sysfs_reg_protect.h
new file mode 100644
index 000000000..fdd6c5fe7
--- /dev/null
+++ include/bhv/sysfs_reg_protect.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <linux/types.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+struct kobject;
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_reg_protect(struct kobject *kobj);
+/***************************************************/
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs_version.h include/bhv/sysfs_version.h
new file mode 100644
index 000000000..4b2915c3f
--- /dev/null
+++ include/bhv/sysfs_version.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_version(struct kobject *kobj);
+/***************************************************/
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/util.h include/bhv/util.h
new file mode 100644
index 000000000..d5b93087e
--- /dev/null
+++ include/bhv/util.h
@@ -0,0 +1,92 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bluerock.io>
+ */
+
+#include <linux/cgroup.h>
+#include <linux/version.h>
+#include <linux/namei.h>
+#include <linux/binfmts.h>
+#include <linux/file.h>
+
+// Various utility functions
+static inline const char *bhv_get_file_path(struct file *file, char *buf,
+					    size_t buf_sz)
+{
+	const char *rv;
+
+	if (buf == NULL)
+		return "UNKNOWN (NO BUF)";
+
+	rv = d_path(&file->f_path, buf, buf_sz);
+	if (IS_ERR(rv))
+		return "UNKNOWN (ERROR)";
+
+	return rv;
+}
+
+static inline const char *bhv_get_dentry_path(struct dentry *dentry, char *buf,
+					      size_t buf_sz)
+{
+	const char *rv;
+
+	if (buf == NULL)
+		return "UNKNOWN (NO BUF)";
+
+	rv = dentry_path(dentry, buf, buf_sz);
+	if (IS_ERR(rv)) {
+		return "UNKNOWN (ERROR)";
+	}
+
+	return rv;
+}
+
+static inline bool bhv_task_in_container(struct task_struct *tsk)
+{
+#define CGRP_NAME_SZ 128
+#define CONTAINER_ID_LEN 64
+	static const char *CONTAINER_CGRP_PREFIX[] = { "libpod-conmon-",
+						       "docker-",
+						       "cri-containerd-" };
+	static const size_t CONTAINER_CGRP_PREFIX_SZ =
+		(ARRAY_SIZE(CONTAINER_CGRP_PREFIX));
+
+	int r = 0;
+	struct cgroup *cgrp;
+	char cgrp_name[CGRP_NAME_SZ];
+	int i;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	while (cgrp != NULL) {
+		r = cgroup_name(cgrp, cgrp_name, CGRP_NAME_SZ);
+		if (r < 0) {
+			goto out;
+		}
+
+		for (i = 0; i < CONTAINER_CGRP_PREFIX_SZ; i++) {
+			size_t prefix_len = strlen(CONTAINER_CGRP_PREFIX[i]);
+			if (strncmp(cgrp_name, CONTAINER_CGRP_PREFIX[i],
+				    prefix_len) == 0) {
+				rcu_read_unlock();
+				return true;
+			}
+		}
+
+		// Try next
+		cgrp = cgroup_parent(cgrp);
+	}
+
+out:
+	rcu_read_unlock();
+	return false;
+}
+
+static inline bool bhv_is_pipe(struct file *f)
+{
+	if (f == NULL)
+		return false;
+
+	return S_ISFIFO(f->f_path.dentry->d_inode->i_mode);
+}
\ No newline at end of file
diff --git include/bhv/vault.h include/bhv/vault.h
new file mode 100644
index 000000000..8e008ea8e
--- /dev/null
+++ include/bhv/vault.h
@@ -0,0 +1,258 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VAULT_H__
+#define __BHV_VAULT_H__
+
+#ifdef __ASSEMBLY__
+
+#ifdef CONFIG_X86_64
+.macro BHV_ASM_PUSH_SECTION_VAULT_SHARED_CODE vault
+#ifdef CONFIG_BHV_VAULT_SPACES
+	.pushsection .bhv.vault.shared.text.\vault,"ax"
+#endif
+.endm
+
+.macro BHV_ASM_POP_SECTION
+#ifdef CONFIG_BHV_VAULT_SPACES
+	.popsection
+#endif
+.endm
+#endif /* !CONFIG_X86_64 */
+
+#else /* !__ASSEMBLY__ */
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool bhv_vault_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(bhv_configuration_bitmap);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+typedef struct {
+	int type;
+	unsigned long start;
+	unsigned long end;
+} bhv_vault_memory_region_helper_t;
+
+typedef struct {
+	void *ep;
+} bhv_vault_entry_point_helper_t;
+
+typedef struct {
+	void *rp;
+} bhv_vault_return_point_helper_t;
+
+#define STRINGIFY(x) #x
+#define ARG_DECL(t, a) t a
+#define RET(t) STRINGIFY(t)
+
+#define void_fn(fn, ...) fn(__VA_ARGS__)
+#define nonvoid_fn(fn, ...) return fn(__VA_ARGS__)
+
+/* Check out system call macros. */
+
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)                               \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(void)                                         \
+	{                                                                      \
+		void_fn(fn);                                                   \
+	}
+
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)                                     \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(void)                                         \
+	{                                                                      \
+		nonvoid_fn(fn);                                                \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)                       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		void_fn(fn, a1);                                               \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)                             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1);                                            \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)                           \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label")))  \
+		bhv_wrapper_##fn##_##a1(void)                                  \
+	{                                                                      \
+		return fn(a1);                                                 \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)               \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		void_fn(fn, a1, a2);                                           \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)                     \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2);                                        \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		void_fn(fn, a1, a2, a3);                                       \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2, a3);                                    \
+	}
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.data." #vault)))
+
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)                                 \
+	__attribute__((section(".bhv.vault.rodata." #vault)))
+
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.text." #vault),                     \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       no_instrument_function))
+
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault)                                \
+	__attribute__((section(".ref.text.bhv.vault.text." #vault),            \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       no_instrument_function))
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)                             \
+	__attribute__((section(".bhv.vault.shared.text." #vault),              \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       no_instrument_function))
+#else
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)
+#endif
+
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)                                 \
+	static bhv_vault_entry_point_helper_t bhv_vault_##vault##_ep_##func    \
+		__attribute__((used,                                           \
+			       section(".bhv.vault." #vault ".eps"))) = {      \
+			.ep = func,                                            \
+		}
+
+#define BHV_VAULT_FOR_EACH(section_name, type_t, elem)                         \
+		/*type_t *elem;	*/					       \
+		for (elem = ({                                         	       \
+		     	extern type_t section_name##_start;                    \
+		     	&section_name##_start;                                 \
+	     	});                                                            \
+	     	elem != ({                                                     \
+		     	extern type_t section_name##_end;                      \
+		     	&section_name##_end;                                   \
+	     	});                                                            \
+	     	++elem)
+
+#define BHV_VAULT_FOR_EACH_SECTION(vault, elem_name)                           \
+	BHV_VAULT_FOR_EACH(bhv_vault_##vault##_regions,                        \
+			   bhv_vault_memory_region_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, elem_name)                       \
+	BHV_VAULT_FOR_EACH(__bhv_vault_##vault##_eps,                          \
+			   bhv_vault_entry_point_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_RETURN_POINT(vault, elem)                           \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_return_sites;                       \
+		     &__start_vault_return_sites;                              \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_return_sites;                        \
+		     &__stop_vault_return_sites;                               \
+	     });                                                               \
+	     ++elem)
+
+#define BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, elem)                          \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_rethunk_sites;                      \
+		     &__start_vault_rethunk_sites;                             \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_rethunk_sites;                       \
+		     &__stop_vault_rethunk_sites;                              \
+	     });                                                               \
+	     ++elem)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#else /* !CONFIG_BHV_VAS */
+
+#define bhv_vault_is_enabled() 0
+
+#endif /* CONFIG_BHV_VAS */
+
+#if !defined(CONFIG_BHV_VAS) || !defined(CONFIG_BHV_VAULT_SPACES)
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault) __ref
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)
+#endif /* !CONFIG_BHV_VAS || !CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* __BHV_VAULT_H__ */
diff --git include/bhv/version.h include/bhv/version.h
new file mode 100644
index 000000000..c19bc2bcb
--- /dev/null
+++ include/bhv/version.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VERSION_H__
+#define __BHV_VERSION_H__
+
+#define BHV_VERSION __BHV_VERSION(24, 52, 0)
+
+#include <bhv/interface/abi_version_autogen.h>
+
+#endif //__BHV_VERSION_H__
diff --git include/keys/system_keyring.h include/keys/system_keyring.h
index 875e002a4..5e57cbb29 100644
--- include/keys/system_keyring.h
+++ include/keys/system_keyring.h
@@ -69,6 +69,9 @@ extern struct key *ima_blacklist_keyring;
 
 static inline struct key *get_ima_blacklist_keyring(void)
 {
+	if (bhv_keyring_verify(ima_blacklist_keyring, &ima_blacklist_keyring))
+		return NULL;
+
 	return ima_blacklist_keyring;
 }
 #else
diff --git include/linux/cgroup-defs.h include/linux/cgroup-defs.h
index 6c6323a01..2faf04e12 100644
--- include/linux/cgroup-defs.h
+++ include/linux/cgroup-defs.h
@@ -71,6 +71,9 @@ enum {
 
 	/* Cgroup is frozen. */
 	CGRP_FROZEN,
+
+	/* Control group has to be killed. */
+	CGRP_KILL,
 };
 
 /* cgroup_root->flags */
diff --git include/linux/filter.h include/linux/filter.h
index 840b2a05c..dad6a3417 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -29,6 +29,9 @@
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
+#include <bhv/module.h>
+#include <bhv/integrity.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -832,6 +835,10 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 	if (!fp->jited) {
 		set_vm_flush_reset_perms(fp);
 		set_memory_ro((unsigned long)fp, fp->pages);
+		if (!bhv_integrity_freeze_create_currently_frozen &&
+		    !bhv_integrity_freeze_update_currently_frozen &&
+		    !bhv_integrity_freeze_patch_currently_frozen)
+			bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
 	}
 #endif
 }
@@ -841,6 +848,7 @@ static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->pages);
 	set_memory_x((unsigned long)hdr, hdr->pages);
+	bhv_bpf_protect_x(hdr, hdr->pages << PAGE_SHIFT);
 }
 
 static inline struct bpf_binary_header *
@@ -878,6 +886,8 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited && !bhv_integrity_freeze_remove_currently_frozen)
+		bhv_bpf_unprotect(fp);
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/highmem.h include/linux/highmem.h
index b25df1f8d..7bed5678f 100644
--- include/linux/highmem.h
+++ include/linux/highmem.h
@@ -9,6 +9,8 @@
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
 
+#include <bhv/domain.h>
+
 #include <asm/cacheflush.h>
 
 #ifndef ARCH_HAS_FLUSH_ANON_PAGE
@@ -321,11 +323,19 @@ static inline void copy_user_highpage(struct page *to, struct page *from,
 {
 	char *vfrom, *vto;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
 	vfrom = kmap_atomic(from);
 	vto = kmap_atomic(to);
 	copy_user_page(vto, vfrom, vaddr, to);
 	kunmap_atomic(vto);
 	kunmap_atomic(vfrom);
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 #endif
diff --git include/linux/jump_label.h include/linux/jump_label.h
index e67ee4d73..ecdb4fa76 100644
--- include/linux/jump_label.h
+++ include/linux/jump_label.h
@@ -230,6 +230,27 @@ extern void static_key_disable(struct static_key *key);
 extern void static_key_enable_cpuslocked(struct static_key *key);
 extern void static_key_disable_cpuslocked(struct static_key *key);
 
+#ifdef CONFIG_MODULES
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+struct static_key_mod {
+        struct static_key_mod *next;
+        struct jump_entry *entries;
+        struct module *mod;
+};
+
+struct jump_label_patch {
+        const void *code;
+        int size;
+};
+
+inline struct static_key_mod *static_key_mod(struct static_key *key);
+#endif
+
+#endif /* CONFIG_MODULES */
+
+inline bool static_key_linked(struct static_key *key);
+
 /*
  * We should be using ATOMIC_INIT() for initializing .enabled, but
  * the inclusion of atomic.h is problematic for inclusion of jump_label.h
diff --git include/linux/lsm_hook_defs.h include/linux/lsm_hook_defs.h
index 35bb13ce1..51588a42a 100644
--- include/linux/lsm_hook_defs.h
+++ include/linux/lsm_hook_defs.h
@@ -395,3 +395,11 @@ LSM_HOOK(void, LSM_RET_VOID, perf_event_free, struct perf_event *event)
 LSM_HOOK(int, 0, perf_event_read, struct perf_event *event)
 LSM_HOOK(int, 0, perf_event_write, struct perf_event *event)
 #endif /* CONFIG_PERF_EVENTS */
+
+LSM_HOOK(void, LSM_RET_VOID, module_loaded, struct module *mod)
+LSM_HOOK(int, 0, unshare, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, setns, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, cgroup_mkdir, struct cgroup *cgrp)
+LSM_HOOK(void, LSM_RET_VOID, cgroup_rmdir, struct cgroup *cgrp)
+LSM_HOOK(int, 0, fd_dup, unsigned int fd, struct file *file)
+LSM_HOOK(void, LSM_RET_VOID, socket_accepted, struct socket *sock, struct socket *newsock)
diff --git include/linux/lsm_hooks.h include/linux/lsm_hooks.h
index bbf9c8c7b..a0050a2b3 100644
--- include/linux/lsm_hooks.h
+++ include/linux/lsm_hooks.h
@@ -1532,6 +1532,24 @@
  * 	Read perf_event security info if allowed.
  * @perf_event_write:
  * 	Write perf_event security info if allowed.
+ *
+ * Security hooks for BlueRock Security Module
+ *
+ * @module_loaded:
+ * 	A module has successfully loaded.
+ *
+ * @unshare:
+ * 	A process unshares a part of its context.
+ *
+ * @setns:
+ * 	A setns system call is made.
+ *
+ * @cgroup_mkdir:
+ * 	A cgroup is created.
+ *
+ * @cgroup_rmdir:
+ * 	A cgroup is destroyed.
+ *
  */
 union security_list_options {
 	#define LSM_HOOK(RET, DEFAULT, NAME, ...) RET (*NAME)(__VA_ARGS__);
diff --git include/linux/mem_namespace.h include/linux/mem_namespace.h
new file mode 100644
index 000000000..58a84fd77
--- /dev/null
+++ include/linux/mem_namespace.h
@@ -0,0 +1,89 @@
+#ifndef _LINUX_MEM_NS_H
+#define _LINUX_MEM_NS_H
+
+#include <linux/kref.h>
+#include <linux/nsproxy.h>
+#include <linux/ns_common.h>
+
+#ifdef CONFIG_MEM_NS
+#define bhv_pr_info(msg, ...)   pr_info("[-BHV-] %s: " msg "\n", __FUNCTION__, ##__VA_ARGS__)
+#else
+#define bhv_pr_info(msg, ...)
+#endif
+
+struct mem_namespace {
+	struct kref kref;
+	struct user_namespace *user_ns;
+	struct ucounts *ucounts;
+	struct ns_common ns;
+	struct mem_namespace *parent;
+	unsigned int level;
+	uint64_t domain;
+} __randomize_layout;
+
+extern struct mem_namespace init_mem_ns;
+
+#ifdef CONFIG_MEM_NS
+static inline struct mem_namespace *get_mem_ns(struct mem_namespace *ns)
+{
+	if (ns != &init_mem_ns)
+		kref_get(&ns->kref);
+	return ns;
+}
+
+extern void free_mem_ns(struct kref *kref);
+
+static inline void put_mem_ns(struct mem_namespace *ns)
+{
+	struct mem_namespace *parent = NULL;
+
+	while (ns != &init_mem_ns) {
+		parent = ns->parent;
+		if (!kref_put(&ns->kref, free_mem_ns))
+			break;
+		ns = parent;
+	}
+}
+
+extern struct mem_namespace *copy_mem_ns(unsigned long flags,
+					 struct user_namespace *user_ns,
+					 struct mem_namespace *old_ns);
+
+extern struct mem_namespace *memns_of_task(const struct task_struct *task);
+
+extern bool current_in_same_mem_ns(const struct task_struct *task);
+
+extern bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns);
+
+static inline unsigned int task_memns_level(struct task_struct *task) {
+	return memns_of_task(task)->level;
+}
+
+#else /* CONFIG_MEM_NS */
+
+static inline void get_mem_ns(struct mem_namespace *ns) {}
+static inline void put_mem_ns(struct mem_namespace *ns) {}
+
+static inline struct mem_namespace *copy_mem_ns(unsigned long flags,
+						struct user_namespace *user_ns,
+						struct mem_namespace *old_ns)
+{
+	if (flags & CLONE_NEWMEM)
+		return ERR_PTR(-EINVAL);
+
+	return old_ns;
+}
+
+static inline bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return true;
+}
+
+static inline unsigned int task_memns_level(struct task_struct *task)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* _LINUX_MEM_NS_H */
diff --git include/linux/module.h include/linux/module.h
index a55a40c28..cd2239c56 100644
--- include/linux/module.h
+++ include/linux/module.h
@@ -159,6 +159,18 @@ extern void cleanup_module(void);
 #define __INITRODATA_OR_MODULE __INITRODATA
 #endif /*CONFIG_MODULES*/
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+/* TODO: Make sure to delete the code in the __init section. */
+#define __bhv_init
+#define __bhv_init_or_module
+#define __bhv_noinstr
+#else
+#define __bhv_init __init
+#define __bhv_init_or_module __init_or_module
+#define __bhv_noinstr noinstr
+#endif
+
+
 /* Generic info of form tag = "info" */
 #define MODULE_INFO(tag, info) __MODULE_INFO(tag, tag, info)
 
diff --git include/linux/nsproxy.h include/linux/nsproxy.h
index cdb171efc..499668ef1 100644
--- include/linux/nsproxy.h
+++ include/linux/nsproxy.h
@@ -11,6 +11,7 @@ struct ipc_namespace;
 struct pid_namespace;
 struct cgroup_namespace;
 struct fs_struct;
+struct mem_namespace;
 
 /*
  * A structure to contain pointers to all per-process
@@ -38,6 +39,7 @@ struct nsproxy {
 	struct time_namespace *time_ns;
 	struct time_namespace *time_ns_for_children;
 	struct cgroup_namespace *cgroup_ns;
+	struct mem_namespace *mem_ns;
 };
 extern struct nsproxy init_nsproxy;
 
diff --git include/linux/proc_ns.h include/linux/proc_ns.h
index 75807ecef..ff1046249 100644
--- include/linux/proc_ns.h
+++ include/linux/proc_ns.h
@@ -34,6 +34,7 @@ extern const struct proc_ns_operations mntns_operations;
 extern const struct proc_ns_operations cgroupns_operations;
 extern const struct proc_ns_operations timens_operations;
 extern const struct proc_ns_operations timens_for_children_operations;
+extern const struct proc_ns_operations memns_operations;
 
 /*
  * We always define these enumerators
@@ -46,6 +47,7 @@ enum {
 	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
 	PROC_CGROUP_INIT_INO	= 0xEFFFFFFBU,
 	PROC_TIME_INIT_INO	= 0xEFFFFFFAU,
+	PROC_MEM_INIT_INO	= 0xEFFFFFF9U,
 };
 
 #ifdef CONFIG_PROC_FS
diff --git include/linux/sched.h include/linux/sched.h
index 3613c3f43..d70cd1c3d 100644
--- include/linux/sched.h
+++ include/linux/sched.h
@@ -37,6 +37,7 @@
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
 struct backing_dev_info;
+struct bhv_vas;
 struct bio_list;
 struct blk_plug;
 struct capture_control;
@@ -1356,6 +1357,10 @@ struct task_struct {
 	int				mce_count;
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	struct bhv_vas 			*bhv_vas;
+#endif
+
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.
diff --git include/linux/security.h include/linux/security.h
index e32e040f0..438dd099e 100644
--- include/linux/security.h
+++ include/linux/security.h
@@ -32,6 +32,7 @@
 #include <linux/string.h>
 #include <linux/mm.h>
 #include <linux/sockptr.h>
+#include <linux/nsproxy.h>
 
 struct linux_binprm;
 struct cred;
@@ -462,6 +463,13 @@ int security_inode_notifysecctx(struct inode *inode, void *ctx, u32 ctxlen);
 int security_inode_setsecctx(struct dentry *dentry, void *ctx, u32 ctxlen);
 int security_inode_getsecctx(struct inode *inode, void **ctx, u32 *ctxlen);
 int security_locked_down(enum lockdown_reason what);
+void security_module_loaded(struct module *mod);
+int security_unshare(struct task_struct *tsk, struct nsset *nsset);
+int security_setns(struct task_struct *tsk, struct nsset *nsset);
+int security_cgroup_mkdir(struct cgroup *cgrp);
+void security_cgroup_rmdir(struct cgroup *cgrp);
+int security_fd_dup(unsigned int fd, struct file *file);
+void security_socket_accepted(struct socket *sock, struct socket *newsock);
 #else /* CONFIG_SECURITY */
 
 static inline int call_blocking_lsm_notifier(enum lsm_event event, void *data)
@@ -1322,6 +1330,31 @@ static inline int security_locked_down(enum lockdown_reason what)
 {
 	return 0;
 }
+static inline void security_module_loaded(struct module *)
+{
+}
+static inline int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return 0;
+}
+static inline void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+}
+static inline int security_fd_dup(unsigned int fd, struct file *file)
+{
+	return 0;
+}
+static inline void security_socket_accepted(struct socket *sock, struct socket *newsock)
+{
+}
 #endif	/* CONFIG_SECURITY */
 
 #if defined(CONFIG_SECURITY) && defined(CONFIG_WATCH_QUEUE)
diff --git include/linux/static_call.h include/linux/static_call.h
index 04e6042d2..d4bc58a58 100644
--- include/linux/static_call.h
+++ include/linux/static_call.h
@@ -97,6 +97,8 @@
 #include <linux/cpu.h>
 #include <linux/static_call_types.h>
 
+#include <bhv/vault.h>
+
 #ifdef CONFIG_HAVE_STATIC_CALL
 #include <asm/static_call.h>
 
@@ -205,7 +207,13 @@ struct static_call_key {
 
 #define static_call_cond(name)	(void)__static_call(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#ifdef CONFIG_BHV_VAULT_SPACES
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	cpus_read_lock();
@@ -213,6 +221,7 @@ void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 	arch_static_call_transform(NULL, tramp, func, false);
 	cpus_read_unlock();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_update);
 
 static inline int static_call_text_reserved(void *start, void *end)
 {
@@ -276,7 +285,13 @@ static inline void __static_call_nop(void) { }
 
 #define static_call_cond(name)	(void)__static_call_cond(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_HAVE_STATIC_CALL)
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	WRITE_ONCE(key->func, func);
diff --git include/linux/user_namespace.h include/linux/user_namespace.h
index 079395186..85d6f589b 100644
--- include/linux/user_namespace.h
+++ include/linux/user_namespace.h
@@ -54,6 +54,7 @@ enum ucount_type {
 	UCOUNT_FANOTIFY_GROUPS,
 	UCOUNT_FANOTIFY_MARKS,
 #endif
+	UCOUNT_MEM_NAMESPACES,
 	UCOUNT_COUNTS,
 };
 
diff --git include/uapi/linux/perf_event.h include/uapi/linux/perf_event.h
index 6ca63ab6b..3d0eacdcb 100644
--- include/uapi/linux/perf_event.h
+++ include/uapi/linux/perf_event.h
@@ -727,6 +727,7 @@ enum {
 	USER_NS_INDEX		= 4,
 	MNT_NS_INDEX		= 5,
 	CGROUP_NS_INDEX		= 6,
+	MEM_NS_INDEX		= 7,
 
 	NR_NAMESPACES,		/* number of available namespaces */
 };
diff --git include/uapi/linux/sched.h include/uapi/linux/sched.h
index 3bac0a8ce..94224cf51 100644
--- include/uapi/linux/sched.h
+++ include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
  * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
  * syscalls only:
  */
+#define CLONE_NEWMEM	0x00000040	/* New memory namespace */
 #define CLONE_NEWTIME	0x00000080	/* New time namespace */
 
 #ifndef __ASSEMBLY__
diff --git init/main.c init/main.c
index b1593bdaf..bcfb18c8b 100644
--- init/main.c
+++ init/main.c
@@ -108,6 +108,10 @@
 
 #include <kunit/test.h>
 
+#include <bhv/init/mm_init.h>
+#include <bhv/init/late_start.h>
+#include <bhv/memory_freeze.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -838,6 +842,8 @@ static void __init mm_init(void)
 	/* Should be run after espfix64 is set up. */
 	pti_init();
 	mm_cache_init();
+
+	bhv_mm_init();
 }
 
 void __init __weak arch_call_rest_init(void)
@@ -903,6 +909,7 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	trap_init();
 	mm_init();
 	poking_init();
+
 	ftrace_init();
 
 	/* trace_printk can be enabled here */
@@ -1323,6 +1330,7 @@ static void __init do_pre_smp_initcalls(void)
 static int run_init_process(const char *init_filename)
 {
 	const char *const *p;
+	int ret;
 
 	argv_init[0] = init_filename;
 	pr_info("Run %s as init process\n", init_filename);
@@ -1332,7 +1340,10 @@ static int run_init_process(const char *init_filename)
 	pr_debug("  with environment:\n");
 	for (p = envp_init; *p; p++)
 		pr_debug("    %s\n", *p);
-	return kernel_execve(init_filename, argv_init, envp_init);
+	ret = kernel_execve(init_filename, argv_init, envp_init);
+	if (!ret)
+		bhv_memory_freeze_init();
+	return ret;
 }
 
 static int try_to_run_init_process(const char *init_filename)
@@ -1408,6 +1419,8 @@ static int __ref kernel_init(void *unused)
 	free_initmem();
 	mark_readonly();
 
+	bhv_late_start();
+
 	/*
 	 * Kernel mappings are now finalized - update the userspace page-table
 	 * to finalize PTI.
diff --git kernel/Makefile kernel/Makefile
index 82e9c8436..40d7731d5 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -76,6 +76,7 @@ obj-$(CONFIG_CGROUPS) += cgroup/
 obj-$(CONFIG_UTS_NS) += utsname.o
 obj-$(CONFIG_USER_NS) += user_namespace.o
 obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_MEM_NS) += mem_namespace.o
 obj-$(CONFIG_IKCONFIG) += configs.o
 obj-$(CONFIG_IKHEADERS) += kheaders.o
 obj-$(CONFIG_SMP) += stop_machine.o
diff --git kernel/bpf/bpf_struct_ops.c kernel/bpf/bpf_struct_ops.c
index ac283f9b2..4afe69e3c 100644
--- kernel/bpf/bpf_struct_ops.c
+++ kernel/bpf/bpf_struct_ops.c
@@ -452,6 +452,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 
 	set_memory_ro((long)st_map->image, 1);
 	set_memory_x((long)st_map->image, 1);
+	bhv_bpf_protect_x(st_map->image, PAGE_SIZE);
 	err = st_ops->reg(kdata);
 	if (likely(!err)) {
 		/* Pair with smp_load_acquire() during lookup_elem().
@@ -470,6 +471,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 	 */
 	set_memory_nx((long)st_map->image, 1);
 	set_memory_rw((long)st_map->image, 1);
+	bhv_bpf_unprotect(st_map->image);
 	bpf_map_put(map);
 
 reset_unlock:
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 33ea6ab12..0f865cf02 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -37,6 +37,8 @@
 #include <asm/barrier.h>
 #include <asm/unaligned.h>
 
+#include <bhv/vault.h>
+
 /* Registers */
 #define BPF_R0	regs[BPF_REG_0]
 #define BPF_R1	regs[BPF_REG_1]
@@ -678,6 +680,7 @@ void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 	bpf_ksym_del(&fp->aux->ksym);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static struct bpf_ksym *bpf_ksym_find(unsigned long addr)
 {
 	struct latch_tree_node *n;
@@ -711,6 +714,7 @@ const char *__bpf_address_lookup(unsigned long addr, unsigned long *size,
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_bpf_text_address(unsigned long addr)
 {
 	bool ret;
@@ -908,6 +912,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 pages = hdr->pages;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(pages);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index 87becf77c..c163f2c3c 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -38,6 +38,11 @@ void *bpf_jit_alloc_exec_page(void)
 	 * everytime new program is attached or detached.
 	 */
 	set_memory_x((long)image, 1);
+	/*
+	 * XXX: Make sure that we foresee all cases that would allow new
+	 * programs to attach/detach and handle the permissions appropriately.
+	 */
+	bhv_bpf_protect_x(image, PAGE_SIZE);
 	return image;
 }
 
@@ -171,6 +176,7 @@ static void __bpf_tramp_image_put_deferred(struct work_struct *work)
 
 	im = container_of(work, struct bpf_tramp_image, work);
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 	bpf_jit_uncharge_modmem(1);
 	percpu_ref_exit(&im->pcref);
@@ -289,6 +295,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key, u32 idx)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 out_uncharge:
 	bpf_jit_uncharge_modmem(1);
diff --git kernel/cgroup/cgroup.c kernel/cgroup/cgroup.c
index 643d8e178..dcc1e12a6 100644
--- kernel/cgroup/cgroup.c
+++ kernel/cgroup/cgroup.c
@@ -3756,6 +3756,47 @@ static ssize_t cgroup_freeze_write(struct kernfs_open_file *of,
 	return nbytes;
 }
 
+static void __cgroup_kill(struct cgroup *cgrp)
+{
+	struct css_task_iter it;
+	struct task_struct *task;
+
+	lockdep_assert_held(&cgroup_mutex);
+
+	spin_lock_irq(&css_set_lock);
+	set_bit(CGRP_KILL, &cgrp->flags);
+	spin_unlock_irq(&css_set_lock);
+
+	css_task_iter_start(&cgrp->self, CSS_TASK_ITER_PROCS | CSS_TASK_ITER_THREADED, &it);
+	while ((task = css_task_iter_next(&it))) {
+		/* Ignore kernel threads here. */
+		if (task->flags & PF_KTHREAD)
+			continue;
+
+		/* Skip tasks that are already dying. */
+		if (__fatal_signal_pending(task))
+			continue;
+
+		send_sig(SIGKILL, task, 0);
+	}
+	css_task_iter_end(&it);
+
+	spin_lock_irq(&css_set_lock);
+	clear_bit(CGRP_KILL, &cgrp->flags);
+	spin_unlock_irq(&css_set_lock);
+}
+
+void cgroup_kill(struct cgroup *cgrp)
+{
+	struct cgroup_subsys_state *css;
+	struct cgroup *dsct;
+
+	lockdep_assert_held(&cgroup_mutex);
+
+	cgroup_for_each_live_descendant_pre(dsct, css, cgrp)
+		__cgroup_kill(dsct);
+}
+
 static int cgroup_file_open(struct kernfs_open_file *of)
 {
 	struct cftype *cft = of->kn->priv;
@@ -5498,6 +5539,10 @@ int cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)
 	if (ret)
 		goto out_destroy;
 
+	ret = security_cgroup_mkdir(cgrp);
+	if (ret)
+		goto out_destroy;
+
 	TRACE_CGROUP_PATH(mkdir, cgrp);
 
 	/* let's create and online css's */
@@ -5696,8 +5741,10 @@ int cgroup_rmdir(struct kernfs_node *kn)
 		return 0;
 
 	ret = cgroup_destroy_locked(cgrp);
-	if (!ret)
+	if (!ret) {
+		security_cgroup_rmdir(cgrp);
 		TRACE_CGROUP_PATH(rmdir, cgrp);
+	}
 
 	cgroup_kn_unlock(kn);
 	return ret;
@@ -6245,6 +6292,8 @@ void cgroup_post_fork(struct task_struct *child,
 		      struct kernel_clone_args *kargs)
 	__releases(&cgroup_threadgroup_rwsem) __releases(&cgroup_mutex)
 {
+	unsigned long cgrp_flags = 0;
+	bool kill = false;
 	struct cgroup_subsys *ss;
 	struct css_set *cset;
 	int i;
@@ -6256,6 +6305,11 @@ void cgroup_post_fork(struct task_struct *child,
 
 	/* init tasks are special, only link regular threads */
 	if (likely(child->pid)) {
+		if (kargs->cgrp)
+			cgrp_flags = kargs->cgrp->flags;
+		else
+			cgrp_flags = cset->dfl_cgrp->flags;
+
 		WARN_ON_ONCE(!list_empty(&child->cg_list));
 		cset->nr_tasks++;
 		css_set_move_task(child, NULL, cset, false);
@@ -6264,23 +6318,32 @@ void cgroup_post_fork(struct task_struct *child,
 		cset = NULL;
 	}
 
-	/*
-	 * If the cgroup has to be frozen, the new task has too.  Let's set
-	 * the JOBCTL_TRAP_FREEZE jobctl bit to get the task into the
-	 * frozen state.
-	 */
-	if (unlikely(cgroup_task_freeze(child))) {
-		spin_lock(&child->sighand->siglock);
-		WARN_ON_ONCE(child->frozen);
-		child->jobctl |= JOBCTL_TRAP_FREEZE;
-		spin_unlock(&child->sighand->siglock);
+	if (!(child->flags & PF_KTHREAD)) {
+		if (unlikely(test_bit(CGRP_FREEZE, &cgrp_flags))) {
+			/*
+			 * If the cgroup has to be frozen, the new task has
+			 * too. Let's set the JOBCTL_TRAP_FREEZE jobctl bit to
+			 * get the task into the frozen state.
+			 */
+			spin_lock(&child->sighand->siglock);
+			WARN_ON_ONCE(child->frozen);
+			child->jobctl |= JOBCTL_TRAP_FREEZE;
+			spin_unlock(&child->sighand->siglock);
+
+			/*
+			 * Calling cgroup_update_frozen() isn't required here,
+			 * because it will be called anyway a bit later from
+			 * do_freezer_trap(). So we avoid cgroup's transient
+			 * switch from the frozen state and back.
+			 */
+		}
 
 		/*
-		 * Calling cgroup_update_frozen() isn't required here,
-		 * because it will be called anyway a bit later from
-		 * do_freezer_trap(). So we avoid cgroup's transient switch
-		 * from the frozen state and back.
+		 * If the cgroup is to be killed notice it now and take the
+		 * child down right after we finished preparing it for
+		 * userspace.
 		 */
+		kill = test_bit(CGRP_KILL, &cgrp_flags);
 	}
 
 	spin_unlock_irq(&css_set_lock);
@@ -6303,6 +6366,10 @@ void cgroup_post_fork(struct task_struct *child,
 		put_css_set(rcset);
 	}
 
+	/* Cgroup has to be killed so take down child immediately. */
+	if (unlikely(kill))
+		do_send_sig_info(SIGKILL, SEND_SIG_NOINFO, child, PIDTYPE_TGID);
+
 	cgroup_css_set_put_fork(kargs);
 }
 
diff --git kernel/cred.c kernel/cred.c
index 54042ebed..1349ce8d3 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -17,6 +17,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -112,6 +114,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 #endif
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -451,6 +454,8 @@ int commit_creds(struct cred *new)
 #endif
 	BUG_ON(atomic_long_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -690,6 +695,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	if (daemon)
 		old = get_task_cred(daemon);
 	else
diff --git kernel/entry/common.c kernel/entry/common.c
index 09f58853f..0c0f096c7 100644
--- kernel/entry/common.c
+++ kernel/entry/common.c
@@ -195,6 +195,10 @@ static void exit_to_user_mode_prepare(struct pt_regs *regs)
 
 	lockdep_assert_irqs_disabled();
 
+
+	// Make sure we are on the current domain before exiting to userspace
+	bhv_domain_enter(current);
+
 	if (unlikely(ti_work & EXIT_TO_USER_MODE_WORK))
 		ti_work = exit_to_user_mode_loop(regs, ti_work);
 
diff --git kernel/events/core.c kernel/events/core.c
index 55033d6c0..5198b294f 100644
--- kernel/events/core.c
+++ kernel/events/core.c
@@ -8039,6 +8039,10 @@ void perf_event_namespaces(struct task_struct *task)
 	perf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],
 			       task, &cgroupns_operations);
 #endif
+#ifdef CONFIG_MEM_NS
+	perf_fill_ns_link_info(&ns_link_info[MEM_NS_INDEX],
+			       task, &memns_operations);
+#endif
 
 	perf_iterate_sb(perf_event_namespaces_output,
 			&namespaces_event,
diff --git kernel/exit.c kernel/exit.c
index af9c8e794..266cacb4a 100644
--- kernel/exit.c
+++ kernel/exit.c
@@ -70,6 +70,8 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 /*
  * The default value should be high enough to not crash a system that randomly
  * crashes its kernel from time to time, but low enough to at least not permit
diff --git kernel/extable.c kernel/extable.c
index b0ea5eb0c..c21ddcc30 100644
--- kernel/extable.c
+++ kernel/extable.c
@@ -15,6 +15,8 @@
 #include <asm/sections.h>
 #include <linux/uaccess.h>
 
+#include <bhv/vault.h>
+
 /*
  * mutex protecting text section modification (dynamic code patching).
  * some users need to sleep (allocating memory...) while they hold this lock.
@@ -62,6 +64,7 @@ const struct exception_table_entry *search_exception_tables(unsigned long addr)
 	return e;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int init_kernel_text(unsigned long addr)
 {
 	if (addr >= (unsigned long)_sinittext &&
@@ -70,6 +73,7 @@ int init_kernel_text(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int notrace core_kernel_text(unsigned long addr)
 {
 	if (addr >= (unsigned long)_stext &&
@@ -100,6 +104,7 @@ int core_kernel_data(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int __kernel_text_address(unsigned long addr)
 {
 	if (kernel_text_address(addr))
@@ -117,6 +122,7 @@ int __kernel_text_address(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int kernel_text_address(unsigned long addr)
 {
 	bool no_rcu;
diff --git kernel/fork.c kernel/fork.c
index 8b8a5a172..8d0feae4d 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -96,6 +96,9 @@
 #include <linux/kasan.h>
 #include <linux/scs.h>
 #include <linux/io_uring.h>
+#include <linux/mem_namespace.h>
+
+#include <bhv/creds.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -603,6 +606,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	}
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
+
 out:
 	mmap_write_unlock(mm);
 	flush_tlb_mm(oldmm);
@@ -1094,6 +1098,11 @@ static inline void __mmput(struct mm_struct *mm)
 {
 	VM_BUG_ON(atomic_read(&mm->mm_users));
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_destroy_pgd(current, mm);
+	bhv_domain_debug_destroy_pgd(current, mm);
+#endif
+
 	uprobe_clear_state(mm);
 	exit_aio(mm);
 	ksm_exit(mm);
@@ -1913,6 +1922,8 @@ static __latent_entropy struct task_struct *copy_process(
 	/*
 	 * If the new process will be in a different pid or user namespace
 	 * do not allow it to share a thread group with the forking task.
+	 *
+	 * XXX: Consider adding additional constraints for memory namespaces.
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
@@ -2108,9 +2119,12 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2123,15 +2137,15 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_signal(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_sighand;
-	retval = copy_mm(clone_flags, p);
+	retval = copy_namespaces(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_signal;
-	retval = copy_namespaces(clone_flags, p);
+	retval = copy_mm(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_mm;
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_io(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_mm;
 	retval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
@@ -2361,13 +2375,13 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);
-bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm) {
 		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
 	}
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2379,6 +2393,7 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
@@ -2878,7 +2893,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
 				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+				CLONE_NEWTIME|CLONE_NEWMEM))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -2981,6 +2996,11 @@ int ksys_unshare(unsigned long unshare_flags)
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
 
+	/*
+	 * XXX: Consider CLONE_NEWMEM! Do we need to unshare the thread group
+	 * via CLONE_THREAD?
+	 */
+
 	err = check_unshare_flags(unshare_flags);
 	if (err)
 		goto bad_unshare_out;
@@ -3006,6 +3026,15 @@ int ksys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_cleanup_cred;
 
 	if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
+		struct nsset nsset = { .flags = unshare_flags,
+				       .nsproxy = new_nsproxy,
+				       .fs = new_fs,
+				       .cred = new_cred };
+
+		err = security_unshare(current, &nsset);
+		if (err)
+			goto bad_unshare_cleanup_cred;
+
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
diff --git kernel/jump_label.c kernel/jump_label.c
index 4ae693ce7..b59a3fea0 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,9 +19,16 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
+#include <bhv/vault.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
+static int jump_label_cmp(const void *a, const void *b);
+static void jump_label_swap(void *a, void *b, int size);
+
 void jump_label_lock(void)
 {
 	mutex_lock(&jump_label_mutex);
@@ -32,6 +39,17 @@ void jump_label_unlock(void)
 	mutex_unlock(&jump_label_mutex);
 }
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_lock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_unlock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, lockdep_assert_cpus_held)
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER2(int, jump_label_cmp, const void *, a, const void *, b);
+BHV_VAULT_FN_WRAPPER3_NORET(void, jump_label_swap, void *, a, void *, b, int, s);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_cmp(const void *a, const void *b)
 {
 	const struct jump_entry *jea = a;
@@ -59,7 +77,9 @@ static int jump_label_cmp(const void *a, const void *b)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_cmp);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_swap(void *a, void *b, int size)
 {
 	long delta = (unsigned long)a - (unsigned long)b;
@@ -75,7 +95,9 @@ static void jump_label_swap(void *a, void *b, int size)
 	jeb->target	= tmp.target + delta;
 	jeb->key	= tmp.key + delta;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_swap);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void
 jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 {
@@ -83,13 +105,23 @@ jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 	void *swapfn = NULL;
 
 	if (IS_ENABLED(CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE))
+#ifdef CONFIG_BHV_VAULT_SPACES
+		swapfn = bhv_wrapper_jump_label_swap;
+#else
 		swapfn = jump_label_swap;
+#endif
 
 	size = (((unsigned long)stop - (unsigned long)start)
 					/ sizeof(struct jump_entry));
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	sort(start, size, sizeof(struct jump_entry), bhv_wrapper_jump_label_cmp, swapfn);
+#else
 	sort(start, size, sizeof(struct jump_entry), jump_label_cmp, swapfn);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key);
 
 /*
@@ -101,6 +133,7 @@ static void jump_label_update(struct static_key *key);
  * 'static_key_disable()', which require bug.h. This should allow jump_label.h
  * to be included from most/all places for CONFIG_JUMP_LABEL.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int static_key_count(struct static_key *key)
 {
 	/*
@@ -111,14 +144,20 @@ int static_key_count(struct static_key *key)
 
 	return n >= 0 ? n : 1;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_count);
 EXPORT_SYMBOL_GPL(static_key_count);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_slow_inc_cpuslocked(struct static_key *key)
 {
 	int v, v1;
 
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	/*
 	 * Careful if we get concurrent static_key_slow_inc() calls;
@@ -138,7 +177,11 @@ void static_key_slow_inc_cpuslocked(struct static_key *key)
 			return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_read(&key->enabled) == 0) {
 		atomic_set(&key->enabled, -1);
 		jump_label_update(key);
@@ -150,8 +193,13 @@ void static_key_slow_inc_cpuslocked(struct static_key *key)
 	} else {
 		atomic_inc(&key->enabled);
 	}
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_slow_inc_cpuslocked);
 
 void static_key_slow_inc(struct static_key *key)
 {
@@ -161,17 +209,26 @@ void static_key_slow_inc(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_slow_inc);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_enable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) > 0) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_read(&key->enabled) == 0) {
 		atomic_set(&key->enabled, -1);
 		jump_label_update(key);
@@ -180,8 +237,13 @@ void static_key_enable_cpuslocked(struct static_key *key)
 		 */
 		atomic_set_release(&key->enabled, 1);
 	}
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_enable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_enable_cpuslocked);
 
 void static_key_enable(struct static_key *key)
@@ -192,21 +254,35 @@ void static_key_enable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_enable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_disable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) != 1) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_cmpxchg(&key->enabled, 1, 0))
 		jump_label_update(key);
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_disable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_disable_cpuslocked);
 
 void static_key_disable(struct static_key *key)
@@ -217,6 +293,7 @@ void static_key_disable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_disable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool static_key_slow_try_dec(struct static_key *key)
 {
 	int val;
@@ -236,18 +313,32 @@ static bool static_key_slow_try_dec(struct static_key *key)
 	return true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_key_slow_dec_cpuslocked(struct static_key *key)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (static_key_slow_try_dec(key))
 		return;
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_dec_and_test(&key->enabled))
 		jump_label_update(key);
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_key_slow_dec_cpuslocked);
 
 static void __static_key_slow_dec(struct static_key *key)
 {
@@ -306,6 +397,7 @@ void jump_label_rate_limit(struct static_key_deferred *key,
 }
 EXPORT_SYMBOL_GPL(jump_label_rate_limit);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 {
 	if (jump_entry_code(entry) <= (unsigned long)end &&
@@ -315,6 +407,7 @@ static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_text_reserved(struct jump_entry *iter_start,
 		struct jump_entry *iter_stop, void *start, void *end, bool init)
 {
@@ -344,27 +437,32 @@ void __weak __init_or_module arch_jump_label_transform_static(struct jump_entry
 	arch_jump_label_transform(entry, type);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct jump_entry *static_key_entries(struct static_key *key)
 {
 	WARN_ON_ONCE(key->type & JUMP_TYPE_LINKED);
 	return (struct jump_entry *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_key_type(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_TRUE;
 }
 
-static inline bool static_key_linked(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline bool static_key_linked(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_clear_linked(struct static_key *key)
 {
 	key->type &= ~JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_set_linked(struct static_key *key)
 {
 	key->type |= JUMP_TYPE_LINKED;
@@ -379,6 +477,7 @@ static inline void static_key_set_linked(struct static_key *key)
  * type is in use and to store the initial branch direction, we use an access
  * function which preserves these bits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_entries(struct static_key *key,
 				   struct jump_entry *entries)
 {
@@ -390,6 +489,7 @@ static void static_key_set_entries(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum jump_label_type jump_label_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -400,6 +500,7 @@ static enum jump_label_type jump_label_type(struct jump_entry *entry)
 	return enabled ^ branch;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool jump_label_can_update(struct jump_entry *entry, bool init)
 {
 	/*
@@ -427,6 +528,7 @@ static bool jump_label_can_update(struct jump_entry *entry, bool init)
 }
 
 #ifndef HAVE_JUMP_LABEL_BATCH
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -438,6 +540,7 @@ static void __jump_label_update(struct static_key *key,
 	}
 }
 #else
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -459,8 +562,10 @@ static void __jump_label_update(struct static_key *key,
 	arch_jump_label_transform_apply();
 }
 #endif
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_update);
 
-void __init jump_label_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __jump_label_init(void)
 {
 	struct jump_entry *iter_start = __start___jump_table;
 	struct jump_entry *iter_stop = __stop___jump_table;
@@ -479,8 +584,13 @@ void __init jump_label_init(void)
 	if (static_key_initialized)
 		return;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 	jump_label_sort_entries(iter_start, iter_stop);
 
 	for (iter = iter_start; iter < iter_stop; iter++) {
@@ -501,12 +611,24 @@ void __init jump_label_init(void)
 		static_key_set_entries(key, iter);
 	}
 	static_key_initialized = true;
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_init);
+
+void __init jump_label_init(void) {
+	__jump_label_init();
+}
+
 
 #ifdef CONFIG_MODULES
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -517,24 +639,29 @@ static enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 	return type ^ branch;
 }
 
+#ifndef CONFIG_BHV_VAULT_SPACES
 struct static_key_mod {
 	struct static_key_mod *next;
 	struct jump_entry *entries;
 	struct module *mod;
 };
+#endif
 
-static inline struct static_key_mod *static_key_mod(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline struct static_key_mod *static_key_mod(struct static_key *key)
 {
 	WARN_ON_ONCE(!static_key_linked(key));
 	return (struct static_key_mod *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+
 /***
  * key->type and key->next are the same via union.
  * This sets key->next and preserves the type bits.
  *
  * See additional comments above static_key_set_entries().
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_mod(struct static_key *key,
 			       struct static_key_mod *mod)
 {
@@ -546,6 +673,7 @@ static void static_key_set_mod(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
@@ -570,6 +698,7 @@ static int __jump_label_mod_text_reserved(void *start, void *end)
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_mod_update(struct static_key *key)
 {
 	struct static_key_mod *mod;
@@ -603,6 +732,7 @@ static void __jump_label_mod_update(struct static_key *key)
  * loads patch these with arch_get_jump_label_nop(), which is specified by
  * the arch specific jump label code.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void jump_label_apply_nops(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -619,7 +749,9 @@ void jump_label_apply_nops(struct module *mod)
 			arch_jump_label_transform_static(iter, JUMP_LABEL_NOP);
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_apply_nops);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_add_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -627,6 +759,9 @@ static int jump_label_add_module(struct module *mod)
 	struct jump_entry *iter;
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, *jlm2;
+#ifndef CONFIG_BHV_VAULT_SPACES
+	int rc;
+#endif
 
 	/* if the module doesn't have jump label entries, just return */
 	if (iter_start == iter_stop)
@@ -634,6 +769,11 @@ static int jump_label_add_module(struct module *mod)
 
 	jump_label_sort_entries(iter_start, iter_stop);
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if ((rc = bhv_jump_label_add_module(mod)))
+		return rc;
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		struct static_key *iterk;
 
@@ -681,6 +821,7 @@ static int jump_label_add_module(struct module *mod)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_del_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -689,6 +830,10 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	bhv_jump_label_del_module(mod);
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
@@ -731,6 +876,7 @@ static void jump_label_del_module(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int
 jump_label_module_notify(struct notifier_block *self, unsigned long val,
 			 void *data)
@@ -738,8 +884,13 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 	struct module *mod = data;
 	int ret = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -754,11 +905,17 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 		break;
 	}
 
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
 
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_module_notify);
 
 static struct notifier_block jump_label_module_nb = {
 	.notifier_call = jump_label_module_notify,
@@ -786,6 +943,7 @@ early_initcall(jump_label_init_module);
  *
  * returns 1 if there is an overlap, 0 otherwise
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int jump_label_text_reserved(void *start, void *end)
 {
 	bool init = system_state < SYSTEM_RUNNING;
@@ -800,7 +958,9 @@ int jump_label_text_reserved(void *start, void *end)
 #endif
 	return ret;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_text_reserved);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key)
 {
 	struct jump_entry *stop = __stop___jump_table;
@@ -825,6 +985,7 @@ static void jump_label_update(struct static_key *key)
 		__jump_label_update(key, entry, stop,
 				    system_state < SYSTEM_RUNNING);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_update);
 
 #ifdef CONFIG_STATIC_KEYS_SELFTEST
 static DEFINE_STATIC_KEY_TRUE(sk_true);
diff --git kernel/kmod.c kernel/kmod.c
index 3cd075ce2..8adb2acbd 100644
--- kernel/kmod.c
+++ kernel/kmod.c
@@ -28,6 +28,8 @@
 
 #include <trace/events/module.h>
 
+#include <bhv/config.h>
+
 /*
  * Assuming:
  *
@@ -58,6 +60,12 @@ static DECLARE_WAIT_QUEUE_HEAD(kmod_wq);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define MODPROBE_PATH bhv_policy_get_modprobe_path()
+#else
+#define MODPROBE_PATH modprobe_path
+#endif
+
 char modprobe_path[KMOD_PATH_LEN] = "/sbin/modprobe";
 
 static void free_modprobe_argv(struct subprocess_info *info)
@@ -84,13 +92,13 @@ static int call_modprobe(char *module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)MODPROBE_PATH;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
 	argv[4] = NULL;
 
-	info = call_usermodehelper_setup(modprobe_path, argv, envp, GFP_KERNEL,
+	info = call_usermodehelper_setup(MODPROBE_PATH, argv, envp, GFP_KERNEL,
 					 NULL, free_modprobe_argv, NULL);
 	if (!info)
 		goto free_module_name;
@@ -135,7 +143,7 @@ int __request_module(bool wait, const char *fmt, ...)
 	 */
 	WARN_ON_ONCE(wait && current_is_async());
 
-	if (!modprobe_path[0])
+	if (!MODPROBE_PATH[0])
 		return -ENOENT;
 
 	va_start(args, fmt);
diff --git kernel/kthread.c kernel/kthread.c
index 9d6cc9c15..6f9677fb4 100644
--- kernel/kthread.c
+++ kernel/kthread.c
@@ -30,6 +30,7 @@
 #include <linux/sched/isolation.h>
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
@@ -1342,6 +1343,9 @@ void kthread_use_mm(struct mm_struct *mm)
 	tsk->mm = mm;
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
@@ -1374,6 +1378,7 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	/* active_mm is still 'mm' */
 	enter_lazy_tlb(mm, tsk);
 	local_irq_enable();
+	bhv_domain_enter(NULL);
 	task_unlock(tsk);
 }
 EXPORT_SYMBOL_GPL(kthread_unuse_mm);
diff --git kernel/mem_namespace.c kernel/mem_namespace.c
new file mode 100644
index 000000000..2d993160a
--- /dev/null
+++ kernel/mem_namespace.c
@@ -0,0 +1,255 @@
+#include <linux/user_namespace.h>
+#include <linux/mem_namespace.h>
+#include <linux/proc_ns.h>
+#include <linux/cred.h>
+#include <linux/sched/task.h>
+#include <linux/slab.h>
+
+#include <bhv/domain.h>
+
+uint64_t get_free_domain(void)
+{
+	uint64_t domain = BHV_INVALID_DOMAIN;
+	bhv_domain_create(&domain);
+	return domain;
+}
+
+void put_domain(uint64_t domain)
+{
+	/*
+	 * XXX: Do we need to destroy nested, higher-level domains that belong
+	 * to the acestor tree of this domain if they are still around?
+	 */
+
+	if (domain == BHV_INVALID_DOMAIN)
+		return;
+
+	/*
+	 * We assume that the caller takes the necessary steps to switch to
+	 * another, valid domain before putting/releasing the given domain.
+	 */
+
+	BUG_ON(bhv_get_domain(current) == domain);
+
+	bhv_domain_destroy(domain);
+}
+
+static struct kmem_cache *mem_ns_cache;
+
+struct mem_namespace init_mem_ns = {
+	.kref = KREF_INIT(2),
+	.user_ns = &init_user_ns,
+	.domain = BHV_INIT_DOMAIN,
+	.ns.inum = PROC_MEM_INIT_INO,
+	.level = 0,
+	.parent = NULL,
+#ifdef CONFIG_MEM_NS
+	.ns.ops = &memns_operations,
+#endif
+};
+EXPORT_SYMBOL_GPL(init_mem_ns);
+
+struct mem_namespace *memns_of_task(const struct task_struct *task)
+{
+	/*
+	 * Kernel threads, and threads that do not act on behalf of a user space
+	 * task, do not have a valid nsproxy. These threads shall switch to the
+	 * default memory namespace that we use for the init_task.
+	 * Alternatively, we can define a dedicated memory namespace, which all
+	 * kernel threads will enter if they do not execute on behalf of a user
+	 * space task.
+	 */
+	if (task == NULL || task->nsproxy == NULL)
+		return init_task.nsproxy->mem_ns;
+
+	return task->nsproxy->mem_ns;
+}
+
+bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return memns_of_task(current) == memns_of_task(task);
+}
+
+static struct ucounts *inc_mem_namespaces(struct user_namespace *ns)
+{
+	return inc_ucount(ns, current_euid(), UCOUNT_MEM_NAMESPACES);
+}
+
+static void dec_mem_namespaces(struct ucounts *ucounts)
+{
+	dec_ucount(ucounts, UCOUNT_MEM_NAMESPACES);
+}
+
+static struct mem_namespace *create_mem_namespace(struct user_namespace *user_ns,
+						  struct mem_namespace *parent_ns)
+{
+	struct mem_namespace *ns = NULL;
+	unsigned int level = parent_ns->level + 1;
+	struct ucounts *ucounts;
+	uint64_t domain = 0;
+	int err = -EINVAL;
+
+	if (!in_userns(parent_ns->user_ns, user_ns))
+		goto out;
+
+	/* XXX: Consider limiting the number of nested memory namespaces. */
+
+	domain = get_free_domain();
+	if (domain == BHV_INVALID_DOMAIN && bhv_domain_is_active())
+		goto out;
+
+	ucounts = inc_mem_namespaces(user_ns);
+	if (!ucounts)
+		goto out_domain;
+
+	err = -ENOMEM;
+	ns = kmem_cache_zalloc(mem_ns_cache, GFP_KERNEL);
+	if (ns == NULL)
+		goto out_dec;
+
+	err = ns_alloc_inum(&ns->ns);
+	if (err)
+		goto out_free;
+
+	kref_init(&ns->kref);
+	ns->level = level;
+	ns->parent = get_mem_ns(parent_ns);
+	ns->ns.ops = &memns_operations;
+	ns->domain = domain;
+	ns->user_ns = get_user_ns(user_ns);
+	ns->ucounts = ucounts;
+
+	return ns;
+
+out_free:
+	kmem_cache_free(mem_ns_cache, ns);
+out_dec:
+	dec_mem_namespaces(ucounts);
+out_domain:
+	put_domain(domain);
+out:
+	return ERR_PTR(err);
+}
+
+struct mem_namespace *copy_mem_ns(unsigned long flags,
+				  struct user_namespace *user_ns,
+				  struct mem_namespace *old_ns)
+{
+	BUG_ON(!old_ns);
+
+	if (!(flags & CLONE_NEWMEM)) {
+		return get_mem_ns(old_ns);
+	}
+
+	/*
+	 * XXX: Consider performing additional checks (see pid_namespaces.c); we
+	 * shall proceed only if the old_ns corresponds to the namespace, in
+	 * which the current task resides.
+	 */
+
+	return create_mem_namespace(user_ns, old_ns);
+}
+
+static void destroy_mem_namespace(struct mem_namespace *ns)
+{
+	put_domain(ns->domain);
+	ns_free_inum(&ns->ns);
+
+	/*
+	 * XXX: Make the namespace leverage RCU (see pid_namespace.c)!
+	 */
+
+	dec_mem_namespaces(ns->ucounts);
+	put_user_ns(ns->user_ns);
+
+	kmem_cache_free(mem_ns_cache, ns);
+}
+
+void free_mem_ns(struct kref *kref)
+{
+	struct mem_namespace *ns = container_of(kref, struct mem_namespace, kref);
+	destroy_mem_namespace(ns);
+}
+
+static inline struct mem_namespace *to_mem_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct mem_namespace, ns);
+}
+
+static struct ns_common *memns_get(struct task_struct *task)
+{
+	struct mem_namespace *ns = NULL;
+	struct nsproxy *nsproxy;
+
+	task_lock(task);
+	nsproxy = task->nsproxy;
+	if (nsproxy) {
+		ns = nsproxy->mem_ns;
+		get_mem_ns(ns);
+	}
+	task_unlock(task);
+
+	return ns ? &ns->ns : NULL;
+}
+
+static void memns_put(struct ns_common *ns)
+{
+	put_mem_ns(to_mem_ns(ns));
+}
+
+bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns)
+{
+	struct mem_namespace *task_ns = memns_of_task(task);
+	struct mem_namespace *ancestor = ns;
+
+	if (ancestor->level < task_ns->level)
+		return false;
+
+	while (ancestor->level > task_ns->level) {
+		ancestor = ancestor->parent;
+	}
+
+	return (ancestor == task_ns);
+}
+
+static int memns_install(struct nsset *nsset, struct ns_common *ns)
+{
+	struct nsproxy *nsproxy = nsset->nsproxy;
+	struct mem_namespace *new = to_mem_ns(ns);
+
+	if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
+	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!task_in_ancestor_memns(current, new))
+		return -EINVAL;
+
+	put_mem_ns(nsproxy->mem_ns);
+	nsproxy->mem_ns = get_mem_ns(new);
+
+	/* XXX: Do we need an explicit mem_ns_for_children? */
+
+	return 0;
+}
+
+static struct user_namespace *memns_owner(struct ns_common *ns)
+{
+	return to_mem_ns(ns)->user_ns;
+}
+
+const struct proc_ns_operations memns_operations = {
+	.name		= "mem",
+	.type		= CLONE_NEWMEM,
+	.get		= memns_get,
+	.put		= memns_put,
+	.install	= memns_install,
+	.owner		= memns_owner,
+};
+
+static int __init mem_ns_init(void)
+{
+	mem_ns_cache = KMEM_CACHE(mem_namespace, SLAB_PANIC);
+	return 0;
+}
+
+__initcall(mem_ns_init);
diff --git kernel/module.c kernel/module.c
index edc7b99cb..b6fb95a2a 100644
--- kernel/module.c
+++ kernel/module.c
@@ -60,6 +60,9 @@
 #include <uapi/linux/module.h>
 #include "module-internal.h"
 
+#include <bhv/module.h>
+#include <bhv/vault.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -88,7 +91,8 @@
  * 3) module_addr_min/module_addr_max.
  * (delete and add uses RCU list operations). */
 DEFINE_MUTEX(module_mutex);
-static LIST_HEAD(modules);
+EXPORT_SYMBOL_GPL(module_mutex);
+LIST_HEAD(modules);
 
 /* Work queue for freeing init sections in success case */
 static void do_free_init(struct work_struct *w);
@@ -267,6 +271,14 @@ static void module_assert_mutex_or_preempt(void)
 }
 
 #ifdef CONFIG_MODULE_SIG
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+#define sig_enforce true
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
@@ -274,6 +286,7 @@ void set_module_sig_enforced(void)
 {
 	sig_enforce = true;
 }
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 #else
 #define sig_enforce false
 #endif
@@ -2244,6 +2257,8 @@ static void free_module(struct module *mod)
 	synchronize_rcu();
 	mutex_unlock(&module_mutex);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
@@ -3625,6 +3640,7 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
 	module_memfree(mod->core_layout.base);
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -3769,6 +3785,7 @@ static noinline int do_init_module(struct module *mod)
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	mod->init_layout.base = NULL;
 	mod->init_layout.size = 0;
 	mod->init_layout.ro_size = 0;
@@ -4083,10 +4100,14 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
 
+	security_module_loaded(mod);
+
 	/* Module is ready to execute: parsing args may do that. */
 	after_dashes = parse_args(mod->name, mod->args, mod->kp, mod->num_kp,
 				  -32768, 32767, mod,
@@ -4653,6 +4674,7 @@ bool is_module_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_address(unsigned long addr)
 {
 	struct module *mod;
@@ -4679,6 +4701,7 @@ struct module *__module_address(unsigned long addr)
  * anywhere in a module.  See kernel_text_address() for testing if an
  * address corresponds to kernel or module code.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_module_text_address(unsigned long addr)
 {
 	bool ret;
@@ -4697,6 +4720,7 @@ bool is_module_text_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_text_address(unsigned long addr)
 {
 	struct module *mod = __module_address(addr);
diff --git kernel/nsproxy.c kernel/nsproxy.c
index 12dd41b39..7589e03fc 100644
--- kernel/nsproxy.c
+++ kernel/nsproxy.c
@@ -16,6 +16,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/utsname.h>
 #include <linux/pid_namespace.h>
+#include <linux/mem_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
 #include <linux/time_namespace.h>
@@ -27,6 +28,8 @@
 #include <linux/cgroup.h>
 #include <linux/perf_event.h>
 
+#include <bhv/domain.h>
+
 static struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = {
@@ -47,6 +50,9 @@ struct nsproxy init_nsproxy = {
 	.time_ns		= &init_time_ns,
 	.time_ns_for_children	= &init_time_ns,
 #endif
+#ifdef CONFIG_MEM_NS
+	.mem_ns			= &init_mem_ns,
+#endif
 };
 
 static inline struct nsproxy *create_nsproxy(void)
@@ -75,6 +81,10 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	if (!new_nsp)
 		return ERR_PTR(-ENOMEM);
 
+	if (bhv_check_memns_enable_flags(flags)) {
+		flags |= CLONE_NEWMEM;
+	}
+
 	new_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);
 	if (IS_ERR(new_nsp->mnt_ns)) {
 		err = PTR_ERR(new_nsp->mnt_ns);
@@ -121,8 +131,19 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	}
 	new_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);
 
+	new_nsp->mem_ns = copy_mem_ns(flags, user_ns, tsk->nsproxy->mem_ns);
+	if (IS_ERR(new_nsp->mem_ns)) {
+		err = PTR_ERR(new_nsp->mem_ns);
+		goto out_mem;
+	}
+
 	return new_nsp;
 
+out_mem:
+	if (new_nsp->time_ns)
+		put_time_ns(new_nsp->time_ns);
+	if (new_nsp->time_ns_for_children)
+		put_time_ns(new_nsp->time_ns_for_children);
 out_time:
 	put_net(new_nsp->net_ns);
 out_net:
@@ -157,7 +178,7 @@ int copy_namespaces(unsigned long flags, struct task_struct *tsk)
 
 	if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			      CLONE_NEWPID | CLONE_NEWNET |
-			      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {
+			      CLONE_NEWCGROUP | CLONE_NEWTIME | CLONE_NEWMEM)))) {
 		if (likely(old_ns->time_ns_for_children == old_ns->time_ns)) {
 			get_nsproxy(old_ns);
 			return 0;
@@ -204,6 +225,8 @@ void free_nsproxy(struct nsproxy *ns)
 		put_time_ns(ns->time_ns);
 	if (ns->time_ns_for_children)
 		put_time_ns(ns->time_ns_for_children);
+	if (ns->mem_ns)
+		put_mem_ns(ns->mem_ns);
 	put_cgroup_ns(ns->cgroup_ns);
 	put_net(ns->net_ns);
 	kmem_cache_free(nsproxy_cachep, ns);
@@ -221,7 +244,7 @@ int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 
 	if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |
-			       CLONE_NEWTIME)))
+			       CLONE_NEWTIME | CLONE_NEWMEM)))
 		return 0;
 
 	user_ns = new_cred ? new_cred->user_ns : current_user_ns();
@@ -250,6 +273,31 @@ void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
 	p->nsproxy = new;
 	task_unlock(p);
 
+	/*
+	 * Move the task's address space to the given domain only if we do not
+	 * destroy the nsproxy that the task is about to switch to is valid.
+	 * Switching to an invalid nsproxy (nsproxy == NULL) means that the task
+	 * is about to be destroyed.
+	 */
+	if (new != NULL) {
+		bhv_domain_transfer_mm(p->mm, ns, new);
+
+		// If we change the domain of the current process, we need to switch.
+		if (current == p) {
+			bhv_domain_enter(p);
+		}
+	} else {
+		/*
+		* Note that  bhv_domain_enter will automatically determine which domain
+		* to switch to (i.e., to the task's domain maintained by its nsproxy or
+		* to the default domain of init_task).
+		*
+		* XXX: If the task switches to an invalid nsproxy, we should consider
+		* switching to the parent's domain.
+		*/
+		bhv_domain_enter(p);
+	}
+
 	if (ns && atomic_dec_and_test(&ns->count))
 		free_nsproxy(ns);
 }
@@ -263,7 +311,7 @@ static int check_setns_flags(unsigned long flags)
 {
 	if (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 				 CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |
-				 CLONE_NEWPID | CLONE_NEWCGROUP)))
+				 CLONE_NEWPID | CLONE_NEWCGROUP | CLONE_NEWMEM)))
 		return -EINVAL;
 
 #ifndef CONFIG_USER_NS
@@ -294,6 +342,10 @@ static int check_setns_flags(unsigned long flags)
 	if (flags & CLONE_NEWTIME)
 		return -EINVAL;
 #endif
+#ifndef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM)
+		return -EINVAL;
+#endif
 
 	return 0;
 }
@@ -476,6 +528,14 @@ static int validate_nsset(struct nsset *nsset, struct pid *pid)
 	}
 #endif
 
+#ifdef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM) {
+		ret = validate_ns(nsset, &nsp->mem_ns->ns);
+		if (ret)
+			goto out;
+	}
+#endif
+
 out:
 	if (pid_ns)
 		put_pid_ns(pid_ns);
@@ -546,6 +606,10 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 			err = -EINVAL;
 		flags = ns->ops->type;
 	} else if (!IS_ERR(pidfd_pid(file))) {
+		if (bhv_check_memns_enable_flags(flags)) {
+			flags |= CLONE_NEWMEM;
+		}
+
 		err = check_setns_flags(flags);
 	} else {
 		err = -EINVAL;
@@ -558,12 +622,26 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 		goto out;
 
 	if (proc_ns_file(file))
+#ifdef CONFIG_MEM_NS
+		/*
+		 * XXX: Note that we cannot piggy-back memory namespaces onto
+		 * pid namespaces during nsset/nsenter if no pidfd is given.
+		 * This is because we cannot identify the associated memory
+		 * namespace without any additional links between the
+		 * pid_namespace and the mem_namespace data structures. Consider
+		 * adding links or using pid namespaces alone for creating
+		 * memory isolation domains.
+		 */
+#endif
 		err = validate_ns(&nsset, ns);
 	else
 		err = validate_nsset(&nsset, file->private_data);
 	if (!err) {
-		commit_nsset(&nsset);
-		perf_event_namespaces(current);
+		err = security_setns(current, &nsset);
+		if (!err) {
+			commit_nsset(&nsset);
+			perf_event_namespaces(current);
+		}
 	}
 	put_nsset(&nsset);
 out:
diff --git kernel/printk/printk.c kernel/printk/printk.c
index a8af93cbc..e3eef9f45 100644
--- kernel/printk/printk.c
+++ kernel/printk/printk.c
@@ -932,7 +932,7 @@ static int devkmsg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations kmsg_fops = {
+const struct file_operations kmsg_fops __section(".rodata") = {
 	.open = devkmsg_open,
 	.read = devkmsg_read,
 	.write_iter = devkmsg_write,
diff --git kernel/rcu/tree.c kernel/rcu/tree.c
index 06bfe61d3..2c57f66a3 100644
--- kernel/rcu/tree.c
+++ kernel/rcu/tree.c
@@ -63,6 +63,8 @@
 #include <linux/kasan.h>
 #include "../time/tick-internal.h"
 
+#include <bhv/vault.h>
+
 #include "tree.h"
 #include "rcu.h"
 
@@ -1104,6 +1106,7 @@ static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)
  * Make notrace because it can be called by the internal functions of
  * ftrace, and making this notrace removes unnecessary recursion calls.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 notrace bool rcu_is_watching(void)
 {
 	bool ret;
diff --git kernel/reboot.c kernel/reboot.c
index e297b35fc..d0de2a5f6 100644
--- kernel/reboot.c
+++ kernel/reboot.c
@@ -18,6 +18,8 @@
 #include <linux/syscore_ops.h>
 #include <linux/uaccess.h>
 
+#include <bhv/config.h>
+
 /*
  * this indicates whether you can reboot with ctrl-alt-del: the default is yes
  */
@@ -417,8 +419,15 @@ void ctrl_alt_del(void)
 		kill_cad_pid(SIGINT, 1);
 }
 
-char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+#define POWEROFF_CMD bhv_policy_get_poweroff_cmd()
+static const char reboot_cmd[] __section(".rodata") = "/sbin/reboot";
+#else
+#define POWEROFF_CMD poweroff_cmd
 static const char reboot_cmd[] = "/sbin/reboot";
+#endif
+
+char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
 
 static int run_cmd(const char *cmd)
 {
@@ -459,7 +468,7 @@ static int __orderly_poweroff(bool force)
 {
 	int ret;
 
-	ret = run_cmd(poweroff_cmd);
+	ret = run_cmd(POWEROFF_CMD);
 
 	if (ret && force) {
 		pr_warn("Failed to start orderly shutdown: forcing the issue\n");
diff --git kernel/sched/core.c kernel/sched/core.c
index 29d8fc3a7..00ed249a8 100644
--- kernel/sched/core.c
+++ kernel/sched/core.c
@@ -17,6 +17,10 @@
 #include <linux/kcov.h>
 #include <linux/scs.h>
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+#include <bhv/integrity.h>
+
 #include <asm/switch_to.h>
 #include <asm/tlb.h>
 
@@ -3770,6 +3774,8 @@ context_switch(struct rq *rq, struct task_struct *prev,
 			mmgrab(prev->active_mm);
 		else
 			prev->active_mm = NULL;
+
+		bhv_pt_protect_check_pgd(next->active_mm);
 	} else {                                        // to user
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
diff --git kernel/sysctl.c kernel/sysctl.c
index abe0f16d5..06d2bd7e5 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -77,6 +77,8 @@
 #include <linux/uaccess.h>
 #include <asm/processor.h>
 
+#include <bhv/config.h>
+
 #ifdef CONFIG_X86
 #include <asm/nmi.h>
 #include <asm/stacktrace.h>
@@ -388,6 +390,30 @@ int proc_dostring(struct ctl_table *table, int write,
 			ppos);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+static int proc_domodprobe_path(struct ctl_table *table, int write,
+				void *buffer, size_t *lenp, loff_t *ppos)
+{
+	// No writes allowed.
+	if (write)
+		return -EPERM;
+
+	return _proc_do_string((char *)bhv_policy_get_modprobe_path(),
+			       table->maxlen, 0, buffer, lenp, ppos);
+}
+
+static int proc_dopoweroff_cmd(struct ctl_table *table, int write,
+			       void *buffer, size_t *lenp, loff_t *ppos)
+{
+	// No writes allowed.
+	if (write)
+		return -EPERM;
+
+	return _proc_do_string((char *)bhv_policy_get_poweroff_cmd(),
+			       table->maxlen, 0, buffer, lenp, ppos);
+}
+#endif
+
 static void proc_skip_spaces(char **buf, size_t *size)
 {
 	while (*size) {
@@ -1183,7 +1209,12 @@ static void validate_coredump_safety(void)
 {
 #ifdef CONFIG_COREDUMP
 	if (suid_dumpable == SUID_DUMP_ROOT &&
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	    bhv_policy_get_core_pattern()[0] != '/' &&
+	    bhv_policy_get_core_pattern()[0] != '|') {
+#else
 	    core_pattern[0] != '/' && core_pattern[0] != '|') {
+#endif
 		printk(KERN_WARNING
 "Unsafe core_pattern used with fs.suid_dumpable=2.\n"
 "Pipe handler or fully qualified core dump path required.\n"
@@ -1206,7 +1237,17 @@ static int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,
 static int proc_dostring_coredump(struct ctl_table *table, int write,
 		  void *buffer, size_t *lenp, loff_t *ppos)
 {
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	int error;
+
+	if (write)
+		return -EPERM;
+	
+	error = _proc_do_string((char *)bhv_policy_get_core_pattern(),
+				table->maxlen, 0, buffer, lenp, ppos);
+#else
 	int error = proc_dostring(table, write, buffer, lenp, ppos);
+#endif
 	if (!error)
 		validate_coredump_safety();
 	return error;
@@ -2028,7 +2069,11 @@ static struct ctl_table kern_table[] = {
 		.procname	= "core_pattern",
 		.data		= core_pattern,
 		.maxlen		= CORENAME_MAX_SIZE,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+#else
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring_coredump,
 	},
 	{
@@ -2196,8 +2241,13 @@ static struct ctl_table kern_table[] = {
 		.procname	= "modprobe",
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+		.proc_handler	= proc_domodprobe_path,
+#else
 		.mode		= 0644,
 		.proc_handler	= proc_dostring,
+#endif
 	},
 	{
 		.procname	= "modules_disabled",
@@ -2659,8 +2709,13 @@ static struct ctl_table kern_table[] = {
 		.procname	= "poweroff_cmd",
 		.data		= &poweroff_cmd,
 		.maxlen		= POWEROFF_CMD_PATH_LEN,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+		.proc_handler	= proc_dopoweroff_cmd,
+#else
 		.mode		= 0644,
 		.proc_handler	= proc_dostring,
+#endif
 	},
 #ifdef CONFIG_KEYS
 	{
diff --git kernel/trace/Kconfig kernel/trace/Kconfig
index 467975300..067e15270 100644
--- kernel/trace/Kconfig
+++ kernel/trace/Kconfig
@@ -126,6 +126,7 @@ if TRACING_SUPPORT
 menuconfig FTRACE
 	bool "Tracers"
 	default y if DEBUG_KERNEL
+	depends on !BHV_LOCKDOWN
 	help
 	  Enable the kernel tracing infrastructure.
 
@@ -360,6 +361,7 @@ config MMIOTRACE
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
+	depends on !BHV_LOCKDOWN
 	select TRACING
 	help
 	  This tracer hooks to various trace points in the kernel,
diff --git kernel/trace/ftrace.c kernel/trace/ftrace.c
index 31fec924b..0bfe805bc 100644
--- kernel/trace/ftrace.c
+++ kernel/trace/ftrace.c
@@ -36,6 +36,8 @@
 #include <linux/rcupdate.h>
 #include <linux/kprobes.h>
 
+#include <bhv/vault.h>
+
 #include <trace/events/sched.h>
 
 #include <asm/sections.h>
@@ -1048,6 +1050,7 @@ struct ftrace_ops global_ops = {
 /*
  * Used by the stack undwinder to know about dynamic ftrace trampolines.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
 {
 	struct ftrace_ops *op = NULL;
@@ -1082,6 +1085,7 @@ struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
  * not return true for either core_kernel_text() or
  * is_module_text_address().
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_ftrace_trampoline(unsigned long addr)
 {
 	return ftrace_ops_trampoline(addr) != NULL;
diff --git kernel/tracepoint.c kernel/tracepoint.c
index 2dff7f1a2..d837c6217 100644
--- kernel/tracepoint.c
+++ kernel/tracepoint.c
@@ -15,6 +15,8 @@
 #include <linux/sched/task.h>
 #include <linux/static_key.h>
 
+#include <bhv/vault.h>
+
 enum tp_func_state {
 	TP_FUNC_0,
 	TP_FUNC_1,
@@ -42,8 +44,10 @@ struct tp_transition_snapshot {
 };
 
 /* Protected by tracepoints_mutex */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct tp_transition_snapshot tp_transition_snapshot[_NR_TP_TRANSITION_SYNC];
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_get_state(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -54,6 +58,7 @@ static void tp_rcu_get_state(enum tp_transition_sync sync)
 	snapshot->ongoing = true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -67,6 +72,7 @@ static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 }
 
 /* Set to 1 to enable tracepoint debug output */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const int tracepoint_debug;
 
 #ifdef CONFIG_MODULES
@@ -104,6 +110,7 @@ static void tp_stub_func(void)
 	return;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void *allocate_probes(int count)
 {
 	struct tp_probes *p  = kmalloc(struct_size(p, probes, count),
@@ -139,6 +146,7 @@ static __init int release_early_probes(void)
 /* SRCU is initialized at core_initcall */
 postcore_initcall(release_early_probes);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void release_probes(struct tracepoint_func *old)
 {
 	if (old) {
@@ -165,6 +173,7 @@ static inline void release_probes(struct tracepoint_func *old)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void debug_print_probes(struct tracepoint_func *funcs)
 {
 	int i;
@@ -176,6 +185,7 @@ static void debug_print_probes(struct tracepoint_func *funcs)
 		printk(KERN_DEBUG "Probe %d : %p\n", i, funcs[i].func);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static struct tracepoint_func *
 func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	 int prio)
@@ -245,6 +255,7 @@ func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	return old;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *func_remove(struct tracepoint_func **funcs,
 		struct tracepoint_func *tp_func)
 {
@@ -315,6 +326,7 @@ static void *func_remove(struct tracepoint_func **funcs,
 /*
  * Count the number of functions (enum tp_func_state) in a tp_funcs array.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum tp_func_state nr_func_state(const struct tracepoint_func *tp_funcs)
 {
 	if (!tp_funcs)
@@ -341,6 +353,7 @@ static void tracepoint_update_call(struct tracepoint *tp, struct tracepoint_func
 /*
  * Add the probe function to a tracepoint.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_add_func(struct tracepoint *tp,
 			       struct tracepoint_func *func, int prio,
 			       bool warn)
@@ -407,6 +420,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_add_func);
 
 /*
  * Remove a probe function from a tracepoint.
@@ -414,6 +428,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
  * function insures that the original callback is not used anymore. This insured
  * by preempt_disable around the call site.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_remove_func(struct tracepoint *tp,
 		struct tracepoint_func *func)
 {
@@ -479,6 +494,7 @@ static int tracepoint_remove_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_remove_func);
 
 /**
  * tracepoint_probe_register_prio_may_exist -  Connect a probe to a tracepoint with priority
@@ -720,6 +736,7 @@ static void tracepoint_module_going(struct module *mod)
 	mutex_unlock(&tracepoint_module_list_mutex);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_module_notify(struct notifier_block *self,
 		unsigned long val, void *data)
 {
@@ -740,6 +757,7 @@ static int tracepoint_module_notify(struct notifier_block *self,
 	}
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_module_notify);
 
 static struct notifier_block tracepoint_module_nb = {
 	.notifier_call = tracepoint_module_notify,
diff --git kernel/ucount.c kernel/ucount.c
index 8d8874f1c..e9a6c7536 100644
--- kernel/ucount.c
+++ kernel/ucount.c
@@ -78,6 +78,7 @@ static struct ctl_table user_table[] = {
 	UCOUNT_ENTRY("max_fanotify_groups"),
 	UCOUNT_ENTRY("max_fanotify_marks"),
 #endif
+	UCOUNT_ENTRY("max_mem_namespaces"),
 	{ }
 };
 #endif /* CONFIG_SYSCTL */
diff --git kernel/umh.c kernel/umh.c
index 3f646613a..50a705e54 100644
--- kernel/umh.c
+++ kernel/umh.c
@@ -30,6 +30,10 @@
 
 #include <trace/events/module.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CAP_BSET	(void *)1
 #define CAP_PI		(void *)2
 
@@ -107,6 +111,12 @@ static int call_usermodehelper_exec_async(void *data)
 
 	commit_creds(new);
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kernel_exec_events())
+		bhv_guestlog_log_kernel_exec(sub_info->path, sub_info->argv,
+					     sub_info->envp);
+#endif
+
 	retval = kernel_execve(sub_info->path,
 			       (const char *const *)sub_info->argv,
 			       (const char *const *)sub_info->envp);
diff --git lib/Kconfig.debug lib/Kconfig.debug
index 24ca61cf8..294e518b8 100644
--- lib/Kconfig.debug
+++ lib/Kconfig.debug
@@ -527,6 +527,7 @@ endmenu
 
 config DEBUG_KERNEL
 	bool "Kernel debugging"
+	depends on !BHV_LOCKDOWN
 	help
 	  Say Y here if you are developing drivers or trying to debug and
 	  identify kernel problems.
diff --git lib/Kconfig.kgdb lib/Kconfig.kgdb
index 05dae05b6..828c9bc65 100644
--- lib/Kconfig.kgdb
+++ lib/Kconfig.kgdb
@@ -28,6 +28,7 @@ config KGDB_HONOUR_BLOCKLIST
 	bool "KGDB: use kprobe blocklist to prohibit unsafe breakpoints"
 	depends on HAVE_KPROBES
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	select KPROBES
 	default y
 	help
diff --git lib/sort.c lib/sort.c
index 3ad454411..ed914a2f0 100644
--- lib/sort.c
+++ lib/sort.c
@@ -16,6 +16,8 @@
 #include <linux/export.h>
 #include <linux/sort.h>
 
+#include <bhv/vault.h>
+
 /**
  * is_aligned - is this pointer & size okay for word-wide copying?
  * @base: pointer to data
@@ -55,6 +57,7 @@ static bool is_aligned(const void *base, size_t size, unsigned char align)
  * subtract (since the intervening mov instructions don't alter the flags).
  * Gcc 8.1.0 doesn't have that problem.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_32(void *a, void *b, size_t n)
 {
 	do {
@@ -80,6 +83,7 @@ static void swap_words_32(void *a, void *b, size_t n)
  * but it's possible to have 64-bit loads without 64-bit pointers (e.g.
  * x32 ABI).  Are there any cases the kernel needs to worry about?
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_64(void *a, void *b, size_t n)
 {
 	do {
@@ -108,6 +112,7 @@ static void swap_words_64(void *a, void *b, size_t n)
  *
  * This is the fallback if alignment doesn't allow using larger chunks.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_bytes(void *a, void *b, size_t n)
 {
 	do {
@@ -126,10 +131,18 @@ static void swap_bytes(void *a, void *b, size_t n)
 #define SWAP_WORDS_32 (swap_func_t)1
 #define SWAP_BYTES    (swap_func_t)2
 
+/*
+ * XXX: Try to remove this!!
+ */
+#ifdef swap
+#undef swap
+#endif
+
 /*
  * The function pointer is last to make tail calls most efficient if the
  * compiler decides not to inline this function.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_swap(void *a, void *b, size_t size, swap_func_t swap_func)
 {
 	if (swap_func == SWAP_WORDS_64)
@@ -144,6 +157,7 @@ static void do_swap(void *a, void *b, size_t size, swap_func_t swap_func)
 
 #define _CMP_WRAPPER ((cmp_r_func_t)0L)
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int do_cmp(const void *a, const void *b, cmp_r_func_t cmp, const void *priv)
 {
 	if (cmp == _CMP_WRAPPER)
@@ -196,6 +210,7 @@ static size_t parent(size_t i, unsigned int lsbit, size_t size)
  * O(n*n) worst-case behavior and extra memory requirements that make
  * it less suitable for kernel use.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort_r(void *base, size_t num, size_t size,
 	    cmp_r_func_t cmp_func,
 	    swap_func_t swap_func,
@@ -263,6 +278,7 @@ void sort_r(void *base, size_t num, size_t size,
 }
 EXPORT_SYMBOL(sort_r);
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort(void *base, size_t num, size_t size,
 	  cmp_func_t cmp_func,
 	  swap_func_t swap_func)
diff --git lib/string.c lib/string.c
index 4288e0158..ca59a06c3 100644
--- lib/string.c
+++ lib/string.c
@@ -28,6 +28,7 @@
 #include <linux/bug.h>
 #include <linux/errno.h>
 #include <linux/slab.h>
+#include <bhv/vault.h>
 
 #include <asm/byteorder.h>
 #include <asm/word-at-a-time.h>
@@ -794,6 +795,7 @@ EXPORT_SYMBOL(__sysfs_match_string);
  *
  * Do not use memset() to access IO space, use memset_io() instead.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset(void *s, int c, size_t count)
 {
 	char *xs = s;
@@ -816,6 +818,7 @@ EXPORT_SYMBOL(memset);
  * of a byte.  Remember that @count is the number of uint16_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset16(uint16_t *s, uint16_t v, size_t count)
 {
 	uint16_t *xs = s;
@@ -838,6 +841,7 @@ EXPORT_SYMBOL(memset16);
  * of a byte.  Remember that @count is the number of uint32_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset32(uint32_t *s, uint32_t v, size_t count)
 {
 	uint32_t *xs = s;
@@ -860,6 +864,7 @@ EXPORT_SYMBOL(memset32);
  * of a byte.  Remember that @count is the number of uint64_ts to
  * store, not the number of bytes.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memset64(uint64_t *s, uint64_t v, size_t count)
 {
 	uint64_t *xs = s;
@@ -881,6 +886,7 @@ EXPORT_SYMBOL(memset64);
  * You should not use this function to access IO space, use memcpy_toio()
  * or memcpy_fromio() instead.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *memcpy(void *dest, const void *src, size_t count)
 {
 	char *tmp = dest;
@@ -933,6 +939,7 @@ EXPORT_SYMBOL(memmove);
  * @count: The size of the area.
  */
 #undef memcmp
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 __visible int memcmp(const void *cs, const void *ct, size_t count)
 {
 	const unsigned char *su1, *su2;
diff --git mm/filemap.c mm/filemap.c
index 3b0d8c6dd..c74a299b8 100644
--- mm/filemap.c
+++ mm/filemap.c
@@ -2893,6 +2893,7 @@ void filemap_map_pages(struct vm_fault *vmf,
 		last_pgoff = xas.xa_index;
 		if (alloc_set_pte(vmf, page))
 			goto unlock;
+
 		unlock_page(head);
 		goto next;
 unlock:
@@ -2904,7 +2905,9 @@ void filemap_map_pages(struct vm_fault *vmf,
 		if (pmd_trans_huge(*vmf->pmd))
 			break;
 	}
+
 	rcu_read_unlock();
+
 	WRITE_ONCE(file->f_ra.mmap_miss, mmap_miss);
 }
 EXPORT_SYMBOL(filemap_map_pages);
diff --git mm/gup.c mm/gup.c
index 11307a8b2..edf6755e9 100644
--- mm/gup.c
+++ mm/gup.c
@@ -21,6 +21,8 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
+#include <linux/mem_namespace.h>
+
 #include "internal.h"
 
 struct follow_page_context {
@@ -923,6 +925,11 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 	int write = (gup_flags & FOLL_WRITE);
 	int foreign = (gup_flags & FOLL_REMOTE);
 
+#ifdef CONFIG_MEM_NS
+	struct mm_struct *mm = vma->vm_mm;
+	struct task_struct *t = mm->owner;
+#endif
+
 	if (vm_flags & (VM_IO | VM_PFNMAP))
 		return -EFAULT;
 
@@ -930,6 +937,17 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 		return -EFAULT;
 
 	if (write) {
+#ifdef CONFIG_MEM_NS
+		/*
+		 * Grant write access to remote address spaces only if both
+		 * processes are executing inside of the same mem_namespace.
+		 */
+		if (!current_in_same_mem_ns(t)) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
 		if (!(vm_flags & VM_WRITE)) {
 			if (!(gup_flags & FOLL_FORCE))
 				return -EFAULT;
@@ -944,17 +962,42 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 			 */
 			if (!is_cow_mapping(vm_flags))
 				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
 		}
-	} else if (!(vm_flags & VM_READ)) {
-		if (!(gup_flags & FOLL_FORCE))
-			return -EFAULT;
+	} else {
+#ifdef CONFIG_MEM_NS
 		/*
-		 * Is there actually any vma we can reach here which does not
-		 * have VM_MAYREAD set?
+		 * Grant read access to remote address spaces only if both
+		 * processes are part of the same ancestor tree branch the
+		 * target mem_namespace.
 		 */
-		if (!(vm_flags & VM_MAYREAD))
-			return -EFAULT;
+		if (!task_in_ancestor_memns(current, memns_of_task(t))) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
+		if (!(vm_flags & VM_READ)) {
+			if (!(gup_flags & FOLL_FORCE))
+				return -EFAULT;
+			/*
+			 * Is there actually any vma we can reach here which does not
+			 * have VM_MAYREAD set?
+			 */
+			if (!(vm_flags & VM_MAYREAD))
+				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
+		}
 	}
+
 	/*
 	 * gups are always data accesses, not instruction
 	 * fetches, so execute=false here
diff --git mm/khugepaged.c mm/khugepaged.c
index 28e18777e..02eb8fa95 100644
--- mm/khugepaged.c
+++ mm/khugepaged.c
@@ -605,6 +605,11 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 	int none_or_zero = 0, shared = 0, result = 0, referenced = 0;
 	bool writable = false;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte+HPAGE_PMD_NR;
 	     _pte++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -728,9 +733,16 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		result = SCAN_SUCCEED;
 		trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 						    referenced, writable, result);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
 		return 1;
 	}
 out:
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	release_pte_pages(pte, _pte, compound_pagelist);
 	trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 					    referenced, writable, result);
@@ -745,6 +757,12 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 {
 	struct page *src_page, *tmp;
 	pte_t *_pte;
+
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 				_pte++, page++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -761,6 +779,8 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 				 * paravirt calls inside pte_clear here are
 				 * superfluous.
 				 */
+				bhv_domain_clear_pte(vma->vm_mm, address, _pte,
+						     *_pte);
 				pte_clear(vma->vm_mm, address, _pte);
 				spin_unlock(ptl);
 			}
@@ -779,6 +799,7 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 			 * paravirt calls inside pte_clear here are
 			 * superfluous.
 			 */
+			bhv_domain_clear_pte(vma->vm_mm, address, _pte, *_pte);
 			pte_clear(vma->vm_mm, address, _pte);
 			page_remove_rmap(src_page, false);
 			spin_unlock(ptl);
@@ -790,6 +811,10 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 		list_del(&src_page->lru);
 		release_pte_page(src_page);
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 static void khugepaged_alloc_sleep(void)
diff --git mm/maccess.c mm/maccess.c
index f6ea117a6..c51a983a6 100644
--- mm/maccess.c
+++ mm/maccess.c
@@ -5,6 +5,17 @@
 #include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/uaccess.h>
+#include <asm-generic/sections.h>
+
+__always_inline static bool is_vault(const void *addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
 
 bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 		size_t size)
@@ -24,6 +35,10 @@ bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 
 long copy_from_kernel_nofault(void *dst, const void *src, size_t size)
 {
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(src, size))
 		return -ERANGE;
 
@@ -68,6 +83,10 @@ long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr, long count)
 
 	if (unlikely(count <= 0))
 		return 0;
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(unsafe_addr, count))
 		return -ERANGE;
 
diff --git mm/memory.c mm/memory.c
index 218300368..50cac204d 100644
--- mm/memory.c
+++ mm/memory.c
@@ -76,6 +76,8 @@
 
 #include <trace/events/kmem.h>
 
+#include <bhv/domain.h>
+
 #include <asm/io.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
@@ -1676,7 +1678,7 @@ static int insert_page_into_pte_locked(struct mm_struct *mm, pte_t *pte,
 	get_page(page);
 	inc_mm_counter_fast(mm, mm_counter_file(page));
 	page_add_file_rmap(page, false);
-	set_pte_at(mm, addr, pte, mk_pte(page, prot));
+	bhv_domain_set_pte_at_kernel(mm, addr, pte, mk_pte(page, prot));
 	return 0;
 }
 
@@ -1996,7 +1998,10 @@ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	}
 
-	set_pte_at(mm, addr, pte, entry);
+	if (pfn_t_devmap(pfn))
+		set_pte_at(mm, addr, pte, entry);
+	else
+		bhv_domain_set_pte_at_kernel(mm, addr, pte, entry);
 	update_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */
 
 out_unlock:
@@ -3963,6 +3968,7 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 		ret = alloc_set_pte(vmf, page);
 	if (vmf->pte)
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
+
 	return ret;
 }
 
@@ -4987,10 +4993,18 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags & FOLL_WRITE;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (mmap_read_lock_killable(mm))
 		return 0;
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(mm->owner);
+#endif
+
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
@@ -5039,6 +5053,11 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
 		buf += bytes;
 		addr += bytes;
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	mmap_read_unlock(mm);
 
 	return buf - old_buf;
@@ -5200,6 +5219,10 @@ static void clear_gigantic_page(struct page *page,
 	struct page *p = page;
 
 	might_sleep();
+
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
+
 	for (i = 0; i < pages_per_huge_page;
 	     i++, p = mem_map_next(p, page, i)) {
 		cond_resched();
@@ -5225,6 +5248,8 @@ void clear_huge_page(struct page *page,
 		return;
 	}
 
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
 	process_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);
 }
 
diff --git mm/mmap.c mm/mmap.c
index 33ebda838..2575cefc7 100644
--- mm/mmap.c
+++ mm/mmap.c
@@ -2557,6 +2557,7 @@ int expand_downwards(struct vm_area_struct *vma,
 			}
 		}
 	}
+
 	anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
 	validate_mm(mm);
@@ -2650,6 +2651,10 @@ static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 		if (vma->vm_flags & VM_ACCOUNT)
 			nr_accounted += nrpages;
 		vm_stat_account(mm, vma->vm_flags, -nrpages);
+		/*
+		 * XXX: Create a hypercall that registers all VMAs that are to
+		 * be released.
+		 */
 		vma = remove_vma(vma);
 	} while (vma);
 	vm_unacct_memory(nr_accounted);
diff --git mm/page_owner.c mm/page_owner.c
index b735a8eaf..ec20d8392 100644
--- mm/page_owner.c
+++ mm/page_owner.c
@@ -635,7 +635,7 @@ static void init_early_allocated_pages(void)
 		init_zones_in_node(pgdat);
 }
 
-static const struct file_operations proc_page_owner_operations = {
+const struct file_operations proc_page_owner_operations __section(".rodata") = {
 	.read		= read_page_owner,
 };
 
diff --git mm/page_poison.c mm/page_poison.c
index ae0482cde..a5d0f8be8 100644
--- mm/page_poison.c
+++ mm/page_poison.c
@@ -132,8 +132,12 @@ void kernel_poison_pages(struct page *page, int numpages, int enable)
 
 	if (enable)
 		unpoison_pages(page, numpages);
-	else
+	else {
+		bhv_domain_map_kernel(
+			current->mm != NULL ? current->mm : current->active_mm,
+			page_to_pfn(page), numpages, true, true, false);
 		poison_pages(page, numpages);
+	}
 }
 
 #ifndef CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC
diff --git mm/pgtable-generic.c mm/pgtable-generic.c
index 4e640baf9..36edd6ed1 100644
--- mm/pgtable-generic.c
+++ mm/pgtable-generic.c
@@ -12,6 +12,10 @@
 #include <linux/pgtable.h>
 #include <asm/tlb.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 /*
  * If a p?d_bad entry is found while walking page tables, report
  * the error, before resetting entry to p?d_none.  Usually (but
@@ -194,7 +198,11 @@ pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)
 pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 		     pmd_t *pmdp)
 {
-	pmd_t old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
+	pmd_t old;
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pmd(vma->vm_mm, address, pmdp, pmd_mkinvalid(*pmdp));
+#endif
+	old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return old;
 }
diff --git mm/shmem.c mm/shmem.c
index e173d83b4..b509f56f6 100644
--- mm/shmem.c
+++ mm/shmem.c
@@ -247,7 +247,7 @@ static inline void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 
 static const struct super_operations shmem_ops;
 static const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -3899,7 +3899,7 @@ static const struct address_space_operations shmem_aops = {
 	.error_remove_page = generic_error_remove_page,
 };
 
-static const struct file_operations shmem_file_operations = {
+const struct file_operations shmem_file_operations __section(".rodata") = {
 	.mmap		= shmem_mmap,
 	.get_unmapped_area = shmem_get_unmapped_area,
 #ifdef CONFIG_TMPFS
diff --git mm/vmalloc.c mm/vmalloc.c
index d6a4794fa..c70bd1191 100644
--- mm/vmalloc.c
+++ mm/vmalloc.c
@@ -39,9 +39,12 @@
 #include <asm/tlbflush.h>
 #include <asm/shmparam.h>
 
+#include <bhv/vault.h>
+
 #include "internal.h"
 #include "pgalloc-track.h"
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_vmalloc_addr(const void *x)
 {
 	unsigned long addr = (unsigned long)x;
@@ -327,6 +330,7 @@ int map_kernel_range(unsigned long start, unsigned long size, pgprot_t prot,
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int is_vmalloc_or_module_addr(const void *x)
 {
 	/*
diff --git mm/vmscan.c mm/vmscan.c
index e2b8cee1d..8343f461b 100644
--- mm/vmscan.c
+++ mm/vmscan.c
@@ -1452,8 +1452,9 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 		 */
 		if (unlikely(PageTransHuge(page)))
 			destroy_compound_page(page);
-		else
+		else {
 			list_add(&page->lru, &free_pages);
+		}
 		continue;
 
 activate_locked_split:
diff --git net/Kconfig net/Kconfig
index a22c3fb88..1d58c9d59 100644
--- net/Kconfig
+++ net/Kconfig
@@ -292,6 +292,7 @@ config BPF_JIT
 	bool "enable BPF Just In Time compiler"
 	depends on HAVE_CBPF_JIT || HAVE_EBPF_JIT
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	help
 	  Berkeley Packet Filter filtering capabilities are normally handled
 	  by an interpreter. This option allows kernel to generate a native
diff --git net/socket.c net/socket.c
index 2a48aa89c..6bae70004 100644
--- net/socket.c
+++ net/socket.c
@@ -146,7 +146,7 @@ static void sock_show_fdinfo(struct seq_file *m, struct file *f)
  *	in the operation structures but are done directly via the socketcall() multiplexor.
  */
 
-static const struct file_operations socket_file_ops = {
+const struct file_operations socket_file_ops = {
 	.owner =	THIS_MODULE,
 	.llseek =	no_llseek,
 	.read_iter =	sock_read_iter,
@@ -1746,6 +1746,8 @@ struct file *do_accept(struct file *file, unsigned file_flags,
 	if (err < 0)
 		goto out_fd;
 
+	security_socket_accepted(sock, newsock);
+
 	if (upeer_sockaddr) {
 		len = newsock->ops->getname(newsock,
 					(struct sockaddr *)&address, 2);
diff --git net/unix/af_unix.c net/unix/af_unix.c
index b7e9c1238..38ff44cc2 100644
--- net/unix/af_unix.c
+++ net/unix/af_unix.c
@@ -220,7 +220,7 @@ static inline void unix_release_addr(struct unix_address *addr)
  *		- if started by zero, it is abstract name.
  */
 
-static int unix_mkname(struct sockaddr_un *sunaddr, int len, unsigned int *hashp)
+int unix_mkname(struct sockaddr_un *sunaddr, int len, unsigned int *hashp)
 {
 	*hashp = 0;
 
@@ -304,7 +304,7 @@ static inline struct sock *unix_find_socket_byname(struct net *net,
 	return s;
 }
 
-static struct sock *unix_find_socket_byinode(struct inode *i)
+struct sock *unix_find_socket_byinode(struct inode *i)
 {
 	struct sock *s;
 
@@ -932,9 +932,8 @@ out:	mutex_unlock(&u->bindlock);
 	return err;
 }
 
-static struct sock *unix_find_other(struct net *net,
-				    struct sockaddr_un *sunname, int len,
-				    int type, unsigned int hash, int *error)
+struct sock *unix_find_other(struct net *net, struct sockaddr_un *sunname,
+			     int len, int type, unsigned int hash, int *error)
 {
 	struct sock *u;
 	struct path path;
diff --git scripts/link-vmlinux.sh scripts/link-vmlinux.sh
index bf534a323..668e8a8eb 100755
--- scripts/link-vmlinux.sh
+++ scripts/link-vmlinux.sh
@@ -63,7 +63,7 @@ objtool_link()
 {
 	local objtoolopt;
 
-	if [ -n "${CONFIG_VMLINUX_VALIDATION}" ]; then
+	if [ -n "${CONFIG_VMLINUX_VALIDATION}" ] || ( [ -n "${CONFIG_BHV_VAULT_SPACES}" ] && [ -n "${CONFIG_X86_64}" ] ); then
 		objtoolopt="check"
 		if [ -n "${CONFIG_CPU_UNRET_ENTRY}" ]; then
 			objtoolopt="${objtoolopt} --unret"
@@ -83,6 +83,9 @@ objtool_link()
 		if [ -n "${CONFIG_SLS}" ]; then
 			objtoolopt="${objtoolopt} --sls"
 		fi
+		if [ -n "${CONFIG_BHV_VAULT_SPACES}" ]; then
+			objtoolopt="${objtoolopt} --vault"
+		fi
 		info OBJTOOL ${1}
 		tools/objtool/objtool ${objtoolopt} ${1}
 	fi
@@ -109,6 +112,9 @@ vmlinux_link()
 	fi
 
 	if [ "${SRCARCH}" != "um" ]; then
+		if [ "$output" = "vmlinux" ] && [ -n "${CONFIG_BHV_VAULT_SPACES}" ]; then
+			KBUILD_VMLINUX_OBJS="vmlinux.o"
+		fi
 		objects="--whole-archive			\
 			${KBUILD_VMLINUX_OBJS}			\
 			--no-whole-archive			\
@@ -372,9 +378,11 @@ mksysmap vmlinux System.map
 if [ -n "${CONFIG_KALLSYMS}" ]; then
 	mksysmap ${kallsyms_vmlinux} .tmp_System.map
 
-	if ! cmp -s System.map .tmp_System.map; then
-		echo >&2 Inconsistent kallsyms data
-		echo >&2 Try "make KALLSYMS_EXTRA_PASS=1" as a workaround
-		exit 1
+	if [ -z "${CONFIG_BHV_VAULT_SPACES}" ]; then
+		if ! cmp -s System.map .tmp_System.map; then
+			echo >&2 Inconsistent kallsyms data
+			echo >&2 Try "make KALLSYMS_EXTRA_PASS=1" as a workaround
+			exit 1
+		fi
 	fi
 fi
diff --git security/Kconfig security/Kconfig
index 9893c316d..61ce04796 100644
--- security/Kconfig
+++ security/Kconfig
@@ -222,6 +222,15 @@ config STATIC_USERMODEHELPER_PATH
 	  If you wish for all usermode helper programs to be disabled,
 	  specify an empty string here (i.e. "").
 
+config MEM_NS
+	bool "Enable memory namespaces"
+	depends on MEMCG
+	depends on BHV_VAS
+	default y
+	help
+	  Enable memory namespaces.
+
+source "security/bhv/Kconfig"
 source "security/selinux/Kconfig"
 source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
@@ -269,11 +278,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf" if DEFAULT_SECURITY_SMACK
-	default "lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
-	default "lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
+	default "lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "lockdown,yama,loadpin,safesetid,integrity,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list will be ignored. This can be
diff --git security/Makefile security/Makefile
index 3baf435de..aa60209b2 100644
--- security/Makefile
+++ security/Makefile
@@ -10,6 +10,7 @@ subdir-$(CONFIG_SECURITY_TOMOYO)        += tomoyo
 subdir-$(CONFIG_SECURITY_APPARMOR)	+= apparmor
 subdir-$(CONFIG_SECURITY_YAMA)		+= yama
 subdir-$(CONFIG_SECURITY_LOADPIN)	+= loadpin
+subdir-$(CONFIG_BHV_VAS)		+= bhv
 subdir-$(CONFIG_SECURITY_SAFESETID)    += safesetid
 subdir-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown
 subdir-$(CONFIG_BPF_LSM)		+= bpf
@@ -28,6 +29,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_BHV_VAS)			+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 000000000..42bb01583
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,84 @@
+config BHV_VAS
+	bool "BHV guest support and VAS LSM"
+	default y
+	depends on (X86_64) || (ARM64 && OF)
+	select VIRTIO_VSOCKETS
+	select VSOCKETS
+	select VIRTIO_VSOCKETS_COMMON
+	select EXT4_FS
+	select XFS_FS
+	select BLOCK
+	help
+	  Say Y if you want to enable the BHV LSM and run Linux in a Virtual
+	  Machine on BHV and benefit from Virtualization-assisted Security.
+
+config BHV_PANIC_ON_FAIL
+	bool "BHV guest panics on Hypercall failure"
+	default y
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	bool "Build BHV guest support with DEBUG information"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_TRACEPOINTS
+	bool "Enable BHV Tracepoints"
+	default n
+	depends on BHV_VAS_DEBUG && TRACEPOINTS
+	help
+	  Say Y if you want to enable BHV tracepoints. Note: do not enable this option in production systems.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	bool "Make all paths passed to call_usermodehelper in the kernel constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in the kernel are constant. This implies
+	  that these paths cannot be updated via sysctl. Paths include
+	  the modprobe path and the poweroff path. This setting is recommended,
+	  since these strings are often updated in exploits.
+
+config BHV_CONST_CALL_USERMODEHELPER_MODULES
+	bool "Make all paths passed to call_usermodehelper in modules constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in drivers constant. This implies that
+	  these paths cannot be updated via sysctl. This setting is recommended,
+	  to harden modules against exploits.
+
+config BHV_LOCKDOWN
+	bool "Enable the most secure BHV settings (Lockdown)"
+	default n
+	depends on BHV_VAS
+	select BHV_PANIC_ON_FAIL
+	select BHV_CONST_MODPROBE_PATH
+	help
+	  Say Y if you want to enable the most secure BHV settings
+
+config BHV_VAULT_SPACES
+    bool "Enable the spaces-based BHV vault to guard code patching"
+    default y
+    depends on BHV_VAS
+    depends on X86_64 || ARM64
+    help
+	  Say Y if you want to enable the spaces-based BHV vault
+
+
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 000000000..5fdada030
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,49 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+#          Tommaso Frassetto <tommaso@bedrocksystems.com>
+
+ccflags-y+=-DKERNEL_COMMIT_HASH=\"cdc42684724904871c0ab962eed168fc3b566010+691397ba11cf5b17e40746a826b6bfc286e1897b\"
+
+obj-$(CONFIG_BHV_VAS)		:= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= abi_autogen.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/mm_init.o
+obj-$(CONFIG_BHV_VAS)		+= init/late_start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_bpf.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn_send.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn_listen.o
+obj-$(CONFIG_BHV_VAS)		+= guestcmd.o
+obj-$(CONFIG_BHV_VAS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_fops.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_integrity_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_version.o
+obj-$(CONFIG_BHV_VAS)		+= vmalloc_to_page.o
+obj-$(CONFIG_BHV_VAS)		+= domain.o
+obj-$(CONFIG_BHV_VAS)		+= drift_detection.o
+obj-$(CONFIG_BHV_VAS)		+= reverse_shell_detection.o
+obj-$(CONFIG_BHV_VAS)		+= lsm.o
+obj-$(CONFIG_BHV_VAS)		+= memory_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= inode.o
+obj-$(CONFIG_BHV_VAS)		+= config.o
+ifeq ($(CONFIG_KEYS),y)
+obj-$(CONFIG_BHV_VAS)		+= keyring.o
+endif
diff --git security/bhv/abi_autogen.c security/bhv/abi_autogen.c
new file mode 100644
index 000000000..0e8cbd490
--- /dev/null
+++ security/bhv/abi_autogen.c
@@ -0,0 +1,365 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2025 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2025-01-20T09:53:35).
+ */
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        pr_info("HypABI__Init__Init__BHVData__BHVConfigBitmap: %s%s%s%s%s%s%s%s%s%s%s%s%s%s",
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(addr) ? "KERNEL_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(addr) ? "PROC_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(addr) ? "DRIVER_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(addr) ? "LOGGING " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(addr) ? "CREDS_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(addr) ? "FILE_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(addr) ? "GUEST_POLICY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(addr) ? "REGISTER_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(addr) ? "STRONG_ISOLATION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(addr) ? "KERNEL_INTEGRITY_PT_PROT " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(addr) ? "INODE_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(addr) ? "KEYRING_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(addr) ? "VAULT " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__MemFlags: %s%s%s",
+                HypABI__Integrity__MemFlags__has_TRANSIENT(addr) ? "TRANSIENT " : "", 
+                HypABI__Integrity__MemFlags__has_MUTABLE(addr) ? "MUTABLE " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__Freeze__FreezeFlags: %s%s%s%s%s",
+                HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(addr) ? "CREATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(addr) ? "UPDATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(addr) ? "REMOVE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(addr) ? "PATCH " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        pr_info("HypABI__Guestlog__Init__GuestlogFlags: %s%s%s%s%s%s%s%s%s",
+                HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(addr) ? "PROCESS_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(addr) ? "DRIVER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(addr) ? "KERNEL_ACCESS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(addr) ? "UNKNOWN_FILEOPS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(addr) ? "KERNEL_EXEC_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(addr) ? "CONTAINER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_SOCKET_EVENTS(addr) ? "SOCKET_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_FILE_EVENTS(addr) ? "FILE_EVENTS " : "", 
+                ""
+        );
+}
+
+
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        pr_info("HypABI__FileProtection__Init__Config: %s%s%s%s",
+                HypABI__FileProtection__Init__Config__has_READ_ONLY(addr) ? "READ_ONLY " : "", 
+                HypABI__FileProtection__Init__Config__has_FILE_OPS(addr) ? "FILE_OPS " : "", 
+                HypABI__FileProtection__Init__Config__has_DIRTY_CRED(addr) ? "DIRTY_CRED " : "", 
+                ""
+        );
+}
+
+struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Remove__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__Patch__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Guestlog__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Configure__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Assign__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Commit__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Verification__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = NULL;
+struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__KernelConfig__arg__slab = NULL;
+struct kmem_cache *HypABI__GuestPolicy__GetPolicy__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Extend__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Delete__arg__slab = NULL;
+
+void HypABI__init_slabs(void)
+{
+        HypABI__Integrity__Create__Mem_Region__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__Mem_Region__slab",
+                sizeof(HypABI__Integrity__Create__Mem_Region__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__Mem_Region__slab)
+                panic("Could not create slab HypABI__Integrity__Create__Mem_Region__slab!\n");
+        HypABI__Integrity__Create__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__arg__slab",
+                sizeof(HypABI__Integrity__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Create__arg__slab!\n");
+        HypABI__Integrity__Update__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Update__arg__slab",
+                sizeof(HypABI__Integrity__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Update__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Update__arg__slab!\n");
+        HypABI__Integrity__Remove__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Remove__arg__slab",
+                sizeof(HypABI__Integrity__Remove__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Remove__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Remove__arg__slab!\n");
+        HypABI__Integrity__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Freeze__arg__slab",
+                sizeof(HypABI__Integrity__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Freeze__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Freeze__arg__slab!\n");
+        HypABI__Integrity__PtpgInit__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__PtpgInit__arg__slab",
+                sizeof(HypABI__Integrity__PtpgInit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__PtpgInit__arg__slab)
+                panic("Could not create slab HypABI__Integrity__PtpgInit__arg__slab!\n");
+        HypABI__Patch__Patch__arg__slab = kmem_cache_create(
+                "HypABI__Patch__Patch__arg__slab",
+                sizeof(HypABI__Patch__Patch__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__Patch__arg__slab)
+                panic("Could not create slab HypABI__Patch__Patch__arg__slab!\n");
+        HypABI__Patch__PatchNoClose__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchNoClose__arg__slab",
+                sizeof(HypABI__Patch__PatchNoClose__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchNoClose__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchNoClose__arg__slab!\n");
+        HypABI__Patch__PatchViolation__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchViolation__arg__slab",
+                sizeof(HypABI__Patch__PatchViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchViolation__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchViolation__arg__slab!\n");
+        HypABI__Acl__ProcessViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__ProcessViolation__arg__slab",
+                sizeof(HypABI__Acl__ProcessViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__ProcessViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__ProcessViolation__arg__slab!\n");
+        HypABI__Acl__DriverViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__DriverViolation__arg__slab",
+                sizeof(HypABI__Acl__DriverViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__DriverViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__DriverViolation__arg__slab!\n");
+        HypABI__Guestlog__Init__arg__slab = kmem_cache_create(
+                "HypABI__Guestlog__Init__arg__slab",
+                sizeof(HypABI__Guestlog__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Guestlog__Init__arg__slab)
+                panic("Could not create slab HypABI__Guestlog__Init__arg__slab!\n");
+        HypABI__Creds__Configure__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Configure__arg__slab",
+                sizeof(HypABI__Creds__Configure__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Configure__arg__slab)
+                panic("Could not create slab HypABI__Creds__Configure__arg__slab!\n");
+        HypABI__Creds__RegisterInitTask__arg__slab = kmem_cache_create(
+                "HypABI__Creds__RegisterInitTask__arg__slab",
+                sizeof(HypABI__Creds__RegisterInitTask__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__RegisterInitTask__arg__slab)
+                panic("Could not create slab HypABI__Creds__RegisterInitTask__arg__slab!\n");
+        HypABI__Creds__Assign__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Assign__arg__slab",
+                sizeof(HypABI__Creds__Assign__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Assign__arg__slab)
+                panic("Could not create slab HypABI__Creds__Assign__arg__slab!\n");
+        HypABI__Creds__AssignPriv__arg__slab = kmem_cache_create(
+                "HypABI__Creds__AssignPriv__arg__slab",
+                sizeof(HypABI__Creds__AssignPriv__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__AssignPriv__arg__slab)
+                panic("Could not create slab HypABI__Creds__AssignPriv__arg__slab!\n");
+        HypABI__Creds__Commit__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Commit__arg__slab",
+                sizeof(HypABI__Creds__Commit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Commit__arg__slab)
+                panic("Could not create slab HypABI__Creds__Commit__arg__slab!\n");
+        HypABI__Creds__Release__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Release__arg__slab",
+                sizeof(HypABI__Creds__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Release__arg__slab)
+                panic("Could not create slab HypABI__Creds__Release__arg__slab!\n");
+        HypABI__Creds__Verification__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Verification__arg__slab",
+                sizeof(HypABI__Creds__Verification__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Verification__arg__slab)
+                panic("Could not create slab HypABI__Creds__Verification__arg__slab!\n");
+        HypABI__Creds__Log__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Log__arg__slab",
+                sizeof(HypABI__Creds__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Log__arg__slab)
+                panic("Could not create slab HypABI__Creds__Log__arg__slab!\n");
+        HypABI__FileProtection__Init__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__Init__arg__slab",
+                sizeof(HypABI__FileProtection__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__Init__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__Init__arg__slab!\n");
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab!\n");
+        HypABI__FileProtection__ViolationFileOps__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationFileOps__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationFileOps__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationFileOps__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationFileOps__arg__slab!\n");
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab!\n");
+        HypABI__RegisterProtection__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__RegisterProtection__Freeze__arg__slab",
+                sizeof(HypABI__RegisterProtection__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__RegisterProtection__Freeze__arg__slab)
+                panic("Could not create slab HypABI__RegisterProtection__Freeze__arg__slab!\n");
+        HypABI__Inode__Register__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Register__arg__slab",
+                sizeof(HypABI__Inode__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Register__arg__slab)
+                panic("Could not create slab HypABI__Inode__Register__arg__slab!\n");
+        HypABI__Inode__Update__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Update__arg__slab",
+                sizeof(HypABI__Inode__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Update__arg__slab)
+                panic("Could not create slab HypABI__Inode__Update__arg__slab!\n");
+        HypABI__Inode__Release__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Release__arg__slab",
+                sizeof(HypABI__Inode__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Release__arg__slab)
+                panic("Could not create slab HypABI__Inode__Release__arg__slab!\n");
+        HypABI__Inode__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Verify__arg__slab",
+                sizeof(HypABI__Inode__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Verify__arg__slab)
+                panic("Could not create slab HypABI__Inode__Verify__arg__slab!\n");
+        HypABI__Inode__Log__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Log__arg__slab",
+                sizeof(HypABI__Inode__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Log__arg__slab)
+                panic("Could not create slab HypABI__Inode__Log__arg__slab!\n");
+        HypABI__Keyring__Register__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Register__arg__slab",
+                sizeof(HypABI__Keyring__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Register__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Register__arg__slab!\n");
+        HypABI__Keyring__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Verify__arg__slab",
+                sizeof(HypABI__Keyring__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Verify__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Verify__arg__slab!\n");
+        HypABI__Keyring__Log__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Log__arg__slab",
+                sizeof(HypABI__Keyring__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Log__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Log__arg__slab!\n");
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab",
+                sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab)
+                panic("Could not create slab HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab!\n");
+        HypABI__Confserver__StrictFileops__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__StrictFileops__arg__slab",
+                sizeof(HypABI__Confserver__StrictFileops__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__StrictFileops__arg__slab)
+                panic("Could not create slab HypABI__Confserver__StrictFileops__arg__slab!\n");
+        HypABI__Confserver__KernelConfig__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__KernelConfig__arg__slab",
+                sizeof(HypABI__Confserver__KernelConfig__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__KernelConfig__arg__slab)
+                panic("Could not create slab HypABI__Confserver__KernelConfig__arg__slab!\n");
+        HypABI__GuestPolicy__GetPolicy__arg__slab = kmem_cache_create(
+                "HypABI__GuestPolicy__GetPolicy__arg__slab",
+                sizeof(HypABI__GuestPolicy__GetPolicy__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__GuestPolicy__GetPolicy__arg__slab)
+                panic("Could not create slab HypABI__GuestPolicy__GetPolicy__arg__slab!\n");
+        HypABI__Wagner__Create__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Create__arg__slab",
+                sizeof(HypABI__Wagner__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Create__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Create__arg__slab!\n");
+        HypABI__Wagner__Extend__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Extend__arg__slab",
+                sizeof(HypABI__Wagner__Extend__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Extend__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Extend__arg__slab!\n");
+        HypABI__Wagner__Delete__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Delete__arg__slab",
+                sizeof(HypABI__Wagner__Delete__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Delete__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Delete__arg__slab!\n");
+}
diff --git security/bhv/acl.c security/bhv/acl.c
new file mode 100644
index 000000000..0945155f4
--- /dev/null
+++ security/bhv/acl.c
@@ -0,0 +1,316 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define PATH_DELIMITER '/'
+
+HypABI__Acl__ProcessInit__arg__T *ProcessInit_acl_config __ro_after_init = NULL;
+HypABI__Acl__DriverInit__arg__T *DriverInit_acl_config __ro_after_init = NULL;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+#define ACL_INIT(name)                                                         \
+	unsigned long r;                                                       \
+	HypABI__Acl__##name##Init__arg__T *acl_config =                        \
+		(HypABI__Acl__##name##Init__arg__T *)__get_free_pages(         \
+			GFP_KERNEL, 0);                                        \
+	if (acl_config == NULL) {                                              \
+		bhv_fail("Cannot allocate acl config");                        \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	acl_config->num_pages = 1;                                             \
+	r = HypABI__Acl__##name##Init__hypercall_noalloc(                      \
+		acl_config, PAGE_SIZE - HypABI__Acl__##name##Init__arg__SZ);   \
+	if (r) {                                                               \
+		pr_err("acl init fail");                                       \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	if (!acl_config->valid) {                                              \
+		uint16_t required_pages = acl_config->num_pages;               \
+		free_pages((unsigned long)acl_config, 0);                      \
+                                                                               \
+		acl_config =                                                   \
+			(HypABI__Acl__##name##Init__arg__T *)__get_free_pages( \
+				GFP_KERNEL, order_base_2(required_pages));     \
+                                                                               \
+		if (acl_config == NULL) {                                      \
+			bhv_fail("Cannot allocate acl config");                \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		r = HypABI__Acl__##name##Init__hypercall_noalloc(              \
+			acl_config,                                            \
+			required_pages * PAGE_SIZE -                           \
+				HypABI__Acl__##name##Init__arg__SZ);           \
+		if (r) {                                                       \
+			pr_err("acl init fail");                               \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		if (!acl_config->valid) {                                      \
+			bhv_fail("host returned invalid config");              \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	/* Protect memory */                                                   \
+	if (bhv_integrity_is_enabled()) {                                      \
+		HypABI__Integrity__Create__Mem_Region__T *region =             \
+			HypABI__Integrity__Create__Mem_Region__ALLOC();        \
+                                                                               \
+		region->start_addr = virt_to_phys(acl_config);                 \
+		region->size = acl_config->num_pages * PAGE_SIZE;              \
+		region->type = HypABI__Integrity__MemType__DATA_READ_ONLY;     \
+		region->flags = HypABI__Integrity__MemFlags__NONE;             \
+		region->next = BHV_INVALID_PHYS_ADDR;                          \
+		strscpy(region->label, "ACL CONFIG",                           \
+			HypABI__Integrity__MAX_LABEL_SIZE);                    \
+                                                                               \
+		r = bhv_create_kern_phys_mem_region_hyp(0, region);            \
+                                                                               \
+		HypABI__Integrity__Create__Mem_Region__FREE(region);           \
+                                                                               \
+		if (r) {                                                       \
+			pr_err("Cannot protect acl config");                   \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	name##Init_acl_config = acl_config;
+
+void __init bhv_mm_init_acl(void)
+{
+	if (bhv_acl_is_proc_acl_enabled()) {
+		ACL_INIT(Process);
+	}
+	if (bhv_acl_is_driver_acl_enabled()) {
+		ACL_INIT(Driver);
+	}
+}
+#undef ACL_INIT
+/************************************************************/
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+#define MATCHES(name)                                                         \
+	static bool _matches_##name(const char *target, bool strip_ext)       \
+	{                                                                     \
+		size_t target_len = 0;                                        \
+		size_t target_ext_len = 0;                                    \
+		uint16_t i;                                                   \
+                                                                              \
+		BUG_ON(target[0] != PATH_DELIMITER);                          \
+                                                                              \
+		/* Get target len */                                          \
+		target_len = strnlen(target, PATH_MAX);                       \
+		if (strip_ext) {                                              \
+			target_ext_len = _get_ext_len(target);                \
+			target_len -= target_ext_len;                         \
+		}                                                             \
+                                                                              \
+		if (target_len == 0 || target_len >= PATH_MAX)                \
+			return false;                                         \
+                                                                              \
+		for (i = 0; i < name##Init_acl_config->list_len; i++) {       \
+			const char *cur = ((char *)name##Init_acl_config) +   \
+					  name##Init_acl_config->list[i];     \
+			size_t cur_len = 0;                                   \
+                                                                              \
+			if (cur[0] != PATH_DELIMITER) {                       \
+				if (_match_names(cur, target, target_ext_len, \
+						 strip_ext))                  \
+					return true;                          \
+				else                                          \
+					continue;                             \
+			}                                                     \
+                                                                              \
+			cur_len = strnlen(cur, PATH_MAX);                     \
+			if (strip_ext) {                                      \
+				cur_len -= _get_ext_len(cur);                 \
+			}                                                     \
+                                                                              \
+			if (cur_len == 0 || cur_len >= PATH_MAX)              \
+				continue;                                     \
+                                                                              \
+			if (cur[cur_len - 1] == '*') {                        \
+				cur_len--;                                    \
+                                                                              \
+				if (target_len < cur_len)                     \
+					continue;                             \
+			} else if (target_len != cur_len)                     \
+				continue;                                     \
+                                                                              \
+			if (strncmp(cur, target, cur_len) == 0)               \
+				return true;                                  \
+		}                                                             \
+                                                                              \
+		return false;                                                 \
+	}
+
+MATCHES(Process)
+MATCHES(Driver)
+#undef MATCHES
+
+#define BLOCK_ENTITY(name)                                                     \
+	static bool _block_entity_##name(const char *target, bool strip_ext)   \
+	{                                                                      \
+		bool rv;                                                       \
+		unsigned long r;                                               \
+		size_t target_len = strlen(target);                            \
+		bool m;                                                        \
+		HypABI__Acl__##name##Violation__arg__T *violation;             \
+                                                                               \
+		if (name##Init_acl_config == NULL ||                           \
+		    !name##Init_acl_config->valid) {                           \
+			bhv_fail(                                              \
+				"unable to resolve entity due to init error"); \
+			return false;                                          \
+		}                                                              \
+                                                                               \
+		m = _matches_##name(target, strip_ext);                        \
+                                                                               \
+		/* Is this entity part of the allow list? */                   \
+		if (m && name##Init_acl_config->is_allow)                      \
+			return false;                                          \
+		/* Is this entity _NOT_ in the deny list? */                   \
+		if (!m && !name##Init_acl_config->is_allow)                    \
+			return false;                                          \
+                                                                               \
+		BUG_ON(target_len >= PAGE_SIZE);                               \
+                                                                               \
+		violation = HypABI__Acl__##name##Violation__arg__ALLOC();      \
+                                                                               \
+		/* Get Context */                                              \
+		r = populate_event_context(&violation->context, true);         \
+		if (r) {                                                       \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			bhv_fail("%s: BHV cannot retrieve event context",      \
+				 __FUNCTION__);                                \
+		}                                                              \
+                                                                               \
+		violation->name_len = target_len,                              \
+		violation->na##me = virt_to_phys((volatile void *)target);     \
+                                                                               \
+		/* Hypercall */                                                \
+		/* We cannot use the HL interface due to stack constraints */  \
+		r = HypABI__Acl__##name##Violation__hypercall_noalloc(         \
+			violation);                                            \
+		if (r) {                                                       \
+			pr_err("entity hypercall failed");                     \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			return true;                                           \
+		}                                                              \
+                                                                               \
+		rv = (bool)violation->block;                                   \
+		HypABI__Acl__##name##Violation__arg__FREE(violation);          \
+                                                                               \
+		return (bool)rv;                                               \
+	}
+
+BLOCK_ENTITY(Process)
+BLOCK_ENTITY(Driver)
+#undef BLOCK_ENTITY
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity_Driver(target, true);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity_Process(target, false);
+}
\ No newline at end of file
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 000000000..a5e1b17bc
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_BHV_TRACEPOINTS
+#define CREATE_TRACE_POINTS
+#endif
+#include <bhv/bhv_trace.h>
+
+bool bhv_initialized __ro_after_init = false;
+unsigned long *bhv_configuration_bitmap __ro_after_init = NULL;
diff --git security/bhv/config.c security/bhv/config.c
new file mode 100644
index 000000000..4d3da14a6
--- /dev/null
+++ security/bhv/config.c
@@ -0,0 +1,353 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/cache.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/config.h>
+#include <bhv/bson.h>
+
+typedef struct {
+	BHV_BSON_ROOT boot;
+} policy;
+
+// This should be in read-only memory and updated through the ABI.
+// Probably the entire struct should become part of the bhv_data_area.
+static policy _policy __ro_after_init;
+
+// These variables are currently just used to provide a better performance.
+bool _bhv_config_userspace_force_nx_stack __ro_after_init;
+bool _bhv_policy_get_new_file_exec_enabled __ro_after_init;
+bool _bhv_policy_get_new_file_exec_container_only __ro_after_init;
+bool _bhv_policy_get_interpreter_new_file_arg_enabled __ro_after_init;
+bool _bhv_policy_get_interpreter_new_file_arg_container_only __ro_after_init;
+BHV_BSON_DOC _bhv_policy_interpreter_new_file_is_interpreter __ro_after_init;
+bool _bhv_policy_get_interpreter_arg_cmd_enabled __ro_after_init;
+bool _bhv_policy_get_interpreter_arg_cmd_container_only __ro_after_init;
+BHV_BSON_DOC _bhv_policy_interpreter_arg_cmd_is_interpreter __ro_after_init;
+BHV_BSON_DOC _bhv_policy_interpreter_arg_cmd_is_interpreter_cmd __ro_after_init;
+bool _bhv_policy_get_interpreter_piped_enabled __ro_after_init;
+bool _bhv_policy_get_interpreter_piped_container_only __ro_after_init;
+BHV_BSON_DOC _bhv_policy_interpreter_piped_is_interpreter __ro_after_init;
+bool _bhv_policy_get_interpreter_binds_enabled __ro_after_init;
+bool _bhv_policy_get_interpreter_binds_container_only __ro_after_init;
+bool _bhv_policy_get_interpreter_binds_detect_transitive __ro_after_init;
+BHV_BSON_DOC _bhv_policy_interpreter_binds_is_interpreter __ro_after_init;
+BHV_BSON_DOC _bhv_policy_protected_unix_sockets __ro_after_init;
+
+static inline bool _policy_get_bool(const char *name, bool def)
+{
+	int err;
+	bool rv;
+
+	err = bson_get_bool(_policy.boot.root, name, &rv);
+	if (err != 0) {
+		pr_err_once("Could not read config: %s", name);
+		return def;
+	}
+
+	return rv;
+}
+
+static inline BHV_BSON_DOC _policy_get_array(const char *name)
+{
+	return bson_get_array(_policy.boot.root, name);
+}
+
+static inline BHV_BSON_DOC _policy_get_doc(const char *name)
+{
+	return bson_get_doc(_policy.boot.root, name);
+}
+
+void __init bhv_mm_init_config(void)
+{
+	int r;
+
+	_policy.boot.root = NULL;
+
+	if (!is_bhv_initialized())
+		return;
+
+	r = bson_get_policy(&_policy.boot);
+	if (r != 0) {
+		panic("Could not get configuration");
+		return;
+	}
+
+	// Init read-only variables.
+	_bhv_config_userspace_force_nx_stack = _policy_get_bool(
+		"GuestKernelConfig::userspace_force_nx_stack", false);
+	_bhv_policy_get_new_file_exec_enabled = _policy_get_bool(
+		"DriftDetection::new_file_exec::enabled", false);
+	_bhv_policy_get_new_file_exec_container_only = _policy_get_bool(
+		"DriftDetection::new_file_exec::remediate", false);
+	_bhv_policy_get_interpreter_new_file_arg_enabled = _policy_get_bool(
+		"DriftDetection::interpreter_arg_new_file::enabled", false);
+	_bhv_policy_get_interpreter_new_file_arg_container_only = _policy_get_bool(
+		"DriftDetection::interpreter_arg_new_file::container_only",
+		true);
+	_bhv_policy_interpreter_new_file_is_interpreter = _policy_get_array(
+		"DriftDetection::interpreter_arg_new_file::interpreter");
+	_bhv_policy_get_interpreter_arg_cmd_enabled = _policy_get_bool(
+		"DriftDetection::interpreter_arg_cmd::enabled", false);
+	_bhv_policy_get_interpreter_arg_cmd_container_only = _policy_get_bool(
+		"DriftDetection::interpreter_arg_cmd::container_only", true);
+	_bhv_policy_interpreter_arg_cmd_is_interpreter = _policy_get_array(
+		"DriftDetection::interpreter_arg_cmd::interpreter");
+	_bhv_policy_interpreter_arg_cmd_is_interpreter_cmd = _policy_get_doc(
+		"DriftDetection::interpreter_arg_cmd::interpreter_cmd");
+	_bhv_policy_get_interpreter_piped_enabled = _policy_get_bool(
+		"DriftDetection::interpreter_piped::enabled", false);
+	_bhv_policy_get_interpreter_piped_container_only = _policy_get_bool(
+		"DriftDetection::interpreter_piped::container_only", true);
+	_bhv_policy_interpreter_piped_is_interpreter = _policy_get_array(
+		"DriftDetection::interpreter_piped::interpreter");
+	_bhv_policy_get_interpreter_binds_enabled = _policy_get_bool(
+		"ReverseShellDetection::interpreter_binds::enabled", false);
+	_bhv_policy_get_interpreter_binds_container_only = _policy_get_bool(
+		"ReverseShellDetection::interpreter_binds::container_only",
+		false);
+	_bhv_policy_get_interpreter_binds_detect_transitive = _policy_get_bool(
+		"ReverseShellDetection::interpreter_binds::transitive", false);
+	_bhv_policy_interpreter_binds_is_interpreter = _policy_get_array(
+		"ReverseShellDetection::interpreter_binds::interpreter");
+	_bhv_policy_protected_unix_sockets = _policy_get_array(
+		"Logging::filter_options::protected_unix_sockets");
+}
+
+bool bhv_config_userspace_force_nx_stack(void)
+{
+	return _bhv_config_userspace_force_nx_stack;
+}
+
+const char *bhv_policy_get_modprobe_path(void)
+{
+	const char *result = NULL;
+	int err;
+
+	err = bson_get_str(_policy.boot.root,
+			   "GuestKernelConfig::modprobe_path", &result);
+	if (err) {
+		pr_err_once("Could not obtain modprobe_path from policy!");
+		return "/sbin/modprobe";
+	}
+
+	return result;
+}
+
+const char *bhv_policy_get_poweroff_cmd(void)
+{
+	const char *result = NULL;
+	int err;
+
+	err = bson_get_str(_policy.boot.root, "GuestKernelConfig::poweroff_cmd",
+			   &result);
+	if (err) {
+		pr_err_once("Could not obtain poweroff_cmd from policy!");
+		return "/sbin/poweroff";
+	}
+
+	return result;
+}
+
+const char *bhv_policy_get_core_pattern(void)
+{
+	const char *result = NULL;
+	int err;
+
+	err = bson_get_str(_policy.boot.root, "GuestKernelConfig::core_pattern",
+			   &result);
+	if (err) {
+		pr_err_once("Could not obtain core_pattern from policy!");
+		return "core";
+	}
+
+	return result;
+}
+
+bool bhv_policy_get_new_file_exec_enabled(void)
+{
+	return _bhv_policy_get_new_file_exec_enabled;
+}
+
+bool bhv_policy_get_new_file_exec_remediate(void)
+{
+	return _policy_get_bool("DriftDetection::new_file_exec::remediate",
+				false);
+}
+
+bool bhv_policy_get_new_file_exec_container_only(void)
+{
+	return _bhv_policy_get_new_file_exec_container_only;
+}
+
+bool bhv_policy_get_interpreter_new_file_arg_enabled(void)
+{
+	return _bhv_policy_get_interpreter_new_file_arg_enabled;
+}
+
+bool bhv_policy_get_interpreter_new_file_arg_remediate(void)
+{
+	return _policy_get_bool(
+		"DriftDetection::interpreter_arg_new_file::remediate", false);
+}
+
+bool bhv_policy_get_interpreter_new_file_arg_container_only(void)
+{
+	return _bhv_policy_get_interpreter_new_file_arg_container_only;
+}
+
+static inline bool _bhv_policy_is_interpreter(BHV_BSON_DOC doc,
+					      const char *path,
+					      const char **result)
+{
+	if (!doc) {
+		*result = NULL;
+		return false;
+	}
+
+	const char *name = strrchr(path, '/');
+
+	if (name == NULL)
+		name = path;
+
+	return bson_array_contains_str(doc, name, result);
+}
+
+bool bhv_policy_interpreter_new_file_is_interpreter(const char *path,
+						    const char **result)
+{
+	return _bhv_policy_is_interpreter(
+		_bhv_policy_interpreter_new_file_is_interpreter, path, result);
+}
+
+bool bhv_policy_get_interpreter_arg_cmd_enabled(void)
+{
+	return _bhv_policy_get_interpreter_arg_cmd_enabled;
+}
+
+bool bhv_policy_get_interpreter_arg_cmd_remediate(void)
+{
+	return _policy_get_bool(
+		"DriftDetection::interpreter_arg_cmd::remediate", false);
+}
+
+bool bhv_policy_get_interpreter_arg_cmd_container_only(void)
+{
+	return _bhv_policy_get_interpreter_arg_cmd_container_only;
+}
+
+bool bhv_policy_interpreter_arg_cmd_is_interpreter(const char *path,
+						   const char **result)
+{
+	return _bhv_policy_is_interpreter(
+		_bhv_policy_interpreter_arg_cmd_is_interpreter, path, result);
+}
+
+bool bhv_policy_interpreter_arg_cmd_is_interpreter_cmd(const char *interpreter,
+						       const char *cmd)
+{
+	BHV_BSON_DOC doc = _bhv_policy_interpreter_arg_cmd_is_interpreter_cmd;
+	if (doc == NULL)
+		return false;
+
+	BHV_BSON_DOC arr = bson_get_array(doc, interpreter);
+	if (arr == NULL)
+		return false;
+
+	return bson_array_contains_str(arr, cmd, NULL);
+}
+
+bool bhv_policy_get_interpreter_piped_enabled(void)
+{
+	return _bhv_policy_get_interpreter_piped_enabled;
+}
+
+bool bhv_policy_get_interpreter_piped_remediate(void)
+{
+	return _policy_get_bool("DriftDetection::interpreter_piped::remediate",
+				false);
+}
+
+bool bhv_policy_get_interpreter_piped_container_only(void)
+{
+	return _bhv_policy_get_interpreter_piped_container_only;
+}
+
+bool bhv_policy_interpreter_piped_is_interpreter(const char *path,
+						 const char **result)
+{
+	return _bhv_policy_is_interpreter(
+		_bhv_policy_interpreter_piped_is_interpreter, path, result);
+}
+
+bool bhv_policy_get_interpreter_binds_enabled(void)
+{
+	return _bhv_policy_get_interpreter_binds_enabled;
+}
+
+bool bhv_policy_get_interpreter_binds_remediate(void)
+{
+	return _policy_get_bool(
+		"ReverseShellDetection::interpreter_binds::remediate", false);
+}
+
+bool bhv_policy_get_interpreter_binds_container_only(void)
+{
+	return _bhv_policy_get_interpreter_binds_container_only;
+}
+
+bool bhv_policy_get_interpreter_binds_detect_transitive(void)
+{
+	return _bhv_policy_get_interpreter_binds_detect_transitive;
+}
+
+bool bhv_policy_interpreter_binds_is_interpreter(const char *path,
+						 const char **result)
+{
+	return _bhv_policy_is_interpreter(
+		_bhv_policy_interpreter_binds_is_interpreter, path, result);
+}
+
+bool bhv_policy_protected_unix_sockets_is_empty(void)
+{
+	if (!_bhv_policy_protected_unix_sockets)
+		return true;
+	return bson_doc_is_empty(
+		(BSON_DOC *)_bhv_policy_protected_unix_sockets);
+}
+
+struct bhv_protected_unit_socket_filter_arg_t {
+	bhv_protected_unit_socket_filter_t func;
+	void *arg;
+};
+
+static bool _bhv_protected_unit_socket_filter(BSON_KEY *key, void *elem,
+					      void *_arg)
+{
+	BSON_ELEMENT_STRING *cur;
+	struct bhv_protected_unit_socket_filter_arg_t *arg =
+		(struct bhv_protected_unit_socket_filter_arg_t *)_arg;
+	if (key->type != STRING)
+		return false;
+	cur = (BSON_ELEMENT_STRING *)elem;
+	return arg->func(cur->value, arg->arg);
+}
+
+bool bhv_match_each_protected_unix_sockets(bhv_protected_unit_socket_filter_t f,
+					   void *arg)
+{
+	struct bhv_protected_unit_socket_filter_arg_t _arg = { .func = f,
+							       .arg = arg };
+
+	if (_bhv_policy_protected_unix_sockets == NULL)
+		return false;
+
+	return (bool)bson_find_in_doc(_bhv_policy_protected_unix_sockets,
+				      _bhv_protected_unit_socket_filter, &_arg);
+}
diff --git security/bhv/creds.c security/bhv/creds.c
new file mode 100644
index 000000000..71edfcb13
--- /dev/null
+++ security/bhv/creds.c
@@ -0,0 +1,472 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/gfp.h>
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/event.h>
+#include <bhv/inode.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/keyring.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#define BHV_CRED_USAGE_SET(arg1, arg2)         \
+	_Generic((arg1), atomic_t *            \
+		 : atomic_set, atomic_long_t * \
+		 : atomic_long_set)(arg1, arg2)
+
+siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	BHV_CRED_USAGE_SET(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+#undef MAX_BUF_SIZE
+}
+
+#define LOG_PREPARE(logarg)                                                    \
+	HypABI__Creds__Log__arg__T *logarg = HypABI__Creds__Log__arg__ALLOC(); \
+	rc = populate_event_context(&logarg->context, true);                   \
+	if (rc) {                                                              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}
+
+#define LOG_SEND(rc, logarg)                                                  \
+	rc = HypABI__Creds__Log__hypercall_noalloc(logarg);                   \
+	if (rc) {                                                             \
+		pr_err("%s: BHV Cannot log event with type=%d", __FUNCTION__, \
+		       type);                                                 \
+	}
+
+#define LOG_FREE(logarg) HypABI__Creds__Log__arg__FREE(logarg)
+
+static int __bhv_cred_assign(struct task_struct *t,
+			     struct task_struct *_current, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Assign__arg__T *assarg = NULL;
+	struct task_struct *parent = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(_current);
+	if (rc)
+		return -EPERM;
+
+	assarg = HypABI__Creds__Assign__arg__ALLOC();
+	if (assarg == NULL)
+		return -ENOMEM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = _current->real_parent;
+	else
+		parent = _current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	assarg->new_task.addr = (uint64_t)t;
+	assarg->new_task.cred = (uint64_t)t->cred;
+	assarg->new_task.hmac = hmac;
+	assarg->parent.addr = (uint64_t)parent;
+	assarg->parent.cred = (uint64_t)parent->cred;
+
+	rc = HypABI__Creds__Assign__hypercall_noalloc(assarg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = assarg->ret;
+
+	HypABI__Creds__Assign__arg__FREE(assarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		/* Note that we currently log only the parent's information. */
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)parent;
+		logarg->task_cred = (uint64_t)parent->cred;
+		logarg->task_pid = parent->pid;
+		strscpy(logarg->task_name, parent->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return __bhv_cred_assign(t, current, clone_flags);
+}
+
+#ifdef VASKM // out of tree
+int __init bhv_cred_assign_init(struct task_struct *t)
+{
+	return __bhv_cred_assign(t, t->real_parent, 0);
+}
+#endif // VASKM
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	HypABI__Creds__AssignPriv__arg__T *aparg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	aparg = HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK();
+	if (aparg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	aparg->cred = (uint64_t)c;
+	aparg->daemon = (uint64_t)daemon;
+
+	rc = HypABI__Creds__AssignPriv__hypercall_noalloc(aparg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = aparg->ret;
+	HypABI__Creds__AssignPriv__arg__FREE(aparg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Commit__arg__T *commarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	commarg = HypABI__Creds__Commit__arg__ALLOC_NOCHECK();
+	if (commarg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	commarg->currnt.cred = (uint64_t)c;
+	commarg->currnt.addr = (uint64_t)current;
+	commarg->currnt.hmac = hmac;
+
+	rc = HypABI__Creds__Commit__hypercall_noalloc(commarg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = commarg->ret;
+	HypABI__Creds__Commit__arg__FREE(commarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+
+		LOG_FREE(logarg);
+	}
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Verification__arg__T *verarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	verarg = HypABI__Creds__Verification__arg__ALLOC_NOCHECK();
+	if (verarg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	verarg->task.cred = (uint64_t)t->cred;
+	verarg->task.addr = (uint64_t)t;
+	verarg->task.hmac = hmac;
+
+	rc = HypABI__Creds__Verification__hypercall_noalloc(verarg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = verarg->ret;
+	HypABI__Creds__Verification__arg__FREE(verarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)t;
+		logarg->task_cred = (uint64_t)t->cred;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, t->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	HypABI__Creds__Release__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->cred = (uint64_t)c;
+
+	rc = HypABI__Creds__Release__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	HypABI__Creds__Release__arg__FREE(arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	HypABI__Creds__RegisterInitTask__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK();
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->init_task.addr = (uint64_t)t;
+	arg->init_task.cred = (uint64_t)c;
+	arg->init_task.hmac = siphash_cred_context(c, t);
+
+	rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	HypABI__Creds__RegisterInitTask__arg__FREE(arg);
+}
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_cred(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_cred_register_init_task(KLN_SYMBOL_P(struct cred *const, init_cred),
+				    &init_task);
+}
+/***********************************************************************/
+
+/***********************************************************************
+ * init
+ ***********************************************************************/
+int __init bhv_init_cred(void)
+{
+	int rc = 0;
+
+	static HypABI__Creds__Configure__arg__T fbarg;
+	HypABI__Creds__Configure__arg__T *arg;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(fbarg);
+
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = HypABI__Creds__Configure__hypercall_noalloc(arg);
+
+	if (!rc) {
+		static_assert(HypABI__Creds__Configure__arg__SZ ==
+			      sizeof(siphash_key_t));
+		memcpy(&bhv_siphash_key, arg->key, sizeof(siphash_key_t));
+		memset(arg->key, 0, sizeof(siphash_key_t));
+	}
+
+	HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(arg, fbarg);
+	if (rc)
+		return -EINVAL;
+
+	rc = bhv_inode_init();
+	if (rc)
+		return rc;
+#ifndef VASKM
+	rc = bhv_init_keyring();
+	if (rc)
+		return rc;
+#endif
+
+	return rc;
+}
+/***********************************************************************/
diff --git security/bhv/domain.c security/bhv/domain.c
new file mode 100644
index 000000000..de7f08a62
--- /dev/null
+++ security/bhv/domain.c
@@ -0,0 +1,911 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifdef CONFIG_MEM_NS
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/nsproxy.h>
+#include <linux/mem_namespace.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+#include <linux/hugetlb.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <asm/bhv/domain.h>
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#include <asm/stacktrace.h>
+#include <asm/unwind.h>
+#endif
+
+DEFINE_PER_CPU(uint64_t, bhv_domain_current_domain);
+EXPORT_PER_CPU_SYMBOL_GPL(bhv_domain_current_domain);
+
+static DEFINE_XARRAY_ALLOC(xa_domids);
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static bhv_domain_batched_arg_t _batch_area;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+bool bhv_domain_initialized __ro_after_init = false;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static bool isolate __ro_after_init = false;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+static bool disallow_forced_mem_access __ro_after_init = true;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_create_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_CREATE_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot create new domain\n", __FUNCTION__);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_create(uint64_t *domid)
+{
+	int rc = 0;
+	unsigned int id;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == NULL)
+		return -EINVAL;
+
+	/*
+	 * Start allocating domain IDs from ID 1. Domain with ID 0 is reserved.
+	 *
+	 * NOTE: This implementation allocates domain IDs inside of the guest.
+	 * The allocations are done sequentially, yet (unless otherwise required
+	 * by BHV), do not necessarily have to be. These can be passed to BHV
+	 * for management purposes or, if needed, allocated directly by BHV
+	 * instead.
+	 *
+	 * XXX: Consider binding struct mem_namespace (or another datastructure)
+	 * to the allocated ID.
+	 */
+	rc = xa_alloc(&xa_domids, &id, NULL, xa_limit_32b, GFP_KERNEL);
+	if (rc) {
+		pr_err("%s: Cannot allocate new domain ID\n", __FUNCTION__);
+		*domid = BHV_INVALID_DOMAIN;
+		return rc;
+	}
+
+	*domid = (uint64_t)id;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate) {
+		rc = bhv_domain_create_isolated_view(*domid);
+		if (rc) {
+			xa_release(&xa_domids, id);
+			*domid = BHV_INVALID_DOMAIN;
+		}
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	return 0;
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_destroy_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_DESTROY_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot destroy domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+void bhv_domain_destroy(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate)
+		rc = bhv_domain_destroy_isolated_view(domid);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	/*
+	 * Release the allocated domain IDs only if BHV did not return an error,
+	 * or if guest isolation was not enabled in the first place.
+	 */
+	if (!rc)
+		xa_release(&xa_domids, domid);
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_switch_isolated_view(uint64_t domid)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_SWITCH_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot switch to domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_switch(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return -EINVAL;
+
+	if (domid == this_cpu_read(bhv_domain_current_domain))
+		return 0;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate) {
+		rc = bhv_domain_switch_isolated_view(domid);
+		if (rc)
+			return rc;
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	this_cpu_write(bhv_domain_current_domain, domid);
+
+	return rc;
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	if (mm == NULL)
+		return -EINVAL;
+
+	if (old_ns == NULL || new_ns == NULL)
+		return -EINVAL;
+
+	arg.domain.id = old_ns->mem_ns->domain;
+	arg.domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.id = new_ns->mem_ns->domain;
+
+	rc = BHV_DOMAIN_TRANSFER_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot transfer PGD 0x%llx to domain[%llu]\n",
+		       __FUNCTION__, arg.domain.pgd, new_ns->mem_ns->domain);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+static inline void bhv_domain_batch_from_info(uint64_t info, uint32_t *head,
+					      uint32_t *tail)
+{
+	(*tail) = (info & 0xffffffff);
+	(*head) = (info >> 32);
+}
+
+static inline uint64_t bhv_domain_batch_to_info(uint32_t head, uint32_t tail)
+{
+	return (((uint64_t)head << 32) | tail);
+}
+
+static inline uint32_t bhv_domain_get_nr_entries(uint32_t head, uint32_t tail)
+{
+	return head <= tail ? tail - head :
+				    BHV_DOMAIN_MAX_ENTRIES - head + tail;
+}
+
+static int bhv_domain_batch_send_locked(void)
+{
+	// LOCK MUST BE HELD!
+	int rc = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	rc = BHV_DOMAIN_BATCH_HYP(batch_area);
+	if (rc) {
+		uint32_t head;
+		uint32_t tail;
+		uint64_t info = atomic_long_read(&_batch_area.info);
+		bhv_domain_batch_from_info(info, &head, &tail);
+		pr_err("%s: BHV could not process %u batched hypercalls!\n",
+		       __FUNCTION__, bhv_domain_get_nr_entries(head, tail));
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+// DISCLAIMER: Disabled for now, since it may change the order of operations.
+#define BHV_BATCH_STEAL_PROCESSED 0
+
+static inline uint32_t bhv_domain_batch_get_slot(void)
+{
+	static atomic_t sending = { false };
+	uint32_t in_send = false;
+
+	uint64_t info = atomic64_read(&_batch_area.info);
+	uint64_t new_info;
+	uint32_t head;
+	uint32_t tail;
+	uint32_t nr_entries;
+
+#if BHV_BATCH_STEAL_PROCESSED
+	uint32_t i;
+#endif
+
+	while (true) {
+		// Update head and tail
+		bhv_domain_batch_from_info(info, &head, &tail);
+		// Calculate the current number of entries
+		nr_entries = bhv_domain_get_nr_entries(head, tail);
+
+		// Check if we need to send the batch area down because it is full
+		// Note that we check whether we reached BHV_DOMAIN_MAX_ENTRIES - 1
+		// as the tail always points to the next free entry
+		if (nr_entries >= BHV_DOMAIN_MAX_ENTRIES - 1) {
+			// Make sure in send is always false when we try to win
+			// the race.
+			in_send = false;
+
+			if (atomic_try_cmpxchg(&sending, &in_send, true)) {
+				// Hooray! We are a winner! Send it down.
+				bhv_domain_batch_send_locked();
+				// Reset sending afterwards.
+				atomic_set(&sending, false);
+			} else {
+#if BHV_BATCH_STEAL_PROCESSED
+				// Lets try to steal an existing entry.
+				// This works as processed entries always come after an INVALID entry
+				// Thus we can try to reuse them in case little space is left in our
+				// batch area.
+				for (i = head; i < nr_entries; i++) {
+					uint32_t cur =
+						i % BHV_DOMAIN_MAX_ENTRIES;
+					uint32_t expected =
+						BHV_VAS_DOMAIN_BATCH_STATE_PROCESSED;
+
+					if (atomic_try_cmpxchg(
+						    &_batch_area.entries[cur]
+							     .state,
+						    &expected,
+						    BHV_VAS_DOMAIN_BATCH_STATE_INVALID)) {
+						// We manged to steal an entry. Lets use it.
+						return cur;
+					}
+				}
+#endif
+			}
+
+			// Reread the the info and try again
+			info = atomic64_read(&_batch_area.info);
+			continue;
+		}
+
+		// There are still free entries! Lets see if we can get one
+		new_info = bhv_domain_batch_to_info(
+			head, (tail + 1) % BHV_DOMAIN_MAX_ENTRIES);
+		if (atomic64_try_cmpxchg(&_batch_area.info, &info, new_info)) {
+			// We won the race!
+			if (atomic_read(&_batch_area.entries[tail].state) !=
+			    BHV_VAS_DOMAIN_BATCH_STATE_INVALID) {
+				pr_err("Found valid/processed slot! (state: %u, head: %u, tail: %u, info: 0x%llx)",
+				       atomic_read(
+					       &_batch_area.entries[tail].state),
+				       head, tail,
+				       atomic64_read(&_batch_area.info));
+				panic("Batching failed!");
+			}
+			return tail;
+		}
+	}
+}
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+static void bhv_domain_add_stack_trace(bhv_domain_batched_entry_arg_t *target)
+{
+	struct unwind_state state;
+	struct stack_info stack_info = { 0 };
+	unsigned long visit_mask = 0;
+	unsigned long *stack = get_stack_pointer(current, NULL);
+	char *cur = &target->stack_trace[0];
+	size_t stack_buf_len = BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+
+	unwind_start(&state, current, NULL, stack);
+
+	stack = PTR_ALIGN(stack, sizeof(long));
+	if (get_stack_info(stack, current, &stack_info, &visit_mask)) {
+		pr_err("Could not get stack info!");
+		target->stack_trace[0] = '\0';
+		return;
+	}
+
+	for (; stack < stack_info.end && stack_buf_len > 0 &&
+	       stack_buf_len <= BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+	     stack++) {
+		unsigned long addr = READ_ONCE_NOCHECK(*stack);
+		unsigned long *ret_addr_p =
+			unwind_get_return_address_ptr(&state);
+		int reliable = 0;
+		int written;
+
+		if (!__kernel_text_address(addr))
+			continue;
+
+		if (stack == ret_addr_p)
+			reliable = 1;
+
+		// Skip unreliable for now.
+		if (!reliable)
+			continue;
+
+		written = snprintf(cur, stack_buf_len, "%c%pB\n",
+				   reliable ? ' ' : '?', (void *)addr);
+		if (written >= stack_buf_len)
+			break;
+		stack_buf_len -= written;
+		cur += written;
+
+		if (!reliable)
+			continue;
+
+		unwind_next_frame(&state);
+	}
+}
+#endif
+
+static int bhv_domain_batch_op(bhv_domain_batched_entry_arg_t *arg)
+{
+	int rc = 0;
+	uint32_t dest;
+	uint32_t invalid_state = BHV_VAS_DOMAIN_BATCH_STATE_INVALID;
+	unsigned long flags = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg->state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	dest = bhv_domain_batch_get_slot();
+
+	// disable interrupts for the completion of the entry.
+	local_irq_save(flags);
+	BUG_ON(atomic_read(&batch_area->entries[dest].state) !=
+	       BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	memcpy(&batch_area->entries[dest], arg,
+	       sizeof(bhv_domain_batched_entry_arg_t));
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+	bhv_domain_add_stack_trace(&batch_area->entries[dest]);
+#endif
+
+	if (!atomic_try_cmpxchg(&batch_area->entries[dest].state,
+				&invalid_state,
+				BHV_VAS_DOMAIN_BATCH_STATE_VALID)) {
+		panic("Could not set valid state!");
+	}
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+
+static int bhv_domain_batch_map(uint32_t op, struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages, bool read, bool write,
+				bool exec, bool kernel)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = op;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.map.read = read;
+	arg.map.write = write;
+	arg.map.exec = exec;
+	arg.map.kernel = kernel;
+	arg.map.pfn.pfn = pfn;
+	arg.map.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec)
+{
+	if (!isolate)
+		return 0;
+
+	return bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pfn, nr_pages,
+				    read, write, exec, true);
+}
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(pte))
+		return;
+
+	if (pte_special(pte) && !pte_exec(pte)) {
+		bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), true);
+	} else {
+		uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+		if (pte_present(*ptep))
+			bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+		bhv_domain_batch_map(bhv_domain_op, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), false);
+	}
+}
+EXPORT_SYMBOL(bhv_domain_set_pte_at);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte)) {
+		set_pte(ptep, pte);
+		return;
+	}
+#endif
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+			     pte_read(pte), pte_write(pte), pte_exec(pte),
+			     true);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	set_pte(ptep, pte);
+}
+
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(pmd))
+		return;
+
+	if (!pmd_large(pmd))
+		return;
+
+	if (pmd_present(*pmdp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pmd_pfn(pmd), 512,
+			     pmd_read(pmd), pmd_write(pmd), pmd_exec(pmd),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pmd_at);
+
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(pud))
+		return;
+
+	if (!pud_large(pud))
+		return;
+
+	if (pud_present(*pudp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pud_pfn(pud), 512 * 512,
+			     pud_read(pud), pud_write(pud), pud_exec(pud),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pud_at);
+
+static int bhv_domain_unmap_pte(struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_UNMAP;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.unmap.pfn.pfn = pfn;
+	arg.unmap.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(*ptep))
+		return;
+
+	bhv_domain_unmap_pte(mm, pte_pfn(*ptep), 1);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pte);
+
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(*pmdp))
+		return;
+
+	if (!pmd_large(*pmdp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pmd_pfn(*pmdp), 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pmd);
+
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(*pudp))
+		return;
+
+	if (!pud_large(*pudp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pud_pfn(*pudp), 512 * 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pud);
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	int rc = 0;
+	bhv_domain_debug_arg_t arg = {};
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	arg.msg_type = BHV_VAS_DOMAIN_DEBUG_DESTROY_PGD;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	rc = BHV_DOMAIN_DEBUG_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV an error occurred during DEBUG destroy pgd hypercall\n",
+		       __FUNCTION__);
+	}
+}
+#endif
+
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_PGD_DESTROY;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	bhv_domain_batch_op(&arg);
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags)
+{
+	int rc = 0;
+	HypABI__Domain__Report__arg__T arg = { 0 };
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return 0;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	rc = populate_event_context(&arg.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context",
+		       __FUNCTION__);
+	}
+
+	arg.domain_src.id = bhv_get_domain(t);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.domain_src.pgd = bhv_virt_to_phys_single(
+		bhv_domain_get_user_pgd(t->active_mm->pgd));
+	arg.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.domain_target.pgd = bhv_virt_to_phys_single(
+		bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.gva_start = vma->vm_start;
+	arg.gva_end = vma->vm_end;
+	arg.write = ((gup_flags & FOLL_WRITE) || (gup_flags & FOLL_FORCE));
+
+	rc = HypABI__Domain__Report__hypercall_noalloc(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+	if (arg.block)
+		rc = -EPERM;
+
+#if 0
+	pr_info("[-BHV-] %s: dom[%llu] -> dom[%llu] write=%d block=%d", __FUNCTION__, arg.report.domain_src.id, arg.report.domain_target.id, arg.report.write, arg.report.block);
+#endif
+
+	return rc;
+}
+
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+				     bool foreign)
+{
+	int rc = 0;
+	HypABI__Domain__ReportForcedMemAccess__arg__T arg = { 0 };
+	struct mm_struct *mm_target = vma->vm_mm;
+
+	if (!bhv_domain_is_active())
+		return true;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return true;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (!disallow_forced_mem_access)
+		return true;
+
+	rc = populate_event_context(&arg.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context",
+		       __FUNCTION__);
+	}
+
+	arg.domain_src.id = bhv_get_domain(current);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.domain_src.pgd = bhv_virt_to_phys_single(
+		bhv_domain_get_user_pgd(current->active_mm->pgd));
+	arg.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.domain_target.pgd = bhv_virt_to_phys_single(
+		bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.gva_start = vma->vm_start;
+	arg.gva_end = vma->vm_end;
+	arg.write = write;
+
+	rc = HypABI__Domain__ReportForcedMemAccess__hypercall_noalloc(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return false;
+	}
+
+	pr_info("[-BHV-] %s: vma @ [0x%lx:0x%lx] write=%d", __FUNCTION__,
+		vma->vm_start, vma->vm_end, write);
+
+	return !arg.block;
+}
+
+static void __init bhv_domain_init(void)
+{
+	int rc;
+	bool pti_active = false;
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	// We use the CPU region before the initialization!
+	// This is important, otherwise this would overwrite nr_entries!!!
+	bhv_domain_arg_t *arg = (bhv_domain_arg_t *)&_batch_area;
+#else
+	static HypABI__Domain__Configure__arg__T args = { 0 };
+	HypABI__Domain__Configure__arg__T *arg = &args;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (bhv_domain_initialized)
+		return;
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		pti_active = 1;
+#endif
+
+	arg->pti = (uint8_t)pti_active;
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	arg->batched_region = bhv_virt_to_phys(&_batch_area);
+#else
+	arg->batched_region = BHV_INVALID_PHYS_ADDR;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	rc = HypABI__Domain__Configure__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot configure StrongIsolation\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	isolate = !!arg->isolate;
+#else // !BHV_VAS_DOMAIN_SPACES_BASED
+	BUG_ON(arg->isolate);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+	disallow_forced_mem_access = !arg->allow_forced_mem_access;
+
+	bhv_domain_initialized = true;
+}
+
+int __init bhv_domain_mm_init(void)
+{
+	int cpu;
+	int rc;
+	
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	uint32_t i;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (!bhv_domain_is_enabled())
+		return -EPERM;
+
+	rc = xa_reserve_bh(&xa_domids, BHV_INIT_DOMAIN, GFP_KERNEL);
+	if (rc) {
+		bhv_fail("%s: BHV: cannot reserve domain ID %lu", __FUNCTION__,
+			 BHV_INIT_DOMAIN);
+		return -EFAULT;
+	}
+
+	bhv_domain_init();
+
+	for_each_possible_cpu(cpu) {
+		per_cpu(bhv_domain_current_domain, cpu) = 0;
+	}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/* Reserve the ID of the first, static memory namespace. */
+
+	atomic64_set(&_batch_area.info, 0);
+
+	for (i = 0; i < BHV_DOMAIN_MAX_ENTRIES; i++) {
+		atomic_set(&_batch_area.entries[i].state,
+			   BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	return 0;
+}
+
+#ifdef CONFIG_MEM_NS
+__initcall(bhv_domain_mm_init);
+#endif
+
+#endif /* CONFIG_MEM_NS */
diff --git security/bhv/drift_detection.c security/bhv/drift_detection.c
new file mode 100644
index 000000000..a6a9c4323
--- /dev/null
+++ security/bhv/drift_detection.c
@@ -0,0 +1,477 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors:  Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 
+ */
+#include <bhv/bhv_print.h>
+
+#include <uapi/linux/magic.h>
+#include <linux/dcache.h>
+#include <linux/xattr.h>
+#include <linux/cgroup.h>
+#include <linux/version.h>
+#include <linux/namei.h>
+#include <linux/binfmts.h>
+#include <linux/file.h>
+
+#include <bhv/drift_detection.h>
+#include <bhv/event.h>
+#include <bhv/bson.h>
+#include <bhv/guestlog.h>
+#include <bhv/util.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define DEBUG 0
+
+/***********************************************************************
+ * CONFIG
+ ***********************************************************************/
+#define BHV_XATTR_PREFIX "security.bhv."
+#define BHV_XATTR_INFO BHV_XATTR_PREFIX "info"
+/***********************************************************************/
+
+/***********************************************************************
+ * Hypercalls
+ ***********************************************************************/
+static bool drift_detection_hyp_block_new_file(struct file *file,
+					       const char *path)
+{
+	int r;
+	bool remediate = bhv_policy_get_new_file_exec_remediate();
+
+	r = bhv_guestlog_log_drift_detection_exec_new_file(path, remediate);
+	if (r != 0) {
+		pr_err("Could not log new file execution violation!\n");
+	}
+
+	return remediate;
+}
+
+static bool drift_detection_hyp_block_interpreter_arg_new_file(
+	const char *interpreter, const char *filename, size_t filename_sz)
+{
+	int r;
+	bool remediate = bhv_policy_get_interpreter_new_file_arg_remediate();
+
+	r = bhv_guestlog_log_drift_detection_interpreter_arg_new_file(
+		interpreter, filename, filename_sz, remediate);
+	if (r != 0) {
+		pr_err("Could not log interpreter executing new file!\n");
+	}
+
+	return remediate;
+}
+
+static bool drift_detection_hyp_block_interpreter_arg_cmd(
+	const char *interpreter, const char *command, size_t command_sz)
+{
+	int r;
+	bool remediate = bhv_policy_get_interpreter_new_file_arg_remediate();
+
+	r = bhv_guestlog_log_drift_detection_interpreter_arg_cmd(
+		interpreter, command, command_sz, remediate);
+	if (r != 0) {
+		pr_err("Could not log interpreter executing command!\n");
+	}
+
+	return remediate;
+}
+
+static bool drift_detection_hyp_block_interpreter_piped(const char *interpreter)
+{
+	int r;
+	bool remediate = bhv_policy_get_interpreter_piped_remediate();
+
+	r = bhv_guestlog_log_drift_detection_interpreter_piped(interpreter,
+							       remediate);
+	if (r != 0) {
+		pr_err("Could not log command piped into interpreter!\n");
+	}
+
+	return remediate;
+}
+/***********************************************************************/
+
+static inline void drift_detection_debug_print_context(struct dentry *dentry)
+{
+#if DEBUG
+	const char *path;
+	char *buf;
+
+	if (dentry == NULL)
+		return;
+
+	buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	path = bhv_get_dentry_path(dentry, buf, PATH_MAX);
+	pr_debug("-> Dentry: %s (%p), Task: %s\n", path, dentry, current->comm);
+
+	path = bhv_get_dentry_path(d_real(dentry, NULL), buf, PATH_MAX);
+	pr_debug("-> Underlying path: %s (%p)\n", path, d_real(dentry, NULL));
+
+	kfree(buf);
+#endif
+}
+
+static inline void drift_detection_debug_print_file(struct file *file)
+{
+#if DEBUG
+	const char *path;
+	char *buf;
+
+	buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	path = bhv_get_file_path(file, buf, PATH_MAX);
+	pr_debug("-> Path: %s\n", path);
+
+	kfree(buf);
+#endif
+}
+
+static inline void drift_detection_debug_print_dentry(struct dentry *dentry)
+{
+#if DEBUG
+	const char *path;
+	char *buf;
+
+	buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	path = bhv_get_dentry_path(dentry, buf, PATH_MAX);
+	pr_debug("-> Path: %s\n", path);
+
+	kfree(buf);
+#endif
+}
+
+struct bhv_xattr_info {
+	bool new_file;
+};
+
+static inline int drift_detection_set_xattr(struct dentry *d,
+					    struct bhv_xattr_info *info)
+{
+	int rv;
+
+	inode_lock(d->d_inode);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 528))
+	rv = __vfs_setxattr_noperm(&nop_mnt_idmap, d, BHV_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	rv = __vfs_setxattr_noperm(&init_user_ns, d, BHV_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#else
+	rv = __vfs_setxattr_noperm(d, BHV_XATTR_INFO, info, sizeof(*info), 0);
+#endif
+	inode_unlock(d->d_inode);
+
+	return rv;
+}
+
+static inline ssize_t drift_detection_get_xattr(struct dentry *d,
+						struct bhv_xattr_info *info)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 528))
+	return vfs_getxattr(&nop_mnt_idmap, d, BHV_XATTR_INFO, info,
+			    sizeof(*info));
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	return vfs_getxattr(&init_user_ns, d, BHV_XATTR_INFO, info,
+			    sizeof(*info));
+#else
+	return vfs_getxattr(d, BHV_XATTR_INFO, info, sizeof(*info));
+#endif
+}
+
+static inline bool is_new_file_by_dentry(struct dentry *dentry)
+{
+	struct bhv_xattr_info info = { .new_file = false };
+	ssize_t sz = drift_detection_get_xattr(d_real(dentry, NULL), &info);
+
+	if (sz != sizeof(info) && sz != 0) {
+		return false;
+	}
+
+	if (info.new_file) {
+		return true;
+	}
+
+	return false;
+}
+
+static inline bool is_new_file_by_file(struct file *file)
+{
+	return is_new_file_by_dentry(file->f_path.dentry);
+}
+
+static inline bool is_new_file_by_filename(const char *filename)
+{
+	struct path path;
+
+	if (kern_path(filename, LOOKUP_FOLLOW, &path)) {
+		// Could not lookup path
+		return false;
+	}
+
+	return is_new_file_by_dentry(path.dentry);
+}
+
+static bool drift_detection_consider_file(struct file *file)
+{
+	if (!d_is_reg(file->f_path.dentry)) {
+		return false;
+	}
+
+	// Filer special file systems
+	switch (file->f_inode->i_sb->s_magic) {
+	case PROC_SUPER_MAGIC:
+		return false;
+	case SYSFS_MAGIC:
+		return false;
+	default:
+		break;
+	}
+
+	return true;
+}
+
+// These functions check whether we should consider the current
+// task for the individual mechanism. In particular, we verify
+// whether "container_only" is set and whether the current task
+// is in a container (if container_only=true)
+static inline bool drift_detection_new_file_proceed(void)
+{
+	if (!bhv_policy_get_new_file_exec_enabled())
+		return false;
+
+	if (bhv_policy_get_new_file_exec_container_only() &&
+	    !bhv_task_in_container(current))
+		return false;
+
+	return true;
+}
+
+static inline bool drift_detection_new_file_arg_proceed(void)
+{
+	if (!bhv_policy_get_interpreter_new_file_arg_enabled())
+		return false;
+
+	if (bhv_policy_get_interpreter_new_file_arg_container_only() &&
+	    !bhv_task_in_container(current))
+		return false;
+
+	return true;
+}
+
+static inline bool drift_detection_interpreter_cmd_proceed(void)
+{
+	if (!bhv_policy_get_interpreter_arg_cmd_enabled())
+		return false;
+
+	if (bhv_policy_get_interpreter_arg_cmd_container_only() &&
+	    !bhv_task_in_container(current))
+		return false;
+
+	return true;
+}
+
+static inline bool drift_detection_interpreter_piped_proceed(void)
+{
+	if (!bhv_policy_get_interpreter_piped_enabled())
+		return false;
+
+	if (bhv_policy_get_interpreter_piped_container_only() &&
+	    !bhv_task_in_container(current))
+		return false;
+
+	return true;
+}
+
+int bhv_drift_detection_file_permission(struct file *file, int mask)
+{
+	if ((mask & MAY_WRITE) == MAY_WRITE &&
+	    (drift_detection_new_file_proceed() ||
+	     drift_detection_new_file_arg_proceed()) &&
+	    drift_detection_consider_file(file)) {
+		int rv;
+		struct bhv_xattr_info info = { .new_file = true };
+
+		struct dentry *d = d_real(file->f_path.dentry, NULL);
+
+		// Do not consider files not in the overlayfs
+		if (d == file->f_path.dentry)
+			return 0;
+
+		rv = drift_detection_set_xattr(d, &info);
+		if (rv) {
+			pr_err("Could not set xattr on file (%d)!\n", rv);
+			drift_detection_debug_print_context(
+				file->f_path.dentry);
+		} else {
+			pr_debug("Set xattr on:\n");
+			drift_detection_debug_print_context(
+				file->f_path.dentry);
+		}
+	}
+
+	return 0;
+}
+
+int bhv_drift_detection_bprm_check_security(struct linux_binprm *bprm,
+					    const char *path)
+{
+	bool proceed_new_file_arg;
+	bool proceed_interpreter_cmd;
+
+	if (!bhv_drift_detection_is_enabled())
+		return 0;
+
+	// Check whether this is a new file
+	if (drift_detection_new_file_proceed() &&
+	    is_new_file_by_file(bprm->file)) {
+		if (drift_detection_hyp_block_new_file(bprm->file, path))
+			return -EPERM;
+	}
+
+	// Check for interpreter piped command
+	if (drift_detection_interpreter_piped_proceed()) {
+		// Check if stdin is a pipe
+		struct file *f = fget(0);
+		bool is_pipe = bhv_is_pipe(f);
+		fput(f);
+
+		if (is_pipe &&
+		    bhv_policy_interpreter_piped_is_interpreter(path, NULL)) {
+			// This interpreter has a pipe for stdin.
+			if (drift_detection_hyp_block_interpreter_piped(path))
+				return -EPERM;
+		}
+	}
+
+	proceed_new_file_arg = drift_detection_new_file_arg_proceed();
+	proceed_interpreter_cmd = drift_detection_interpreter_cmd_proceed();
+
+	if (proceed_new_file_arg || proceed_interpreter_cmd) {
+		int argc, i;
+		unsigned long pos;
+		char *arg_buf = NULL;
+
+		pr_debug("Processing binary: '%s'\n", path);
+		drift_detection_debug_print_file(bprm->file);
+
+		// Check whether a new file is passed as argument or interpreter launched with -c
+		argc = bprm->argc;
+		pos = bprm->p;
+		i = 1;
+		arg_buf = vmalloc(MAX_ARG_STRLEN + 1);
+		if (arg_buf == NULL) {
+			pr_err("Could not allocate memory");
+			// Failing open for now.
+			return 0;
+		}
+
+		while (argc > 0) {
+			int len = 0;
+			int cur_buf_pos = 0;
+
+			pr_debug("%d args left. Reading...\n", argc);
+
+			// We need to read from the new mm
+			len = access_remote_vm(bprm->mm, pos, arg_buf,
+					       MAX_ARG_STRLEN + 1,
+#if (defined(VASKM_IS_UBUNTU) &&                                               \
+     (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0))) ||                       \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 480))
+					       FOLL_FORCE);
+#else
+					       FOLL_REMOTE | FOLL_FORCE);
+#endif
+
+			pr_debug("Read %d bytes...\n", len);
+
+			// Process all strings in the buffer
+			while (len > 0 && argc > 0) {
+				char *cur = arg_buf + cur_buf_pos;
+				size_t cur_len = strnlen(cur, len);
+
+				if (cur_len == len &&
+				    arg_buf[cur_buf_pos + len - 1] != '\0') {
+					// Incomplete arg. Trigger reread.
+					break;
+				}
+
+				pr_debug(
+					"Processing argv[%d] = '%s' (%lu, %d)\n",
+					i, cur, cur_len, len);
+
+				// Check if this is a command passed to an interpreter
+				if (proceed_interpreter_cmd) {
+					const char *interpreter = NULL;
+
+					if (bhv_policy_interpreter_arg_cmd_is_interpreter(
+						    path, &interpreter) &&
+					    bhv_policy_interpreter_arg_cmd_is_interpreter_cmd(
+						    interpreter, cur)) {
+						// Command passed as part of the command line.
+						if (drift_detection_hyp_block_interpreter_arg_cmd(
+							    path, cur, len)) {
+							vfree(arg_buf);
+							return -EPERM;
+						}
+					}
+				}
+
+				if (proceed_new_file_arg &&
+				    is_new_file_by_filename(cur) &&
+				    bhv_policy_interpreter_new_file_is_interpreter(
+					    path, NULL)) {
+					if (drift_detection_hyp_block_interpreter_arg_new_file(
+						    path, cur, len)) {
+						vfree(arg_buf);
+						return -EPERM;
+					}
+				}
+
+				pos += (cur_len + 1);
+				len -= (cur_len + 1);
+				cur_buf_pos += (cur_len + 1);
+				argc--;
+				i++;
+			}
+
+			vfree(arg_buf);
+			pr_debug("Done processing arguments!\n");
+		}
+	}
+
+	return 0;
+}
+
+int bhv_drift_detection_inode_setxattr(const char *name)
+{
+	if (!bhv_drift_detection_is_enabled())
+		return 0;
+
+	if (strncmp(BHV_XATTR_PREFIX, name, strlen(BHV_XATTR_PREFIX)) == 0) {
+		return -EPERM;
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git security/bhv/file_protection.c security/bhv/file_protection.c
new file mode 100644
index 000000000..12a89aed5
--- /dev/null
+++ security/bhv/file_protection.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/file_protection.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init = HypABI__FileProtection__Init__Config__NONE;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void)
+{
+	unsigned long r;
+
+	HypABI__FileProtection__Init__arg__T *bhv_arg;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+
+	r = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+	if (r) {
+		pr_err("File protection init failed");
+	} else {
+		bhv_file_protection_config = bhv_arg->feature_bitmap;
+	}
+
+	HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+}
+/***********************************************************************/
+
+#define READ_ONLY_FUNC(T)                                                      \
+	bool bhv_block_read_only_file_write_##T(const char *target)            \
+	{                                                                      \
+		unsigned long r;                                               \
+		bool rv;                                                       \
+		HypABI__FileProtection__##T##__arg__T *volatile violation =    \
+			HypABI__FileProtection__##T##__arg__ALLOC();           \
+                                                                               \
+		rv = populate_event_context(&violation->context, true);        \
+		if (rv) {                                                      \
+			bhv_fail("%s: BHV cannot retrieve event context",      \
+				 __FUNCTION__);                                \
+		}                                                              \
+                                                                               \
+		/* Setup arg */                                                \
+		violation->name_len =                                          \
+			strlen(target) + 1 /* NULL terminator */;              \
+		violation->name =                                              \
+			bhv_virt_to_phys((void *)target, violation->name_len); \
+                                                                               \
+		/* Hypercall */                                                \
+		r = HypABI__FileProtection__##T##__hypercall_noalloc(          \
+			violation);                                            \
+		if (r) {                                                       \
+			pr_err("file protection violation hypercall failed");  \
+			rv = true;                                             \
+		} else {                                                       \
+			/* Note: in case of dirtycred, "block" halts */        \
+			/* the guest with a panic */                           \
+			/* Read block and free */                              \
+			rv = (bool)violation->block;                           \
+		}                                                              \
+		HypABI__FileProtection__##T##__arg__FREE(violation);           \
+                                                                               \
+		return rv;                                                     \
+	}
+
+READ_ONLY_FUNC(ViolationWriteReadOnlyFile)
+READ_ONLY_FUNC(ViolationDirtyCredWrite)
+
+#undef READ_ONLY_FUNC
\ No newline at end of file
diff --git security/bhv/fileops_protection.c security/bhv/fileops_protection.c
new file mode 100644
index 000000000..67956b096
--- /dev/null
+++ security/bhv/fileops_protection.c
@@ -0,0 +1,380 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <uapi/linux/magic.h>
+#include <linux/sort.h>
+#include <linux/bsearch.h>
+#include <linux/moduleparam.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/file_protection.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/fileops_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/guestlog.h>
+
+#include <asm/sections.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+static bool bhv_strict_fileops __ro_after_init = false;
+
+bool bhv_strict_fileops_enforced(void)
+{
+	return bhv_strict_fileops;
+}
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+u8 bhv_fileops_type(u32 fs_magic)
+{
+	switch (fs_magic) {
+#if defined CONFIG_EXT4_FS || defined VASKM
+	case EXT4_SUPER_MAGIC:
+		return FT(EXT4);
+#endif
+#if defined CONFIG_XFS_FS || defined VASKM
+	case XFS_SUPER_MAGIC:
+		return FT(XFS);
+#endif
+	case TMPFS_MAGIC:
+		return FT(TMPFS);
+	case PROC_SUPER_MAGIC:
+		return FT(PROC);
+	case CGROUP2_SUPER_MAGIC:
+		fallthrough;
+	case SYSFS_MAGIC:
+		return FT(SYSFS);
+	case DEBUGFS_MAGIC:
+		return FT(DEBUGFS);
+	default:
+		return FT(UNSUPPORTED);
+	}
+}
+#undef FT
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)) ||                         \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 480))
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod && ((unsigned long)mod->mem[MOD_RODATA].base <= addr) &&
+	       ((unsigned long)mod->mem[MOD_RODATA].base +
+			mod->mem[MOD_RODATA].size >
+		addr);
+}
+#else
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.text_size <=
+		addr) &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.ro_size >
+		addr);
+}
+#endif
+
+bool bhv_fileops_is_ro(u64 f_op)
+{
+	struct module *mod;
+	if (f_op >= KLN_SYM(__start_rodata) && f_op < KLN_SYM(__end_rodata))
+		return true;
+
+	preempt_disable();
+	mod = __module_address(f_op);
+	preempt_enable();
+	if (mod == NULL)
+		return false;
+
+	if (_is_module_ro_data(f_op, mod))
+		return true;
+
+	return false;
+}
+
+bool bhv_block_fileops(const char *target, u8 fops_type, bool is_dir,
+		       const void *fops_ptr)
+{
+	unsigned long err;
+	bool retval;
+	HypABI__FileProtection__ViolationFileOps__arg__T *volatile violation;
+	size_t path_sz;
+	int rv;
+
+	if (!bhv_fileops_file_protection_is_enabled())
+		return false;
+
+	violation = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ) {
+		path_sz =
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ -
+			1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->fops_type = fops_type;
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+	violation->fops_ptr = (uint64_t)fops_ptr;
+
+	// pr_err("Bad fops %d %s %d %px %pS\n", violation->fops_type, violation->path_name, violation->is_dir, (void*)violation->fops_ptr, (void*)violation->fops_ptr);
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+
+	return retval;
+}
+
+#ifndef VASKM // inside kernel tree
+
+const fops_t fileops_map[] __section(".rodata") = {
+#define FOPS_MAP(_, idx, file_ops, dir_ops) [idx] = { &file_ops, &dir_ops },
+#define FOPS_MAP_DIRNULL(_, idx, file_ops) [idx] = { &file_ops, NULL },
+#include <bhv/fileops_internal_fopsmap.h>
+};
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[] __ro_after_init = {
+#define FOPS(_)
+#define FOPS_PROC(sym) &sym,
+#include <bhv/fileops_internal_symlist.h>
+};
+
+#define init_fileops_data()
+
+#else // out of tree
+fops_t *fileops_map;
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[0
+#define FOPS(_)
+#define FOPS_PROC(_) +1
+#include <bhv/fileops_internal_symlist.h>
+] __ro_after_init = {};
+
+static void init_fileops_data(void)
+{
+	// Here we use UNSUPPORTED for simplicity. If this assert fails, we
+	// will have to write a more clever check. For now it works fine.
+	static_assert(
+		sizeof(fops_t) *
+			HypABI__FileProtection__ViolationFileOps__FopsType__UNSUPPORTED <=
+		PAGE_SIZE);
+	fileops_map = (fops_t *)vzalloc(PAGE_SIZE);
+	BUG_ON(!fileops_map);
+	BUG_ON(!PAGE_ALIGNED(fileops_map));
+
+#define KFOPS(sym) KLN_SYMBOL_P(const struct file_operations *, sym)
+#define FOPS_MAP(_, idx, file_ops, dir_ops)    \
+	fileops_map[idx][0] = KFOPS(file_ops); \
+	fileops_map[idx][1] = KFOPS(dir_ops);
+#define FOPS_MAP_DIRNULL(_, idx, file_ops)     \
+	fileops_map[idx][0] = KFOPS(file_ops); \
+	fileops_map[idx][1] = NULL;
+#include <bhv/fileops_internal_fopsmap.h>
+
+	int c = 0;
+#define FOPS(_)
+#define FOPS_PROC(sym) proc_fops[c++] = KFOPS(sym);
+#include <bhv/fileops_internal_symlist.h>
+	BUG_ON(c != ARRAY_SIZE(proc_fops));
+}
+#endif // VASKM
+
+static int cmp_fileops(const void *a_ptr, const void *b_ptr)
+{
+	u64 a = *(u64 *)a_ptr;
+	u64 b = *(u64 *)b_ptr;
+	return ((a == b) ? 0 : (a > b ? 1 : -1));
+}
+
+/***************************************************************
+ * init
+ ***************************************************************/
+void __init bhv_init_fileops(void)
+{
+	static HypABI__Confserver__StrictFileops__arg__T bhv_arg_fb;
+	int rc;
+
+	HypABI__Confserver__StrictFileops__arg__T *bhv_arg =
+		HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(
+			bhv_arg_fb);
+
+	init_fileops_data();
+
+	rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+
+	if (rc == 0) {
+		bhv_strict_fileops = bhv_arg->strict_fileops;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(bhv_arg,
+								    bhv_arg_fb);
+
+	if (rc == 0 && bhv_file_protection_is_enabled()) {
+		sort(proc_fops, ARRAY_SIZE(proc_fops), sizeof(proc_fops[0]),
+		     cmp_fileops, NULL);
+	}
+}
+/***************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **fop_ptr)
+{
+	if (bsearch(fop_ptr, proc_fops, ARRAY_SIZE(proc_fops),
+		    sizeof(proc_fops[0]), cmp_fileops))
+		return true;
+
+	return false;
+}
+
+#ifndef VASKM // inside kernel tree
+extern int full_proxy_release(struct inode *inode, struct file *filp);
+extern loff_t full_proxy_llseek(struct file *, loff_t, int);
+extern ssize_t full_proxy_read(struct file *, char __user *, size_t, loff_t *);
+extern ssize_t full_proxy_write(struct file *, const char __user *, size_t,
+				loff_t *);
+extern __poll_t full_proxy_poll(struct file *, struct poll_table_struct *);
+extern long full_proxy_unlocked_ioctl(struct file *, unsigned int,
+				      unsigned long);
+#endif
+
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr)
+{
+	size_t i;
+
+	if (fop_ptr == NULL)
+		return false;
+
+	// Check whether the fop points to one of three proxy fops
+	// see fs/debugfs/file.c
+	if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    debugfs_noop_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_open_proxy_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_full_proxy_file_operations))
+		return true;
+
+	// Allow directory operations. Might be set by debugfs_create_automount()
+	// (see  fs/debugfs/inode.c)
+	if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    empty_dir_operations) ||
+	    fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    simple_dir_operations)) {
+		return true;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+	// simple_offset_dir_operations available in versions >= 6.6
+	if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    simple_offset_dir_operations)) {
+		return true;
+	}
+#endif
+
+	// No. This should be a dynamically allocated fops struct.
+	// We will validate the pointer within fops.
+	// See __full_proxy_fops_init in fs/debugfs/file.c
+	if (fop_ptr->llseek != NULL &&
+	    (u64)fop_ptr->llseek != KLN_SYM(full_proxy_llseek))
+		return false;
+
+	if (fop_ptr->read != NULL &&
+	    (u64)fop_ptr->read != KLN_SYM(full_proxy_read))
+		return false;
+
+	if (fop_ptr->write != NULL &&
+	    (u64)fop_ptr->write != KLN_SYM(full_proxy_write))
+		return false;
+
+	if (fop_ptr->read_iter != NULL)
+		return false;
+
+	if (fop_ptr->write_iter != NULL)
+		return false;
+
+	if (fop_ptr->iopoll != NULL)
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 5, 0)
+	if (fop_ptr->iterate != NULL)
+		return false;
+#endif
+
+	if (fop_ptr->iterate_shared != NULL)
+		return false;
+
+	if (fop_ptr->poll != NULL &&
+	    (u64)fop_ptr->poll != KLN_SYM(full_proxy_poll))
+		return false;
+
+	if (fop_ptr->unlocked_ioctl != NULL &&
+	    (u64)fop_ptr->unlocked_ioctl != KLN_SYM(full_proxy_unlocked_ioctl))
+		return false;
+
+	if (fop_ptr->compat_ioctl != NULL)
+		return false;
+
+	if (fop_ptr->mmap != NULL)
+		return false;
+
+	if (fop_ptr->open != NULL)
+		return false;
+
+	if (fop_ptr->flush != NULL)
+		return false;
+
+	if ((u64)fop_ptr->release != KLN_SYM(full_proxy_release))
+		return false;
+
+	// Remainder of pointers must be NULL as well
+	for (i = offsetof(struct file_operations, fsync);
+	     i < sizeof(struct file_operations); i++) {
+		if (((uint8_t *)fop_ptr)[i] != '\0') {
+			return false;
+		}
+	}
+
+	return true;
+}
diff --git security/bhv/guestcmd.c security/bhv/guestcmd.c
new file mode 100644
index 000000000..dbacc5619
--- /dev/null
+++ security/bhv/guestcmd.c
@@ -0,0 +1,142 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bluerock.io>
+ */
+
+#include <linux/cgroup.h>
+
+#include <bhv/guestconn.h>
+
+#include <bhv/guestcmd.h>
+
+static void bhv_guestcmd_kill_proc_handler(void *buf, size_t len)
+{
+	int r;
+	SendConnABI__CmdKillProc__T *cmd;
+	struct task_struct *tsk;
+
+	if (len != sizeof(SendConnABI__CmdKillProc__T)) {
+		pr_err("bhv: kill proc command length mismatch!\n");
+		return;
+	}
+	cmd = buf;
+
+	tsk = find_task_by_pid_ns(cmd->pid, &init_pid_ns);
+	if (tsk == NULL) {
+		pr_err("bhv: kill proc command failed: no task with pid %llu!\n",
+		       cmd->pid);
+		return;
+	}
+
+	r = send_sig(SIGKILL, tsk, 0);
+	if (r) {
+		pr_err("bhv: kill proc command failed with %d!\n", r);
+		return;
+	}
+}
+
+#ifndef VASKM // inside kernel tree
+static bool bhv_guestcmd_is_target_container(struct cgroup *cur,
+					     const char *target)
+{
+	static char buf[128];
+
+	buf[0] = '\0';
+	cgroup_name(cur, buf, sizeof(buf));
+
+	return strstr(buf, target) != NULL;
+}
+
+// Kill containers is only supported in the VAS kernel.
+extern bool cgroup_is_threaded(struct cgroup *cgrp);
+extern void cgroup_kill(struct cgroup *cgrp);
+static void bhv_guestcmd_kill_container_handler(void *buf, size_t len)
+{
+	struct cgroup_subsys_state *pos;
+	struct cgroup *cur;
+	SendConnABI__CmdKillContainer__T *cmd;
+
+	if (len != sizeof(SendConnABI__CmdKillContainer__T)) {
+		pr_err("bhv: kill container command length mismatch!\n");
+		return;
+	}
+	cmd = buf;
+
+	rcu_read_lock();
+	css_for_each_descendant_pre (pos, &cgrp_dfl_root.cgrp.self) {
+		cur = container_of(pos, struct cgroup, self);
+		if (bhv_guestcmd_is_target_container(cur, cmd->container_id))
+			break;
+
+		cur = NULL;
+	}
+	rcu_read_unlock();
+
+	if (cur == NULL) {
+		pr_err("bhv: kill container command failed! Could not find container with ID '%s'!\n",
+		       cmd->container_id);
+		return;
+	}
+
+	if (cgroup_is_threaded(cur)) {
+		pr_err("bhv: kill container command failed! Cannot kill threaded container!\n");
+		return;
+	}
+
+	cgroup_kill(cur);
+}
+#else // out of tree
+static void bhv_guestcmd_kill_container_handler(void *, size_t) {
+	pr_err("bhv: kill container command is not supported!\n");
+}
+#endif // VASKM
+
+static const __section(".rodata") bhv_guestconn_backend_handler_t
+	bhv_guestcmd_cmd_handlers[SendConnABI__NUM_CMDS] = {
+		bhv_guestcmd_kill_proc_handler,
+		bhv_guestcmd_kill_container_handler,
+	};
+
+static int bhv_guestcmd_get_cmd(void *buf, size_t len, uint64_t *cmd)
+{
+	if (len < sizeof(uint64_t))
+		return -EOVERFLOW;
+
+	*cmd = *(uint64_t *)buf;
+
+	if (*cmd >= SendConnABI__NUM_CMDS)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void bhv_guestcmd_handler(void *buf, size_t len)
+{
+	int r;
+	uint64_t cmd;
+
+	r = bhv_guestcmd_get_cmd(buf, len, &cmd);
+	if (r) {
+		pr_err("BHV: Guestcmd: Invalid message! (%d)\n", r);
+		return;
+	}
+
+	bhv_guestcmd_cmd_handlers[cmd](buf, len);
+}
+
+/*********************************************************
+ * init
+ *********************************************************/
+void __init bhv_init_guestcmd(void)
+{
+	int r;
+
+	r = bhv_guestconn_register_backend(SendConnABI__CMD_BACKEND,
+					   bhv_guestcmd_handler);
+	if (r) {
+		bhv_fail("bhv: unable to register guestcmd handler!\n");
+		return;
+	}
+}
+/*********************************************************/
diff --git security/bhv/guestconn.c security/bhv/guestconn.c
new file mode 100644
index 000000000..09b5ec5b4
--- /dev/null
+++ security/bhv/guestconn.c
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#include "guestconn.h"
+
+uint32_t bhv_guestconn_cid __ro_after_init = 0;
+uint32_t bhv_guestconn_port __ro_after_init = 0;
+
+static int bhv_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	bhv_guestconn_send_reboot();
+	bhv_guestconn_listen_reboot();
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bhv_guestconn_reboot_notifier = {
+	.notifier_call = bhv_guestconn_reboot,
+	.priority = 0,
+};
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_guestconn(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	register_reboot_notifier(&bhv_guestconn_reboot_notifier);
+
+	bhv_start_guestconn_send(bhv_guestconn_cid, bhv_guestconn_port);
+	bhv_start_guestconn_listen(bhv_guestconn_cid, bhv_guestconn_port+1);
+}
+/**********************************************************/
+
+/**********************************************************
+ * mm_init
+ **********************************************************/
+void __init bhv_mm_init_guestconn(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	bhv_mm_init_guestconn_send();
+	bhv_mm_init_guestconn_listen();
+}
+/**********************************************************/
+
+/**********************************************************
+ * init
+ **********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	bhv_guestconn_cid = cid;
+	bhv_guestconn_port = port;
+	return 0;
+}
+/**********************************************************/
diff --git security/bhv/guestconn.h security/bhv/guestconn.h
new file mode 100644
index 000000000..7cf59dd3c
--- /dev/null
+++ security/bhv/guestconn.h
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_INTERNAL_H__
+#define __BHV_GUESTCONN_INTERNAL_H__
+void __init bhv_mm_init_guestconn_send(void);
+void bhv_start_guestconn_send(uint32_t bhv_guestconn_cid, uint32_t bhv_guestconn_port);
+void bhv_guestconn_send_reboot(void);
+
+void __init bhv_mm_init_guestconn_listen(void);
+void bhv_start_guestconn_listen(uint32_t bhv_guestconn_cid, uint32_t bhv_guestconn_port);
+void bhv_guestconn_listen_reboot(void);
+#endif /* __BHV_GUESTCONN_INTERNAL_H__ */
\ No newline at end of file
diff --git security/bhv/guestconn_listen.c security/bhv/guestconn_listen.c
new file mode 100644
index 000000000..23ba29a90
--- /dev/null
+++ security/bhv/guestconn_listen.c
@@ -0,0 +1,253 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+// #include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/kthread.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+#include <net/vsock_addr.h>
+#include <net/sock.h>
+
+#include <bhv/bhv.h>
+
+#include "guestconn.h"
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+typedef struct {
+	u32 cid;
+	u32 port;
+} guestconn_conn_t;
+
+static struct task_struct *guestconn_listen_tsk;
+
+static bhv_guestconn_backend_handler_t bhv_guestconn_backend_handlers
+	[SendConnABI__NUM_BACKENDS] __ro_after_init = { NULL };
+
+static int guestconn_recv(struct socket *vsock, u8 *buf, size_t sz)
+{
+	size_t bytes_read = 0;
+	struct msghdr msg = { .msg_flags = 0 };
+	int len;
+	struct kvec iov;
+
+	while (bytes_read < sz) {
+		iov.iov_base = (void *)(buf + bytes_read);
+		iov.iov_len = sz - bytes_read;
+		len = kernel_recvmsg(vsock, &msg, &iov, 1, iov.iov_len,
+				     msg.msg_flags);
+		if (unlikely(kthread_should_stop()))
+			return -ERESTARTSYS;
+		if (unlikely((len < 0))) {
+			if (len == -EAGAIN || len == -ETIMEDOUT)
+				continue;
+			return len;
+		}
+		bytes_read += len;
+	}
+	return 0;
+}
+
+static int guestconn_get_hdr(struct socket *vsock, SendConnABI__Header__T *hdr)
+{
+	return guestconn_recv(vsock, (u8 *)hdr, sizeof(SendConnABI__Header__T));
+}
+
+static int guestconn_get_body(struct socket *vsock, void *buf, size_t read_len)
+{
+	return guestconn_recv(vsock, (u8 *)buf, read_len);
+}
+
+static void deliver_body(uint16_t backend, void *buf, size_t len)
+{
+	BUG_ON(buf == NULL);
+	if (len == 0)
+		return;
+
+	if (backend >= SendConnABI__NUM_BACKENDS) {
+		pr_err("recieved unknown backend id (%u)\n", backend);
+		return;
+	}
+
+	if (bhv_guestconn_backend_handlers[backend] == NULL) {
+		pr_err("no handler registered for backend (%u)\n", backend);
+		return;
+	}
+
+	bhv_guestconn_backend_handlers[backend](buf, len);
+}
+
+static int guestconn_listen_kthread_main(void *arg)
+{
+	int rv = 0;
+	int err;
+	struct sockaddr_vm addr;
+	struct socket *vsock;
+	SendConnABI__Header__T hdr;
+	void *body_buf;
+	int body_buf_order = 0;
+	size_t body_buf_read_len;
+	uint32_t cid = ((guestconn_conn_t *)arg)->cid;
+	uint32_t port = ((guestconn_conn_t *)arg)->port;
+
+	kfree((guestconn_conn_t *)arg);
+
+	body_buf = (void *)__get_free_pages(GFP_KERNEL, body_buf_order);
+	if (body_buf == NULL) {
+		bhv_fail("GuestConn: Listen: Could allocate body buffer");
+		return -ENOMEM;
+	}
+
+	vsock_addr_init(&addr, cid, port);
+	pr_info("bhv guestconn listener started with cid %u, port %u", cid,
+		port);
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		free_pages((unsigned long)body_buf, body_buf_order);
+		bhv_fail(
+			"GuestConn: Listen: Could not create kernel socket (%d)",
+			err);
+		return err;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		free_pages((unsigned long)body_buf, body_buf_order);
+		bhv_fail("GuestConn: Listen: Could not connect to host (%d)",
+			 err);
+		return err;
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+	/*
+	 * 2 second TO for 5.10.x as the listening thread refuses to come out
+	 * of the recv when a signal is sent.
+	 */
+	vsock->sk->sk_rcvtimeo = 2 * HZ;
+#endif
+
+	while (!kthread_should_stop()) {
+		err = guestconn_get_hdr(vsock, &hdr);
+		if (err) {
+			if (err != -ERESTARTSYS) {
+				bhv_fail(
+					"GuestConn: Listen: error during recv (%d)",
+					err);
+				rv = err;
+			}
+			break;
+		}
+
+		if (hdr.sz < sizeof(SendConnABI__Header__T)) {
+			bhv_fail("Guestconn: Listen: protocol error");
+			return -EPROTO;
+		}
+
+		body_buf_read_len = hdr.sz - sizeof(SendConnABI__Header__T);
+
+		if (body_buf_read_len == 0) {
+			pr_warn("recieved msg of size 0\n");
+			continue;
+		}
+
+		if (((1ULL << body_buf_order) * PAGE_SIZE) < body_buf_read_len) {
+			free_pages((unsigned long)body_buf, body_buf_order);
+			body_buf_order = get_order(body_buf_read_len);
+			body_buf = (void *)__get_free_pages(GFP_KERNEL,
+							    body_buf_order);
+			if (body_buf == NULL) {
+				bhv_fail(
+					"GuestConn: Listen: Could allocate body buffer");
+				rv = -ENOMEM;
+				break;
+			}
+		}
+
+		err = guestconn_get_body(vsock, body_buf, body_buf_read_len);
+		if (err) {
+			if (err != -ERESTARTSYS) {
+				bhv_fail(
+					"GuestConn: Listen: error during recv (%d)",
+					err);
+				rv = err;
+			}
+			break;
+		}
+
+		deliver_body(hdr.backend, body_buf, body_buf_read_len);
+	}
+
+	sock_release(vsock);
+	if (body_buf != NULL)
+		free_pages((unsigned long)body_buf, body_buf_order);
+	return rv;
+}
+
+void bhv_guestconn_listen_reboot(void)
+{
+	kthread_stop(guestconn_listen_tsk);
+}
+
+int __init bhv_guestconn_register_backend(uint16_t backend,
+					  bhv_guestconn_backend_handler_t cb)
+{
+	if (backend >= SendConnABI__NUM_BACKENDS) {
+		pr_err("BHV: invalid backend registration (%u)\n", backend);
+		return -EINVAL;
+	}
+	if (bhv_guestconn_backend_handlers[backend] != NULL) {
+		pr_err("BHV: backend registration attempt with existing handler (%u)\n",
+		       backend);
+		return -EINVAL;
+	}
+	bhv_guestconn_backend_handlers[backend] = cb;
+	return 0;
+}
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_guestconn_listen(uint32_t bhv_guestconn_cid,
+				uint32_t bhv_guestconn_port)
+{
+	guestconn_conn_t *guestconn_conn;
+
+	guestconn_conn = kzalloc(sizeof(guestconn_conn_t), GFP_KERNEL);
+	if (guestconn_conn == NULL) {
+		bhv_fail(
+			"GuestConn: Could not allocate memory for guestconn arg\n");
+		return;
+	}
+
+	guestconn_conn->cid = bhv_guestconn_cid;
+	guestconn_conn->port = bhv_guestconn_port;
+
+	guestconn_listen_tsk =
+		kthread_create(guestconn_listen_kthread_main, guestconn_conn,
+			       "guestconn_listen");
+	if (IS_ERR(guestconn_listen_tsk)) {
+		bhv_fail("GuestConn: Could not create kthread (%ld)\n",
+			 PTR_ERR(guestconn_listen_tsk));
+		guestconn_listen_tsk = NULL;
+		kfree(guestconn_conn);
+		return;
+	}
+
+	wake_up_process(guestconn_listen_tsk);
+}
+/**********************************************************/
+
+/**********************************************************
+ * mm_init
+ **********************************************************/
+void __init bhv_mm_init_guestconn_listen(void)
+{
+}
+/**********************************************************/
diff --git security/bhv/guestconn_send.c security/bhv/guestconn_send.c
new file mode 100644
index 000000000..2e7e28b2b
--- /dev/null
+++ security/bhv/guestconn_send.c
@@ -0,0 +1,268 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022-2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include "guestconn.h"
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#define bhv_guestconn_send_configured true
+#else // out of tree
+static bool bhv_guestconn_send_configured = false;
+#endif // VASKM
+
+typedef struct {
+	struct list_head list;
+	size_t to_send;
+	GuestConnABI__Header__T *msg;
+} bhv_guestconn_send_item_t;
+
+static struct socket *guestconn_send_vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+static LIST_HEAD(bhv_guestconn_msg_list);
+static DEFINE_RAW_SPINLOCK(bhv_guestconn_msg_lock);
+
+static struct workqueue_struct *bhv_guestconn_workqueue = NULL;
+static struct work_struct bhv_guestconn_work_struct;
+static struct kmem_cache *bhv_guestconn_send_item_cache;
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void)
+{
+	_Static_assert(GuestConnABI__MAX_MSG_SZ <= PAGE_SIZE,
+		       "GuestConnABI__MAX_MSG_SZ > PAGE_SIZE");
+	GuestConnABI__Header__T *rv =
+		(GuestConnABI__Header__T *)__get_free_page(GFP_KERNEL);
+	memset(rv, 0, PAGE_SIZE);
+	return rv;
+}
+
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg)
+{
+	free_page((unsigned long)msg);
+}
+
+static inline size_t __bhv_send(void *data, size_t size, size_t to_send,
+				struct msghdr *msghdr)
+{
+	int r;
+	struct kvec vec;
+
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+
+		r = kernel_sendmsg(guestconn_send_vsock, msghdr, &vec, 1,
+				   vec.iov_len);
+		if (r == -EAGAIN)
+			return to_send;
+
+		if (r < 0) {
+			pr_err("BHV GuestLog: Send Failed (%d)", r);
+			return 0;
+		}
+
+		to_send -= r;
+	}
+
+	return 0;
+}
+
+static inline size_t bhv_send_blocking(void *data, size_t size, size_t to_send)
+{
+	struct msghdr msghdr = { .msg_flags = 0 };
+	return __bhv_send(data, size, to_send, &msghdr);
+}
+
+static inline size_t bhv_send_non_blocking(void *data, size_t size,
+					   size_t to_send)
+{
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	return __bhv_send(data, size, to_send, &msghdr);
+}
+
+static void bhv_guestconn_sendmsg(struct work_struct *ws)
+{
+	bhv_guestconn_send_item_t *item;
+	unsigned long flags;
+
+	while (true) {
+		raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+		if (list_empty(&bhv_guestconn_msg_list)) {
+			raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock,
+						   flags);
+			return;
+		}
+		item = list_first_entry(&bhv_guestconn_msg_list,
+					bhv_guestconn_send_item_t, list);
+
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+		item->to_send = bhv_send_non_blocking(item->msg, item->msg->sz,
+						      item->to_send);
+		if (item->to_send != 0) {
+			item->to_send = bhv_send_blocking(
+				item->msg, item->msg->sz, item->to_send);
+		}
+
+		BUG_ON(item->to_send != 0);
+
+		raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+		list_del(&(item->list));
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+		bhv_guestconn_free_msg(item->msg);
+		kmem_cache_free(bhv_guestconn_send_item_cache, item);
+	}
+}
+
+int bhv_guestconn_send(uint16_t backend, GuestConnABI__Header__T *msg,
+		       size_t size)
+{
+	bhv_guestconn_send_item_t *cur;
+	unsigned long flags;
+
+	if (!bhv_guestconn_send_configured) {
+		bhv_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		bhv_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+
+	pr_debug("BHV GuestConn: Queuing msg for backend %u with size %lu",
+		 backend, size);
+
+	BUG_ON(!bhv_guestconn_send_item_cache);
+
+	cur = kmem_cache_alloc(bhv_guestconn_send_item_cache, GFP_ATOMIC);
+	if (cur == NULL) {
+		bhv_guestconn_free_msg(msg);
+		bhv_fail("BHV: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	cur->msg = msg;
+
+	cur->msg->backend = backend;
+	cur->msg->sz = GuestConnABI__Header__SZ + size;
+
+	cur->to_send = cur->msg->sz;
+
+	raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+
+	list_add_tail(&(cur->list), &bhv_guestconn_msg_list);
+	raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+	if (atomic_read(&workqueue_ready))
+		queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+
+	return 0;
+}
+
+void bhv_guestconn_send_reboot(void)
+{
+	atomic_set(&reboot_in_progress, 1);
+	if (atomic_read(&workqueue_ready)) {
+		atomic_set(&workqueue_ready, 0);
+		// Drain the workqueue
+		drain_workqueue(bhv_guestconn_workqueue);
+	}
+	if (guestconn_send_vsock != NULL) {
+		// We assume all messages are gone now and we shut down the socket
+		sock_release(guestconn_send_vsock);
+		guestconn_send_vsock = NULL;
+	}
+	if (bhv_guestconn_send_item_cache != NULL) {
+		kmem_cache_destroy(bhv_guestconn_send_item_cache);
+		bhv_guestconn_send_item_cache = NULL;
+	}
+}
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_guestconn_send(uint32_t bhv_guestconn_cid,
+			      uint32_t bhv_guestconn_port)
+{
+	int err;
+	struct sockaddr_vm addr;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	vsock_addr_init(&addr, bhv_guestconn_cid, bhv_guestconn_port);
+	pr_info("bhv guestconn sender started with cid %u, port %u",
+		bhv_guestconn_cid, bhv_guestconn_port);
+#ifdef VASKM // out of tree
+	bhv_guestconn_send_configured = true;
+#endif // VASKM
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0,
+			       &guestconn_send_vsock);
+	if (err < 0) {
+		bhv_fail("GuestConn: Send: Could not create kernel socket (%d)",
+			 err);
+		return;
+	}
+
+	err = kernel_connect(guestconn_send_vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		bhv_fail("GuestConn: Send: Could not connect to host (%d)",
+			 err);
+		return;
+	}
+
+	// Initialize work queue
+	INIT_WORK(&bhv_guestconn_work_struct, bhv_guestconn_sendmsg);
+	bhv_guestconn_workqueue =
+		alloc_workqueue("bhv_guestconn_workqueue", WQ_UNBOUND, 1);
+	// queue = create_singlethread_workqueue("bhv_guestlog_work_queue");
+	if (bhv_guestconn_workqueue == NULL) {
+		bhv_fail("BHV: Could not allocate work queue!");
+		return;
+	}
+	atomic_inc(&workqueue_ready);
+
+	queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+}
+/**********************************************************/
+
+/**********************************************************
+ * mm_init
+ **********************************************************/
+void __init bhv_mm_init_guestconn_send(void)
+{
+	// Create cache
+	bhv_guestconn_send_item_cache = kmem_cache_create(
+		"bhv_guestconn_send_item_cache",
+		sizeof(bhv_guestconn_send_item_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_guestconn_send_item_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+}
+/**********************************************************/
diff --git security/bhv/guestlog.c security/bhv/guestlog.c
new file mode 100644
index 000000000..85f223e56
--- /dev/null
+++ security/bhv/guestlog.c
@@ -0,0 +1,905 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/types.h>
+
+#include <linux/cgroup.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/time_namespace.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/namei.h>
+#include <linux/fs_struct.h>
+#include <net/net_namespace.h>
+#include <net/sock.h>
+#include <net/inet_common.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+
+#include <bhv/capability.h>
+#include <bhv/config.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/util.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include "../../fs/internal.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+int unix_mkname(struct sockaddr_un *sunaddr, int len, unsigned int *hashp);
+struct sock *unix_find_other(struct net *net, struct sockaddr_un *sunname,
+			     int len, int type, unsigned int hash, int *error);
+#else
+struct sock *unix_find_other(struct net *net, struct sockaddr_un *sunaddr,
+			     int addr_len, int type);
+#endif
+struct sock *unix_find_socket_byinode(struct inode *i);
+#endif /* VASKM */
+
+HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init = { 0,
+								       false };
+
+#define BHV_GUESTLOG_MAX_PAYLOAD_SZ                                            \
+	BHV_GUESTCONN_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__Message__SZ
+
+#define BHV_GUESTLOG_MAX_BUF_SZ(EVT_T)                                         \
+	BHV_GUESTLOG_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__##EVT_T##__SZ
+
+static inline uint16_t bhv_guestlog_calc_msg_sz(uint16_t event_id,
+						size_t buf_sz)
+{
+	size_t size = GuestConnABI__GuestLog__Message__SZ;
+	switch (event_id) {
+	case GuestConnABI__GuestLog__StringMsg__EVT_ID:
+		size += GuestConnABI__GuestLog__StringMsg__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessFork__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessFork__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessExec__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__ProcessExit__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExit__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriverLoad__EVT_ID:
+		size += GuestConnABI__GuestLog__DriverLoad__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelAccess__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelAccess__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__FopsUnknown__EVT_ID:
+		size += GuestConnABI__GuestLog__FopsUnknown__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelExec__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__CgroupCreate__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupCreate__SZ;
+		break;
+	case GuestConnABI__GuestLog__CgroupDestroy__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupDestroy__SZ;
+		break;
+	case GuestConnABI__GuestLog__NamespaceChange__EVT_ID:
+		size += GuestConnABI__GuestLog__NamespaceChange__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriftDetectionNewFileExecution__EVT_ID:
+		size += GuestConnABI__GuestLog__DriftDetectionNewFileExecution__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__EVT_ID:
+		size += GuestConnABI__GuestLog__DriftDetectionInterpreterArgNewFile__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__EVT_ID:
+		size += GuestConnABI__GuestLog__DriftDetectionInterpreterArgCmd__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__EVT_ID:
+		size += GuestConnABI__GuestLog__DriftDetectionInterpreterPiped__SZ;
+		break;
+	case GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__EVT_ID:
+		size += GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__SZ;
+		break;
+	case GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__EVT_ID:
+		size += GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__SZ;
+		break;
+	case GuestConnABI__GuestLog__Capable__EVT_ID:
+		size += GuestConnABI__GuestLog__Capable__SZ;
+		break;
+	case GuestConnABI__GuestLog__ElfExecStack__EVT_ID:
+		size += GuestConnABI__GuestLog__ElfExecStack__SZ;
+		break;
+	case GuestConnABI__GuestLog__SocketConnection__EVT_ID:
+		size += GuestConnABI__GuestLog__SocketConnection__SZ;
+		break;
+	case GuestConnABI__GuestLog__SocketAccept__EVT_ID:
+		size += GuestConnABI__GuestLog__SocketAccept__SZ;
+		break;
+	case GuestConnABI__GuestLog__FileOpen__EVT_ID:
+		size += GuestConnABI__GuestLog__FileOpen__SZ;
+		break;
+	default:
+		BUG();
+	}
+	size += buf_sz;
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+	return size;
+}
+
+#define ALLOC_MSG(EVT, EVT_T)                                                  \
+	GuestConnABI__Header__T *__gc_hdr;                                     \
+	GuestConnABI__GuestLog__Message__T *__msg;                             \
+	GuestConnABI__GuestLog__##EVT_T##__T *EVT;                             \
+                                                                               \
+	__gc_hdr = bhv_guestconn_alloc_msg();                                  \
+	if (!__gc_hdr)                                                         \
+		return -ENOMEM;                                                \
+                                                                               \
+	__msg = (GuestConnABI__GuestLog__Message__T *)__gc_hdr->payload;       \
+	EVT = (GuestConnABI__GuestLog__##EVT_T##__T *)__msg->payload
+
+#define SEND_MSG(EVT, EVT_T, BUF_SIZE, FULL_PATH)                              \
+	__msg->header.evt = GuestConnABI__GuestLog__##EVT_T##__EVT_ID;         \
+	__msg->header.sz = bhv_guestlog_calc_msg_sz(                           \
+		GuestConnABI__GuestLog__##EVT_T##__EVT_ID, BUF_SIZE);          \
+                                                                               \
+	if (populate_event_context(&__msg->context, FULL_PATH)) {              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}                                                                      \
+                                                                               \
+	return bhv_guestconn_send(GuestConnABI__GuestLog__BACKEND_ID,          \
+				  __gc_hdr, __msg->header.sz)
+
+#define SEND_MSG_STRLEN(EVT, EVT_T, FULL_PATH)                                 \
+	SEND_MSG(EVT, EVT_T,                                                   \
+		 strnlen(EVT->buf, BHV_GUESTLOG_MAX_BUF_SZ(EVT_T)) + 1,        \
+		 FULL_PATH);
+
+int bhv_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+
+	ALLOC_MSG(evt, StringMsg);
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(evt->buf, BHV_GUESTLOG_MAX_BUF_SZ(StringMsg), fmt,
+			 args);
+	va_end(args);
+
+	SEND_MSG(evt, StringMsg, len, true);
+}
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _bhv_get_incoming_ns_inums(struct task_struct *tsk,
+				       struct nsset *nsset,
+				       HypABI__Context__Inums__T *inums)
+{
+	if (nsset->nsproxy) {
+		inums->cgroup_ns_inum = NS_DEREF(nsset->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(nsset->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(nsset->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(nsset->nsproxy->net_ns);
+		inums->time_ns_inum = NS_DEREF(nsset->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->time_ns_for_children);
+		inums->uts_ns_inum = NS_DEREF(nsset->nsproxy->uts_ns);
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->pid_ns_for_children);
+	} else {
+		inums->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		inums->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		inums->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+	}
+
+	inums->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+	if (nsset->flags & CLONE_NEWUSER) {
+		inums->user_ns_inum = NS_DEREF(nsset->cred->user_ns);
+	} else {
+		rcu_read_lock();
+		inums->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+	}
+}
+#undef NS_DEREF
+
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm)
+{
+	ALLOC_MSG(evt, ProcessFork);
+
+	evt->child_pid = child_pid;
+	evt->parent_pid = parent_pid;
+	evt->child_comm_offset = 0;
+	strscpy(evt->buf, child_comm, TASK_COMM_LEN);
+	evt->parent_comm_offset = strnlen(evt->buf, TASK_COMM_LEN) + 1;
+	strscpy(&(evt->buf[evt->parent_comm_offset]), parent_comm,
+		TASK_COMM_LEN);
+
+	SEND_MSG(evt, ProcessFork,
+		 evt->parent_comm_offset +
+			 strnlen(&evt->buf[evt->parent_comm_offset],
+				 TASK_COMM_LEN) +
+			 1,
+		 true);
+}
+
+static inline struct page *bhv_get_page(struct linux_binprm *bprm,
+					unsigned long addr)
+{
+	struct page *page;
+#ifdef CONFIG_MMU
+	/*
+		 * This is called at execve() time in order to dig around
+		 * in the argv/environment of the new proceess
+		 * (represented by bprm).  'current' is the process doing
+		 * the execve().
+		 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 10))
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL) <=
+	    0)
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL,
+				  NULL) <= 0)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+		return NULL;
+#else /* CONFIG_MMU */
+	page = bprm->page[addr / PAGE_SIZE];
+#endif /* CONFIG_MMU */
+	return page;
+}
+
+static uint32_t bhv_get_args_env(struct linux_binprm *bprm,
+				 uint32_t start_offset, uint32_t count,
+				 char *dst, size_t dst_sz)
+{
+	uint32_t rv = 0;
+	char *kaddr;
+	struct page *page;
+	int cur = 0;
+	unsigned long i, j;
+	unsigned long pos = bprm->p + start_offset;
+	unsigned int offset = pos % PAGE_SIZE;
+
+	if (dst_sz <= 0)
+		return 0;
+
+	if (count <= 0) {
+		dst[0] = '\0';
+		return 1;
+	}
+
+	page = bhv_get_page(bprm, pos);
+	if (page == NULL) {
+		pr_err("BHV: unable to find user page\n");
+		return rv;
+	}
+	kaddr = kmap_atomic(page);
+
+	for (i = 0, j = 0; i < dst_sz; i++) {
+		dst[i] = *(char *)(kaddr + (offset + j));
+
+		if (dst[i] == '\0') {
+			cur++;
+			if (cur == count) {
+				rv = i + 1;
+				break;
+			}
+			dst[i] = ' ';
+		}
+
+		if ((offset + j + 1) >= PAGE_SIZE) {
+			kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+			put_page(page);
+#endif
+
+			page = bhv_get_page(bprm, pos + i);
+			if (page == NULL) {
+				pr_err("BHV: unable to find user page\n");
+				dst[dst_sz - 1] = '\0';
+				return i + 1;
+			}
+			kaddr = kmap_atomic(page);
+
+			offset = 0;
+			j = 0;
+		} else {
+			j++;
+		}
+	}
+	dst[dst_sz - 1] = '\0';
+
+	kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+	put_page(page);
+#endif
+	return rv;
+}
+
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm)
+{
+	uint32_t env_offset;
+
+	ALLOC_MSG(evt, ProcessExec);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	strscpy(evt->name, comm, TASK_COMM_LEN);
+	env_offset =
+		bhv_get_args_env(bprm, 0, bprm->argc, evt->args,
+				 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+	bhv_get_args_env(bprm, env_offset, bprm->envc, evt->env,
+			 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+
+	SEND_MSG(evt, ProcessExec, 0, true);
+}
+
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm, uint32_t exit_code)
+{
+	ALLOC_MSG(evt, ProcessExit);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	evt->exit_code = exit_code;
+	strscpy(evt->buf, comm, TASK_COMM_LEN);
+
+	SEND_MSG_STRLEN(evt, ProcessExit, false);
+}
+
+int bhv_guestlog_log_elf_load_exec_stack(struct linux_binprm *bprm)
+{
+	int len;
+	if (!bhv_guestlog_log_process_events()) {
+		return 0;
+	}
+
+	ALLOC_MSG(evt, ElfExecStack);
+
+	len = 1 /* null terminator */ +
+	      snprintf(evt->buf, BHV_GUESTLOG_MAX_BUF_SZ(ElfExecStack), "%pD4",
+		       bprm->file /* binary */);
+
+	SEND_MSG(evt, ElfExecStack, len, true);
+}
+
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupCreate);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupCreate, 0, true);
+}
+
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupDestroy);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupDestroy, 0, true);
+}
+
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset)
+{
+	ALLOC_MSG(evt, NamespaceChange);
+
+	evt->target_task_pid = tsk->pid;
+	strncpy(evt->target_task_name, tsk->comm,
+		GuestConnABI__GuestLog__PROC_COMM_SZ);
+
+	_bhv_get_incoming_ns_inums(tsk, nsset, &evt->incoming_inums);
+
+	SEND_MSG(evt, NamespaceChange, 0, true);
+}
+
+int bhv_guestlog_log_capable(int cap)
+{
+	ALLOC_MSG(evt, Capable);
+
+	evt->cap = (u64)cap;
+	strncpy(evt->cap_str, cap2str(cap),
+		GuestConnABI__GuestLog__Capable__MAX_CAP_STR_SZ);
+
+	SEND_MSG(evt, Capable, 0, true);
+}
+
+int bhv_guestlog_log_driver_load(const char *name)
+{
+	ALLOC_MSG(evt, DriverLoad);
+
+	strscpy(evt->buf, name, BHV_GUESTLOG_MAX_BUF_SZ(DriverLoad));
+
+	SEND_MSG_STRLEN(evt, DriverLoad, true);
+}
+
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t type)
+{
+	ALLOC_MSG(evt, KernelAccess);
+
+	evt->address = addr;
+	evt->type = type;
+
+	SEND_MSG(evt, KernelAccess, 0, true);
+}
+
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint8_t type, uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr)
+{
+	ALLOC_MSG(evt, FopsUnknown);
+
+	evt->magic = (u64)magic;
+	evt->struct_type = type;
+	evt->special_major = major;
+	evt->special_minor = minor;
+	evt->address = (u64)fops_ptr;
+	strscpy(evt->buf, pathname, BHV_GUESTLOG_MAX_BUF_SZ(FopsUnknown));
+
+	SEND_MSG_STRLEN(evt, FopsUnknown, true);
+}
+
+static void bhv_guestlog_char_array_into_buf(char *dst, size_t dst_size,
+					     char **arr)
+{
+	char *cur;
+	unsigned int i;
+	ssize_t cur_len = 0;
+	ssize_t total_len = 0;
+
+	BUG_ON(dst == NULL);
+
+	if (arr == NULL) {
+		if (dst_size > 0)
+			dst[0] = '\0';
+
+		return;
+	}
+
+	for (i = 0, cur = arr[0]; cur != NULL; i++, cur = arr[i]) {
+		if (i != 0) {
+			// Replace NULL of previous round with a space
+			dst[total_len] = ' ';
+			// Increase len since strscpy returns len without NULL
+			total_len++;
+		}
+
+		cur_len = strscpy(&dst[total_len], cur, dst_size);
+
+		// No more room. We are done
+		if (cur_len == -E2BIG)
+			return;
+
+		// Update size
+		dst_size -= cur_len;
+		total_len += cur_len;
+
+		// No more room. We are done. The last character is '\0'
+		if (dst_size <= 1)
+			return;
+	}
+}
+
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp)
+{
+	ALLOC_MSG(evt, KernelExec);
+
+	strscpy(evt->path, path, sizeof(evt->path));
+	bhv_guestlog_char_array_into_buf(evt->args, sizeof(evt->args), argv);
+	bhv_guestlog_char_array_into_buf(evt->env, sizeof(evt->env), envp);
+
+	SEND_MSG(evt, KernelExec, 0, true);
+}
+
+int bhv_guestlog_log_drift_detection_exec_new_file(const char *path,
+						   bool blocked)
+{
+	ALLOC_MSG(evt, DriftDetectionNewFileExecution);
+
+	strncpy(evt->path, path, sizeof(evt->path));
+	evt->path[sizeof(evt->path) - 1] = '\0';
+	evt->blocked = blocked;
+
+	SEND_MSG(evt, DriftDetectionNewFileExecution, 0, true);
+}
+
+int bhv_guestlog_log_drift_detection_interpreter_arg_new_file(
+	const char *interpreter, const char *path, size_t filename_sz,
+	bool blocked)
+{
+	size_t sz;
+
+	ALLOC_MSG(evt, DriftDetectionInterpreterArgNewFile);
+
+	strncpy(evt->interpreter_name, interpreter,
+		sizeof(evt->interpreter_name));
+	evt->interpreter_name[sizeof(evt->interpreter_name) - 1] = '\0';
+
+	sz = min(filename_sz, sizeof(evt->path));
+	strncpy(evt->path, path, sz);
+	evt->path[sz - 1] = '\0';
+
+	evt->blocked = blocked;
+
+	SEND_MSG(evt, DriftDetectionInterpreterArgNewFile, 0, true);
+}
+
+int bhv_guestlog_log_drift_detection_interpreter_arg_cmd(
+	const char *interpreter, const char *cmd, size_t cmd_sz, bool blocked)
+{
+	size_t sz;
+
+	ALLOC_MSG(evt, DriftDetectionInterpreterArgCmd);
+
+	strncpy(evt->interpreter_name, interpreter,
+		sizeof(evt->interpreter_name));
+	evt->interpreter_name[sizeof(evt->interpreter_name) - 1] = '\0';
+
+	sz = min(cmd_sz, sizeof(evt->command));
+	strncpy(evt->command, cmd, sz);
+	evt->command[sz - 1] = '\0';
+
+	evt->blocked = blocked;
+
+	SEND_MSG(evt, DriftDetectionInterpreterArgCmd, 0, true);
+}
+
+int bhv_guestlog_log_drift_detection_interpreter_piped(const char *interpreter,
+						       bool blocked)
+{
+	ALLOC_MSG(evt, DriftDetectionInterpreterPiped);
+
+	strncpy(evt->interpreter_name, interpreter,
+		sizeof(evt->interpreter_name));
+	evt->interpreter_name[sizeof(evt->interpreter_name) - 1] = '\0';
+	evt->blocked = blocked;
+
+	SEND_MSG(evt, DriftDetectionInterpreterPiped, 0, true);
+}
+
+static inline void _bhv_guestlog_add_ip_info_sockaddr(char *dest,
+						      size_t dest_sz,
+						      struct sockaddr *addr)
+{
+	snprintf(dest, dest_sz, "%pISpc", addr);
+}
+
+static inline void _bhv_guestlog_add_ip_info_socket(char *dest, size_t dest_sz,
+						    struct socket *sock,
+						    bool dest_addr)
+{
+	if (sock->sk->sk_family == AF_INET || sock->sk->sk_family == AF_INET6) {
+		struct sockaddr addr;
+		inet_getname(sock, &addr, dest_addr);
+		_bhv_guestlog_add_ip_info_sockaddr(dest, dest_sz, &addr);
+	} else {
+		strncpy(dest, "UNKNOWN", dest_sz);
+		dest[dest_sz - 1] = '\0';
+	}
+}
+
+static inline void _bhv_guestlog_add_ip_info_file(char *dest, size_t dest_sz,
+						  struct file *file,
+						  bool dest_addr)
+{
+	struct socket *sock;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+	int err;
+	if (file && (sock = sock_from_file(file, &err)))
+#else
+	if (file && (sock = sock_from_file(file)))
+#endif
+	{
+		_bhv_guestlog_add_ip_info_socket(dest, dest_sz, sock,
+						 dest_addr);
+	} else {
+		strncpy(dest, "UNKNOWN", dest_sz);
+		dest[dest_sz - 1] = '\0';
+	}
+}
+
+int bhv_guestlog_log_reverse_shell_detection_interpreter_bound(
+	const char *interpreter, uint32_t fd, struct file *file, bool blocked)
+{
+	ALLOC_MSG(evt, ReverseShellDetectionInterpreterBound);
+
+	strncpy(evt->interpreter_name, interpreter,
+		sizeof(evt->interpreter_name));
+	evt->interpreter_name[sizeof(evt->interpreter_name) - 1] = '\0';
+	evt->fd = fd;
+	evt->blocked = blocked;
+	_bhv_guestlog_add_ip_info_file(
+		evt->destination_addr,
+		GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__MAX_ADDR_STR_SZ,
+		file, true);
+	_bhv_guestlog_add_ip_info_file(
+		evt->source_addr,
+		GuestConnABI__GuestLog__ReverseShellDetectionInterpreterBound__MAX_ADDR_STR_SZ,
+		file, false);
+
+	SEND_MSG(evt, ReverseShellDetectionInterpreterBound, 0, true);
+}
+
+int bhv_guestlog_log_reverse_shell_detection_interpreter_transitive(
+	struct task_struct *interpreter, uint32_t interpreter_fd,
+	struct task_struct *transitive, uint32_t transitive_pipe_fd,
+	uint32_t transitive_socket_fd, struct sockaddr *dest_address,
+	bool blocked)
+{
+	struct file *file;
+
+	ALLOC_MSG(evt, ReverseShellDetectionInterpreterTransitive);
+
+	get_path_from_task(interpreter, evt->interpreter_name,
+			   sizeof(evt->interpreter_name), true);
+	evt->interpreter_fd = interpreter_fd;
+	evt->interpreter_pid = interpreter->tgid;
+	evt->blocked = blocked;
+	get_path_from_task(transitive, evt->transitive_path,
+			   sizeof(evt->transitive_path), true);
+	evt->transitive_pid = transitive->tgid;
+	evt->transitive_pipe_fd = transitive_pipe_fd;
+	evt->transitive_socket_fd = transitive_socket_fd;
+
+	file = fget_task(transitive, transitive_socket_fd);
+
+	if (dest_address) {
+		_bhv_guestlog_add_ip_info_sockaddr(
+			evt->destination_addr,
+			GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ,
+			dest_address);
+	} else {
+		_bhv_guestlog_add_ip_info_file(
+			evt->destination_addr,
+			GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ,
+			file, true);
+	}
+
+	_bhv_guestlog_add_ip_info_file(
+		evt->source_addr,
+		GuestConnABI__GuestLog__ReverseShellDetectionInterpreterTransitive__MAX_ADDR_STR_SZ,
+		file, false);
+
+	fput(file);
+
+	SEND_MSG(evt, ReverseShellDetectionInterpreterTransitive, 0, true);
+}
+
+int bhv_guestlog_log_net_socket_connection(bool local, struct sockaddr *address)
+{
+	ALLOC_MSG(evt, SocketConnection);
+
+	evt->local = local;
+	evt->domain = false;
+	evt->protected_sock = false;
+	_bhv_guestlog_add_ip_info_sockaddr(
+		evt->address,
+		GuestConnABI__GuestLog__SocketConnection__MAX_ADDR_STR_SZ,
+		address);
+
+	SEND_MSG(evt, SocketConnection, 0, true);
+}
+
+#ifdef VASKM
+/***********************************************
+ * This mechanism is not supported by the vaskm
+ * at the moment as the functions required are
+ * inlined by many kernels.
+ ***********************************************/
+static bool _bhv_guestlog_is_runtime_sock(struct socket *sock,
+					  struct sockaddr_un *address,
+					  int addrlen)
+{
+	return false;
+}
+#else /* VASKM */
+static struct sock *_bhv_get_unix_sock(struct sockaddr_un *sunaddr,
+				       int addr_len)
+{
+	struct inode *inode;
+	struct path path;
+	struct sock *sk;
+	int err;
+
+	err = vfs_path_lookup(init_task.fs->root.dentry, init_task.fs->root.mnt,
+			      sunaddr->sun_path,
+			      LOOKUP_FOLLOW | LOOKUP_AUTOMOUNT, &path);
+	if (err)
+		return ERR_PTR(err);
+
+	inode = d_backing_inode(path.dentry);
+	if (!S_ISSOCK(inode->i_mode)) {
+		path_put(&path);
+		return ERR_PTR(-ECONNREFUSED);
+	}
+
+	sk = unix_find_socket_byinode(inode);
+	if (!sk) {
+		path_put(&path);
+		return ERR_PTR(-ECONNREFUSED);
+	}
+
+	path_put(&path);
+	return sk;
+}
+
+static bool _bhv_guestlog_runtime_sock_filter(const char *path, void *arg)
+{
+	struct sockaddr_un cur_addr;
+	struct sock *target, *cur_sock;
+
+	target = arg;
+
+	cur_addr.sun_family = AF_UNIX;
+	strncpy(cur_addr.sun_path, path, UNIX_PATH_MAX);
+	cur_addr.sun_path[UNIX_PATH_MAX - 1] = '\0';
+	cur_sock = _bhv_get_unix_sock(&cur_addr, sizeof(cur_addr));
+	if (!IS_ERR(cur_sock)) {
+		sock_put(cur_sock);
+		if (cur_sock == target) {
+			return true;
+		}
+	}
+	return false;
+}
+
+static bool _bhv_guestlog_is_runtime_sock(struct socket *sock,
+					  struct sockaddr_un *address,
+					  int addrlen)
+{
+	struct sock *target;
+	bool found;
+
+	if (address->sun_family != AF_UNIX ||
+	    bhv_policy_protected_unix_sockets_is_empty())
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+	{
+		unsigned int hash;
+		int err;
+		err = unix_mkname(address, addrlen, &hash);
+		if (err < 0)
+			return false;
+
+		target = unix_find_other(sock_net(sock->sk), address, addrlen,
+					 sock->type, hash, &err);
+		if (!target)
+			return false;
+	}
+#else
+	target = unix_find_other(sock_net(sock->sk), address, addrlen,
+				 sock->type);
+	if (IS_ERR(target))
+		return false;
+#endif
+
+	found = bhv_match_each_protected_unix_sockets(
+		_bhv_guestlog_runtime_sock_filter, target);
+	sock_put(target);
+	return found;
+}
+#endif /* VASKM */
+
+int bhv_guestlog_log_unix_socket_connection(struct socket *sock,
+					    struct sockaddr_un *address,
+					    int addrlen)
+{
+	ALLOC_MSG(evt, SocketConnection);
+
+	evt->local = false;
+	evt->domain = true;
+	evt->protected_sock =
+		_bhv_guestlog_is_runtime_sock(sock, address, addrlen);
+	strncpy(evt->address, address->sun_path,
+		GuestConnABI__GuestLog__SocketConnection__MAX_ADDR_STR_SZ);
+
+	SEND_MSG(evt, SocketConnection, 0, true);
+}
+
+int bhv_guestlog_log_socket_accept(struct socket *sock)
+{
+	ALLOC_MSG(evt, SocketAccept);
+
+	_bhv_guestlog_add_ip_info_socket(
+		evt->src_address,
+		GuestConnABI__GuestLog__SocketAccept__MAX_ADDR_STR_SZ, sock,
+		false);
+	_bhv_guestlog_add_ip_info_socket(
+		evt->dest_address,
+		GuestConnABI__GuestLog__SocketAccept__MAX_ADDR_STR_SZ, sock,
+		true);
+
+	SEND_MSG(evt, SocketAccept, 0, true);
+}
+
+int bhv_guestlog_log_file_open(struct file *file)
+{
+	const char *path;
+	char *tmp;
+	ALLOC_MSG(evt, FileOpen);
+
+	/* Avoid allocating this buffer on the stack. */
+	tmp = kmalloc(HypABI__Context__MAX_PATH_SZ, GFP_KERNEL);
+	if (tmp == NULL) {
+		bhv_guestconn_free_msg(__gc_hdr);
+		return -ENOMEM;
+	}
+
+	path = bhv_get_file_path(file, tmp, HypABI__Context__MAX_PATH_SZ);
+	strncpy(evt->path, path, HypABI__Context__MAX_PATH_SZ);
+	evt->path[HypABI__Context__MAX_PATH_SZ - 1] = '\0';
+
+	kfree(tmp);
+
+	evt->is_link = S_ISLNK(file->f_inode->i_mode);
+	evt->is_dir = S_ISDIR(file->f_inode->i_mode);
+	evt->is_fifo = S_ISFIFO(file->f_inode->i_mode);
+	evt->is_socket = S_ISSOCK(file->f_inode->i_mode);
+
+	evt->mode_write = (bool)(file->f_mode & FMODE_WRITE);
+
+	SEND_MSG(evt, FileOpen, 0, true);
+}
+
+/*****************************************************************
+ * init
+ *****************************************************************/
+int __init bhv_init_guestlog()
+{
+	static HypABI__Guestlog__Init__arg__T glc_fb;
+	HypABI__Guestlog__Init__arg__T *glc;
+	int rc;
+
+	if (!bhv_guestlog_enabled())
+		return 0;
+
+	glc = HypABI__Guestlog__Init__arg__ALLOC_STATICFALLBACK(glc_fb);
+
+	rc = HypABI__Guestlog__Init__hypercall_noalloc(glc);
+	if (rc == 0) {
+		bhv_guestlog_config = *glc;
+	} else {
+		bhv_fail("BHV: guestlog init failed");
+	}
+
+	HypABI__Guestlog__Init__arg__FREE_STATICFALLBACK(glc, glc_fb);
+
+	return rc;
+}
+/***************************************************************/
diff --git security/bhv/init/init.c security/bhv/init/init.c
new file mode 100644
index 000000000..4a496a9e7
--- /dev/null
+++ security/bhv/init/init.c
@@ -0,0 +1,604 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include "bhv/bhv.h"
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/syscall.h>
+#include <linux/jump_label.h>
+#include <linux/kmod.h>
+#include <linux/mm.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv_print.h>
+
+#include <bhv/bhv.h>
+#include <bhv/config.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestcmd.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/creds.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/creds.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_comm_start[];
+extern char __bhv_vault_comm_end[];
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_text_jump_label_start[];
+extern char __bhv_vault_text_jump_label_end[];
+
+extern char __start_static_call_sites[];
+extern char __stop_static_call_sites[];
+extern char __start_static_call_tramp_key[];
+extern char __stop_static_call_tramp_key[];
+
+extern char __alt_instructions[];
+extern char __alt_instructions_end[];
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+extern char __start_bhv_tp_vault[];
+extern char __end_bhv_tp_vault[];
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+
+extern uint8_t __bhv_text_start[];
+extern uint8_t __bhv_text_end[];
+#endif // VASKM
+
+bool __bhv_init_done __ro_after_init = false;
+
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define BHV_VAULT_PRINT_DEBUG_INFO(v, vault_name)			\
+{									\
+	int i = 0;							\
+									\
+	pr_info("Registering vault '%s': ep=%u rp=%u:\n",		\
+		#vault_name, v->nr_entry_points, v->nr_return_points);	\
+									\
+	for (i = 0; i < v->nr_entry_points; ++i) {			\
+		pr_info("\t Entry point @ 0x%llx\n",			\
+		        (uint64_t)(&v->transit_points)[i]);		\
+	}								\
+									\
+	pr_info("\t CODE Region     @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+		 v->code.gpa, v->code.gpa + v->code.size,		\
+		 __bhv_vault_text_##vault_name##_start,			\
+		 __bhv_vault_text_##vault_name##_end);			\
+	pr_info("\t REF_CODE Region @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+		 v->ref_code.gpa,					\
+		 v->ref_code.gpa + v->ref_code.size,			\
+		 __bhv_vault_ref_text_##vault_name##_start,		\
+		 __bhv_vault_ref_text_##vault_name##_end);		\
+	pr_info("\t DATA Region     @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+		 v->data.gpa, v->data.gpa + v->data.size,		\
+		 __bhv_vault_data_##vault_name##_start,			\
+		 __bhv_vault_data_##vault_name##_end);			\
+	pr_info("\t RODATA Region   @ [0x%llx:0x%llx] -> [0x%px:0x%px]\n", \
+		 v->ro_data.gpa, v->ro_data.gpa + v->ro_data.size,	\
+		 __bhv_vault_ro_data_##vault_name##_start,		\
+		 __bhv_vault_ro_data_##vault_name##_end);		\
+}
+#else /* !CONFIG_BHV_VAS_DEBUG */
+#define BHV_VAULT_PRINT_DEBUG_INFO(v, vault_name)
+#endif
+
+#ifdef CONFIG_RETPOLINE
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)                        \
+	{                                                                \
+		extern char __indirect_thunk_start[];                    \
+		extern char __indirect_thunk_end[];                      \
+                                                                         \
+		vault->thunks.gpa =                                      \
+			bhv_virt_to_phys_single(__indirect_thunk_start); \
+		vault->thunks.size =                                     \
+			__indirect_thunk_end - __indirect_thunk_start;   \
+	}
+#else /* !CONFIG_RETPOLINE */
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)			\
+{									\
+	vault->thunks.gpa = 0;						\
+	vault->thunks.size = 0;						\
+}
+#endif /* CONFIG_RETPOLINE */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_ALTINSTR_AUX(vault)                         \
+{                                                                      \
+	extern char __altinstr_aux_start[];                             \
+	extern char __altinstr_aux_end[];                               \
+                                                                       \
+	vault->altinstr_aux.gpa =                                       \
+		bhv_virt_to_phys_single(__altinstr_aux_start);          \
+	vault->altinstr_aux.size =                                      \
+		__altinstr_aux_end - __altinstr_aux_start;              \
+}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_ALTINSTR_AUX(vault)                         \
+{                                                                      \
+	vault->altinstr_aux.gpa = 0;                                    \
+	vault->altinstr_aux.size = 0;                                   \
+}
+#endif /* CONFIG_X86_64 */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_SHARED_CODE(vault, vault_name)               \
+{                                                                       \
+	extern char __bhv_vault_shared_text_##vault_name##_start[];     \
+	extern char __bhv_vault_shared_text_##vault_name##_end[];       \
+                                                                        \
+	vault->shared_code.gpa = bhv_virt_to_phys_single(              \
+		__bhv_vault_shared_text_##vault_name##_start);          \
+	vault->shared_code.size =                                      \
+		__bhv_vault_shared_text_##vault_name##_end -            \
+		__bhv_vault_shared_text_##vault_name##_start;           \
+}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_SHARED_CODE(vault, vault_name)               \
+{                                                                       \
+	vault->shared_code.gpa = 0;                                     \
+	vault->shared_code.size = 0;                                    \
+}
+#endif /* CONFIG_X86_64 */
+
+#ifdef CONFIG_X86_64
+#define BHV_VAULT_REGISTER_NOINSTR_CODE(vault)                         \
+{                                                                      \
+	extern char __noinstr_text_start[];                             \
+	extern char __noinstr_text_end[];                               \
+                                                                       \
+	vault->noinstr_text.gpa =                                      \
+		bhv_virt_to_phys_single(__noinstr_text_start);          \
+	vault->noinstr_text.size =                                     \
+		__noinstr_text_end - __noinstr_text_start;              \
+}
+#else /* !CONFIG_X86_64 */
+#define BHV_VAULT_REGISTER_NOINSTR_CODE(vault)                         \
+{                                                                      \
+	vault->noinstr_text.gpa = 0;                                    \
+	vault->noinstr_text.size = 0;                                   \
+}
+#endif /* CONFIG_X86_64 */
+
+
+#define BHV_VAULT_REGISTER(vault, buf, buf_size)                       \
+{                                                                      \
+	extern char __bhv_vault_text_##vault##_start[];                 \
+	extern char __bhv_vault_text_##vault##_end[];                   \
+	extern char __bhv_vault_ref_text_##vault##_start[];             \
+	extern char __bhv_vault_ref_text_##vault##_end[];               \
+	extern char __bhv_vault_data_##vault##_start[];                 \
+	extern char __bhv_vault_data_##vault##_end[];                   \
+	extern char __bhv_vault_ro_data_##vault##_start[];              \
+	extern char __bhv_vault_ro_data_##vault##_end[];                \
+	uint32_t ep_ctr = 0;                                            \
+	uint32_t rp_ctr = 0;                                            \
+	uint32_t __cur_ctr = 0;                                         \
+	int __r = 0;                                                    \
+                                                                       \
+	HypABI__Wagner__Create__arg__T *_vault =                        \
+        (HypABI__Wagner__Create__arg__T *)buf;                          \
+                                                                        \
+	bhv_vault_return_point_helper_t *ret;                           \
+	bhv_vault_entry_point_helper_t *entry;                          \
+	BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                    \
+	{                                                               \
+        	ep_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +                \
+        			sizeof(uint64_t) * ep_ctr) > buf_size); \
+                                                                        \
+	_vault->nr_entry_points = ep_ctr;                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                    \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys_single(entry->ep);    \
+        	__cur_ctr++;                                            \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                     \
+	{                                                               \
+        	rp_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                    \
+	{                                                               \
+        	rp_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +                \
+               (sizeof(uint64_t) * ep_ctr) +                            \
+               (sizeof(uint64_t) * rp_ctr)) > buf_size);                \
+                                                                        \
+	__cur_ctr = _vault->nr_entry_points;                            \
+	_vault->nr_return_points = rp_ctr;                              \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                     \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys_single(ret->rp);      \
+        	__cur_ctr++;                                            \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                    \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys_single(ret->rp);      \
+        	__cur_ctr++;                                            \
+	}                                                               \
+	_vault->transit_points =                                        \
+        bhv_virt_to_phys_single((uint64_t *)&_vault->transit_points +   \
+                        sizeof(uint64_t));                              \
+                                                                        \
+	_vault->code.gpa =                                              \
+        bhv_virt_to_phys_single(__bhv_vault_text_##vault##_start);      \
+	_vault->code.size = __bhv_vault_text_##vault##_end -            \
+        __bhv_vault_text_##vault##_start;                               \
+	_vault->ref_code.gpa = bhv_virt_to_phys_single(                 \
+        		__bhv_vault_ref_text_##vault##_start);          \
+	_vault->ref_code.size = __bhv_vault_ref_text_##vault##_end -    \
+				__bhv_vault_ref_text_##vault##_start;   \
+	_vault->data.gpa = bhv_virt_to_phys_single(                     \
+		__bhv_vault_data_##vault##_start);                      \
+	_vault->data.size = __bhv_vault_data_##vault##_end -            \
+        __bhv_vault_data_##vault##_start;                               \
+	_vault->ro_data.gpa =                                           \
+        bhv_virt_to_phys_single(__bhv_vault_ro_data_##vault##_start);   \
+	_vault->ro_data.size = __bhv_vault_ro_data_##vault##_end -      \
+        __bhv_vault_ro_data_##vault##_start;                            \
+	_vault->entry_text.start = (uint64_t)__entry_text_start;        \
+	_vault->entry_text.end = (uint64_t)__entry_text_end;            \
+                                                                       \
+	BHV_VAULT_REGISTER_ALTINSTR_AUX(_vault)				\
+	BHV_VAULT_REGISTER_INDIRECT_THUNKS(_vault)                      \
+	BHV_VAULT_REGISTER_SHARED_CODE(_vault, vault)			\
+	BHV_VAULT_REGISTER_NOINSTR_CODE(_vault)				\
+                                                                       \
+	BHV_VAULT_PRINT_DEBUG_INFO(_vault, vault)			\
+                                                                        \
+	__r = HypABI__Wagner__Create__hypercall_noalloc(_vault);        \
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+static inline void
+bhv_section_run_ctor(HypABI__Init__Init__BHVSectionRun__T *curr_item,
+		     HypABI__Init__Init__BHVSectionRun__T *prev_item,
+		     uint64_t gpa_start, uint64_t size, uint8_t type)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (HypABI__Init__Init__BHVSectionRun__T){
+		.gpa_start = gpa_start,
+		.size = size,
+		.type = type,
+		.next = BHV_INVALID_PHYS_ADDR,
+	};
+
+	if (prev_item)
+		prev_item->next = bhv_virt_to_phys_single(curr_item);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static int __init bhv_vault_extend(void)
+{
+	int r = 0;
+	HypABI__Wagner__Extend__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return 0;
+
+	/*
+	 * Note: jump lables ((__start|__stop)___jump_table) and static calls
+	 * ((__start|__stop)_static_call_sites) are part of the __ro_after_init
+	 * section. As such, they will be protected by BRASS integrity and do
+	 * not need to be explicitly propagated to the spaces-based BRASS vault.
+	 */
+
+	/* Alternative instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+#ifdef CONFIG_PARAVIRT
+	/* Paravirt instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__parainstructions);
+	vault.mem.size = (unsigned long)__parainstructions_end - (unsigned long)__parainstructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+#ifdef CONFIG_RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys_single(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end - (unsigned long)__retpoline_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+	vault.mem.gpa = bhv_virt_to_phys_single(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end - (unsigned long)__return_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+#ifdef CONFIG_TRACEPOINTS
+	/* Tracepoints */
+	vault.mem.gpa = bhv_virt_to_phys_single(__start_bhv_tp_vault);
+	vault.mem.size = (unsigned long)__end_bhv_tp_vault - (unsigned long)__start_bhv_tp_vault;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+	return 0;
+}
+
+#endif
+
+static int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size)
+{
+	unsigned long r;
+	unsigned int region_counter = 0;
+
+	struct {
+		HypABI__Init__Init__arg__T init_arg;
+		bhv_mem_region_t mem_regions[BHV_INIT_MAX_REGIONS];
+		HypABI__Init__Init__BHVSectionRun__T bhv_section_runs[];
+	} *arg = bhv_data;
+	BUG_ON((void *)&arg->bhv_section_runs[2] - bhv_data > +bhv_data_size);
+
+#ifndef VASKM // inside kernel tree
+#define BI_ALIGN_START(start) (unsigned long)(start)
+#else // out of tree
+#define BI_ALIGN_START(start) round_down((unsigned long)(start), PAGE_SIZE)
+#endif // VASKM
+
+#define BI_ALIGN_START_SIZE(start, end)                                        \
+	(bhv_virt_to_phys_single((void *)BI_ALIGN_START(start))),              \
+		(round_up((unsigned long)(end), PAGE_SIZE) -                   \
+		 BI_ALIGN_START(start))
+#define BI_ALIGN_START_SIZE_KLN(start, end)                                    \
+	BI_ALIGN_START_SIZE(KLN_SYM(start), KLN_SYM(end))
+#define BI_LL_FIRST(ll) &(ll)[region_counter], NULL
+#define BI_LL_NEXT(ll) &(ll)[region_counter], &(ll)[region_counter - 1]
+
+	bhv_mem_region_create_ctor(BI_LL_FIRST(arg->mem_regions),
+				   BI_ALIGN_START_SIZE_KLN(_stext, _etext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL TEXT SECTION");
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sinittext, _einittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL INIT TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sexittext, _eexittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL EXIT TEXT SECTION");
+	region_counter++;
+#endif // VASKM
+
+	bhv_init_hyp_arch(arg->mem_regions, &region_counter);
+
+	region_counter = 0;
+	BUG_ON((unsigned long)bhv_data & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_FIRST(arg->bhv_section_runs),
+		bhv_virt_to_phys(bhv_data, bhv_data_size), bhv_data_size,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA);
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		BI_ALIGN_START_SIZE(__bhv_text_start, __bhv_text_end),
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+	region_counter++;
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else // out of tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		vmalloc_to_phys(__bhv_text_start), PAGE_SIZE,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+
+	for (uint8_t *p = __bhv_text_start + PAGE_SIZE; p < __bhv_text_end;
+	     p += PAGE_SIZE) {
+		phys_addr_t phy = vmalloc_to_phys(p);
+		if (phy == arg->bhv_section_runs[region_counter].gpa_start +
+				   arg->bhv_section_runs[region_counter].size) {
+			arg->bhv_section_runs[region_counter].size += PAGE_SIZE;
+		} else {
+			region_counter++;
+			BUG_ON((uint64_t)&arg->bhv_section_runs[region_counter +
+								1] -
+				       (uint64_t)arg >
+			       bhv_data_size);
+
+			bhv_section_run_ctor(
+				BI_LL_NEXT(arg->bhv_section_runs), phy,
+				PAGE_SIZE,
+				HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+		}
+	}
+	region_counter++;
+#endif // VASKM
+
+#ifdef BHV_CONST_MODPROBE_PATH
+	arg->init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	arg->init_arg.modprobe_path =
+		bhv_virt_to_phys((void *)&modprobe_path, KMOD_PATH_LEN);
+#else
+	arg->init_arg.modprobe_path_sz = 0;
+	arg->init_arg.modprobe_path = BHV_INVALID_PHYS_ADDR;
+#endif /* BHV_CONST_MODPROBE_PATH */
+
+	arg->init_arg.owner = 0;
+	arg->init_arg.region_head = BHV_VIRT_TO_PHYS_SIZEOF(&arg->mem_regions);
+	arg->init_arg.bhv_region_head =
+		bhv_virt_to_phys_single(&arg->bhv_section_runs);
+
+	r = HypABI__Init__Init__hypercall_noalloc(&arg->init_arg);
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+
+void __init bhv_init_platform(void)
+{
+	int rv;
+	uint32_t cid, port;
+	void *bhv_data_ptr = NULL;
+	HypABI__Init__Init__BHVData__T *data_ptr = NULL;
+	static_assert(sizeof(uint64_t) ==
+		      sizeof(unsigned long)); //for pointer cast below
+
+	bhv_init_arch();
+
+#ifndef VASKM // inside kernel tree
+	rv = bhv_init_hyp(__bhv_data_start, __bhv_data_end - __bhv_data_start);
+	bhv_data_ptr = __bhv_data_start;
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+#else // out of tree
+	bhv_data_ptr = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	// no kfree on purpose
+	if (!bhv_data_ptr) {
+		pr_err("BHV: cannot allocate bhv_data\n");
+		return;
+	}
+
+	rv = bhv_init_hyp(bhv_data_ptr, PAGE_SIZE);
+#endif // VASKM
+
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return;
+	}
+
+	bhv_initialized = true;
+	data_ptr = bhv_data_ptr;
+	bhv_configuration_bitmap =
+		(unsigned long *)&data_ptr
+			->config_bitmap; // see static_cast above
+	cid = data_ptr->vsocket_cid;
+	port = data_ptr->vsocket_port;
+
+#if !defined(VASKM) && defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_vault_is_enabled()) {
+		rv = (BHV_VAULT_REGISTER(jump_label, __bhv_vault_comm_start,
+  				         __bhv_vault_comm_end - __bhv_vault_comm_start));
+		if (rv) {
+			bhv_fail("BHV: Cannot create spaces-based Vault");
+			return;
+		}
+
+		rv = bhv_vault_extend();
+		if (rv) {
+			bhv_fail("BHV: Cannot extend the spaces-based Vault");
+			return;
+		}
+	}
+#endif
+
+	rv = bhv_init_guestconn(cid, port);
+	if (rv) {
+		bhv_fail(
+			"BHV: Cannot configure the BHV guest connection subsystem");
+		return;
+	}
+
+	rv = bhv_init_guestlog();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV logging subsystem");
+		return;
+	}
+
+	bhv_init_guestcmd();
+
+	rv = bhv_init_cred();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV creds subsystem");
+		return;
+	}
+
+	bhv_init_fileops();
+
+#ifndef VASKM // inside kernel tree
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+#endif // VASKM
+
+#if !defined(VASKM) && defined(CONFIG_BHV_VAULT_SPACES)
+	/*
+	 * Initialize static keys.
+	 *
+	 * XXX: Consider moving this to another place.
+	 */
+	bhv_init_jump_label();
+	bhv_init_alternatives();
+	bhv_init_static_call();
+#endif
+
+	__bhv_init_done = true;
+}
diff --git security/bhv/init/late_start.c security/bhv/init/late_start.c
new file mode 100644
index 000000000..068599788
--- /dev/null
+++ security/bhv/init/late_start.c
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+#include <bhv/init/late_start.h>
+
+void bhv_late_start(void)
+{
+	int r = bhv_late_start_init_ptpg();
+	if (r) {
+		bhv_fail("ptpg init failed");
+	}
+}
diff --git security/bhv/init/mm_init.c security/bhv/init/mm_init.c
new file mode 100644
index 000000000..e490e0827
--- /dev/null
+++ security/bhv/init/mm_init.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/file_protection.h>
+#include <bhv/config.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/acl.h>
+#endif // VASKM
+
+#include <bhv/init/mm_init.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+void __init bhv_mm_init(void)
+{
+	HypABI__init_slabs();
+
+	bhv_mm_init_config();
+	bhv_mm_init_integrity();
+#ifndef VASKM // inside kernel tree
+	bhv_mm_init_acl();
+#endif // VASKM
+	bhv_mm_init_guestconn();
+	bhv_mm_init_cred();
+	bhv_mm_init_file_protection();
+}
diff --git security/bhv/init/start.c security/bhv/init/start.c
new file mode 100644
index 000000000..29031d543
--- /dev/null
+++ security/bhv/init/start.c
@@ -0,0 +1,218 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <linux/types.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/guestconn.h>
+
+#include <bhv/init/start.h>
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+#include <linux/kprobes.h>
+#endif
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_SECURITY_SELINUX
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#endif // VASKM
+
+#if defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+static int disable_patching_vault_regions(void)
+{
+	extern char __bhv_vault_text_jump_label_start[];
+	extern char __bhv_vault_text_jump_label_end[];
+	extern char __bhv_vault_ref_text_jump_label_start[];
+	extern char __bhv_vault_ref_text_jump_label_end[];
+
+	int rc = 0;
+
+	/*
+	 * Add the vault .text and .ref.text segments into the kprobes blacklist
+	 * to prohibit setting unauthorized kprobes inside of the vault.
+	 * This function must be called after initializing kmem_caches.
+	 *
+	 * Note: we do not prohibit patching the vault's shared code segment.
+	 */
+
+	rc = kprobe_add_area_blacklist((unsigned long)__bhv_vault_text_jump_label_start,
+				       (unsigned long)__bhv_vault_text_jump_label_end);
+	if (rc)
+		return rc;
+
+	rc = kprobe_add_area_blacklist((unsigned long)__bhv_vault_ref_text_jump_label_start,
+				       (unsigned long)__bhv_vault_ref_text_jump_label_end);
+
+	return rc;
+}
+#endif
+
+static inline void do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	HypABI__Init__Start__arg__T *config =
+		(HypABI__Init__Start__arg__T *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = HypABI__Init__Start__hypercall_noalloc(
+		config, num_pages * PAGE_SIZE - HypABI__Init__Start__arg__SZ);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (HypABI__Init__Start__arg__T *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = HypABI__Init__Start__hypercall_noalloc(
+			config,
+			num_pages * PAGE_SIZE - HypABI__Init__Start__arg__SZ);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#if !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		if ((sizeof(HypABI__Init__Start__arg__T) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#else // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		bhv_fail("guest policy available without target LSM");
+#endif // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+bool __init_km bhv_start(void)
+{
+	int rc;
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_node_t *n[2];
+#endif // VASKM
+
+	if (!is_bhv_initialized())
+		return false;
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_ptpg();
+
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return false;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region.remove));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+#endif // VASKM
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+#ifndef VASKM // inside kernel tree
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_start_delete_alternatives();
+	}
+#elif defined(CONFIG_BHV_VAULT_SPACES) && defined(CONFIG_KPROBES)
+	rc = disable_patching_vault_regions();
+	if (rc)
+		pr_err("BHV: Cannot disable patching the vault's code segments.\n");
+#endif
+
+	bhv_start_guestconn();
+#endif // VASKM
+
+	do_start();
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_sysfs();
+	}
+
+	if (bhv_reg_protect_is_enabled()) {
+		bhv_start_reg_protect();
+	}
+
+	return true;
+}
diff --git security/bhv/inode.c security/bhv/inode.c
new file mode 100644
index 000000000..c07b7e8e8
--- /dev/null
+++ security/bhv/inode.c
@@ -0,0 +1,393 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fs.h>
+#include <linux/siphash.h>
+#include <linux/uidgid.h>
+#include <linux/version.h>
+
+#include <bhv/creds.h>
+#include <bhv/event.h>
+#include <bhv/inode.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_INODE_DEBUG 0
+
+static bool bhv_inode_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_inode_is_active(void)
+{
+	if (!bhv_inode_initialized)
+		return false;
+
+	return bhv_inode_is_enabled();
+}
+
+static size_t collect_inode_invariants(char *buf, const struct inode *inode,
+				       size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t i_addr = (uint64_t)inode;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(umode_t) +
+				       sizeof(kuid_t) + sizeof(kgid_t) +
+				       sizeof(unsigned long);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	/*
+	 * Consider tracking the integrity of the remaining fields as well (not
+	 * just the below). Determine, which of them are subject to change
+	 * during the inode's life time. Otherwise, tracking the integrity of
+	 * those would require an inode tag update every time the fields change.
+	 *
+	 * NOTE: The difficulty in tracking these fields is that we need to make
+	 * sure that we track every time any of the below fields change at
+	 * run-time.
+	 */
+
+	_buf = memcpy(_buf, &i_addr, sizeof(i_addr));
+	_buf += sizeof(i_addr);
+
+	_buf = memcpy(_buf, &inode->i_mode, sizeof(umode_t));
+	_buf += sizeof(umode_t);
+
+	_buf = memcpy(_buf, &inode->i_uid, sizeof(kuid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_gid, sizeof(kgid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_ino, sizeof(unsigned long));
+	_buf += sizeof(unsigned long);
+
+	return buf_size;
+}
+
+static uint64_t siphash_inode_state(const struct inode *inode)
+{
+#define MAX_BUF_SIZE                                                           \
+	sizeof(uint64_t) + sizeof(umode_t) + sizeof(kuid_t) + sizeof(kgid_t) + \
+		sizeof(unsigned long)
+
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_inode_invariants(buf, inode, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static inline bool __is_setuid(struct cred *new, const struct cred *old)
+{
+	return !uid_eq(new->euid, old->uid);
+}
+
+static inline bool __is_setgid(struct cred *new, const struct cred *old)
+{
+	return !gid_eq(new->egid, old->gid);
+}
+
+static inline void bhv_inode_register(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Register__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Register inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_register.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_update(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Update__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the UPDATE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Update inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_update.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_release(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Release__HYPERCALL(.inode = { .addr = (uint64_t)
+								  inode });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the RELEASE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu\n", __FUNCTION__, inode->i_ino);
+#endif
+}
+
+static inline int bhv_inode_verify(struct inode *inode, struct file *file)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Inode__EventType__EVENT_NONE;
+	int block;
+
+	if (!inode)
+		return 0;
+
+	rc = HypABI__Inode__Verify__HYPERCALL(
+		&type, .inode = { .addr = (uint64_t)inode,
+				  .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Verify inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_verify.inode.hmac);
+#endif
+
+	if (!rc && type != HypABI__Inode__EventType__EVENT_NONE) {
+		HypABI__Inode__Log__arg__T *log_arg =
+			HypABI__Inode__Log__arg__ALLOC();
+		populate_event_context(&log_arg->context, true);
+		rc = get_file_path(&file->f_path, log_arg->file_path,
+				   HypABI__Context__MAX_PATH_SZ);
+		if (rc) {
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		log_arg->event_type = type;
+		log_arg->inode_addr = (uint64_t)(inode);
+		log_arg->inode_uid = (uint32_t)(__kuid_val(inode->i_uid));
+		log_arg->inode_gid = (uint32_t)(__kgid_val(inode->i_gid));
+		log_arg->inode_mode = (uint16_t)(inode->i_mode);
+
+		rc = HypABI__Inode__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[-BHV-] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Inode__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   const struct file *file)
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+{
+	int rc = 0;
+	bool is_setxid = false;
+	const struct cred *old = current_cred();
+	struct cred *new = bprm->cred;
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return 0;
+
+	is_setxid = __is_setuid(new, old) || __is_setgid(new, old);
+	if (!is_setxid)
+		return 0;
+
+	inode = d_backing_inode(file->f_path.dentry);
+
+	/*
+	 * We consider the following cases if is_setxid == true:
+	 * - The new executable has the SXID bits set: verify its inode.
+	 * - The new executable does not have the SXID bits set: verify
+	 *   credentials of the current process.
+	 */
+
+	inode_lock(inode);
+
+	if (is_sxid(inode->i_mode)) {
+		// For simplicity, we make sure the pointer is not const.
+		// This makes it easier since from 6.7 this function gets a
+		// const pointer as an argument.
+		rc = bhv_inode_verify(inode, (struct file *)file);
+		inode_unlock(inode);
+	} else {
+		inode_unlock(inode);
+		rc = bhv_cred_verify(current);
+	}
+
+	return rc;
+}
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+/* Caller holds semaphore inode->i_rwsem. */
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid,
+			    umode_t old_mode)
+{
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return;
+
+	/*
+	 * Unfortunately, we cannot verify the current task's credentials at
+	 * this point. This function is sometimes called after having
+	 * temporarily overwritten the current tasks' credentials via
+	 * override_creds; this is done e.g., by the overlay fs to temporarily
+	 * elevate permissions to the filesystem's owner so allow ops on the
+	 * filesystem. As long as we do not track the temporarily overwritten
+	 * credentials, we cannot verify the current task's credentials.
+	 */
+
+	/* We are interested only in mode, UID, and GID changes. */
+	if (!(ia_valid & (ATTR_MODE | ATTR_UID | ATTR_GID)))
+		return;
+
+	/* Note that the caller of this function locked the inode. */
+	inode = d_backing_inode(dentry);
+
+	/* Mode changes should be updated only if the SXID bits are set. */
+	if ((old_mode & (S_ISUID | S_ISGID)) ==
+	    (inode->i_mode & ((S_ISUID | S_ISGID)))) {
+		if (!is_sxid(inode->i_mode))
+			return;
+	}
+
+	/* Register inode if the previous mode did not have SXID bits set. */
+	if (!is_sxid(old_mode) && is_sxid(inode->i_mode)) {
+		bhv_inode_register(inode);
+		return;
+	}
+
+	/* Release tracked inode if setattr removed SXID bits. */
+	if (is_sxid(old_mode) && !is_sxid(inode->i_mode)) {
+		bhv_inode_release(inode);
+		return;
+	}
+
+	/* In any other case, the inode is tracked: update the changes */
+	bhv_inode_update(inode);
+}
+
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+	if (!is_sxid(inode->i_mode))
+		return;
+
+	/*
+	 * Note: we could check whether the inode has any existing dentry
+	 * aliases, via d_find_any_alias, and if so return. Yet, we chose to do
+	 * this check from within BHV to ensure that an attacker cannot modify
+	 * this value to skip inode registrations.
+	 */
+
+	spin_lock(&inode->i_lock);
+	bhv_inode_register(inode);
+	spin_unlock(&inode->i_lock);
+}
+
+/* Caller holds the spinlock inode->i_lock. */
+void bhv_inode_iput_final(struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu i_count=%d i_nlink=%d\n",
+		__FUNCTION__, inode->i_ino, atomic_read(&inode->i_count),
+		inode->i_nlink);
+#endif
+
+	/*
+	 * Note: when releasing inodes, we unconditionally call into BHV (i.e.,
+	 * disregarding whether the inode is privileged or not) to avoid
+	 * potential reuse attacks.  Otherwise, the attacker could maliciously
+	 * modify a privileged (and by BHV registered) inode, cause the kernel
+	 * to drop the inode in  the kernel, and then re-allocate an
+	 * unprivileged file, the meta data of which can be adjusted to match
+	 * the previous meta information that were incorporated into the HMAC of
+	 * the previously privileged inode.
+	 *
+	 * XXX: Consider conditionally calling into BHV at this point if the
+	 * above renders to be not critical (every new inode allocation receives
+	 * a new inode->i_ino). This would reduce the number of hypercalls.
+	 */
+
+	bhv_inode_release(inode);
+}
+
+int __init bhv_inode_init(void)
+{
+	if (!bhv_inode_is_enabled())
+		return -EPERM;
+
+	if (bhv_inode_initialized)
+		return -EPERM;
+
+	bhv_inode_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/integrity.c security/bhv/integrity.c
new file mode 100644
index 000000000..1a5dca2fe
--- /dev/null
+++ security/bhv/integrity.c
@@ -0,0 +1,215 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/filter.h>
+#include <linux/module.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+/************************************************************/
+
+int bhv_integrity_freeze_events(uint64_t flags)
+{
+	if (HypABI__Integrity__Freeze__HYPERCALL(.flags = flags))
+		return -EINVAL;
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+extern struct mutex module_mutex;
+extern struct list_head modules;
+#else // out of tree
+#include <kln.h>
+#endif //VASKM
+
+static void lock_all_modules(void)
+{
+	static bool all_modules_locked = false;
+	struct module *mptr;
+
+	if (all_modules_locked)
+		return;
+
+	mutex_lock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+
+	list_for_each_entry (mptr, KLN_SYMBOL_P(struct list_head *, modules),
+			     list) {
+		if (mptr != THIS_MODULE) {
+			if (!try_module_get(mptr)) {
+				printk(KERN_WARNING
+				       "%s: Cannot lock module %s\n",
+				       __FUNCTION__, mptr->name);
+			}
+		}
+	}
+
+	mutex_unlock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+	all_modules_locked = true;
+	return;
+}
+
+bool bhv_allow_kmod_loads = true;
+bool bhv_allow_patch = true;
+bool bhv_integrity_freeze_create_currently_frozen = false;
+bool bhv_integrity_freeze_update_currently_frozen = false;
+bool bhv_integrity_freeze_remove_currently_frozen = false;
+bool bhv_integrity_freeze_patch_currently_frozen = false;
+
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks)
+{
+	int ret;
+	if (!skip_locks) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_kmod_loads = false;
+			lock_all_modules();
+		}
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_patch = false;
+		}
+	} else {
+		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+		       __FUNCTION__);
+	}
+
+	ret = bhv_integrity_freeze_events(flags);
+
+	if (!ret) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE)
+			bhv_integrity_freeze_create_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE)
+			bhv_integrity_freeze_update_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE)
+			bhv_integrity_freeze_remove_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH)
+			bhv_integrity_freeze_patch_currently_frozen = true;
+	}
+
+	return ret;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Create__HYPERCALL(.owner = owner,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Update__HYPERCALL(.region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) false,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) true,
+						    .owner = owner);
+}
+
+static uint64_t _ptpg_pgd_offset __ro_after_init;
+static uint64_t _ptpg_pgd_value __ro_after_init;
+static atomic_t _ptpg_ready __ro_after_init = ATOMIC_INIT(0);
+
+/**************************************************************
+ * start
+ **************************************************************/
+void __init_km bhv_start_ptpg(void)
+{
+	_ptpg_pgd_offset = 0;
+	_ptpg_pgd_value = 0;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+
+	bhv_start_get_pt_protect_pgd_data(&_ptpg_pgd_offset, &_ptpg_pgd_value);
+
+	atomic_inc(&_ptpg_ready);
+}
+/**************************************************************/
+
+/**************************************************************
+ * late_start
+ **************************************************************/
+int bhv_late_start_init_ptpg(void)
+{
+	unsigned long r;
+	HypABI__Integrity__PtpgInit__arg__T *arg;
+
+	if (!bhv_integrity_pt_prot_is_enabled() || !atomic_read(&_ptpg_ready)) {
+		return 0;
+	}
+
+	BUG_ON(sizeof(HypABI__Integrity__PtpgInit__arg__T) > PAGE_SIZE);
+
+	arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+
+	bhv_late_start_get_pt_protect_data(arg);
+
+	r = HypABI__Integrity__PtpgInit__hypercall_noalloc(arg);
+
+	HypABI__Integrity__PtpgInit__arg__FREE(arg);
+
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+/**************************************************************/
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+	unsigned long r;
+	bool success;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+	success = bhv_pt_protect_check_pgd_arch(mm, _ptpg_pgd_offset,
+						_ptpg_pgd_value);
+	if (!success) {
+		r = HypABI__Integrity__PtpgReport__hypercall_noalloc();
+		if (r) {
+			pr_err("BHV: error reporting pt violation to host!");
+		}
+	}
+}
diff --git security/bhv/keyring.c security/bhv/keyring.c
new file mode 100644
index 000000000..24c0e101a
--- /dev/null
+++ security/bhv/keyring.c
@@ -0,0 +1,198 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/key-type.h>
+#include <linux/siphash.h>
+
+#include <bhv/event.h>
+#include <bhv/keyring.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_KEYRING_DEBUG 0
+
+static bool bhv_keyring_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_keyring_is_active(void)
+{
+	if (!bhv_keyring_initialized)
+		return false;
+
+	return bhv_keyring_is_enabled();
+}
+
+static size_t collect_keyring_invariants(char *buf, const struct key *keyring,
+					 uint64_t anchor, size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct key keyring_copy;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct key);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&keyring_copy, keyring, sizeof(struct key));
+
+	/* Exclude mutable fields from the keyring to be hashed. */
+
+	refcount_set(&keyring_copy.usage, 0);
+	memset(&keyring_copy.graveyard_link, 0, sizeof(struct list_head));
+	memset(&keyring_copy.serial_node, 0, sizeof(struct rb_node));
+#ifdef CONFIG_KEY_NOTIFICATIONS
+	keyring_copy.watchers = NULL;
+#endif
+	memset(&keyring_copy.sem, 0, sizeof(struct rw_semaphore));
+	keyring_copy.last_used_at = 0;
+	keyring_copy.datalen = 0;
+	memset(&keyring_copy.payload, 0, sizeof(union key_payload));
+
+	bound_context = (uint64_t)keyring ^ anchor;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &keyring_copy, sizeof(struct key));
+
+	return buf_size;
+}
+
+static uint64_t siphash_keyring_state(const struct key *keyring,
+				      uint64_t anchor)
+{
+#define MAX_BUF_SIZE sizeof(uint64_t) + sizeof(struct key)
+	char buf[MAX_BUF_SIZE];
+	size_t size =
+		collect_keyring_invariants(buf, keyring, anchor, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Keyring__EventType__EVENT_NONE;
+	int block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	rc = HypABI__Keyring__Verify__HYPERCALL(
+		&type, .keyring = { .addr = (uint64_t)anchor,
+				    .hmac = siphash_keyring_state(
+					    keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	if (!rc && type != HypABI__Keyring__EventType__EVENT_NONE) {
+		HypABI__Keyring__Log__arg__T *log_arg =
+			HypABI__Keyring__Log__arg__ALLOC();
+		populate_event_context(&log_arg->context, true);
+
+		log_arg->event_type = type;
+		log_arg->keyring_addr = (uint64_t)(keyring);
+		log_arg->keyring_uid = (uint32_t)(__kuid_val(keyring->uid));
+		log_arg->keyring_gid = (uint32_t)(__kgid_val(keyring->gid));
+		log_arg->keyring_perm = (uint32_t)(keyring->perm);
+		log_arg->keyring_serial = (uint32_t)(keyring->serial);
+
+		strncpy(log_arg->keyring_desc, keyring->description,
+			HypABI__Context__MAX_PATH_SZ);
+
+		rc = HypABI__Keyring__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[BHV] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Keyring__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Keyring__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+int bhv_keyring_verify(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	down_read(&keyring->sem);
+	rc = bhv_keyring_verify_locked(keyring, anchor);
+	up_read(&keyring->sem);
+
+	return rc;
+}
+
+static int bhv_keyring_register(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	/* Note: we index system-trusted keyrings by its global anchor. */
+
+	rc = HypABI__Keyring__Register__HYPERCALL(
+		&block, .keyring = { .addr = (uint64_t)anchor,
+				     .hmac = siphash_keyring_state(
+					     keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	return block ? -EPERM : 0;
+}
+
+int bhv_keyring_register_system_trusted(struct key **k)
+{
+	void *anchor = k;
+	return bhv_keyring_register(*k, anchor);
+}
+
+int __init bhv_init_keyring(void)
+{
+	if (!bhv_keyring_is_enabled())
+		return -EPERM;
+
+	if (bhv_keyring_initialized)
+		return -EPERM;
+
+	bhv_keyring_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/lsm.c security/bhv/lsm.c
new file mode 100644
index 000000000..66993d0c5
--- /dev/null
+++ security/bhv/lsm.c
@@ -0,0 +1,900 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fdtable.h>
+#include <linux/highmem.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <uapi/linux/magic.h>
+#include <linux/net.h>
+#include <net/sock.h>
+#include <net/inet_common.h>
+
+#include <bhv/acl.h>
+#include <bhv/bhv.h>
+#include <bhv/drift_detection.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/guestlog.h>
+#include <bhv/inode.h>
+#include <bhv/integrity.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/reverse_shell_detection.h>
+#include <bhv/util.h>
+
+#ifndef VASKM // inside kernel tree
+#include <linux/lsm_hooks.h>
+#include <bhv/kernel-kln.h>
+#define static_vk static
+
+#else // out of tree
+#include <module.h>
+#include <kln.h>
+#include <fsreg.h>
+#define static_vk
+
+#pragma GCC diagnostic ignored "-Wmissing-prototypes"
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+struct bhv_vas {
+	volatile unsigned long cap_raised; // bit mask of raised capabilities
+};
+
+static int bhv_read_file(struct file *file, enum kernel_read_file_id id,
+			 bool whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+
+static int bhv_load_data(enum kernel_load_data_id id, bool contents)
+{
+	const char *origin = kernel_read_file_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			bhv_guestlog_log_driver_load("[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+#endif // VASKM
+
+static_vk int bhv_task_alloc(struct task_struct *target,
+			     long unsigned int clone_flags)
+{
+#ifndef VASKM
+	struct bhv_vas *vas;
+
+	BUILD_BUG_ON_MSG((CAP_LAST_CAP + 1) > (sizeof(vas->cap_raised) << 3),
+			 "Number of capabilities exceeds size of bitmask.");
+
+	vas = kmalloc(sizeof(*vas), GFP_KERNEL);
+	if (!vas) {
+		bhv_fail("BHV: Unable to allocate bhv task vas struct");
+		return -ENOMEM;
+	}
+
+	vas->cap_raised = 0UL;
+
+	target->bhv_vas = vas;
+#endif // VASKM
+
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_fork(target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+static_vk void bhv_task_free(struct task_struct *target)
+{
+#ifndef VASKM
+	kfree(target->bhv_vas);
+	target->bhv_vas = NULL;
+#endif /* VASKM */
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exit(target->pid, target->parent->pid,
+					      target->comm,
+					      (uint32_t)target->exit_code);
+	}
+}
+
+static inline const char *bhv_bprm_get_file_name(struct linux_binprm *bprm,
+						 char **buf)
+{
+	char *_buf;
+
+	_buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (_buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+	}
+
+	(*buf) = _buf;
+	// Can be called with when _buf is NULL.
+	return bhv_get_file_path(bprm->file, _buf, PATH_MAX);
+}
+
+static_vk int bhv_bprm_check_security(struct linux_binprm *bprm)
+{
+	int rv = 0;
+	char *buf = NULL;
+	const char *path = NULL;
+
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+	if (bhv_acl_is_proc_acl_enabled()) {
+		path = bhv_bprm_get_file_name(bprm, &buf);
+
+		// check executed filename (name on cli)
+		if (bhv_block_process(bprm->filename)) {
+			rv = -EPERM;
+			goto out;
+		}
+
+		// check underlying file (e.g., busybox or interpreter)
+		if (bhv_block_process(path)) {
+			rv = -EPERM;
+			goto out;
+		}
+	}
+
+	if (bhv_drift_detection_is_enabled()) {
+		if (path == NULL) {
+			path = bhv_bprm_get_file_name(bprm, &buf);
+		}
+
+		rv = bhv_drift_detection_bprm_check_security(bprm, path);
+		if (rv != 0)
+			goto out;
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exec(bprm, current->pid,
+					      current->parent->pid,
+					      bprm->filename);
+	}
+
+out:
+	if (buf)
+		kfree(buf);
+
+	return rv;
+}
+
+static const inline char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+static bool try_update_bhv_fops_map(u8 bhv_fops, bool is_dir,
+				    const struct file_operations *fops_ptr)
+{
+	BUG_ON(fileops_map[bhv_fops][is_dir == true ? 1 : 0]);
+
+	if (!bhv_allow_update_fileops_map)
+		return false;
+
+	if (is_module_ro_data((unsigned long)fops_ptr)) {
+		fileops_map[bhv_fops][is_dir == true ? 1 : 0] = fops_ptr;
+		pr_info("%s: Added fops %d %d\n", __FUNCTION__, bhv_fops,
+			is_dir);
+		return true;
+	}
+	return false;
+}
+#endif // defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+static bool bhv_perform_check_fileops(struct file *file, u8 bhv_fops,
+				      bool is_dir)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+	bool block = false;
+
+	if (bhv_fops != FT(UNSUPPORTED)) {
+		if (file->f_op ==
+		    fileops_map[bhv_fops][is_dir == true ? 1 : 0]) {
+			// fops matches
+			return false;
+		}
+	}
+
+	switch (bhv_fops) {
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS // out of tree
+	case FT(EXT4):
+	case FT(XFS):
+		if (!fileops_map[bhv_fops][is_dir == true ? 1 : 0])
+			if (try_update_bhv_fops_map(bhv_fops, is_dir,
+						    file->f_op))
+				return false;
+		break;
+#endif // VASKM
+	case FT(UNSUPPORTED): {
+		if (bhv_strict_fileops_enforced()) {
+			/*
+			 * strict mode configured or forced by kernel boot option
+			 * continue to report and optionally block
+			 */
+			break;
+		}
+
+		/*
+		 * fallback in case of unknown fops:
+		 * check if pointer points to ro section
+		 */
+		if (bhv_fileops_is_ro((u64)file->f_op)) {
+			uint64_t struct_type = 0;
+			uint32_t minor = 0;
+			uint64_t major = 0;
+
+			if (!bhv_guestlog_log_unknown_fileops()) {
+				return false;
+			}
+
+			// set the type for logging
+			if (d_is_reg(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE;
+			} else if (d_can_lookup(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY;
+			} else if (d_is_special(file->f_path.dentry)) {
+				// save i_rdev which contains major and minor
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL;
+				major = MAJOR(file->f_inode->i_rdev);
+				minor = MINOR(file->f_inode->i_rdev);
+			}
+
+			// log info about unsupported
+			pathname_buf = kzalloc(
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+				GFP_KERNEL);
+			pathname = get_pathname(
+				file, pathname_buf,
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+
+			bhv_guestlog_log_fops_unknown(
+				file->f_inode->i_sb->s_magic, pathname,
+				struct_type, major, minor,
+				(uint64_t)file->f_op);
+
+			pathname = NULL;
+			kfree(pathname_buf);
+			// OK
+			return false;
+		}
+
+		break;
+	}
+	// additional check for empty dir ops
+	case FT(SYSFS):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       empty_dir_operations)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	// additional check for dummy fops
+	case FT(DEV_TTY):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       hung_up_tty_fops)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+#ifndef VASKM
+	// Note: not supported for vaskm
+	// additional check for not-allowed-to-open file ops
+	case FT(SOCKFS):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       no_open_fops)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+#endif
+	case FT(DEV_NULL):
+	case FT(DEV_URANDOM):
+	case FT(DEV_RANDOM):
+	case FT(DEV_CONSOLE):
+	case FT(DEV_KMSG):
+	case FT(DEV_MEM):
+	case FT(DEV_ZERO):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       def_chr_fops)) {
+			// we're all set
+			return false;
+		}
+		break;
+	case FT(PROC):
+		// additional check for proc fops:
+		if (is_valid_proc_fop(&(file->f_op)))
+			return false;
+		break;
+	case FT(DEBUGFS):
+		if (is_valid_debugfs_fop(file->f_op))
+			return false;
+		break;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+	case FT(TMPFS):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       simple_offset_dir_operations))
+			return false;
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(6, 6, 0)
+
+	default:
+		break;
+	}
+
+	// by now we have decided that the fops ptr is bad
+
+	pathname_buf =
+		kzalloc(HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+	pathname = get_pathname(
+		file, pathname_buf,
+		HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+	if (bhv_block_fileops(pathname, bhv_fops, is_dir, file->f_op)) {
+		// block file operation
+		block = true;
+	}
+
+	pathname = NULL;
+	kfree(pathname_buf);
+
+	return block;
+}
+
+static bool bhv_check_fileops(struct file *file)
+{
+	// set up fops check data
+	u8 bhv_fops = FT(UNSUPPORTED);
+	bool is_dir = false;
+
+	unsigned dev_major = imajor(file->f_inode);
+	unsigned dev_minor = iminor(file->f_inode);
+
+	if (d_can_lookup(file->f_path.dentry)) {
+		/* directory S_ISDIR(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+		is_dir = true;
+	} else if (d_is_reg(file->f_path.dentry)) {
+		/* regular file  S_ISREG(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+	} else if (d_is_special(file->f_path.dentry)) {
+		// DCACHE_SPECIAL_TYPE
+		if (S_ISSOCK(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == SOCKFS_MAGIC
+			bhv_fops = FT(SOCKFS);
+		} else if (S_ISFIFO(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == PIPEFS_MAGIC
+			bhv_fops = FT(PIPEFS);
+		} else if (S_ISCHR(file->f_inode->i_mode)) {
+			// character device
+			if (dev_major == 1) {
+				// mem
+				switch (dev_minor) {
+				case 1: // DEVMEM_MINOR
+					// /dev/mem
+					bhv_fops = FT(DEV_MEM);
+					break;
+				case 3:
+					// /dev/null
+					bhv_fops = FT(DEV_NULL);
+					break;
+				case 4:
+					// /dev/port
+					bhv_fops = FT(DEV_PORT);
+					break;
+				case 5:
+					// /dev/zero
+					bhv_fops = FT(DEV_ZERO);
+					break;
+				case 7:
+					// /dev/full
+					bhv_fops = FT(DEV_FULL);
+					break;
+				case 8:
+					// /dev/random
+					bhv_fops = FT(DEV_RANDOM);
+					break;
+				case 9:
+					// /dev/urandom
+					bhv_fops = FT(DEV_URANDOM);
+					break;
+				case 11:
+					// /dev/kmsg
+					bhv_fops = FT(DEV_KMSG);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 5) {
+				switch (dev_minor) {
+				case 0:
+					// /dev/tty
+					bhv_fops = FT(DEV_TTY);
+					break;
+				case 1:
+					// /dev/console
+					bhv_fops = FT(DEV_CONSOLE);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 4 && dev_minor == 64) {
+				// /dev/ttyS0
+				bhv_fops = FT(DEV_TTY);
+			} else if (dev_major == 229 && dev_minor == 0) {
+				// /dev/hvc0
+				bhv_fops = FT(DEV_TTY);
+			}
+		}
+	}
+
+#ifdef DEBUG
+	if (bhv_fops == FT(UNSUPPORTED)) {
+		char *pathname_buf = kzalloc(
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+		const char *pathname = get_pathname(
+			file, pathname_buf,
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+		pr_debug(
+			"name: %s magic: 0x%lx dentry_type: 0x%x stat: 0o%o path: %s fops: 0x%px",
+			file->f_inode->i_sb->s_type->name,
+			file->f_inode->i_sb->s_magic,
+			file->f_path.dentry->d_flags & DCACHE_ENTRY_TYPE,
+			(file->f_inode->i_mode & S_IFMT), pathname, file->f_op);
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+#endif
+
+	// perform fops check
+	return bhv_perform_check_fileops(file, bhv_fops, is_dir);
+}
+#undef FT
+
+#define BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ 256
+
+static_vk int bhv_check_files_dirty_pipe(const void *address_space,
+					 struct file *file, unsigned int number)
+{
+	char *filename_buf = NULL;
+	const char *filename = NULL;
+	int rv;
+
+	struct inode *ipipe =
+		address_space != NULL ?
+			((struct address_space *)address_space)->host :
+			NULL;
+	struct inode *ifile = d_real_inode(file->f_path.dentry);
+
+	if (ipipe == NULL || ifile == NULL || !virt_addr_valid(ipipe))
+		return 0;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (ifile->i_ino == ipipe->i_ino && ifile->i_sb == ipipe->i_sb &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Allocate memory
+		filename_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+
+		if (filename_buf == NULL) {
+			pr_err("Could not allocate file buffer");
+			filename = "UNKNOWN";
+		} else {
+			// Get Path of the file we are trying to write
+			filename = d_path(&file->f_path, filename_buf,
+					  BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+			if (IS_ERR(filename)) {
+				pr_err("Could not retrieve file name (%ld)",
+				       PTR_ERR(filename));
+				filename = "UNKNOWN";
+			}
+		}
+
+		// Ask the HOST whether we should block this attempt.
+		rv = bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+			filename);
+
+		if (filename_buf != NULL) {
+			kfree(filename_buf);
+		}
+
+		return rv;
+	}
+
+	return 0;
+}
+
+/*
+ * Dirty cred detection
+ * When a write happens this checks whether the struct file object is opened for
+ * writing. If this is not the case notify and optionally stop the write
+ * attempt immediately.
+ */
+void bhv_check_file_dirty_cred(struct file *file, int mask)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+
+	if (!bhv_dirtycred_file_protection_is_enabled())
+		return;
+
+	// No need to continue if no write attempt is ongoing.
+	if (!(mask & MAY_WRITE))
+		return;
+
+	if (!(file->f_mode & FMODE_WRITE) ||
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		pathname_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+		pathname = get_pathname(file, pathname_buf,
+					BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+		pr_err("Write to read-only file \"%s\" detected!", pathname);
+		// At this point a struct file object was already placed illegitimately
+		// at the location of another. When we're be called from a LSM file
+		// permission hook, we could still bail out gracefully. However, as
+		// illegitimately placing struct file objects often happens after
+		// permission and LSM hooks, we need to check deep inside write attempts
+		// (e.g. in mm/filemap.c:generic_perform_write()). There, we do not have
+		// much choice as the attempts might have already corrupted the
+		// read-only file. Hence, on a block, we panic after having informed the
+		// HOST.
+		if (bhv_block_read_only_file_write_ViolationDirtyCredWrite(
+			    pathname)) {
+			panic("possible dirtycred compromise detected");
+		}
+
+		kfree(pathname_buf);
+	}
+}
+
+static_vk int bhv_file_open(struct file *file)
+{
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	if (bhv_guestlog_log_file_events())
+		bhv_guestlog_log_file_open(file);
+
+	return 0;
+}
+
+static_vk int bhv_file_permission(struct file *file, int mask)
+{
+	struct pipe_inode_info *info;
+	struct pipe_buffer *buf;
+	int rv;
+
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	/* check for dirty cred inside LSM hook (e.g. for aio_write()) */
+	bhv_check_file_dirty_cred(file, mask);
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return 0;
+
+	/*
+	 * Dirty pipe detection. Whenever we write to a file and this file is
+	 * a pipe, we are going to check whether this pipe points to a read-only
+	 * file. This check happens in `bhv_check_files_dirty_pipe` above.
+	 */
+	if ((mask & MAY_WRITE) == MAY_WRITE &&
+	    (info = get_pipe_info(file, false))) {
+		// Check current buffer in the pipe for dirty pipe
+		buf = &info->bufs[(info->head - 1) & (info->ring_size - 1)];
+		// Iterate over all open files and see whether the pipe points to the same file.
+		if (buf && buf->page && current->files &&
+		    virt_addr_valid(buf->page->mapping)) {
+			if (iterate_fd(current->files, 0,
+				       bhv_check_files_dirty_pipe,
+				       buf->page->mapping)) {
+				return -EACCES;
+			}
+		}
+	}
+
+	if (bhv_drift_detection_is_enabled() &&
+	    (rv = bhv_drift_detection_file_permission(file, mask)) != 0)
+		return rv;
+
+	return 0;
+}
+
+static_vk int bhv_cgroup_mkdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_create(cgrp);
+	return 0;
+}
+
+static_vk void bhv_cgroup_rmdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_destroy(cgrp);
+}
+
+static_vk int bhv_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+static_vk int bhv_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+static_vk int bhv_socket_connect(struct socket *sock, struct sockaddr *address,
+				 int addrlen)
+{
+	int rv;
+
+	if (bhv_guestlog_log_socket_events() && sock) {
+		if (sock->sk->sk_family == AF_INET ||
+		    sock->sk->sk_family == AF_INET6)
+			bhv_guestlog_log_net_socket_connection(false, address);
+		else if (sock->sk->sk_family == AF_UNIX)
+			bhv_guestlog_log_unix_socket_connection(
+				sock, (struct sockaddr_un *)address, addrlen);
+	}
+
+	if (bhv_reverse_shell_detetection_is_enabled()) {
+		rv = bhv_reverse_shell_detection_socket_connect(sock, address,
+								addrlen);
+		if (rv != 0)
+			return rv;
+	}
+
+	return 0;
+}
+
+static_vk int bhv_socket_bind(struct socket *sock, struct sockaddr *address,
+			      int addrlen)
+{
+	if (bhv_guestlog_log_socket_events() && sock &&
+	    (sock->sk->sk_family == AF_INET || sock->sk->sk_family == AF_INET6))
+		bhv_guestlog_log_net_socket_connection(true, address);
+
+	return 0;
+}
+
+static_vk void bhv_socket_accepted(struct socket *sock, struct socket *newsock)
+{
+	if (bhv_guestlog_log_socket_events() && newsock &&
+	    (newsock->sk->sk_family == AF_INET ||
+	     newsock->sk->sk_family == AF_INET6)) {
+		bhv_guestlog_log_socket_accept(newsock);
+	}
+}
+
+static_vk int bhv_fd_dup(unsigned int fd, struct file *file)
+{
+	int rv;
+
+	if (bhv_reverse_shell_detetection_is_enabled()) {
+		rv = bhv_reverse_shell_fd_dup(fd, file);
+		if (rv != 0)
+			return rv;
+	}
+
+	return 0;
+}
+
+#ifndef VASKM
+static void _bhv_capable_check(const struct cred *cred,
+			       struct user_namespace *ns, int cap,
+			       unsigned int opts)
+{
+	bool task_is_null;
+	rcu_read_lock();
+	task_is_null = task_dfl_cgroup(current) == NULL;
+	rcu_read_unlock();
+
+	if (task_is_null)
+		return;
+
+	if (cap_raised(current_real_cred()->cap_permitted, cap))
+		if (!test_and_set_bit(cap, &current->bhv_vas->cap_raised))
+			bhv_guestlog_log_capable(cap);
+
+	return;
+}
+
+static int bhv_capable(const struct cred *cred, struct user_namespace *ns,
+		       int cap, unsigned int opts)
+{
+	if (bhv_guestlog_log_container_events())
+		_bhv_capable_check(cred, ns, cap, opts);
+	return 0;
+}
+#endif /* VASKM */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 528))
+static_vk int bhv_inode_setxattr(struct mnt_idmap *mnt_idmap,
+				 struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+static_vk int bhv_inode_setxattr(struct user_namespace *mnt_userns,
+				 struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#else
+static_vk int bhv_inode_setxattr(struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#endif
+	int rv;
+
+	if (!bhv_drift_detection_is_enabled())
+		return 0;
+
+	if ((rv = bhv_drift_detection_inode_setxattr(name)) != 0)
+		return rv;
+
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+static void bhv_module_loaded(struct module *mod)
+{
+	if (bhv_guestlog_log_driver_events()) {
+		bhv_guestlog_log_driver_load(mod->name);
+	}
+}
+
+static struct security_hook_list bhv_hooks[] __lsm_ro_after_init = {
+	LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+	LSM_HOOK_INIT(kernel_load_data, bhv_load_data), // init_module
+	LSM_HOOK_INIT(task_alloc, bhv_task_alloc), // fork
+	LSM_HOOK_INIT(task_free, bhv_task_free), // exit
+	LSM_HOOK_INIT(bprm_check_security, bhv_bprm_check_security), // execve
+	LSM_HOOK_INIT(file_permission, bhv_file_permission), // file read/write
+	LSM_HOOK_INIT(file_open, bhv_file_open), // file open
+	LSM_HOOK_INIT(module_loaded,
+		      bhv_module_loaded), // module has successfully loaded
+
+	/* Inode protection */
+	LSM_HOOK_INIT(bprm_creds_from_file, bhv_inode_bprm_creds_from_file),
+	LSM_HOOK_INIT(task_fix_setuid, bhv_inode_task_fix_setuid),
+	LSM_HOOK_INIT(task_fix_setgid, bhv_inode_task_fix_setgid),
+	LSM_HOOK_INIT(d_instantiate, bhv_inode_d_instantiate),
+
+	/* Container visibility */
+	LSM_HOOK_INIT(cgroup_mkdir, bhv_cgroup_mkdir),
+	LSM_HOOK_INIT(cgroup_rmdir, bhv_cgroup_rmdir),
+	LSM_HOOK_INIT(unshare, bhv_unshare),
+	LSM_HOOK_INIT(setns, bhv_setns),
+	LSM_HOOK_INIT(capable, bhv_capable),
+
+	/* Xattr */
+	LSM_HOOK_INIT(inode_setxattr, bhv_inode_setxattr),
+
+	/* Socket */
+	LSM_HOOK_INIT(socket_connect, bhv_socket_connect),
+	LSM_HOOK_INIT(socket_bind, bhv_socket_bind),
+	LSM_HOOK_INIT(socket_accepted, bhv_socket_accepted),
+
+	/* Dup */
+	LSM_HOOK_INIT(fd_dup, bhv_fd_dup),
+};
+
+static int __init bhv_lsm_init(void)
+{
+	pr_info("[bhv] LSM active");
+	security_add_hooks(bhv_hooks, ARRAY_SIZE(bhv_hooks), "bhv");
+	return 0;
+}
+
+DEFINE_LSM(bhv) = {
+	.name = "bhv",
+	.init = bhv_lsm_init,
+};
+#endif // VASKM
diff --git security/bhv/memory_freeze.c security/bhv/memory_freeze.c
new file mode 100644
index 000000000..3a9a7bc6e
--- /dev/null
+++ security/bhv/memory_freeze.c
@@ -0,0 +1,105 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/printk.h>
+#include <linux/umh.h>
+
+#include <bhv/integrity.h>
+#include <bhv/guestlog.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+static int __maybe_unused bhv_memory_freeze_main(void *_)
+{
+	static const __section(".rodata") char *const argv[] = {
+		"/usr/bin/systemctl", "is-system-running", NULL
+	};
+	static const __section(".rodata") char *const envp[] = {
+		"HOME=/", "PATH=/sbin:/bin:/usr/sbin:/usr/bin", NULL
+	};
+	int ret;
+
+	while (true) {
+		msleep(500);
+
+		ret = call_usermodehelper(
+			argv[0], (char **)argv, (char **)envp,
+			UMH_WAIT_PROC /* wait for the process to complete */);
+
+		if (ret == -ENOENT) {
+			pr_info("%s: Non-systemd filesystem\n", __FUNCTION__);
+			msleep(120000);
+			pr_info("%s: Assuming system is up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+
+		if (ret == 0) {
+			pr_info("%s: System up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+	}
+}
+
+void bhv_memory_freeze_init(void)
+{
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg;
+	int rc;
+	bool fmab = false;
+
+	if (!is_bhv_initialized())
+		return;
+
+	arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+
+	rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(arg);
+
+	if (rc == 0) {
+		fmab = arg->freeze_memory_after_boot;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(arg);
+
+	if (fmab) {
+#ifdef CONFIG_STATIC_USERMODEHELPER
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.");
+#endif
+#ifdef CONFIG_BPF_JIT
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_BPF_JIT.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_BPF_JIT.");
+#endif
+
+#ifndef NO_FREEZE
+		if (ERR_PTR(-ENOMEM) == kthread_run(bhv_memory_freeze_main,
+						    NULL, "memory_freeze")) {
+			panic("%s: Could not create memory_freeze thread",
+			      __FUNCTION__);
+		}
+		pr_err("%s: started thread", __FUNCTION__);
+#endif
+	}
+}
\ No newline at end of file
diff --git security/bhv/module.c security/bhv/module.c
new file mode 100644
index 000000000..5848192e8
--- /dev/null
+++ security/bhv/module.c
@@ -0,0 +1,634 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/bhv_print.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/module.h>
+#include <bhv/patch.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t, const char *);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags, label);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t type,
+				    uint64_t flags, const char *unused2)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3, const char *unused4)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags,
+				  label);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags, label);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags,
+			       const char *label)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, label,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	type &= ~HypABI__Integrity__MemFlags__MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, "INVALID",
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size,
+				HypABI__Integrity__MemType__UNKNOWN,
+				HypABI__Integrity__MemFlags__NONE, "INVALID",
+				_bhv_link_node_op_remove);
+}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)) ||                         \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 480))
+static void bhv_prepare_mod_init(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_INIT_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_INIT_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static void bhv_prepare_mod_core(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_TEXT].base,
+				   mod->mem[MOD_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_RODATA].base,
+				   mod->mem[MOD_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (mod->mem[MOD_RO_AFTER_INIT].size) {
+		bhv_create_section(head, mod->mem[MOD_RO_AFTER_INIT].base,
+				   mod->mem[MOD_RO_AFTER_INIT].size,
+				   HypABI__Integrity__MemType__DATA,
+				   base_flags |
+					   HypABI__Integrity__MemFlags__MUTABLE,
+				   "MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_DATA].base,
+				   mod->mem[MOD_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout,
+				   unsigned long base_flags)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   HypABI__Integrity__MemType__CODE_PATCHABLE,
+			   base_flags, "MODULE TEXT SECTION");
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			HypABI__Integrity__MemType__DATA,
+			base_flags | HypABI__Integrity__MemFlags__MUTABLE,
+			"MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static inline void bhv_prepare_mod_init(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout, base_flags);
+}
+
+static inline void bhv_prepare_mod_core(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->core_layout, base_flags);
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_init(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+
+#ifndef VASKM // inside kernel tree
+	bhv_prepare_mod_core(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 480))
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	/* Prepare the module region's .text section. */
+	if (mod->mem[MOD_INIT_TEXT].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size);
+
+	/* Prepare the module region's .rodata section. */
+	if (mod->mem[MOD_INIT_RODATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size);
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size);
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	const struct module_layout *layout = &mod->init_layout;
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_complete_free_init(const struct module *mod)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	_bhv_complete_free_init(mod, &bhv_region_list_head);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod,
+				     unsigned long base_flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) ||                           \
+	(defined(VASKM_IS_RHEL) &&                                             \
+	 (LINUX_VERSION_CODE >= KERNEL_VERSION(5, 14, 0)) &&                   \
+	 (VASKM_RHEL_RELEASE >= 480))
+	void *base = mod->mem[MOD_RO_AFTER_INIT].base;
+	unsigned int size = mod->mem[MOD_RO_AFTER_INIT].size;
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   HypABI__Integrity__MemType__DATA_READ_ONLY,
+			   base_flags);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region.update);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#ifndef VASKM // inside kernel tree
+	bhv_update_ro_after_init(mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_update_ro_after_init(mod,
+					 HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_update_ro_after_init(
+			mod, HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+	bhv_complete_free_init(mod);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags,
+			   "BPF SECTION");
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__CODE_PATCHABLE,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_add_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT,
+			       ((size + PAGE_SIZE - 1) >> PAGE_SHIFT));
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_rm_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+	bhv_release_memory_by_owner((uint64_t)base);
+}
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+	BUG_ON(owner == 0);
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	bhv_create_section(&bhv_region_list_head, base, size, type, flags,
+			   description);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+#endif //VASKM
diff --git security/bhv/patch_alternative.c security/bhv/patch_alternative.c
new file mode 100644
index 000000000..a698883eb
--- /dev/null
+++ security/bhv/patch_alternative.c
@@ -0,0 +1,236 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/sync_core.h>
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/patch.h>
+#include <bhv/kversion.h>
+#include <bhv/vault.h>
+
+#include <asm/bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+/**************************************************/
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		pr_err("BHV: %s: Unknown module!\n", __FUNCTION__);
+		rv = -EACCES;
+		goto out;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+out:
+	// Close vault.
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(&search, arch,
+					  bhv_alternatives_find_by_alt);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+/* XXX: CONSIDER MOVING THIS PART INTO TEXT_POKE_EARLY!! */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len)
+{
+	int rc;
+	unsigned long flags;
+
+	/* XXX: Do we still need this? */
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(addr, opcode, len);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+}
+
+#endif
diff --git security/bhv/patch_bpf.c security/bhv/patch_bpf.c
new file mode 100644
index 000000000..13aaa72cb
--- /dev/null
+++ security/bhv/patch_bpf.c
@@ -0,0 +1,219 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com> 
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+void init_xmem(void);
+
+#include <linux/version.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+struct bhv_bpf_code_range {
+	uint64_t start_pfn;
+	size_t num_pages;
+	struct list_head next;
+};
+
+static DEFINE_MUTEX(bhv_bpf_mutex);
+static LIST_HEAD(bhv_bpf_code_ranges_head);
+
+static void __always_inline bhv_bpf_lock(void)
+{
+	mutex_lock(&bhv_bpf_mutex);
+}
+
+static void __always_inline bhv_bpf_unlock(void)
+{
+	mutex_unlock(&bhv_bpf_mutex);
+}
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages)
+{
+	struct bhv_bpf_code_range *n =
+		kzalloc(sizeof(struct bhv_bpf_code_range), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->start_pfn = pfn;
+	n->num_pages = num_pages;
+
+	bhv_bpf_lock();
+	list_add(&(n->next), &bhv_bpf_code_ranges_head);
+	bhv_bpf_unlock();
+}
+
+void bhv_rm_bpf_code_range(uint64_t pfn)
+{
+	struct bhv_bpf_code_range *i, *tmp;
+
+	bhv_bpf_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_bpf_code_ranges_head, next) {
+		if (i->start_pfn == pfn) {
+			list_del(&(i->next));
+			kfree(i);
+			break;
+		}
+	}
+	bhv_bpf_unlock();
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool __bhv_text bhv_bpf_check_write(void *dst, size_t sz)
+{
+	struct bhv_bpf_code_range *i;
+	uint64_t start_pfn = ((uint64_t)dst) >> PAGE_SHIFT;
+	size_t num_pages = ((sz + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	uint64_t end_pfn = (start_pfn + num_pages) - 1;
+
+	list_for_each_entry(i, &bhv_bpf_code_ranges_head, next) {
+		if (start_pfn >= i->start_pfn &&
+		    start_pfn < (i->start_pfn + i->num_pages)) {
+			if (end_pfn >= i->start_pfn &&
+			    end_pfn < (i->start_pfn + i->num_pages))
+				return true;
+		}
+	}
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_write_vault(void *dst, void *src, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall(dst, src, sz);
+#else
+	rv = bhv_patch_hypercall(dst, src, sz, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_write_vault);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_invalidate_vault(void *dst, uint8_t b, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall_memset(dst, sz, b);
+#else
+	rv = bhv_patch_hypercall_memset(dst, sz, b, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_invalidate_vault);
+
+int bhv_bpf_write(void *dst, void *src, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_write_vault(dst, src, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_invalidate_vault(dst, b, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
diff --git security/bhv/patch_jump_label.c security/bhv/patch_jump_label.c
new file mode 100644
index 000000000..e404629a3
--- /dev/null
+++ security/bhv/patch_jump_label.c
@@ -0,0 +1,574 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static void __always_inline bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static void __always_inline bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+static enum jump_label_type __bhv_text
+bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+typedef enum {
+	FAIL = 0,
+	SKIP,
+	SUCCESS,
+} jump_label_validation_t;
+
+static jump_label_validation_t __bhv_text
+validate_jmp_labels(struct jump_entry *entry, const void *opcode, size_t len,
+		    char *error_msg)
+{
+	struct bhv_static_key_mod *i;
+	unsigned long addr = (unsigned long)jump_entry_code(entry);
+	struct jump_entry *iter;
+	unsigned long tmp_addr;
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		// We should only modify the kernel text section
+		if (!kernel_text_address(addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				addr);
+			return FAIL;
+		}
+
+		if (!bhv_jump_label_validate_opcode(
+			    entry, bhv_jump_label_type(entry), opcode, len)) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Jump label opcode validation failed");
+			return FAIL;
+		}
+
+		// Search for overlapping jump labels.
+		for (iter = KLN_SYMBOL(struct jump_entry *,
+				       __start___jump_table);
+		     iter <
+		     KLN_SYMBOL(struct jump_entry *, __stop___jump_table);
+		     iter++) {
+			if (iter == entry)
+				continue;
+
+			tmp_addr = (unsigned long)jump_entry_code(iter);
+
+			if (addr <= tmp_addr && tmp_addr < addr + len) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label (0x%lx) overlaps with jump label (0x%lx)",
+					(unsigned long)(entry),
+					(unsigned long)(iter));
+				return FAIL;
+			}
+		}
+
+		return SUCCESS;
+	}
+
+	list_for_each_entry(i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			struct module *tmp;
+
+			if (i->mod->state != MODULE_STATE_COMING &&
+			    i->mod->state != MODULE_STATE_LIVE) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Invalid module state for jump label (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			if ((jump_entry_is_init(entry) &&
+			     i->mod->state != MODULE_STATE_COMING)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Trying to apply jump label in incorrect module state (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			// Module entries should only point to the text section
+			// of the module
+			// Note: We use the kernel API to check this. We could perform
+			// the check manually but using the kernel API seems more stable.
+			tmp = __module_text_address(addr);
+			if (tmp == NULL) {
+				// No module found
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label trying to write address outside of module (0x%lx)",
+					addr);
+				return FAIL;
+			}
+
+			if (tmp != i->mod) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label (0x%lx<->0x%lx)",
+					(unsigned long)tmp,
+					(unsigned long)i->mod);
+				return FAIL;
+			}
+
+			if (!bhv_jump_label_validate_opcode(
+				    entry, bhv_jump_label_type(entry), opcode,
+				    len)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label");
+				return FAIL;
+			}
+
+			// Search for overlapping jump labels.
+			for (iter = i->entries_start; iter < i->entries_stop;
+			     iter++) {
+				if (iter == entry)
+					continue;
+
+				tmp_addr = (unsigned long)jump_entry_code(iter);
+
+				if (addr <= tmp_addr && tmp_addr < addr + len) {
+					snprintf(
+						error_msg,
+						HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+						"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+						(unsigned long)entry,
+						(unsigned long)iter);
+					return FAIL;
+				}
+			}
+
+			return SUCCESS;
+		}
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+	return FAIL;
+}
+
+static int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+						 const void *opcode, size_t len)
+{
+	int rv = 0;
+	unsigned long r;
+	jump_label_validation_t validation_ok;
+	void *dest_virt_addr = (void *)jump_entry_code(entry);
+	char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return -E2BIG;
+	}
+
+	validation_ok = validate_jmp_labels(entry, opcode, len, message);
+	if (validation_ok == SKIP) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return 0;
+	}
+
+	if (validation_ok == FAIL) {
+		if (bhv_patch_violation_hypercall(dest_virt_addr, message)) {
+			// Block this patch
+			if (HypABI__Richard__Close__hypercall())
+				pr_err("%s: could not close vault",
+				       __FUNCTION__);
+			return -EACCES;
+		}
+
+		// The violation should only be logged. Thus we continue.
+	}
+
+	r = bhv_patch_hypercall(dest_virt_addr, opcode, (uint64_t)len, true);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+	return rv;
+}
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool has_overlapping_entries(struct jump_entry *entry,
+				    struct jump_entry *start,
+				    struct jump_entry *stop, struct module *mod,
+				    size_t op_size,
+				    char *error_msg)
+{
+	unsigned long addr = jump_entry_code(entry);
+	struct jump_entry *iter = NULL;
+	struct module *tmp_mod;
+
+	/*
+        * Module entries should only point to the text section of the module.
+        */
+	preempt_disable();
+	tmp_mod = __module_text_address(addr);
+	preempt_enable();
+
+	if (tmp_mod == NULL) {
+		snprintf(
+			error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			"Jump label trying to write address outside of module (0x%lx | entry @ 0x%px)",
+			addr, entry);
+		return false;
+	}
+
+	if (tmp_mod != mod) {
+		snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			 "Found invalid module for jump label (0x%lx<->0x%lx)",
+			 (unsigned long)tmp_mod, (unsigned long)mod);
+		return false;
+	}
+
+	/* Disallow overlapping entries. */
+	for (iter = start; iter < stop; iter++) {
+		unsigned long tmp_addr;
+		int tmp_size;
+
+		if (iter == entry)
+			continue;
+
+		tmp_addr = (unsigned long)jump_entry_code(iter);
+#ifdef JUMP_LABEL_NOP_SIZE
+		tmp_size = JUMP_LABEL_NOP_SIZE;
+#else
+		arch_jump_entry_size(entry);
+#endif
+
+		if ((addr <= tmp_addr && tmp_addr < addr + op_size) ||
+		    (addr >= tmp_addr && addr < tmp_addr + tmp_size)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+				(unsigned long)entry, (unsigned long)iter);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_mod(struct jump_entry *entry,
+					size_t op_size,
+					char *error_msg)
+{
+	struct static_key_mod *m;
+	struct static_key_mod *jlm;
+
+	struct jump_entry *iter_start;
+	struct jump_entry *iter_stop;
+
+	struct static_key *key = jump_entry_key(entry);
+
+	if (!static_key_linked(key)) {
+		struct module *mod;
+
+		preempt_disable();
+		mod = __module_address((unsigned long)key);
+		if (mod) {
+			iter_start = mod->jump_entries;
+			iter_stop = mod->jump_entries + mod->num_jump_entries;
+		}
+		preempt_enable();
+
+		if (!mod) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Unknown module jump label (%lx)",
+				 (unsigned long)entry);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	jlm = static_key_mod(jump_entry_key(entry));
+
+	//pr_info("%s:%d\n", __FUNCTION__, __LINE__);
+
+	for (m = jlm; m; m = m->next) {
+		struct module *mod = NULL;
+
+		if (!m->entries)
+			continue;
+
+		mod = m->mod;
+
+		if (!mod)
+			continue;
+
+		//pr_info("%s:%d | %s addr @ 0x%lx\n", __FUNCTION__, __LINE__, mod->name, addr);
+
+		iter_start = mod->jump_entries;
+		iter_stop = iter_start + mod->num_jump_entries;
+
+		/* Continue if we did not find the correct jump label table. */
+		if (entry < iter_start || entry >= iter_stop)
+			continue;
+
+		if (mod->state != MODULE_STATE_COMING &&
+		    mod->state != MODULE_STATE_LIVE) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Invalid module state for jump label (%u)",
+				 mod->state);
+			return false;
+		}
+
+		if (jump_entry_is_init(entry) &&
+		    mod->state != MODULE_STATE_COMING) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Trying to apply jump label in incorrect module state (%u)",
+				mod->state);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_kern(struct jump_entry *entry,
+					 char *error_msg)
+{
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		/*
+                 * Ensure that this jump label only modifies the kernel .text
+                 * segment.
+                 */
+		if (!__kernel_text_address((unsigned long)dest_addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				(unsigned long)dest_addr);
+			return false;
+		}
+
+		/*
+                 * Note: we do not search for overlapping target addresses of
+                 * jump labels that are part of the kernel image; the section
+                 * holding the jump labels cannot be overwritten from the
+                 * outside of the vault.
+                 */
+
+		//pr_info("%s:%d | kern addr @ 0x%px\n", __FUNCTION__, __LINE__, dest_addr);
+
+		return true;
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Jump label destination (0x%lx) outside of kernel text region",
+		 (unsigned long)dest_addr);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int bhv_validate_jump_label_entry(struct jump_entry *entry,
+					 const void *opcode, size_t op_size)
+{
+	bool is_valid = false;
+	HypABI__Patch__PatchViolation__arg__T violation_arg;
+	char *error_msg_ptr = violation_arg.message;
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		is_valid = bhv_validate_jump_label_kern(entry, error_msg_ptr);
+	} else {
+		is_valid = bhv_validate_jump_label_mod(entry, op_size, error_msg_ptr);
+	}
+
+	if (!is_valid) {
+		if (bhv_patch_violation_hypercall(
+			    (void *)jump_entry_code(entry), error_msg_ptr)) {
+			return -EACCES;
+		}
+
+		dump_stack();
+		BUG();
+
+		/* XXX: Shall we continue without reporting an issue? */
+		return -1;
+	}
+
+	return 0;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t op_size)
+{
+	int rc = 0;
+	unsigned long flags;
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	if (op_size > HypABI__Patch__PatchViolation__MAX_MSG_SZ) {
+		return -E2BIG;
+	}
+
+#ifdef VASKM // out of tree
+	bhv_arg_ptr =
+		(bhv_patch_arg_t *)kmalloc(sizeof(bhv_patch_arg_t), GFP_KERNEL);
+
+	if (!bhv_arg_ptr) {
+		pr_err("BHV: cannot allocate bhv_arg\n");
+		return -ENOMEM;
+	}
+#endif // VASKM
+
+	rc = bhv_validate_jump_label_entry(entry, opcode, op_size);
+	if (rc) {
+		dump_stack();
+		BUG();
+
+		return rc;
+	}
+
+	//pr_info("[BHV] Patching addr @ 0x%px with %llx (size=%d)\n", dest_addr, *((uint64_t *)opcode), op_size);
+
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(dest_addr, opcode, (uint64_t)op_size);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git security/bhv/reg_protect.c security/bhv/reg_protect.c
new file mode 100644
index 000000000..5cae2a0da
--- /dev/null
+++ security/bhv/reg_protect.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield)
+{
+	return HypABI__RegisterProtection__Freeze__HYPERCALL(
+			.register_selector = reg_selector,
+			.freeze_bitfield = freeze_bitfield);
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void)
+{
+	bhv_start_reg_protect_arch();
+}
+/***************************************************/
diff --git security/bhv/reverse_shell_detection.c security/bhv/reverse_shell_detection.c
new file mode 100644
index 000000000..8817ddccc
--- /dev/null
+++ security/bhv/reverse_shell_detection.c
@@ -0,0 +1,477 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors:  Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 
+ */
+
+#include <linux/binfmts.h>
+#include <linux/file.h>
+#include <linux/wait.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/sched/signal.h>
+#include <linux/fdtable.h>
+#include <net/sock.h>
+
+#include <bhv/reverse_shell_detection.h>
+#include <bhv/guestlog.h>
+#include <bhv/util.h>
+#include <bhv/config.h>
+
+#define STDIN 0
+#define STDOUT 1
+#define STDERR 2
+
+/***********************************************************************
+ * Hypercalls
+ ***********************************************************************/
+static inline bool rsd_hyp_block_interpreter_bound(const char *interpreter,
+						   uint32_t fd,
+						   struct file *file)
+{
+	int r;
+
+	bool remediate = bhv_policy_get_interpreter_binds_remediate();
+
+	r = bhv_guestlog_log_reverse_shell_detection_interpreter_bound(
+		interpreter, fd, file, remediate);
+	if (r != 0) {
+		pr_err("Could not log interpreter bound event!\n");
+	}
+
+	return remediate;
+}
+
+typedef struct {
+	struct task_struct *interpreter; // The interpreter with the pipe.
+	uint32_t interpreter_fd;
+	struct task_struct *transitive; // The process containing the socket.
+	uint32_t transitive_sock_fd;
+	uint32_t transitive_pipe_fd;
+	struct sockaddr *address;
+} transitive_info;
+
+static inline bool rsd_hyp_block_interpreter_transitive(transitive_info *info)
+{
+	int r;
+
+	bool remediate = bhv_policy_get_interpreter_binds_remediate();
+
+	r = bhv_guestlog_log_reverse_shell_detection_interpreter_transitive(
+		info->interpreter, info->interpreter_fd, info->transitive,
+		info->transitive_pipe_fd, info->transitive_sock_fd,
+		info->address, remediate);
+	if (r != 0) {
+		pr_err("Could not log interpreter transitive event!\n");
+	}
+
+	return remediate;
+}
+/***********************************************************************/
+
+static inline bool is_interpreter(struct task_struct *task)
+{
+	char *buf;
+	const char *path;
+	bool rv;
+
+	if (!task || !task->active_mm || !task->active_mm->exe_file)
+		return false;
+
+	// Check for interpreter
+	buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+		return false;
+	}
+
+	path = bhv_get_file_path(task->active_mm->exe_file, buf, PATH_MAX);
+	rv = bhv_policy_interpreter_binds_is_interpreter(path, NULL);
+
+	kfree(buf);
+	return rv;
+}
+
+static inline bool is_socket(struct file *f)
+{
+	if (f == NULL)
+		return false;
+
+	return S_ISSOCK(f->f_path.dentry->d_inode->i_mode);
+}
+
+static inline bool is_remote_socket(struct file *f)
+{
+	struct socket *sock;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+	int err;
+#endif
+
+	if (!is_socket(f))
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+	sock = sock_from_file(f, &err);
+#else
+	sock = sock_from_file(f);
+#endif
+	BUG_ON(sock == NULL);
+
+	// Only consider INET sockets
+	if (sock->sk->sk_family != AF_INET && sock->sk->sk_family != AF_INET6) {
+		return false;
+	}
+
+	return true;
+}
+
+static inline bool is_standard_fd(uint32_t fd)
+{
+	if (fd >= STDIN && fd <= STDERR)
+		return true;
+
+	return false;
+}
+
+static bool _bhv_reverse_shell_find_fd(struct task_struct *task, bool socket,
+				       uint32_t start_fd, uint32_t end_fd,
+				       struct file *target, uint32_t *out)
+{
+	struct files_struct *files;
+	struct file *cur;
+	bool rv = false;
+	uint32_t i = start_fd;
+
+	task_lock(task);
+	files = task->files;
+	if (!files)
+		goto out;
+
+	while ((cur = files_lookup_fd_raw(files, i)) && i <= end_fd) {
+		if (target) {
+			if (bhv_is_pipe(cur) &&
+			    cur->private_data == target->private_data) {
+				goto found;
+			}
+		} else {
+			if (socket && is_remote_socket(cur)) {
+				goto found;
+			} else if (!socket && bhv_is_pipe(cur)) {
+				goto found;
+			}
+		}
+
+		i++;
+	}
+
+	// no match
+	goto out;
+
+found:
+	rv = true;
+	*out = i;
+
+out:
+	task_unlock(task);
+	return rv;
+}
+
+static bool _bhv_reverse_shell_has_socket(struct task_struct *task,
+					  uint32_t start_fd, uint32_t *out)
+{
+	return _bhv_reverse_shell_find_fd(task, true, start_fd, 0xffffffff,
+					  NULL, out);
+}
+
+static bool _bhv_reverse_shell_has_pipe(struct task_struct *task,
+					uint32_t start_fd, uint32_t end_fd,
+					uint32_t *out)
+{
+	return _bhv_reverse_shell_find_fd(task, false, start_fd, end_fd, NULL,
+					  out);
+}
+
+static bool _bhv_reverse_shell_find_pipe(struct task_struct *task,
+					 uint32_t start_fd, uint32_t end_fd,
+					 struct file *f, uint32_t *out)
+{
+	return _bhv_reverse_shell_find_fd(task, false, start_fd, end_fd, f,
+					  out);
+}
+
+typedef struct {
+	struct task_struct *task;
+	struct list_head list;
+} task_skip_list;
+
+static inline bool _bhv_reverse_shell_in_skip_list(struct task_struct *task,
+						   struct list_head *skip_list)
+{
+	task_skip_list *iter;
+
+	list_for_each_entry (iter, skip_list, list) {
+		if (task == iter->task) {
+			return true;
+		}
+	}
+
+	return false;
+}
+
+static int bhv_reverse_shell_find_transitive_helper(transitive_info *info,
+						    uint32_t current_fd,
+						    struct file *current_file,
+						    struct task_struct *skip,
+						    struct list_head *skip_list)
+{
+	struct task_struct *cur;
+	struct task_struct *proc;
+	uint32_t transitive_fd;
+	uint32_t tmp_fd;
+	int rv = 0;
+
+	task_skip_list *skip_entry;
+
+	if (!current_file)
+		return 0;
+
+	skip_entry = kmalloc(sizeof(task_skip_list), GFP_KERNEL);
+	if (skip_entry == NULL) {
+		pr_err("Could not allocate memory for skip list!\n");
+		return -EPERM;
+	}
+	skip_entry->task = skip;
+	list_add_tail(&skip_entry->list, skip_list);
+
+	for_each_process (proc) {
+		for_each_thread (proc, cur) {
+			if (!pid_alive(cur))
+				continue;
+
+			// Skip kernel threads
+			if ((cur->flags & PF_KTHREAD))
+				continue;
+
+			// Did we already consider this process?
+			if (_bhv_reverse_shell_in_skip_list(cur, skip_list))
+				continue;
+
+			// Find the current pipe we are looking for.
+			// If the current FD is stdin, we are looking for stdou and vice versa.
+			if (_bhv_reverse_shell_find_pipe(
+				    cur, current_fd == STDIN ? STDOUT : STDIN,
+				    current_fd == STDIN ? STDERR : STDIN,
+				    current_file, &transitive_fd)) {
+				// We found a matching file.
+				// Do we already have an interpreter?
+				if (info->interpreter &&
+				    _bhv_reverse_shell_has_socket(cur, 0,
+								  &tmp_fd)) {
+					// This is a transitive shell!
+					info->transitive = cur;
+					info->transitive_pipe_fd =
+						transitive_fd;
+					info->transitive_sock_fd = tmp_fd;
+					if (rsd_hyp_block_interpreter_transitive(
+						    info)) {
+						rv = -EPERM;
+						goto out;
+					}
+				} else if (!info->interpreter &&
+					   is_standard_fd(transitive_fd) &&
+					   is_interpreter(cur)) {
+					info->interpreter = cur;
+					info->interpreter_fd = transitive_fd;
+					// An interpreter is the endpoint of the pipe. This is also a transitive shell!
+					if (rsd_hyp_block_interpreter_transitive(
+						    info)) {
+						rv = -EPERM;
+						goto out;
+					}
+				} else {
+					// This is not a transitive shell, lets look for other pipes to follow.
+					uint32_t i = STDIN ? STDOUT : STDIN;
+					struct file *tmp_file;
+					while (_bhv_reverse_shell_has_pipe(
+						cur, i,
+						current_fd == STDIN ? STDERR :
+								      STDIN,
+						&tmp_fd)) {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+						tmp_file =
+							task_lookup_fdget_rcu(
+								cur, tmp_fd);
+#else
+						tmp_file = task_lookup_fd_rcu(
+							cur, tmp_fd);
+#endif
+						rv = bhv_reverse_shell_find_transitive_helper(
+							info, tmp_fd, tmp_file,
+							cur, skip_list);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 7, 0)
+						if (tmp_file)
+							fput(tmp_file);
+#endif
+
+						// We found something in our extended search.
+						if (rv) {
+							goto out;
+						}
+
+						// Search for the next
+						i = tmp_fd + 1;
+					}
+				}
+			}
+		}
+	}
+
+out:
+	list_del(&skip_entry->list);
+	kfree(skip_entry);
+
+	return rv;
+}
+
+static int bhv_reverse_shell_find_transitive(transitive_info *info,
+					     uint32_t current_fd,
+					     struct file *current_file,
+					     struct task_struct *skip)
+{
+	int rv = 0;
+	LIST_HEAD(skip_list);
+
+	rcu_read_lock();
+	rv = bhv_reverse_shell_find_transitive_helper(
+		info, current_fd, current_file, skip, &skip_list);
+	rcu_read_unlock();
+
+	return rv;
+}
+
+int bhv_reverse_shell_fd_dup(unsigned int fd, struct file *file)
+{
+	bool socket = false;
+	bool pipe = false;
+	const char *path;
+	char *buf;
+	int rv = 0;
+
+	if (!bhv_policy_get_interpreter_binds_enabled())
+		return 0;
+
+	// We are only considering stdin, stdout, and stderr
+	if (!is_standard_fd(fd))
+		return 0;
+
+	if (bhv_policy_get_interpreter_binds_container_only() &&
+	    !bhv_task_in_container(current)) {
+		return 0;
+	}
+
+	// Check if this file is a socket or pipe.
+	socket = is_remote_socket(file);
+
+	if (bhv_policy_get_interpreter_binds_detect_transitive()) {
+		pipe = bhv_is_pipe(file);
+	}
+
+	if (!socket && !pipe)
+		return 0;
+
+	if (!current->active_mm || !current->active_mm->exe_file) {
+		pr_warn("Unknown binary!\n");
+		return 0;
+	}
+
+	// Check for interpreter
+	buf = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (buf == NULL) {
+		pr_err("Cannot allocate memory!\n");
+		return -ENOMEM;
+	}
+
+	path = bhv_get_file_path(current->active_mm->exe_file, buf, PATH_MAX);
+	if (!bhv_policy_interpreter_binds_is_interpreter(path, NULL)) {
+		rv = 0;
+		goto out;
+	}
+
+	if (socket) {
+		if (rsd_hyp_block_interpreter_bound(path, fd, file)) {
+			rv = -EPERM;
+			goto out;
+		}
+	} else if (pipe) {
+		// Check for transitive reverse shells
+		transitive_info info;
+		info.interpreter = current;
+		info.interpreter_fd = fd;
+		info.address = NULL;
+		rv = bhv_reverse_shell_find_transitive(&info, fd, file,
+						       current);
+	}
+
+out:
+	kfree(buf);
+	return rv;
+}
+
+int bhv_reverse_shell_detection_socket_connect(struct socket *sock,
+					       struct sockaddr *address,
+					       int addrlen)
+{
+	uint32_t tmp_fd;
+	uint32_t i;
+	int rv;
+	struct file *file;
+	transitive_info info;
+
+	if (!bhv_policy_get_interpreter_binds_detect_transitive())
+		return 0;
+
+	if (sock->sk->sk_family == AF_UNIX || sock->sk->sk_family == AF_LOCAL)
+		return 0;
+
+	info.transitive = current;
+	info.address = address;
+	info.interpreter = NULL;
+
+	i = STDIN;
+	while (_bhv_reverse_shell_has_socket(current, i, &tmp_fd)) {
+		int err = 0;
+		struct socket *tmp_sock = sockfd_lookup(tmp_fd, &err);
+
+		if (err) {
+			pr_err("Could not find socket!\n");
+			return 0;
+		}
+
+		if (tmp_sock == sock) {
+			info.transitive_sock_fd = tmp_fd;
+			break;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	i = STDIN;
+	while (_bhv_reverse_shell_has_pipe(current, i, STDERR, &tmp_fd)) {
+		info.transitive_pipe_fd = tmp_fd;
+		file = fget_task(current, tmp_fd);
+
+		rv = bhv_reverse_shell_find_transitive(&info, tmp_fd, file,
+						       current);
+
+		fput(file);
+
+		if (rv) {
+			return rv;
+		}
+
+		i = tmp_fd + 1;
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git security/bhv/sysfs.c security/bhv/sysfs.c
new file mode 100644
index 000000000..53e8633c1
--- /dev/null
+++ security/bhv/sysfs.c
@@ -0,0 +1,69 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef CONFIG_SYSFS
+#error CONFIG_SYSFS required!
+#endif
+
+#include <linux/kobject.h>
+
+#include <bhv/bhv.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/sysfs_reg_protect.h>
+#include <bhv/sysfs_version.h>
+
+/**********************************************************
+ * start
+ **********************************************************/
+void __init_km bhv_start_sysfs(void)
+{
+	struct kobject *bhv_kobj;
+	struct kobject *version_kobj;
+	struct kobject *integrity_kobj;
+	struct kobject *integrity_freeze_kobj;
+	struct kobject *register_kobj;
+	struct kobject *register_freeze_kobj;
+	struct kobject *fops_kobj;
+	struct kobject *fopsstatus_kobj;
+
+#define CREATE_KOBJ(kobj, name, parent)                                        \
+	kobj = kobject_create_and_add(name, parent);                           \
+	BUG_ON(!kobj);
+
+#ifndef CONFIG_SYS_HYPERVISOR
+	struct kobject *hypervisor_kobj;
+	CREATE_KOBJ(hypervisor_kobj, "hypervisor", NULL);
+#endif // CONFIG_SYS_HYPERVISOR
+
+	CREATE_KOBJ(bhv_kobj, "bhv", hypervisor_kobj);
+	
+	CREATE_KOBJ(version_kobj, "version", bhv_kobj);
+	bhv_start_sysfs_version(version_kobj);
+
+	CREATE_KOBJ(integrity_kobj, "integrity", bhv_kobj);
+	CREATE_KOBJ(integrity_freeze_kobj, "freeze", integrity_kobj);
+	bhv_start_sysfs_integrity_freeze(integrity_freeze_kobj);
+
+	if (bhv_reg_protect_is_enabled()) {
+		CREATE_KOBJ(register_kobj, "register", bhv_kobj);
+		CREATE_KOBJ(register_freeze_kobj, "freeze", register_kobj);
+
+		bhv_start_sysfs_reg_protect(register_freeze_kobj);
+	}
+
+	if (bhv_fileops_file_protection_is_enabled()) {
+		CREATE_KOBJ(fops_kobj, "fileops_protection", bhv_kobj);
+		CREATE_KOBJ(fopsstatus_kobj, "status", fops_kobj);
+
+		bhv_start_sysfs_fileops_protection(fops_kobj, fopsstatus_kobj);
+	}
+}
+/**********************************************************/
diff --git security/bhv/sysfs_fops.c security/bhv/sysfs_fops.c
new file mode 100644
index 000000000..e83ee4cc6
--- /dev/null
+++ security/bhv/sysfs_fops.c
@@ -0,0 +1,167 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kobject.h>
+#include <linux/printk.h>
+#include <asm-generic/set_memory.h>
+
+#include <bhv/bhv.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/module.h>
+#include <bhv/sysfs_fops.h>
+
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf);
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf);
+
+struct bhv_fopsstatus_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint8_t param;
+} static const __section(".rodata") _bhv_fopsstatus_sysfs_data[] = {
+#define FOPS_MAP(name, idx, _, __)                                             \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected,     \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FOPS_MAP_DIRNULL(name, idx, _)                                         \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected_dn,  \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FILEOPS_INTERNAL_FOPSMAP_ALL
+#include <bhv/fileops_internal_fopsmap.h>
+};
+#define attr_to_bsd(ptr)                                                       \
+	container_of((ptr), struct bhv_fopsstatus_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c %c\n",
+			 fileops_map[data->param][0] ? '1' : '0',
+			 fileops_map[data->param][1] ? '1' : '0');
+}
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n",
+			 fileops_map[data->param][0] ? '1' : '0');
+}
+#undef attr_to_bsd
+
+#ifdef VASKM // out of tree
+
+bool bhv_allow_update_fileops_map = true;
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf);
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count);
+
+static char _bhv_fopsfreeze_flag;
+struct bhv_fopsfreeze_sysfs_data {
+	const struct kobj_attribute attr;
+} static _bhv_fopsfreeze_sysfs_data[] = {
+	{
+		.attr = __ATTR(freeze, 0644, _bhv_intfr_sysfs_show_freeze,
+			       _bhv_fops_sysfs_store_freeze),
+	},
+};
+
+int bhv_freeze_fops_map(void)
+{
+	int rc;
+	bhv_allow_update_fileops_map = false;
+	rc = set_memory_ro((unsigned long)fileops_map, 1);
+	if (rc)
+		return rc;
+	bhv_protect_generic_memory((unsigned long)THIS_MODULE, fileops_map,
+				   PAGE_SIZE,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "FILEOPS_MAP");
+	_bhv_fopsfreeze_flag = '1';
+	return 0;
+}
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%c\n", _bhv_fopsfreeze_flag);
+}
+
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1')) {
+		if (_bhv_fopsfreeze_flag == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((_bhv_fopsfreeze_flag == '0') && (buf[0] == '1')) {
+			int ret = bhv_freeze_fops_map();
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_fileops_protection(struct kobject *fops,
+						  struct kobject *status)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsstatus_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsstatus_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(status, attr_array));
+
+#ifdef VASKM // out of tree
+	static_assert(ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) >=
+		      ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data));
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsfreeze_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(fops, attr_array));
+#endif // VASKM
+}
+/***************************************************/
diff --git security/bhv/sysfs_integrity_freeze.c security/bhv/sysfs_integrity_freeze.c
new file mode 100644
index 000000000..d17c158be
--- /dev/null
+++ security/bhv/sysfs_integrity_freeze.c
@@ -0,0 +1,103 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/sysfs_integrity_freeze.h>
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count);
+
+struct bhv_intfr_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t param;
+	const bool *const flagp;
+} static const __section(".rodata") _bhv_intfr_sysfs_data[] = {
+	{ .attr = __ATTR(create, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__CREATE,
+	  .flagp = &bhv_integrity_freeze_create_currently_frozen },
+	{ .attr = __ATTR(update, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__UPDATE,
+	  .flagp = &bhv_integrity_freeze_update_currently_frozen },
+	{ .attr = __ATTR(remove, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+	  .flagp = &bhv_integrity_freeze_remove_currently_frozen },
+	{ .attr = __ATTR(patch, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__PATCH,
+	  .flagp = &bhv_integrity_freeze_patch_currently_frozen },
+};
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_intfr_sysfs_data, attr)
+#define cur_flag_val(bisdp) (*((bisdp)->flagp) ? '1' : '0')
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n", cur_flag_val(data));
+}
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1' || buf[0] == '2')) {
+		struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+		if (cur_flag_val(data) == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((cur_flag_val(data) == '0') &&
+			   (buf[0] == '1' || buf[0] == '2')) {
+			int ret = bhv_enable_integrity_freeze_flag(
+				data->param, buf[0] == '2');
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_integrity_freeze(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_intfr_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_intfr_sysfs_data); ++i)
+		attr_array[i] = &_bhv_intfr_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_reg_protect.c security/bhv/sysfs_reg_protect.c
new file mode 100644
index 000000000..bcaded347
--- /dev/null
+++ security/bhv/sysfs_reg_protect.c
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs_reg_protect.h>
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count);
+
+struct bhv_rp_sysfs_data {
+	const struct kobj_attribute attr;
+	const enum HypABI__RegisterProtection__Freeze__RegisterSelector reg;
+} static const __section(".rodata") _bhv_rp_sysfs_data[] = {
+#define OP(regn)                                                               \
+	{ .attr = __ATTR(regn, 0640, _bhv_rp_sysfs_show, _bhv_rp_sysfs_store), \
+	  .reg = HypABI__RegisterProtection__Freeze__RegisterSelector__##regn },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+struct bhv_rp_sysfs_data_var {
+	uint64_t curr_status;
+} static _bhv_rp_sysfs_data_var[] = {
+#define OP(regn) { .curr_status = 0UL },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+static_assert(ARRAY_SIZE(_bhv_rp_sysfs_data) ==
+	      ARRAY_SIZE(_bhv_rp_sysfs_data_var));
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_rp_sysfs_data, attr)
+#define attr_to_bsd_var(ptr) \
+	_bhv_rp_sysfs_data_var + (attr_to_bsd(ptr) - _bhv_rp_sysfs_data)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static DEFINE_MUTEX(_bhv_rp_sysfs_lock);
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int b;
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	b = scnprintf(buf, PAGE_SIZE, "%016llx\n", data_var->curr_status);
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+	return b;
+}
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	uint64_t nval;
+	int rv = 0;
+	int i = sscanf(buf, "%llx", &nval);
+	if (i != 1) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	nval |= data_var->curr_status;
+
+	rv = bhv_reg_protect_freeze(data->reg, nval);
+
+	if (!rv)
+		data_var->curr_status = nval;
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+
+	if (rv < 0) {
+		return rv;
+	} else {
+		return count;
+	}
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_reg_protect(struct kobject *kobj)
+{
+	int i;
+	const struct attribute *attr_array
+		[1 +
+		 HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_rp_sysfs_data); ++i)
+		attr_array[i] = &_bhv_rp_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_version.c security/bhv/sysfs_version.c
new file mode 100644
index 000000000..bf81c6c86
--- /dev/null
+++ security/bhv/sysfs_version.c
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_version.h>
+
+#define __BHV_VERSION(a, b, c) a, b, c
+#define __BHV_VAS_ABI_VERSION(a, b, c, d) a, b, c, d
+#include <bhv/version.h>
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf);
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf);
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#endif // VASKM
+
+
+struct bhv_version_sysfs_data {
+	const struct kobj_attribute attr;
+} static const __section(".rodata") _bhv_version_sysfs_data[] = {
+
+	{
+		.attr = __ATTR(bhv_version, 0600,
+			       _bhv_version_sysfs_show_bhv_version, NULL),
+	},
+	{
+		.attr = __ATTR(bhv_abi_version, 0600,
+			       _bhv_version_sysfs_show_bhv_abi_version, NULL),
+	},
+#ifndef VASKM // inside kernel tree
+	{
+		.attr = __ATTR(linux_commit, 0600,
+			       _bhv_version_sysfs_show_linux_commit, NULL),
+	},
+#else // out of tree
+	{
+		.attr = __ATTR(vaskm_commit, 0600,
+			       _bhv_version_sysfs_show_vaskm_commit, NULL),
+	},
+#endif // VASKM
+};
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02d.%02d.%d\n", BHV_VERSION);
+}
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02u.%02u.%02u:%05u\n", HypABI__ABI_VERSION);
+}
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", KERNEL_COMMIT_HASH);
+}
+
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", VASKM_COMMIT_HASH);
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_version(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_version_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_version_sysfs_data); ++i)
+		attr_array[i] = &_bhv_version_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/vmalloc_to_page.c security/bhv/vmalloc_to_page.c
new file mode 100644
index 000000000..8abf8b3a0
--- /dev/null
+++ security/bhv/vmalloc_to_page.c
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copied from kernel v6.1, mm/vmalloc.c
+/*
+ *  Copyright (C) 1993  Linus Torvalds
+ *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
+ *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
+ *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002
+ *  Numa awareness, Christoph Lameter, SGI, June 2005
+ *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <bhv/vault.h>
+
+#ifdef VASKM // out of tree
+#include <kln.h>
+#undef pgd_offset_k
+#define pgd_offset_k(address)                                                  \
+	pgd_offset(KLN_SYMBOL(struct mm_struct *, init_mm), (address))
+#endif // VASKM
+
+
+
+/*
+ * Walk a vmap address to the struct page it maps. Huge vmap mappings will
+ * return the tail page that corresponds to the base page address, which
+ * matches small vmap mappings.
+ */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr)
+{
+	unsigned long addr = (unsigned long) vmalloc_addr;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	/*
+	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
+	 * architectures that do not vmalloc module space
+	 */
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (pgd_none(*pgd))
+		return NULL;
+	if (WARN_ON_ONCE(pgd_leaf(*pgd)))
+		return NULL; /* XXX: no allowance for huge pgd */
+	if (WARN_ON_ONCE(pgd_bad(*pgd)))
+		return NULL;
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return NULL;
+	if (p4d_leaf(*p4d))
+		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(p4d_bad(*p4d)))
+		return NULL;
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return NULL;
+	if (pud_leaf(*pud))
+		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return NULL;
+	if (pmd_leaf(*pmd))
+		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pmd_bad(*pmd)))
+		return NULL;
+
+	ptep = pte_offset_map(pmd, addr);
+	pte = *ptep;
+	if (pte_present(pte))
+		page = pte_page(pte);
+	pte_unmap(ptep);
+
+	return page;
+}
diff --git security/integrity/digsig.c security/integrity/digsig.c
index de442af7b..f141fc961 100644
--- security/integrity/digsig.c
+++ security/integrity/digsig.c
@@ -17,6 +17,8 @@
 #include <crypto/public_key.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
+
 #include "integrity.h"
 
 static struct key *keyring[INTEGRITY_KEYRING_MAX];
@@ -44,6 +46,7 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 		return ERR_PTR(-EINVAL);
 
 	if (!keyring[id]) {
+		int rc = 0;
 		keyring[id] =
 			request_key(&key_type_keyring, keyring_name[id], NULL);
 		if (IS_ERR(keyring[id])) {
@@ -52,6 +55,19 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 			keyring[id] = NULL;
 			return ERR_PTR(err);
 		}
+
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets registered
+		 * directly in set_platform_trusted_keys.
+		 */
+		rc = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
+
+	} else {
+		int rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
 	}
 
 	return keyring[id];
@@ -109,6 +125,19 @@ static int __init __integrity_init_keyring(const unsigned int id,
 			keyring_name[id], err);
 		keyring[id] = NULL;
 	} else {
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets also registered
+		 * in set_platform_trusted_keys to allow passing the keyring
+		 * directly and indirectly (through INTEGRITY_KEYRING_PLATFORM)
+		 * to verify_pkcs7_message_sig.
+		 */
+		err = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (err) {
+			key_put(keyring[id]);
+			keyring[id] = NULL;
+			return err;
+		}
+
 		if (id == INTEGRITY_KEYRING_PLATFORM)
 			set_platform_trusted_keys(keyring[id]);
 	}
@@ -156,6 +185,10 @@ int __init integrity_add_key(const unsigned int id, const void *data,
 	if (!keyring[id])
 		return -EINVAL;
 
+	rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+	if (rc)
+		return rc;
+
 	key = key_create_or_update(make_key_ref(keyring[id], 1), "asymmetric",
 				   NULL, data, size, perm,
 				   KEY_ALLOC_NOT_IN_QUOTA);
diff --git security/integrity/ima/ima_mok.c security/integrity/ima/ima_mok.c
index 95cc31525..87c7d9703 100644
--- security/integrity/ima/ima_mok.c
+++ security/integrity/ima/ima_mok.c
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
 
 struct key *ima_blacklist_keyring;
 
@@ -44,6 +45,10 @@ static __init int ima_mok_init(void)
 
 	if (IS_ERR(ima_blacklist_keyring))
 		panic("Can't allocate IMA blacklist keyring.");
+	else
+		if (bhv_keyring_register_system_keyring(&ima_blacklist_keyring))
+			panic("Can't register IMA blacklist keyring.");
+
 	return 0;
 }
 device_initcall(ima_mok_init);
diff --git security/security.c security/security.c
index f836f292e..77728b4ad 100644
--- security/security.c
+++ security/security.c
@@ -2631,3 +2631,38 @@ int security_perf_event_write(struct perf_event *event)
 	return call_int_hook(perf_event_write, 0, event);
 }
 #endif /* CONFIG_PERF_EVENTS */
+
+void security_module_loaded(struct module *mod)
+{
+	call_void_hook(module_loaded, mod);
+}
+
+int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(unshare, 0, tsk, nsset);
+}
+
+int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(setns, 0, tsk, nsset);
+}
+
+int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return call_int_hook(cgroup_mkdir, 0, cgrp);
+}
+
+void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+	call_void_hook(cgroup_rmdir, cgrp);
+}
+
+int security_fd_dup(unsigned int fd, struct file *file)
+{
+	return call_int_hook(fd_dup, 0, fd, file);
+}
+
+void security_socket_accepted(struct socket *sock, struct socket *newsock)
+{
+	call_void_hook(socket_accepted, sock, newsock);
+}
diff --git security/selinux/hooks.c security/selinux/hooks.c
index 46c00a68b..dfe4ce793 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -125,7 +125,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index d893c2280..23e42ceca 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -611,9 +615,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
 	struct selinux_load_state load_state;
@@ -667,6 +671,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2246,6 +2276,40 @@ static int __init init_sel_fs(void)
 
 __initcall(init_sel_fs);
 
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinuxfs_mount->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&fsi->state->policy_mutex);
+
+	rv = security_load_policy(fsi->state, data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(fsi->state, &load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(fsi->state, &load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&fsi->state->policy_mutex);
+	return rv;
+}
+#endif
+
 #ifdef CONFIG_SECURITY_SELINUX_DISABLE
 void exit_sel_fs(void)
 {
diff --git tools/objtool/Build tools/objtool/Build
index b7222d5cc..7f848718d 100644
--- tools/objtool/Build
+++ tools/objtool/Build
@@ -10,6 +10,7 @@ objtool-$(SUBCMD_ORC) += orc_dump.o
 
 objtool-y += builtin-check.o
 objtool-y += builtin-orc.o
+objtool-y += vault.o
 objtool-y += elf.o
 objtool-y += objtool.o
 
diff --git tools/objtool/builtin-check.c tools/objtool/builtin-check.c
index 447a49c03..5e20931b5 100644
--- tools/objtool/builtin-check.c
+++ tools/objtool/builtin-check.c
@@ -19,7 +19,7 @@
 #include "objtool.h"
 
 bool no_fp, no_unreachable, retpoline, module, backtrace, uaccess, stats,
-     validate_dup, vmlinux, sls, unret, rethunk;
+     validate_dup, vmlinux, sls, unret, rethunk, vault;
 
 static const char * const check_usage[] = {
 	"objtool check [<options>] file.o",
@@ -39,6 +39,7 @@ const struct option check_options[] = {
 	OPT_BOOLEAN('d', "duplicate", &validate_dup, "duplicate validation for vmlinux.o"),
 	OPT_BOOLEAN('l', "vmlinux", &vmlinux, "vmlinux.o validation"),
 	OPT_BOOLEAN('S', "sls", &sls, "validate straight-line-speculation"),
+	OPT_BOOLEAN('v', "vault", &vault, "generate vault metadata"),
 	OPT_END(),
 };
 
diff --git tools/objtool/builtin.h tools/objtool/builtin.h
index 61d8d49db..3b0a1d9af 100644
--- tools/objtool/builtin.h
+++ tools/objtool/builtin.h
@@ -9,7 +9,7 @@
 
 extern const struct option check_options[];
 extern bool no_fp, no_unreachable, retpoline, module, backtrace, uaccess, stats,
-            validate_dup, vmlinux, sls, unret, rethunk;
+            validate_dup, vmlinux, sls, unret, rethunk, vault;
 
 extern int cmd_check(int argc, const char **argv);
 extern int cmd_orc(int argc, const char **argv);
diff --git tools/objtool/check.c tools/objtool/check.c
index 0506a48f1..b3584e7e4 100644
--- tools/objtool/check.c
+++ tools/objtool/check.c
@@ -15,6 +15,7 @@
 #include "special.h"
 #include "warn.h"
 #include "arch_elf.h"
+#include "vault.h"
 
 #include <linux/objtool.h>
 #include <linux/hashtable.h>
@@ -57,8 +58,8 @@ static struct instruction *next_insn_same_sec(struct objtool_file *file,
 	return next;
 }
 
-static struct instruction *next_insn_same_func(struct objtool_file *file,
-					       struct instruction *insn)
+struct instruction *next_insn_same_func(struct objtool_file *file,
+					struct instruction *insn)
 {
 	struct instruction *next = list_next_entry(insn, list);
 	struct symbol *func = insn->func;
@@ -705,6 +706,154 @@ static int create_return_sites_sections(struct objtool_file *file)
 	return 0;
 }
 
+#define SEC_VAULT_TEXT ".bhv.vault.text.jump_label"
+#define SEC_VAULT_TEXT_SHARED ".bhv.vault.shared.text.jump_label"
+#define SEC_VAULT_TEXT_REF ".ref.text.bhv.vault.text.jump_label"
+
+static bool is_sec_in_vault(struct section *sec)
+{
+	if (!sec)
+		return false;
+
+	if (strncmp(sec->name, VAULT_TEXT_SECTION,
+		    strlen(VAULT_TEXT_SECTION)) &&
+	    strncmp(sec->name, VAULT_REF_TEXT_SECTION,
+		    strlen(VAULT_REF_TEXT_SECTION)))
+		return false;
+
+	return true;
+}
+
+static struct reloc *insn_reloc(struct objtool_file *file,
+				struct instruction *insn);
+
+static int add_vault_return_site(struct objtool_file *file,
+				 struct instruction *insn,
+				 struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_return_list,
+			    vault_return_node) {
+		if (insn_iter == next_insn)
+			return 0;
+	}
+
+	list_add_tail(&next_insn->vault_return_node, &file->vault_return_list);
+
+	return 0;
+}
+
+static int add_vault_rethunk_site(struct objtool_file *file,
+				  struct instruction *insn,
+				  struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+		if (insn_iter == insn)
+			return 0;
+	}
+
+	list_add_tail(&insn->vault_rethunk_node, &file->vault_rethunk_list);
+
+	return 0;
+}
+
+static int collect_vault_return_sites(struct objtool_file *file,
+				      struct section *sec)
+{
+	struct symbol *func;
+	struct instruction *insn;
+
+	list_for_each_entry(func, &sec->symbol_list, list) {
+		func_for_each_insn(file, func, insn) {
+			struct section *dest_sec = NULL;
+			struct symbol *sym = NULL;
+
+			/* We are interested only in jumps and calls */
+			if (!is_jump(insn) && !is_call(insn))
+				continue;
+
+			if (insn->jump_dest) {
+				sym = insn->jump_dest->func;
+				dest_sec = insn->jump_dest->sec;
+			} else if (insn->call_dest) {
+				sym = insn->call_dest;
+				dest_sec = sym->sec;
+			}
+
+			if (!dest_sec) {
+				struct reloc *reloc = insn_reloc(file, insn);
+				if (!reloc)
+					continue;
+
+				sym = reloc->sym;
+				dest_sec = sym->sec;
+			}
+
+			if (!is_sec_in_vault(dest_sec)) {
+				if (!sym)
+					continue;
+
+				if (sym->retpoline_thunk)
+					add_vault_rethunk_site(file, insn, sym);
+				else
+					add_vault_return_site(file, insn, sym);
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int inspect_vault(struct objtool_file *file)
+{
+	struct section *sec;
+
+	sec = find_section_by_name(file->elf, ".bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	sec = find_section_by_name(file->elf,
+				   ".ref.text.bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	return 0;
+}
+
 /*
  * Warnings shouldn't be reported for ignored functions.
  */
@@ -3524,7 +3673,7 @@ int check(struct objtool_file *file)
 	if (list_empty(&file->insn_list))
 		goto out;
 
-	if (vmlinux && !validate_dup) {
+	if (vmlinux && !validate_dup && !vault) {
 		ret = validate_vmlinux_functions(file);
 		if (ret < 0)
 			goto out;
@@ -3587,6 +3736,24 @@ int check(struct objtool_file *file)
 		warnings += ret;
 	}
 
+	if (vault) {
+		ret = inspect_vault(file);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		//ret = create_vault_entries_section(file);
+		ret = create_vault_section(file, VAULT_SECTION_RETURN_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		ret = create_vault_section(file, VAULT_SECTION_RETHUNK_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+	}
+
 	if (stats) {
 		printf("nr_insns_visited: %ld\n", nr_insns_visited);
 		printf("nr_cfi: %ld\n", nr_cfi);
diff --git tools/objtool/check.h tools/objtool/check.h
index 7f34a7f9c..3c88c8d42 100644
--- tools/objtool/check.h
+++ tools/objtool/check.h
@@ -40,6 +40,9 @@ struct instruction {
 	struct list_head list;
 	struct hlist_node hash;
 	struct list_head call_node;
+	struct list_head vault_entry_node;
+	struct list_head vault_return_node;
+	struct list_head vault_rethunk_node;
 	struct section *sec;
 	unsigned long offset;
 	unsigned int len;
@@ -86,9 +89,27 @@ static inline bool is_jump(struct instruction *insn)
 	return is_static_jump(insn) || is_dynamic_jump(insn);
 }
 
+static inline bool is_static_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL;
+}
+
+static inline bool is_dynamic_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL_DYNAMIC;
+}
+
+static inline bool is_call(struct instruction *insn)
+{
+        return is_static_call(insn) || is_dynamic_call(insn);
+}
+
 struct instruction *find_insn(struct objtool_file *file,
 			      struct section *sec, unsigned long offset);
 
+struct instruction *next_insn_same_func(struct objtool_file *file,
+				        struct instruction *insn);
+
 #define for_each_insn(file, insn)					\
 	list_for_each_entry(insn, &file->insn_list, list)
 
diff --git tools/objtool/objtool.c tools/objtool/objtool.c
index cb2c6acd9..4c6e3b00e 100644
--- tools/objtool/objtool.c
+++ tools/objtool/objtool.c
@@ -64,6 +64,9 @@ struct objtool_file *objtool_open_read(const char *_objname)
 	INIT_LIST_HEAD(&file.retpoline_call_list);
 	INIT_LIST_HEAD(&file.return_thunk_list);
 	INIT_LIST_HEAD(&file.static_call_list);
+	INIT_LIST_HEAD(&file.vault_entry_list);
+	INIT_LIST_HEAD(&file.vault_return_list);
+	INIT_LIST_HEAD(&file.vault_rethunk_list);
 	file.c_file = !vmlinux && find_section_by_name(file.elf, ".comment");
 	file.ignore_unreachables = no_unreachable;
 	file.hints = false;
diff --git tools/objtool/objtool.h tools/objtool/objtool.h
index bf64946e7..3411597ee 100644
--- tools/objtool/objtool.h
+++ tools/objtool/objtool.h
@@ -21,6 +21,9 @@ struct objtool_file {
 	struct list_head retpoline_call_list;
 	struct list_head return_thunk_list;
 	struct list_head static_call_list;
+	struct list_head vault_entry_list;
+	struct list_head vault_return_list;
+	struct list_head vault_rethunk_list;
 	bool ignore_unreachables, c_file, hints, rodata;
 };
 
diff --git tools/objtool/vault.c tools/objtool/vault.c
new file mode 100644
index 000000000..2fb10e278
--- /dev/null
+++ tools/objtool/vault.c
@@ -0,0 +1,175 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) 2024 Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <assert.h>
+#include "arch.h"
+#include "builtin.h"
+#include "vault.h"
+#include "warn.h"
+#include "check.h"
+
+static const char *vault_section_names[] = { ".vault_entries", ".vault_sites",
+					     ".vault_rethunks" };
+
+static int create_entry_points(struct objtool_file *file)
+{
+	/* XXX: We do not identify vault entry points, yet. */
+	return 0;
+}
+
+static int create_return_sites(struct objtool_file *file)
+{
+	int idx = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETURN_SITES];
+
+	printf("%s: start\n", __FUNCTION__);
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_return_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_return_list))
+		return 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node)
+		idx++;
+
+	printf("Found %d returns to the vault (file %s)\n", idx, file->elf->name);
+
+	sec = elf_create_section(file->elf, sec_name, 0, sizeof(unsigned long),
+				 idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node) {
+		unsigned long *loc = (unsigned long *)sec->data->d_buf + idx;
+		memset(loc, 0, sizeof(unsigned long));
+
+		if (elf_add_reloc_to_insn(
+			    file->elf, sec, idx * sizeof(unsigned long),
+			    R_X86_64_64, insn->sec, insn->offset)) {
+			WARN("Cannot add entry[%d] to section %s\n", idx,
+			     sec->name);
+			return -1;
+		}
+
+		printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			idx, insn->offset, insn->func->name,
+			sec->name);
+
+		idx++;
+	}
+
+	return 0;
+}
+
+static int create_rethunk_sites(struct objtool_file *file)
+{
+	int idx = 0, i = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETHUNK_SITES];
+
+	static const int NUM_POSSIBLE_RETURNS = 2;
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_rethunk_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_rethunk_list))
+		return 0;
+
+	/* Account for all possible return points to the patched instruction. */
+	list_for_each_entry(insn, &file->vault_rethunk_list, vault_rethunk_node) {
+		/*
+		 * Note: depending on the system's configuration, retpolines can
+		 * patch a jump/call instruction differently. Yet, there can be
+		 * only two possible return locations, to which the retpoline
+		 * can return to:
+		 *
+		 *   (i)  instruction, right after a short jump (2 bytes)
+		 *   (ii) next instruction after the original jmp/call
+		 */
+		idx += NUM_POSSIBLE_RETURNS;
+	}
+
+	printf("Found %d possible thunk returns to the vault\n", idx);
+
+	sec = elf_create_section(file->elf, sec_name, 0, sizeof(unsigned long),
+				 idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+		unsigned long insn_offsets[NUM_POSSIBLE_RETURNS];
+		struct instruction *next_insn = next_insn_same_func(file, insn);
+		if (next_insn == NULL) {
+        		printf("Cannot find next instruction of %s:0x%lx\n",
+               			insn->sec->name, insn->offset);
+        		return -1;
+		}
+
+		insn_offsets[0] = insn->offset + 2;
+		insn_offsets[1] = next_insn->offset;
+
+		for (i = 0; i < NUM_POSSIBLE_RETURNS; ++i) {
+			unsigned long *loc =
+				(unsigned long *)sec->data->d_buf + idx;
+			memset(loc, 0, sizeof(unsigned long));
+
+			if (elf_add_reloc_to_insn(file->elf, sec,
+						  (idx) * sizeof(unsigned long),
+						  R_X86_64_64, insn->sec,
+						  insn_offsets[i])) {
+				WARN("Cannot add entry[%d] to section %s\n",
+				     idx, sec->name);
+				return -1;
+			}
+
+			printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			       idx, insn_offsets[i], insn->func->name,
+			       sec->name);
+
+			idx++;
+		}
+	}
+
+	return 0;
+}
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t)
+{
+	switch (t) {
+	case VAULT_SECTION_ENTRY_POINTS:
+		return create_entry_points(file);
+	case VAULT_SECTION_RETURN_SITES:
+		return create_return_sites(file);
+	case VAULT_SECTION_RETHUNK_SITES:
+		return create_rethunk_sites(file);
+	default:
+		WARN("Unknown vault section type");
+		return -1;
+	}
+
+	return 0;
+}
+
diff --git tools/objtool/vault.h tools/objtool/vault.h
new file mode 100644
index 000000000..d3a7d14a5
--- /dev/null
+++ tools/objtool/vault.h
@@ -0,0 +1,20 @@
+#ifndef _VAULT_H
+#define _VAULT_H
+
+#include <stdbool.h>
+#include "check.h"
+#include "elf.h"
+
+/* XXX: Change names! */
+#define VAULT_TEXT_SECTION 		".bhv.vault.text.jump_label"
+#define VAULT_REF_TEXT_SECTION 		".ref.text.bhv.vault.text.jump_label"
+
+enum vault_section_type {
+	VAULT_SECTION_ENTRY_POINTS,
+	VAULT_SECTION_RETURN_SITES,
+	VAULT_SECTION_RETHUNK_SITES,
+};
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t);
+
+#endif /* _VAULT_H */
