diff --git arch/Kconfig arch/Kconfig
index e959abf969e..a85122eda8a 100644
--- arch/Kconfig
+++ arch/Kconfig
@@ -52,6 +52,7 @@ config KPROBES
 	bool "Kprobes"
 	depends on MODULES
 	depends on HAVE_KPROBES
+	depends on !BHV_LOCKDOWN
 	select KALLSYMS
 	select TASKS_RCU if PREEMPTION
 	help
diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index 5bfbf7d79c9..d21e14e59a0 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,6 +3,7 @@ obj-y			+= kernel/ mm/ net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
 obj-$(subst m,y,$(CONFIG_HYPERV))	+= hyperv/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
 
 # for cleaning
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 00000000000..b812d00cf4c
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/arm64/bhv/init/init.c arch/arm64/bhv/init/init.c
new file mode 100644
index 00000000000..db713f9cf2b
--- /dev/null
+++ arch/arm64/bhv/init/init.c
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <bhv/integrity.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
+
+void __init bhv_init_arch(void)
+{
+}
diff --git arch/arm64/bhv/init/start.c arch/arm64/bhv/init/start.c
new file mode 100644
index 00000000000..3a4715d6cb0
--- /dev/null
+++ arch/arm64/bhv/init/start.c
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/integrity.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 00000000000..2c86d47e8dc
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,115 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+extern char vdso_start[], vdso_end[];
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif
+
+/************************************************************
+ * start
+ ************************************************************/
+int __init_km bhv_start_integrity_arch(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 3
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON(KLN_SYM(vdso_start) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_start) >= KLN_SYM(__end_rodata));
+	BUG_ON(KLN_SYM(vdso_end) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_end) >= KLN_SYM(__end_rodata));
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 */
+	bhv_mem_region_create_ctor(
+		&n[0]->region, NULL,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, __start_rodata)),
+		KLN_SYM(vdso_start) - KLN_SYM(__start_rodata),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[1]->region, &n[0]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_end)),
+		KLN_SYM(__end_rodata) - KLN_SYM(vdso_end),
+		HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[2]->region, &n[1]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_start)),
+		KLN_SYM(vdso_end) - KLN_SYM(vdso_start),
+		HypABI__Integrity__MemType__VDSO,
+		HypABI__Integrity__MemFlags__NONE, "KERNEL VDSO SECTION");
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+}
+/************************************************************/
+
+/************************************************************
+ * start
+ ************************************************************/
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	*pgd_offset = 0;
+	*pgd_value = 0;
+}
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	init_ptpg_arg->init_pgd = 0;
+	init_ptpg_arg->pt_levels = 0;
+	init_ptpg_arg->num_ranges = 0;
+}
+/************************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	return true;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 00000000000..6b70f8e42b4
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,455 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+#if defined(BHV_KVERS_6_1)
+#include <asm/cacheflush.h>
+#endif
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_patch_hypercall((void *)&updptr[i], (uint8_t *)&insn,
+				    sizeof(insn), false);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+#if defined(BHV_KVERS_6_1)
+#define ALT_CAP(a) ((a)->cpufeature & ~ARM64_CB_BIT)
+#define ALT_HAS_CB(a) ((a)->cpufeature & ARM64_CB_BIT)
+
+extern DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(
+			 ctr_el0, CTR_EL0_DminLine_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+		int cap = ALT_CAP(alt);
+
+		if (!test_bit(cap, feature_mask))
+			continue;
+
+		if (!cpus_have_cap(cap))
+			continue;
+
+		if (ALT_HAS_CB(alt))
+			BUG_ON(alt->alt_len != 0);
+		else
+			BUG_ON(alt->alt_len != alt->orig_len);
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (ALT_HAS_CB(alt)) {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		} else {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	/*
+	 * The core module code takes care of cache maintenance in
+	 * flush_module_icache().
+	 */
+	if (!mod->arch.is_module) {
+		dsb(ish);
+		icache_inval_all_pou();
+		isb();
+
+		/* Ignore ARM64_CB bit from feature mask */
+		bitmap_or(applied_alternatives, applied_alternatives,
+			  feature_mask, ARM64_NCAPS);
+		bitmap_and(applied_alternatives, applied_alternatives,
+			   cpu_hwcaps, ARM64_NCAPS);
+	}
+
+	return 0;
+}
+#else /* BHV_KVERS_6_1 */
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(ctr_el0,
+							   CTR_DMINLINE_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+
+		if (!test_bit(alt->cpufeature, feature_mask))
+			continue;
+
+		/* Use ARM64_CB_PATCH as an unconditional patch */
+		if (alt->cpufeature < ARM64_CB_PATCH &&
+		    !cpus_have_cap(alt->cpufeature))
+			continue;
+
+		if (alt->cpufeature == ARM64_CB_PATCH) {
+			if (alt->alt_len != 0) {
+				return -EACCES;
+			}
+		} else {
+			if (alt->alt_len != alt->orig_len) {
+				return -EACCES;
+			}
+		}
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (alt->cpufeature < ARM64_CB_PATCH) {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst);
+		} else {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	return 0;
+}
+#endif /* BHV_KVERS_6_1 */
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 00000000000..5c47935b515
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+u32 __always_inline bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/reg_protect.c arch/arm64/bhv/reg_protect.c
new file mode 100644
index 00000000000..49dd0470f09
--- /dev/null
+++ arch/arm64/bhv/reg_protect.c
@@ -0,0 +1,13 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+}
+/***************************************************/
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index a38b92e1181..8e9c87360e2 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -10,6 +10,8 @@
 #include <linux/types.h>
 #include <linux/stddef.h>
 
+#include <bhv/interface/abi_base_autogen.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
diff --git arch/arm64/include/asm/bhv/domain.h arch/arm64/include/asm/bhv/domain.h
new file mode 100644
index 00000000000..70b96ab43bb
--- /dev/null
+++ arch/arm64/include/asm/bhv/domain.h
@@ -0,0 +1,76 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) pgd
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_RDONLY);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pte_read(pmd_pte(pmd));
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pte_read(pud_pte(pud));
+}
+
+static inline bool pte_exec(pte_t pte)
+{
+	return !!(pte_val(pte) & (PTE_PXN | PTE_UXN));
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return pte_exec(pmd_pte(pmd));
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return pte_exec(pud_pte(pud));
+}
+
+static inline bool pmd_large(pmd_t pmd)
+{
+	return pmd_thp_or_huge(pmd);
+}
+
+static inline bool pud_large(pud_t pud)
+{
+	return pud_huge(pud);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pte_val(pte) & PTE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return bhv_domain_is_user_pte(pmd_pte(pmd));
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return bhv_domain_is_user_pte(pud_pte(pud));
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 00000000000..7c9d75eb2ff
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 00000000000..a4282166ef1
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+#ifndef VASKM // inside kernel tree
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+#endif // VASKM
+
+#else /* !CONFIG_BHV_VAS */
+#ifndef VASKM // inside kernel tree
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+#endif // VASKM
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 7784081088e..1e6c06211f8 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -117,6 +117,10 @@ alternative_cb_end
 #include <asm/mmu_context.h>
 #include <asm/kvm_host.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
 void kvm_compute_layout(void);
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 91263d09ea6..3703c446487 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -20,6 +20,10 @@
 #include <asm/vdso.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/patch.h>
+
 #define __ALT_PTR(a, f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -30,7 +34,7 @@
 /* Volatile, as we may be patching the guts of READ_ONCE() */
 static volatile int all_alternatives_applied;
 
-static DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
+DECLARE_BITMAP(applied_alternatives, ARM64_NCAPS);
 
 struct alt_region {
 	struct alt_instr *begin;
@@ -147,6 +151,14 @@ static void __apply_alternatives(const struct alt_region *region,
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 		int cap = ALT_CAP(alt);
@@ -217,6 +229,13 @@ void apply_alternatives_vdso(void)
 		.end	= (void *)hdr + alt->sh_offset + alt->sh_size,
 	};
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 false);
+	}
+#endif
+
 	__apply_alternatives(&region, false, &all_capabilities[0]);
 }
 
@@ -288,6 +307,10 @@ void apply_alternatives_module(void *start, size_t length)
 
 	bitmap_fill(all_capabilities, ARM64_NCAPS);
 
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
 #endif
@@ -295,7 +318,19 @@ void apply_alternatives_module(void *start, size_t length)
 noinstr void alt_cb_patch_nops(struct alt_instr *alt, __le32 *origptr,
 			       __le32 *updptr, int nr_inst)
 {
-	for (int i = 0; i < nr_inst; i++)
+	for (int i = 0; i < nr_inst; i++) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_patch_hypercall((void *)&updptr[i],
+					    (uint8_t *)&insn, sizeof(insn),
+					    false);
+		} else {
+			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 EXPORT_SYMBOL(alt_cb_patch_nops);
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index faf88ec9c48..09c366c4712 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -10,10 +10,14 @@
 #include <asm/insn.h>
 #include <asm/patching.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -24,5 +28,12 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 }
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index bfce41c2a53..fd06ee72f1b 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -32,6 +32,13 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -589,8 +596,20 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
@@ -598,8 +617,8 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
  * to call into firmware to adjust the mitigation state.
  */
 void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst)
 {
 	u32 insn;
 
@@ -616,7 +635,17 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1077,24 +1106,49 @@ void spectre_bhb_enable_mitigation(const struct arm64_cpu_capabilities *entry)
 
 /* Patched to NOP when enabled */
 void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
-						     __le32 *origptr,
-						      __le32 *updptr, int nr_inst)
+						      __le32 *origptr,
+						      __le32 *updptr,
+						      int nr_inst)
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_LOOP, &system_bhb_mitigations))
+	if (test_bit(BHB_LOOP, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to NOP when enabled */
 void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
-						   __le32 *origptr,
-						   __le32 *updptr, int nr_inst)
+						     __le32 *origptr,
+						     __le32 *updptr,
+						     int nr_inst)
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_FW, &system_bhb_mitigations))
+	if (test_bit(BHB_FW, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+					    sizeof(insn), false);
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to correct the immediate */
@@ -1115,12 +1169,22 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, loop_count, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to mov WA3 when supported */
-void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt, __le32 *origptr,
+				   __le32 *updptr, int nr_inst)
 {
 	u8 rd;
 	u32 insn;
@@ -1141,20 +1205,47 @@ void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 	if (WARN_ON_ONCE(insn == AARCH64_BREAK_FAULT))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to NOP when not supported */
-void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt,
-				   __le32 *origptr, __le32 *updptr, int nr_inst)
+void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt, __le32 *origptr,
+				       __le32 *updptr, int nr_inst)
 {
 	BUG_ON(nr_inst != 2);
 
 	if (test_bit(BHB_INSN, &system_bhb_mitigations))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+		updptr++;
+
+		insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_patch_hypercall((void *)updptr, (uint8_t *)&insn,
+				    sizeof(insn), false);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index fea3223704b..482eae69b2a 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -52,6 +52,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/init/init.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -326,6 +328,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT) && ((u64)_text % MIN_KIMG_ALIGN) != 0)
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index 45131e354e2..3fe0821a34c 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -169,6 +169,7 @@ SECTIONS
 			KPROBES_TEXT
 			HYPERVISOR_TEXT
 			IDMAP_TEXT
+			BHV_TEXT
 			*(.gnu.warning)
 		. = ALIGN(16);
 		*(.got)			/* Global offset table		*/
@@ -218,13 +219,27 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
 		__alt_instructions = .;
 		*(.altinstructions)
@@ -294,6 +309,17 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index 91b22a01461..7f72e1acd89 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -13,6 +13,11 @@
 #include <asm/kvm_mmu.h>
 #include <asm/memory.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -151,8 +156,21 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn, bhv_patch_arg_t *bhv_arg)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+	bhv_patch_hypercall((void *)dest, &le32_insn, sizeof(le32_insn), false,
+			    bhv_arg);
+}
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst,
+			       bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	int i;
 
@@ -170,7 +188,17 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (cpus_have_cap(ARM64_HAS_VIRT_HOST_EXTN) || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop(),
+						  bhv_arg);
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -181,12 +209,26 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn, bhv_arg);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
+#ifdef CONFIG_BHV_VAS
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst,
+			     bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
 void kvm_patch_vector_branch(struct alt_instr *alt,
 			     __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u64 addr;
 	u32 insn;
@@ -217,15 +259,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -233,12 +289,28 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst)
@@ -257,7 +329,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 16) & 0xffff), lsl #16 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -265,7 +345,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 16,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -273,7 +361,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 48) & 0xffff), lsl #48 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -281,7 +377,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 48,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 void kvm_get_kimage_voffset(struct alt_instr *alt,
diff --git arch/arm64/mm/fault.c arch/arm64/mm/fault.c
index 6b6b8a82f29..f94134ead4a 100644
--- arch/arm64/mm/fault.c
+++ arch/arm64/mm/fault.c
@@ -43,6 +43,10 @@
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 struct fault_info {
 	int	(*fn)(unsigned long far, unsigned long esr,
 		      struct pt_regs *regs);
@@ -382,12 +386,28 @@ static void __do_kernel_fault(unsigned long addr, unsigned long esr,
 	}
 
 	if (is_el1_permission_fault(addr, esr, regs)) {
+#ifdef CONFIG_BHV_VAS
+		uint8_t type;
+		if (esr & ESR_ELx_WNR) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+			msg = "write to read-only memory";
+		} else if (is_el1_instruction_abort(esr)) {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+			msg = "execute from non-executable memory";
+		} else {
+			type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+			msg = "read from unreadable memory";
+		}
+		if (bhv_guestlog_log_kaccess_events())
+			bhv_guestlog_log_kaccess((uint64_t)addr, type);
+#else /* CONFIG_BHV_VAS */
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
 		else if (is_el1_instruction_abort(esr))
 			msg = "execute from non-executable memory";
 		else
 			msg = "read from unreadable memory";
+#endif /* CONFIG_BHV_VAS */
 	} else if (addr < PAGE_SIZE) {
 		msg = "NULL pointer dereference";
 	} else {
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 4b4651ee47f..d91ac740131 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -46,6 +46,8 @@
 #include <asm/alternative.h>
 #include <asm/xen/swiotlb-xen.h>
 
+#include <bhv/init/start.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -483,6 +485,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+	bhv_start();
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
 			   POISON_FREE_INITMEM, "unused kernel");
diff --git arch/x86/Kbuild arch/x86/Kbuild
index 5a83da703e8..806d919796d 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -15,6 +15,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 00000000000..11409f4fb56
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,16 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
diff --git arch/x86/bhv/init/init.c arch/x86/bhv/init/init.c
new file mode 100644
index 00000000000..3cf5e88e1bc
--- /dev/null
+++ arch/x86/bhv/init/init.c
@@ -0,0 +1,147 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/types.h> // This is here so that hypervisor.h knows bool
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+
+#include <vdso/datapage.h>
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+#include <asm/vvar.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_64.data),
+				   vdso_image_64.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 64");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_x32.data),
+				   vdso_image_x32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE X32");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_32.data),
+				   vdso_image_32.size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL VDSO IMAGE 32");
+	(*region_counter)++;
+
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static __always_inline void
+bhv_init_add_vvar(bhv_mem_region_t *init_phys_mem_regions,
+		  unsigned int *region_counter)
+{
+	BUG_ON((*region_counter) == 0 ||
+	       (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   __pa_symbol(&__vvar_page), PAGE_SIZE,
+				   HypABI__Integrity__MemType__VVAR, HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL VVAR PAGE");
+	(*region_counter)++;
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vvar(init_phys_mem_regions, region_counter);
+}
+
+static uint32_t __init bhv_detect(void)
+{
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0; /* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return hypervisor_cpuid_base("BHV.VMM.VAS.", 0);
+
+	return 0;
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform
+};
+
+#else // out of tree
+
+#include <common.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	// We handle VDSOs in integrity.c, so no need to do anything here.
+}
+
+#endif // VASKM
+
+void __init bhv_init_arch(void)
+{
+#ifndef VASKM // inside kernel tree
+	setup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);
+#endif // VASKM
+}
diff --git arch/x86/bhv/init/start.c arch/x86/bhv/init/start.c
new file mode 100644
index 00000000000..dfa7a8a6543
--- /dev/null
+++ arch/x86/bhv/init/start.c
@@ -0,0 +1,12 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <bhv/integrity.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 00000000000..703bcae22c4
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,587 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/pgtable.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_EFI
+#include <linux/efi.h>
+#endif /* CONFIG_EFI */
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <linux/pagewalk.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif //VASKM
+
+struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} typedef table_data_t;
+
+static table_data_t table_data __ro_after_init = { 0 };
+
+struct {
+	uint64_t start_addr;
+	uint64_t size;
+	const char *label;
+	uint32_t mem_type;
+} typedef ro_region_t;
+
+/**********************************************************
+ * start
+ **********************************************************/
+static int __init_km bhv_start_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = bhv_virt_to_phys((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(
+		head, addr, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+		HypABI__Integrity__MemFlags__NONE, "IDT");
+}
+
+static int __init_km bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_start_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region.create);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#ifndef VASKM // inside kernel tree
+
+static inline int __init_km bhv_start_rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km bhv_start_rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int __init_km bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = bhv_start_rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = bhv_start_rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#endif // VASKM
+
+static void __init_km bhv_start_integrity_add_vdso_common(
+	uint64_t start_addr, uint64_t size, const char *label,
+	ro_region_t *range, unsigned int *cur_entry, size_t range_sz)
+{
+	unsigned int i;
+	uint64_t end = start_addr + size;
+	uint64_t cur_end = 0;
+
+	BUG_ON(size == 0);
+	BUG_ON((*cur_entry) >= range_sz);
+	BUG_ON(start_addr < range[0].start_addr);
+
+	// Check for overlaps
+	for (i = 0; i < (*cur_entry); i++) {
+		cur_end = range[i].start_addr + range[i].size;
+
+		// No overlap. Nothing to do.
+		if (end <= range[i].start_addr || start_addr >= cur_end)
+			continue;
+
+		// No range that we add should be exactly the same as an
+		// existing one.
+		if (start_addr == range[i].start_addr && end == cur_end) {
+			BUG(); // Range already exists
+		}
+
+		// Overlap. Split range.
+		// Case 1 (overlap left): New range starts before current range
+		//                        with/before the current range.
+		//                        Update the new range to end when cur starts.
+		//                        Thus creating range A and B.
+		//  -----------------------
+		// |      A      ##########B##########
+		// |     new     #         |   cur   #
+		// |             ##########|##########
+		// ------------------------
+		if (start_addr < range[i].start_addr && end <= cur_end) {
+			size = range[i].start_addr - start_addr;
+			continue;
+		}
+
+		// Case 2 (overlap right): New range starts within current range
+		//                         and ends with/after the current range.
+		//                         Update cur to end when new starts.
+		//                         Thus creating range A and B.
+		//                         -----------------------
+		//              #####A####|##########  B          |
+		//              #   cur   |         #     new     |
+		//              ##########|##########             |
+		//                        ------------------------
+		if (range[i].start_addr < start_addr && cur_end <= end) {
+			range[i].size = start_addr - range[i].start_addr;
+			continue;
+		}
+
+		// Case 3: New range fully encompasses current range.
+		//         Create three ranges A, B, C.
+		//  --------------------------------------------
+		// |     A     ##########B##########     C      |
+		// |    new    #        cur        #    new     |
+		// |           #####################            |
+		// ---------------------------------------------
+		if (start_addr <= range[i].start_addr && cur_end <= end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				// No. Add new Range A.
+				range[(*cur_entry)].start_addr = start_addr;
+				range[(*cur_entry)].size =
+					range[i].start_addr - start_addr;
+				range[(*cur_entry)].label = label;
+				range[(*cur_entry)].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Update new to be C. It will be added after the loop.
+				// B is already in our range list.
+				start_addr = cur_end;
+				size = end - cur_end;
+			} else {
+				// Remaining ranges are the same. Just update B with new label/type.
+				// Since a range can only have one label, it will take the
+				// label of the new range. This generally seems to make sense
+				// since we add the entire read-only section and then split it
+				// with smaller sections.
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+			continue;
+		}
+
+		// Case 4: Current range fully encompasses new range.
+		//         Create three ranges A, B, C.
+		// ##############################################
+		// #     A      ---------B---------      C      #
+		// #    cur    |        new        |    cur     #
+		// #            -------------------             #
+		// ##############################################
+		if (range[i].start_addr <= start_addr && end <= cur_end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				range[(*cur_entry)].start_addr =
+					range[i].start_addr;
+				range[(*cur_entry)].size =
+					start_addr - range[i].start_addr;
+				range[(*cur_entry)].label = range[i].label;
+				range[(*cur_entry)].mem_type =
+					range[i].mem_type;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Create C. B will be added after the loop.
+				range[i].start_addr = end;
+				range[i].size = cur_end - end;
+			} else {
+				// Remaining ranges are the same.
+				// Shrink the current range to B. We also update its label/type.
+				// This generally seems to make sense since we add the
+				// entire read-only section and then split it with smaller
+				// sections.
+				range[i].start_addr = start_addr;
+				range[i].size = size;
+				range[i].label = label;
+				range[i].mem_type =
+					HypABI__Integrity__MemType__VDSO;
+				return;
+			}
+
+			continue;
+		}
+
+		BUG(); // "Unexpected case"
+	}
+
+	if (size != 0) {
+		range[(*cur_entry)].start_addr = start_addr;
+		range[(*cur_entry)].size = size;
+		range[(*cur_entry)].label = label;
+		range[(*cur_entry)].mem_type = HypABI__Integrity__MemType__VDSO;
+		(*cur_entry)++;
+	}
+}
+
+#define VDSO_KLN_SYM(sym) KLN_SYMBOL_P(const struct vdso_image *, sym)
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_64_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_64)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_64)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 64",
+					    range, cur_entry, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_x32_to_range(ro_region_t *range,
+						unsigned int *cur_entry,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_x32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_x32)->size;
+	bhv_start_integrity_add_vdso_common(start, size,
+					    "KERNEL VDSO IMAGE X32", range,
+					    cur_entry, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_32_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_32)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 32",
+					    range, cur_entry, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void __init_km bhv_start_integrity_get_ro_ranges(ro_region_t *range,
+							unsigned int *cur_entry,
+							size_t range_sz)
+{
+	BUG_ON(range_sz < 1);
+
+	range[(*cur_entry)].start_addr =
+		bhv_virt_to_phys((void *)(KLN_SYM(__start_rodata) & PAGE_MASK));
+	range[(*cur_entry)].size = PAGE_ALIGN(KLN_SYM(__end_rodata)) -
+				   (KLN_SYM(__start_rodata) & PAGE_MASK);
+	range[(*cur_entry)].label = "KERNEL READ-ONLY DATA SECTION";
+	range[(*cur_entry)].mem_type =
+		HypABI__Integrity__MemType__DATA_READ_ONLY;
+	(*cur_entry)++;
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, cur_entry,
+						       range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, cur_entry,
+							range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, cur_entry,
+						       range_sz);
+}
+
+static int __init_km bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+	unsigned int i;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	unsigned int nr_ro_ranges = 0;
+	ro_region_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_MAX_RO_RANGES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, &nr_ro_ranges,
+					  BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   nr_ro_ranges, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < nr_ro_ranges; i++) {
+		bhv_mem_region_create_ctor(
+			&n[i]->region, (prev ? &prev->region : NULL),
+			ro_ranges[i].start_addr, ro_ranges[i].size,
+			ro_ranges[i].mem_type,
+			HypABI__Integrity__MemFlags__NONE, ro_ranges[i].label);
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region.create));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, nr_ro_ranges, (void **)&n);
+
+	return rc;
+}
+
+#ifdef CONFIG_EFI
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		bhv_mem_region_create_ctor(&n->region, NULL, md->phys_addr,
+					   (md->num_pages) << PAGE_SHIFT,
+					   HypABI__Integrity__MemType__CODE,
+					   HypABI__Integrity__MemFlags__NONE,
+					   "EFI REGION");
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1,
+							 &(n->region.create));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+#else /* CONFIG_EFI */
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	return 0;
+}
+#endif /* CONFIG_EFI */
+
+int __init_km bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+#ifndef VASKM // inside kernel tree
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+#endif // VASKM
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+	return bhv_start_integrity_add_ro();
+}
+/**********************************************************/
+
+#define BHV_DIR_PTR_ENTRY_MASK 0x800FFFFFFFFFF89D
+
+/**********************************************************
+ * start
+ **********************************************************/
+struct bhv_pw_data {
+	bool set;
+	uint64_t pgd_offset;
+	uint64_t pgd_value;
+};
+
+static int __init_km bhv_start_pgd_entry(pgd_t *pgd, unsigned long addr,
+					 unsigned long next,
+					 struct mm_walk *walk)
+{
+	struct bhv_pw_data *data = (struct bhv_pw_data *)walk->private;
+	pr_info("%s: pgd=%llx (%llx) addr=%lx next=%lx\n", __FUNCTION__,
+		(uint64_t)pgd, bhv_virt_to_phys(pgd), addr, next);
+	BUG_ON(data->set);
+	data->pgd_offset = ((uint64_t)pgd & 0xFFF);
+	data->pgd_value = *((uint64_t *)pgd);
+	data->set = true;
+	return 1;
+}
+
+static const struct mm_walk_ops bhv_start_walk_ops = {
+	.pgd_entry = bhv_start_pgd_entry,
+	.p4d_entry = NULL,
+	.pud_entry = NULL,
+	.pmd_entry = NULL,
+	.pte_entry = NULL,
+	.pte_hole = NULL,
+	.hugetlb_entry = NULL,
+	.test_walk = NULL,
+	.pre_vma = NULL,
+	.post_vma = NULL
+};
+
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value)
+{
+	struct bhv_pw_data text_pw_data = { 0 };
+	struct bhv_pw_data ro_pw_data = { 0 };
+
+	mmap_write_lock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(_stext), KLN_SYM(_etext),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &text_pw_data);
+	walk_page_range_novma(KLN_SYMBOL_P(struct mm_struct *, init_mm),
+			      KLN_SYM(__start_rodata), KLN_SYM(__end_rodata),
+			      &bhv_start_walk_ops,
+			      KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd,
+			      &ro_pw_data);
+	mmap_write_unlock(KLN_SYMBOL_P(struct mm_struct *, init_mm));
+
+	BUG_ON(text_pw_data.pgd_offset != ro_pw_data.pgd_offset ||
+	       text_pw_data.pgd_value != ro_pw_data.pgd_value);
+
+	*pgd_offset = text_pw_data.pgd_offset;
+	*pgd_value = (text_pw_data.pgd_value & BHV_DIR_PTR_ENTRY_MASK);
+}
+/**********************************************************/
+
+/**********************************************************
+ * late_start
+ **********************************************************/
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg)
+{
+	BUG_ON(CONFIG_PGTABLE_LEVELS < 4 || CONFIG_PGTABLE_LEVELS > 5);
+
+	init_ptpg_arg->init_pgd = (uint64_t)bhv_virt_to_phys(
+		KLN_SYMBOL_P(struct mm_struct *, init_mm)->pgd);
+	init_ptpg_arg->pt_levels = pgtable_l5_enabled() ? 5 : 4;
+	init_ptpg_arg->num_ranges = 2;
+
+	init_ptpg_arg->ranges[0] = (uint64_t)KLN_SYM(_stext);
+	init_ptpg_arg->ranges[1] = (uint64_t)KLN_SYM(_etext);
+	init_ptpg_arg->ranges[2] = (uint64_t)KLN_SYM(__start_rodata);
+	init_ptpg_arg->ranges[3] = (uint64_t)KLN_SYM(__end_rodata);
+}
+/**********************************************************/
+
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value)
+{
+	uint64_t value = *((uint64_t *)(((uint8_t *)mm->pgd) + pgd_offset));
+	return ((value & BHV_DIR_PTR_ENTRY_MASK) == pgd_value);
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 00000000000..d5a23494ded
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,846 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/kversion.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+#define NOPS x86_nops
+#elif defined BHV_KVERS_5_10
+#define NOPS ideal_nops
+#endif // BHV_KVERS
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end,
+				      const s32 *locks_begin,
+				      const s32 *locks_end, u8 *text_begin,
+				      u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len, bool patch)
+{
+	size_t total_length = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) > sizeof(buf))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(buf + total_length, NOPS[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, NOPS[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_patch_hypercall(insns, buf, total_length, false);
+	}
+}
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off, bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(u8 *target,
+								   bool lock)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target,
+				    "Invalid altinst smp unlock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch.
+		}
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode) {
+			if (bhv_patch_violation_hypercall(
+				    target, "Invalid altinst smp lock patch")) {
+				// Block attempt.
+				return -EACCES;
+			}
+
+			// Allow patch
+		}
+
+		opcode = unlock_opcode;
+	}
+
+	r = bhv_patch_hypercall((void *)target, &opcode, 1, false);
+
+	if (r) {
+		panic("BHV patch hypercall failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+		    ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+		bytes[i++] = BYTES_NOP1;
+#elif defined BHV_KVERS_5_10
+		bytes[i++] = GENERIC_NOP1;
+#endif // BHV_KVERS
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (invalid op)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+
+	// Retpolines may be disabled or there is another error
+	// this is not an attack.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst retpoline (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr,
+			    "Invalid altinst retpoline (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_optimize_nops(bytes, len, true);
+	bhv_patch_hypercall((void *)addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+#ifdef CONFIG_RETHUNK
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		return;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+	    WARN_ONCE(dest != &__x86_return_thunk,
+		      "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+		      addr))
+		return;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	// Feature may be disabled.
+	if (len < 0)
+		goto out;
+
+	if (len != insn.length) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (wrong length)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (bhv_patch_violation_hypercall(
+			    addr, "Invalid altinst return (patch too big)")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+	bhv_patch_hypercall(addr, bytes, len, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE)*/
+
+#ifdef CONFIG_PARAVIRT
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+
+#if defined(BHV_KVERS_5_10)
+	used = pv_ops.init.patch(p->type, insn_buff, (unsigned long)p->instr,
+				 p->len);
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+#endif // BHV_KVERS
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, false);
+
+	bhv_patch_hypercall((void *)(p->instr), insn_buff, p->len, false);
+
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+#endif /* CONFIG_PARAVIRT */
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_PARAVIRT) */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		if (bhv_patch_violation_hypercall(
+			    instr, "Invalid altinst patch (too big)")) {
+			// Block attempt.
+			return -EACCES;
+		}
+
+		// Allow patch.
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		// This can happen legitmately. We just return.
+		return -EINVAL;
+	}
+
+	if (!boot_cpu_has(a->cpuid & ~ALTINSTR_FLAG_INV) ==
+	    !(a->cpuid & ALTINSTR_FLAG_INV)) {
+		bhv_optimize_nops(instr, a->instrlen, true);
+		return 0;
+	}
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+#if defined(BHV_KVERS_5_10)
+	if (a->instrlen > a->replacementlen) {
+		bhv_add_nops(insn_buff + a->replacementlen,
+			     a->instrlen - a->replacementlen, false);
+		insn_buff_sz += a->instrlen - a->replacementlen;
+	}
+
+#elif defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+#endif // BHV_KVERS
+
+	if (insn_buff_sz >= HypABI__Patch__MAX_PATCH_SZ)
+		panic("Instruction buffer size too small!");
+
+	rv = bhv_patch_hypercall((void *)instr, insn_buff, insn_buff_sz, false);
+
+#if defined(BHV_KVERS_5_15) || defined(BHV_KVERS_6_1)
+	bhv_optimize_nops(instr, a->instrlen, true);
+#endif // BHV_KVERS_5_15
+
+	return rv;
+}
+
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a);
+		else
+			bhv_alternatives_patch_vault(a);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+	    cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) && \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __bhv_text bhv_apply_ibt_endbr_vault(s32 *s)
+{
+	int ret;
+	u32 endbr, poison;
+	void *addr;
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	poison = gen_endbr_poison();
+	addr = (void *)s + *s;
+
+	if (get_kernel_nofault(endbr, addr))
+		goto out;
+
+	if (!is_endbr(endbr))
+		goto out;
+
+	bhv_patch_hypercall(addr, (uint8_t *)&poison, 4, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __init_or_module bhv_apply_ibt_endbr(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_ibt_endbr_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 00000000000..2e5e3aec0a1
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,100 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+#include <bhv/vault.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif // VASKM
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len == 5) {
+		DEF_CHECK_NOP(STATIC_KEY_INIT_NOP);
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *,
+				     ideal_nops)[NOP_ATOMIC5]);
+	}
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len == 2) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[2]);
+	} else if (len == 5) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[5]);
+	}
+#endif // LINUX_VERSION_CODE
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len != 5)
+		return false;
+
+	code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+
+
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+#endif // LINUX_VERSION_CODE
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
+#endif
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 00000000000..be74be068f6
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <linux/version.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static void __always_inline bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static void __always_inline bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+extern void __static_call_return(void);
+
+static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
+{
+	u8 ret = 0;
+
+	if (insn[0] == 0x0f) {
+		u8 tmp = insn[1];
+		if ((tmp & 0xf0) == 0x80)
+			ret = tmp;
+	}
+
+	return ret;
+}
+#endif // LINUX_VERSION_CODE >= 6.1
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func)
+{
+	int ret;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	const void *emulate = NULL;
+#endif // LINUX_VERSION_CODE >= 5.12
+	int size = CALL_INSN_SIZE;
+	const void *code;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	u8 buf[6];
+#endif // LINUX_VERSION_CODE >= 6.1
+
+	ret = HypABI__Richard__Open__hypercall();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+#endif // LINUX_VERSION_CODE >= 5.12
+
+		break;
+
+	case NOP:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 13, 0)
+		code = KLN_SYMBOL(const uint8_t *const *, x86_nops)[5];
+#else // LINUX_VERSION_CODE <= 5.13
+		code = KLN_SYMBOL(const uint8_t *const *,
+				  ideal_nops)[NOP_ATOMIC5];
+#endif // LINUX_VERSION_CODE
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+#else // LINUX_VERSION_CODE <= 5.14
+		code = text_gen_insn(RET_INSN_OPCODE, insn, func);
+		size = RET_INSN_SIZE;
+#endif // LINUX_VERSION_CODE
+		break;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	case JCC:
+		if (!func) {
+			func = KLN_SYMBOL(void *, __static_call_return);
+			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+				func = __x86_return_thunk;
+		}
+
+		buf[0] = 0x0f;
+		__text_gen_insn(buf + 1, __is_Jcc(insn), insn + 1, func, 5);
+		code = buf;
+		size = 6;
+
+		break;
+#endif // LINUX_VERSION_CODE >= 6.1
+	}
+
+	if (memcmp(insn, code, size) == 0) {
+		if (bhv_patch_violation_hypercall(insn,
+						  "Invalid static call")) {
+			// Block attempt.
+			goto out;
+		}
+
+		// Allow patch
+	}
+
+	if (size > HypABI__Patch__MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_patch_hypercall((void *)insn, code, size, false);
+
+out:
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+}
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func)
+{
+	unsigned long flags;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git arch/x86/bhv/reg_protect.c arch/x86/bhv/reg_protect.c
new file mode 100644
index 00000000000..76b062fef63
--- /dev/null
+++ arch/x86/bhv/reg_protect.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#include <asm/processor-flags.h>
+
+#define BHV_CR0_FREEZE X86_CR0_WP
+#define BHV_CR4_FREEZE 0x0ULL
+#define BHV_EFER_FREEZE 0x0ULL
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect_arch(void)
+{
+	int r;
+
+	if (BHV_CR0_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR0,
+			BHV_CR0_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR0!\n");
+	}
+
+	if (BHV_CR4_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__CR4,
+			BHV_CR4_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze CR4!\n");
+	}
+
+	if (BHV_EFER_FREEZE) {
+		r = bhv_reg_protect_freeze(
+			HypABI__RegisterProtection__Freeze__RegisterSelector__EFER,
+			BHV_EFER_FREEZE);
+		if (r)
+			pr_err("BHV: Unable to freeze EFER!\n");
+	}
+}
+/***************************************************/
diff --git arch/x86/entry/common.c arch/x86/entry/common.c
index e72dac09224..5ae3d2e9adb 100644
--- arch/x86/entry/common.c
+++ arch/x86/entry/common.c
@@ -36,6 +36,8 @@
 #include <asm/syscall.h>
 #include <asm/irq_stack.h>
 
+#include <bhv/integrity.h>
+
 #ifdef CONFIG_X86_64
 
 static __always_inline bool do_syscall_x64(struct pt_regs *regs, int nr)
@@ -76,6 +78,8 @@ __visible noinstr void do_syscall_64(struct pt_regs *regs, int nr)
 	add_random_kstack_offset();
 	nr = syscall_enter_from_user_mode(regs, nr);
 
+	bhv_pt_protect_check_pgd(current->active_mm);
+
 	instrumentation_begin();
 
 	if (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {
diff --git arch/x86/entry/entry_64.S arch/x86/entry/entry_64.S
index 6624806e690..9e409f07f94 100644
--- arch/x86/entry/entry_64.S
+++ arch/x86/entry/entry_64.S
@@ -248,6 +248,11 @@ SYM_FUNC_START(__switch_to_asm)
 	pushq	%r14
 	pushq	%r15
 
+#ifdef CONFIG_MEM_NS
+	movq	PER_CPU_VAR(bhv_domain_current_domain), %r12
+	pushq	%r12
+#endif
+
 	/* switch stack */
 	movq	%rsp, TASK_threadsp(%rdi)
 	movq	TASK_threadsp(%rsi), %rsp
@@ -266,6 +271,17 @@ SYM_FUNC_START(__switch_to_asm)
 	 */
 	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
 
+#ifdef CONFIG_MEM_NS
+	// Save arguments in r12 and r13
+	movq	%rdi, %r12
+	movq	%rsi, %r13
+	popq	%rdi
+	callq	bhv_domain_switch
+	// Restore original arguments
+	movq	%r12, %rdi
+	movq	%r13, %rsi
+#endif
+
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
diff --git arch/x86/include/asm/bhv/domain.h arch/x86/include/asm/bhv/domain.h
new file mode 100644
index 00000000000..bbfece7fad2
--- /dev/null
+++ arch/x86/include/asm/bhv/domain.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) kernel_to_user_pgdp(pgd)
+#endif
+
+static inline bool pte_read(pte_t pte)
+{
+	return pte_present(pte);
+}
+
+static inline bool pmd_read(pmd_t pmd)
+{
+	return pmd_present(pmd);
+}
+
+static inline bool pud_read(pud_t pud)
+{
+	return pud_present(pud);
+}
+
+static inline bool pmd_exec(pmd_t pmd)
+{
+	return !(pgprot_val(pmd_pgprot(pmd)) & _PAGE_NX);
+}
+
+static inline bool pud_exec(pud_t pud)
+{
+	return !(pgprot_val(pud_pgprot(pud)) & _PAGE_NX);
+}
+
+static inline bool bhv_domain_is_user_pte(pte_t pte)
+{
+	return !!(pgprot_val(pte_pgprot(pte)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pmd(pmd_t pmd)
+{
+	return !!(pgprot_val(pmd_pgprot(pmd)) & _PAGE_USER);
+}
+
+static inline bool bhv_domain_is_user_pud(pud_t pud)
+{
+	return !!(pgprot_val(pud_pgprot(pud)) & _PAGE_USER);
+}
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 00000000000..99a1e495563
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 00000000000..786be1e4599
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+#endif // VASKM
+#else /* CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 00000000000..f2d2bd51f7e
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#include <bhv/vault.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text
+bhv_alternatives_apply_vault_arch(struct bhv_alternatives_mod *mod, void *arch);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL is used in 6.1
+#if defined(CONFIG_RETPOLINE) &&                                               \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __bhv_text bhv_apply_retpolines_vault(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __bhv_text bhv_apply_returns_vault(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#ifdef CONFIG_PARAVIRT
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+#endif // VASKM
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0)
+	JCC = 4,
+#endif // LINUX_VERSION_CODE >= 6.1
+};
+
+void __bhv_static_call_transform(void *insn, enum insn_type type, void *func);
+
+#if defined BHV_KVERS_5_15 || defined(BHV_KVERS_6_1)
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func, bool modinit)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+
+#elif defined BHV_KVERS_5_10
+static inline void bhv_static_call_transform(void *insn, enum insn_type type,
+					     void *func)
+{
+	return __bhv_static_call_transform(insn, type, func);
+}
+#endif // BHV_KVERS
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec41..591e48b0509 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/include/asm/pgtable.h arch/x86/include/asm/pgtable.h
index 286a71810f9..40d51550ee8 100644
--- arch/x86/include/asm/pgtable.h
+++ arch/x86/include/asm/pgtable.h
@@ -24,6 +24,8 @@
 #include <asm-generic/pgtable_uffd.h>
 #include <linux/page_table_check.h>
 
+#include <bhv/domain_pt.h>
+
 extern pgd_t early_top_pgt[PTRS_PER_PGD];
 bool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
@@ -1007,6 +1009,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
 	page_table_check_pte_set(mm, addr, ptep, pte);
+	bhv_domain_set_pte_at(mm, addr, ptep, pte);
 	set_pte(ptep, pte);
 }
 
@@ -1014,6 +1017,7 @@ static inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,
 			      pmd_t *pmdp, pmd_t pmd)
 {
 	page_table_check_pmd_set(mm, addr, pmdp, pmd);
+	bhv_domain_set_pmd_at(mm, addr, pmdp, pmd);
 	set_pmd(pmdp, pmd);
 }
 
@@ -1021,6 +1025,7 @@ static inline void set_pud_at(struct mm_struct *mm, unsigned long addr,
 			      pud_t *pudp, pud_t pud)
 {
 	page_table_check_pud_set(mm, addr, pudp, pud);
+	bhv_domain_set_pud_at(mm, addr, pudp, pud);
 	native_set_pud(pudp, pud);
 }
 
@@ -1050,7 +1055,9 @@ extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
 {
-	pte_t pte = native_ptep_get_and_clear(ptep);
+	pte_t pte;
+	bhv_domain_clear_pte(mm, addr, ptep, *ptep);
+	pte = native_ptep_get_and_clear(ptep);
 	page_table_check_pte_clear(mm, addr, pte);
 	return pte;
 }
@@ -1066,6 +1073,7 @@ static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
 		 * Full address destruction in progress; paravirt does not
 		 * care about updates and native needs no locking
 		 */
+		bhv_domain_clear_pte(mm, addr, ptep, pte);
 		pte = native_local_ptep_get_and_clear(ptep);
 		page_table_check_pte_clear(mm, addr, pte);
 	} else {
@@ -1079,6 +1087,7 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pte_t *ptep)
 {
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)&ptep->pte);
+	bhv_domain_set_pte_at(mm, addr, ptep, *ptep);
 }
 
 #define flush_tlb_fix_spurious_fault(vma, address) do { } while (0)
@@ -1114,7 +1123,9 @@ static inline int pmd_write(pmd_t pmd)
 static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pmd_t *pmdp)
 {
-	pmd_t pmd = native_pmdp_get_and_clear(pmdp);
+	pmd_t pmd;
+	bhv_domain_clear_pmd(mm, addr, pmdp, *pmdp);
+	pmd = native_pmdp_get_and_clear(pmdp);
 
 	page_table_check_pmd_clear(mm, addr, pmd);
 
@@ -1125,7 +1136,11 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long
 static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 					unsigned long addr, pud_t *pudp)
 {
-	pud_t pud = native_pudp_get_and_clear(pudp);
+	pud_t pud;
+
+	bhv_domain_clear_pud(mm, addr, pudp, *pudp);
+
+	pud = native_pudp_get_and_clear(pudp);
 
 	page_table_check_pud_clear(mm, addr, pud);
 
@@ -1136,6 +1151,7 @@ static inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,
 static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 				      unsigned long addr, pmd_t *pmdp)
 {
+	bhv_domain_set_pmd_at(mm, addr, pmdp, *pmdp);
 	clear_bit(_PAGE_BIT_RW, (unsigned long *)pmdp);
 }
 
diff --git arch/x86/include/asm/switch_to.h arch/x86/include/asm/switch_to.h
index c08eb0fdd11..4dc97bd5e18 100644
--- arch/x86/include/asm/switch_to.h
+++ arch/x86/include/asm/switch_to.h
@@ -19,6 +19,9 @@ asmlinkage void ret_from_fork(void);
  * order of the fields must match the code in __switch_to_asm().
  */
 struct inactive_task_frame {
+#ifdef CONFIG_MEM_NS
+	unsigned long domain;
+#endif
 #ifdef CONFIG_X86_64
 	unsigned long r15;
 	unsigned long r14;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 69f85e27461..dd460e914dc 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -31,12 +31,32 @@
 #include <asm/paravirt.h>
 #include <asm/asm-prototypes.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/domain.h>
+
+#include <bhv/vault.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
 
 #define MAX_PATCH_LEN (255-1)
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, sync_core);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled, X86_FEATURE_RETHUNK);
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled, X86_FEATURE_RETPOLINE);
+BHV_VAULT_FN_WRAPPER1_MACRO(bool, cpu_feature_enabled,
+			    X86_FEATURE_RETPOLINE_LFENCE);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
 static int __initdata_or_module debug_alternative;
 
 static int __init debug_alt(char *str)
@@ -101,7 +121,18 @@ const unsigned char * const x86_nops[ASM_NOP_MAX+1] =
 	x86nops + 1 + 2 + 3 + 4 + 5 + 6 + 7,
 };
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_alternatives(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_alternatives);
+#endif
+
 /* Use this to add nops to a buffer, then text_poke the whole buffer. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __init_or_module add_nops(void *insns, unsigned int len)
 {
 	while (len > 0) {
@@ -124,13 +155,15 @@ void text_poke_early(void *addr, const void *opcode, size_t len);
 /*
  * Are we looking at a near JMP with a 1 or 4-byte displacement.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool is_jmp(const u8 opcode)
 {
 	return opcode == 0xeb || opcode == 0xe9;
 }
 
-static void __init_or_module
-recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insn_buff)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __init_or_module recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					    u8 *repl_insn, u8 *insn_buff)
 {
 	u8 *next_rip, *tgt_rip;
 	s32 n_dspl, o_dspl;
@@ -195,6 +228,7 @@ recompute_jump(struct alt_instr *a, u8 *orig_insn, u8 *repl_insn, u8 *insn_buff)
  *
  * Return: number of NOPs found (and replaced).
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline int optimize_nops_range(u8 *instr, u8 instrlen, int off)
 {
 	unsigned long flags;
@@ -225,7 +259,8 @@ static __always_inline int optimize_nops_range(u8 *instr, u8 instrlen, int off)
  * "noinline" to cause control flow change and thus invalidate I$ and
  * cause refetch after modification.
  */
-static void __init_or_module noinline optimize_nops(u8 *instr, size_t len)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __bhv_init_or_module noinline optimize_nops(u8 *instr, size_t len)
 {
 	struct insn insn;
 	int i = 0;
@@ -262,13 +297,23 @@ static void __init_or_module noinline optimize_nops(u8 *instr, size_t len)
  * Marked "noinline" to cause control flow change and thus insn cache
  * to refetch changed I$ lines.
  */
-void __init_or_module noinline apply_alternatives(struct alt_instr *start,
-						  struct alt_instr *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_alternatives(struct alt_instr *start,
+						      struct alt_instr *end)
 {
 	struct alt_instr *a;
 	u8 *instr, *replacement;
 	u8 insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif
+#endif
+
 	DPRINTK("alt table %px, -> %px", start, end);
 
 	/*
@@ -298,16 +343,26 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 		instr = (u8 *)&a->instr_offset + a->instr_offset;
 		replacement = (u8 *)&a->repl_offset + a->repl_offset;
 		BUG_ON(a->instrlen > sizeof(insn_buff));
+#ifdef CONFIG_BHV_VAULT_SPACES
+		/* XXX: Why is this failing in the VAS kernel? */
+		if (feature >= (NCAPINTS + NBUGINTS) * 32)
+			continue;
+#else
 		BUG_ON(feature >= (NCAPINTS + NBUGINTS) * 32);
-
+#endif
 		/*
 		 * Patch if either:
 		 * - feature is present
 		 * - feature not present but ALTINSTR_FLAG_INV is set to mean,
 		 *   patch if feature is *NOT* present.
 		 */
-		if (!boot_cpu_has(feature) == !(a->cpuid & ALTINSTR_FLAG_INV))
+		if (!boot_cpu_has(feature) == !(a->cpuid & ALTINSTR_FLAG_INV)) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			memcpy(insn_buff, instr, a->instrlen);
+			insn_buff_sz = a->instrlen;
+#endif
 			goto next;
+		}
 
 		DPRINTK("feat: %s%d*32+%d, old: (%pS (%px) len: %d), repl: (%px, len: %d)",
 			(a->cpuid & ALTINSTR_FLAG_INV) ? "!" : "",
@@ -343,15 +398,26 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 
 		DUMP_BYTES(insn_buff, insn_buff_sz, "%px: final_insn: ", instr);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+next:
+		optimize_nops(insn_buff, insn_buff_sz);
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(instr, insn_buff, insn_buff_sz);
+		else
+			text_poke_early(instr, insn_buff, insn_buff_sz);
+#else
 		text_poke_early(instr, insn_buff, insn_buff_sz);
 
 next:
 		optimize_nops(instr, a->instrlen);
+#endif
 	}
 
 	kasan_enable_current();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_alternatives);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool is_jcc32(struct insn *insn)
 {
 	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
@@ -363,6 +429,7 @@ static inline bool is_jcc32(struct insn *insn)
 /*
  * CALL/JMP *%\reg
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int emit_indirect(int op, int reg, u8 *bytes)
 {
 	int i = 0;
@@ -412,6 +479,7 @@ static int emit_indirect(int op, int reg, u8 *bytes)
  *
  * It also tries to inline spectre_v2=retpoline,lfence when size permits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 {
 	retpoline_thunk_t *target;
@@ -427,8 +495,13 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE() &&
+	    !bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE())
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+#endif
 		return -1;
 
 	op = insn->opcode.bytes[0];
@@ -460,7 +533,11 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/*
 	 * For RETPOLINE_LFENCE: prepend the indirect CALL/JMP with an LFENCE.
 	 */
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETPOLINE_LFENCE()) {
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+#endif
 		bytes[i++] = 0x0f;
 		bytes[i++] = 0xae;
 		bytes[i++] = 0xe8; /* LFENCE */
@@ -489,10 +566,23 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 /*
  * Generated by 'objtool --retpoline'.
  */
-void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
+
+/* XXX: Consider moving module_finalze in module.c into the vault. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_retpolines(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -530,10 +620,18 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 			optimize_nops(bytes, len);
 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_retpolines);
 
 #ifdef CONFIG_RETHUNK
 
@@ -548,11 +646,16 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
  *
  *   RET
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 {
 	int i = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (bhv_wrapper_cpu_feature_enabled_X86_FEATURE_RETHUNK()) {
+#else
 	if (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {
+#endif
 		if (x86_return_thunk == __x86_return_thunk)
 			return -1;
 
@@ -567,10 +670,21 @@ static int patch_return(void *addr, struct insn *insn, u8 *bytes)
 	return i;
 }
 
-void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module __apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_returns(s);
+		return;
+	}
+#endif /* !CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *dest = NULL, *addr = (void *)s + *s;
 		struct insn insn;
@@ -600,10 +714,23 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 		if (len == insn.length) {
 			DUMP_BYTES(((u8*)addr),  len, "%px: orig: ", addr);
 			DUMP_BYTES(((u8*)bytes), len, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(addr, bytes, len);
+			else
+				text_poke_early(addr, bytes, len);
+#else
 			text_poke_early(addr, bytes, len);
+#endif
 		}
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __apply_returns);
+
+void __init_or_module noinline apply_returns(s32 *start, s32 *end)
+{
+	__apply_returns(start, end);
+}
 #else
 void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 #endif /* CONFIG_RETHUNK */
@@ -620,10 +747,21 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end) { }
 /*
  * Generated by: objtool --ibt
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_ibt_endbr(s);
+		return;
+	}
+#endif /* !CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		u32 endbr, poison = gen_endbr_poison();
 		void *addr = (void *)s + *s;
@@ -641,9 +779,17 @@ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end)
 		 */
 		DUMP_BYTES(((u8*)addr), 4, "%px: orig: ", addr);
 		DUMP_BYTES(((u8*)&poison), 4, "%px: repl: ", addr);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(addr, &poison, 4);
+		else
+			text_poke_early(addr, &poison, 4);
+#else
 		text_poke_early(addr, &poison, 4);
+#endif
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_ibt_endbr);
 
 #else
 
@@ -652,8 +798,9 @@ void __init_or_module noinline apply_ibt_endbr(s32 *start, s32 *end) { }
 #endif /* CONFIG_X86_KERNEL_IBT */
 
 #ifdef CONFIG_SMP
-static void alternatives_smp_lock(const s32 *start, const s32 *end,
-				  u8 *text, u8 *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void alternatives_smp_lock(const s32 *start, const s32 *end, u8 *text,
+				  u8 *text_end)
 {
 	const s32 *poff;
 
@@ -663,11 +810,21 @@ static void alternatives_smp_lock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn DS segment override prefix into lock prefix */
-		if (*ptr == 0x3e)
-			text_poke(ptr, ((unsigned char []){0xf0}), 1);
+		if (*ptr == 0x3e) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0xf0 }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0xf0 }), 1);
+#endif
+		}
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 				    u8 *text, u8 *text_end)
 {
@@ -679,11 +836,19 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 		if (!*poff || ptr < text || ptr >= text_end)
 			continue;
 		/* turn lock prefix into DS segment override prefix */
-		if (*ptr == 0xf0)
-			text_poke(ptr, ((unsigned char []){0x3E}), 1);
+		if (*ptr == 0xf0) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (static_branch_likely(&bhv_integrity_enabled_key))
+				bhv_apply_alternatives(
+					ptr, ((unsigned char[]){ 0x3E }), 1);
+			else
+				text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#else
+			text_poke(ptr, ((unsigned char[]){ 0x3E }), 1);
+#endif
+		}
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -702,14 +867,27 @@ struct smp_alt_module {
 static LIST_HEAD(smp_alt_modules);
 static bool uniproc_patched = false;	/* protected by text_mutex */
 
-void __init_or_module alternatives_smp_module_add(struct module *mod,
-						  char *name,
-						  void *locks, void *locks_end,
-						  void *text,  void *text_end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_add(struct module *mod,
+						      char *name, void *locks,
+						      void *locks_end,
+						      void *text,
+						      void *text_end)
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	if (!uniproc_patched)
 		goto unlock;
 
@@ -734,16 +912,44 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
-	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end,
+							 locks, locks_end, text,
+							 text_end);
+			// Apply
+			smp_lock = false;
+			p.locks_begin = locks;
+			p.locks_end = locks_end;
+			bhv_alternatives_apply_custom_filter(
+				&p, &smp_lock, bhv_alternatives_find_by_lock);
+		}
+	} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
 unlock:
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_add);
 
-void __init_or_module alternatives_smp_module_del(struct module *mod)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module alternatives_smp_module_del(struct module *mod)
 {
 	struct smp_alt_module *item;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	list_for_each_entry(item, &smp_alt_modules, next) {
 		if (mod != item->mod)
 			continue;
@@ -751,9 +957,15 @@ void __init_or_module alternatives_smp_module_del(struct module *mod)
 		kfree(item);
 		break;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_smp_module_del);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void alternatives_enable_smp(void)
 {
 	struct smp_alt_module *mod;
@@ -761,25 +973,50 @@ void alternatives_enable_smp(void)
 	/* Why bother if there are no other CPUs? */
 	BUG_ON(num_possible_cpus() == 1);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (uniproc_patched) {
 		pr_info("switching to SMP code\n");
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
-			alternatives_smp_lock(mod->locks, mod->locks_end,
-					      mod->text, mod->text_end);
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_VAS */
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+		}
 		uniproc_patched = false;
 	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_enable_smp);
 
 /*
  * Return 1 if the address range is reserved for SMP-alternatives.
  * Must hold text_mutex.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int alternatives_text_reserved(void *start, void *end)
 {
 	struct smp_alt_module *mod;
@@ -802,15 +1039,27 @@ int alternatives_text_reserved(void *start, void *end)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, alternatives_text_reserved);
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_PARAVIRT
-void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
-				     struct paravirt_patch_site *end)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module apply_paravirt(struct paravirt_patch_site *start,
+					 struct paravirt_patch_site *end)
 {
 	struct paravirt_patch_site *p;
 	char insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		for (p = start; p < end; p++)
+			bhv_apply_paravirt(p);
+		return;
+	}
+#endif
+#endif
+
 	for (p = start; p < end; p++) {
 		unsigned int used;
 
@@ -823,9 +1072,18 @@ void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
 
 		/* Pad the rest with nops */
 		add_nops(insn_buff + used, p->len - used);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (static_branch_likely(&bhv_integrity_enabled_key))
+			bhv_apply_alternatives(p->instr, insn_buff, p->len);
+		else
+			text_poke_early(p->instr, insn_buff, p->len);
+#else
 		text_poke_early(p->instr, insn_buff, p->len);
+#endif
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, apply_paravirt);
+
 extern struct paravirt_patch_site __start_parainstructions[],
 	__stop_parainstructions[];
 #endif	/* CONFIG_PARAVIRT */
@@ -846,6 +1104,7 @@ extern struct paravirt_patch_site __start_parainstructions[],
  * convention such that we can 'call' it from assembly.
  */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 extern void int3_magic(unsigned int *ptr); /* defined in asm */
 
 asm (
@@ -912,7 +1171,8 @@ static noinline void __init int3_selftest(void)
 	unregister_die_notifier(&int3_exception_nb);
 }
 
-void __init alternative_instructions(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __alternative_instructions(void)
 {
 	int3_selftest();
 
@@ -991,6 +1251,12 @@ void __init alternative_instructions(void)
 	restart_nmi();
 	alternatives_patched = 1;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __alternative_instructions);
+
+void __init alternative_instructions(void)
+{
+	__alternative_instructions();
+}
 
 /**
  * text_poke_early - Update instructions on a live kernel at boot time
@@ -1004,11 +1270,25 @@ void __init alternative_instructions(void)
  * instructions. And on the local CPU you need to be protected against NMI or
  * MCE handlers seeing an inconsistent instruction while you patch.
  */
-void __init_or_module text_poke_early(void *addr, const void *opcode,
-				      size_t len)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void __bhv_init_or_module text_poke_early(void *addr, const void *opcode,
+					  size_t len)
 {
 	unsigned long flags;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	if (boot_cpu_has(X86_FEATURE_NX) &&
 	    is_module_text_address((unsigned long)addr)) {
 		/*
@@ -1020,7 +1300,11 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 	} else {
 		local_irq_save(flags);
 		memcpy(addr, opcode, len);
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_sync_core();
+#else
 		sync_core();
+#endif
 		local_irq_restore(flags);
 
 		/*
@@ -1029,6 +1313,7 @@ void __init_or_module text_poke_early(void *addr, const void *opcode,
 		 */
 	}
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_early);
 
 typedef struct {
 	struct mm_struct *mm;
@@ -1047,6 +1332,7 @@ typedef struct {
  *          loaded, thereby preventing interrupt handler bugs from overriding
  *          the kernel memory protection.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 {
 	temp_mm_state_t temp_state;
@@ -1063,6 +1349,9 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
 	switch_mm_irqs_off(NULL, mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -1081,10 +1370,14 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 	return temp_state;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
 	lockdep_assert_irqs_disabled();
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(prev_state.mm == NULL ? NULL : prev_state.mm->owner);
+#endif
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
@@ -1097,11 +1390,13 @@ static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 __ro_after_init struct mm_struct *poking_mm;
 __ro_after_init unsigned long poking_addr;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_memcpy(void *dst, const void *src, size_t len)
 {
 	memcpy(dst, src, len);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_memset(void *dst, const void *src, size_t len)
 {
 	int c = *(const int *)src;
@@ -1111,6 +1406,7 @@ static void text_poke_memset(void *dst, const void *src, size_t len)
 
 typedef void text_poke_f(void *dst, const void *src, size_t len);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t len)
 {
 	bool cross_page_boundary = offset_in_page(addr) + len > PAGE_SIZE;
@@ -1176,7 +1472,22 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
 	prev = use_temporary_mm(poking_mm);
 
 	kasan_disable_current();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives((u8 *)poking_addr + offset_in_page(addr),
+				       src, len);
+	} else {
+		if (bhv_integrity_is_enabled())
+			bhv_apply_alternatives((u8 *)poking_addr +
+						       offset_in_page(addr),
+					       src, len);
+		else
+			func((u8 *)poking_addr + offset_in_page(addr), src,
+			     len);
+	}
+#else
 	func((u8 *)poking_addr + offset_in_page(addr), src, len);
+#endif
 	kasan_enable_current();
 
 	/*
@@ -1216,6 +1527,7 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
 	pte_unmap_unlock(ptep, ptl);
 	return addr;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __text_poke);
 
 /**
  * text_poke - Update instructions on a live kernel
@@ -1233,6 +1545,7 @@ static void *__text_poke(text_poke_f func, void *addr, const void *src, size_t l
  * by registering a module notifier, and ordering module removal and patching
  * trough a mutex.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void *text_poke(void *addr, const void *opcode, size_t len)
 {
 	lockdep_assert_held(&text_mutex);
@@ -1324,11 +1637,17 @@ void *text_poke_set(void *addr, int c, size_t len)
 	return addr;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_sync_core(void *info)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_sync_core();
+#else
 	sync_core();
+#endif
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void text_poke_sync(void)
 {
 	on_each_cpu(do_sync_core, NULL, 1);
@@ -1358,6 +1677,7 @@ struct bp_patching_desc {
 
 static struct bp_patching_desc bp_desc;
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline
 struct bp_patching_desc *try_get_desc(void)
 {
@@ -1369,6 +1689,7 @@ struct bp_patching_desc *try_get_desc(void)
 	return desc;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void put_desc(void)
 {
 	struct bp_patching_desc *desc = &bp_desc;
@@ -1377,11 +1698,13 @@ static __always_inline void put_desc(void)
 	arch_atomic_dec(&desc->refs);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline void *text_poke_addr(struct text_poke_loc *tp)
 {
 	return _stext + tp->rel_addr;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static __always_inline int patch_cmp(const void *key, const void *elt)
 {
 	struct text_poke_loc *tp = (struct text_poke_loc *) elt;
@@ -1476,7 +1799,9 @@ noinstr int poke_int3_handler(struct pt_regs *regs)
 }
 
 #define TP_VEC_MAX (PAGE_SIZE / sizeof(struct text_poke_loc))
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct text_poke_loc tp_vec[TP_VEC_MAX];
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static int tp_vec_nr;
 
 /**
@@ -1500,6 +1825,7 @@ static int tp_vec_nr;
  *		  replacing opcode
  *	- sync cores
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries)
 {
 	unsigned char int3 = INT3_INSN_OPCODE;
@@ -1623,6 +1949,7 @@ static void text_poke_bp_batch(struct text_poke_loc *tp, unsigned int nr_entries
 		atomic_cond_read_acquire(&bp_desc.refs, !VAL);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
 			       const void *opcode, size_t len, const void *emulate)
 {
@@ -1702,6 +2029,7 @@ static void text_poke_loc_init(struct text_poke_loc *tp, void *addr,
  * We hard rely on the tp_vec being ordered; ensure this is so by flushing
  * early if needed.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool tp_order_fail(void *addr)
 {
 	struct text_poke_loc *tp;
@@ -1719,6 +2047,7 @@ static bool tp_order_fail(void *addr)
 	return false;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void text_poke_flush(void *addr)
 {
 	if (tp_vec_nr == TP_VEC_MAX || tp_order_fail(addr)) {
@@ -1727,12 +2056,24 @@ static void text_poke_flush(void *addr)
 	}
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void text_poke_finish(void)
+{
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		text_poke_flush(NULL);
+	}
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_finish);
+#else
 void text_poke_finish(void)
 {
 	text_poke_flush(NULL);
 }
+#endif
 
-void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_queue(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc *tp;
 
@@ -1741,11 +2082,25 @@ void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const voi
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	text_poke_flush(addr);
 
 	tp = &tp_vec[tp_vec_nr++];
 	text_poke_loc_init(tp, addr, opcode, len, emulate);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_queue);
 
 /**
  * text_poke_bp() -- update instructions on live kernel on SMP
@@ -1758,10 +2113,24 @@ void __ref text_poke_queue(void *addr, const void *opcode, size_t len, const voi
  * dynamically allocated memory. This function should be used when it is
  * not possible to allocate memory.
  */
-void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+void text_poke_bp(void *addr, const void *opcode, size_t len, const void *emulate)
 {
 	struct text_poke_loc tp;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+
+	/* We need this check to allow patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_apply_alternatives(addr, opcode, len);
+		return;
+	}
+#endif
+
 	if (unlikely(system_state == SYSTEM_BOOTING)) {
 		text_poke_early(addr, opcode, len);
 		return;
@@ -1770,3 +2139,4 @@ void __ref text_poke_bp(void *addr, const void *opcode, size_t len, const void *
 	text_poke_loc_init(&tp, addr, opcode, len, emulate);
 	text_poke_bp_batch(&tp, 1);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, text_poke_bp);
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3a1..20bfa373b05 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index f5a3374e62c..f37c74110f1 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -11,6 +11,7 @@
 #include <asm/desc.h>
 #include <asm/hw_irq.h>
 #include <asm/idtentry.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -302,6 +303,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index f5b8ef02d17..60a22d351c8 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -16,22 +16,52 @@
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
 #include <asm/insn.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER2_NORET(void, insn_decode_kernel, struct insn *, insn, void *, entry);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_jump_label(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_jump_label);
+#endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int arch_jump_entry_size(struct jump_entry *entry)
 {
 	struct insn insn = {};
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_insn_decode_kernel(&insn, (void *)jump_entry_code(entry));
+#else
 	insn_decode_kernel(&insn, (void *)jump_entry_code(entry));
+#endif
 	BUG_ON(insn.length != 2 && insn.length != 5);
 
 	return insn.length;
 }
 
+#if !defined(CONFIG_BHV_VAS) || !defined(CONFIG_BHV_VAULT_SPACES)
 struct jump_label_patch {
 	const void *code;
 	int size;
 };
+#endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static struct jump_label_patch
 __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 {
@@ -78,14 +108,33 @@ __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 
 	return (struct jump_label_patch){.code = code, .size = size};
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_patch);
 
-static __always_inline void
-__jump_label_transform(struct jump_entry *entry,
-		       enum jump_label_type type,
-		       int init)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type,
+					  int init)
 {
 	const struct jump_label_patch jlp = __jump_label_patch(entry, type);
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+
+	/* We need this check to patch patching the above static key. */
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+#else
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+		return;
+	}
+#endif
+
 	/*
 	 * As long as only a single processor is running and the code is still
 	 * not marked as RO, text_poke_early() can be used; Checking that
@@ -105,21 +154,32 @@ __jump_label_transform(struct jump_entry *entry,
 	text_poke_bp((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
 }
 
-static void __ref jump_label_transform(struct jump_entry *entry,
-				       enum jump_label_type type,
-				       int init)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void jump_label_transform(struct jump_entry *entry,
+				 enum jump_label_type type,
+				 int init)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	__jump_label_transform(entry, type, init);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
 	jump_label_transform(entry, type, 0);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool arch_jump_label_transform_queue(struct jump_entry *entry,
 				     enum jump_label_type type)
 {
@@ -133,16 +193,61 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		return true;
 	}
 
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+
+	/* We need this check to patch patching the above static key. */
+#else /* !CONFIG_BHV_VAULT_SPACES */
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_BHV_SPACES */
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 	jlp = __jump_label_patch(entry, type);
 	text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void arch_jump_label_transform_apply(void)
+{
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (!static_branch_unlikely(&bhv_integrity_enabled_key)) {
+		bhv_wrapper_mutex_lock(&text_mutex);
+#else
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+#endif
+		text_poke_finish();
+#ifdef CONFIG_BHV_VAULT_SPACES
+		bhv_wrapper_mutex_unlock(&text_mutex);
+#else
+		mutex_unlock(&text_mutex);
+#endif
+	}
+}
+#else /* !CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index c032edcd3d9..72170ce459e 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -256,6 +259,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*retpolines = NULL, *returns = NULL, *ibt_endbr = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".text", secstrings + s->sh_name))
 			text = s;
@@ -293,9 +302,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks && text) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = (void *)text->sh_addr;
+		arch.text_end = (void *)text->sh_addr + text->sh_size;
+	}
+#endif
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (ibt_endbr) {
diff --git arch/x86/kernel/paravirt.c arch/x86/kernel/paravirt.c
index 7ca2d46c08c..2f72bbaad4f 100644
--- arch/x86/kernel/paravirt.c
+++ arch/x86/kernel/paravirt.c
@@ -33,6 +33,8 @@
 #include <asm/tlb.h>
 #include <asm/io_bitmap.h>
 
+#include <bhv/vault.h>
+
 /*
  * nop stub, which must not clobber anything *including the stack* to
  * avoid confusing the entry prologues.
@@ -95,6 +97,7 @@ void __init native_pv_lock_init(void)
 		static_branch_disable(&virt_spin_lock_key);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 unsigned int paravirt_patch(u8 type, void *insn_buff, unsigned long addr,
 			    unsigned int len)
 {
diff --git arch/x86/kernel/process.c arch/x86/kernel/process.c
index 279b5e9be80..da4e1a7d15a 100644
--- arch/x86/kernel/process.c
+++ arch/x86/kernel/process.c
@@ -48,6 +48,8 @@
 #include <asm/unwind.h>
 #include <asm/tdx.h>
 
+#include <bhv/domain.h>
+
 #include "process.h"
 
 /*
@@ -173,6 +175,10 @@ int copy_thread(struct task_struct *p, const struct kernel_clone_args *args)
 	frame->flags = X86_EFLAGS_FIXED;
 #endif
 
+#ifdef CONFIG_MEM_NS
+	frame->domain = bhv_get_domain(p);
+#endif
+
 	fpu_clone(p, clone_flags, args->fn);
 
 	/* Kernel thread ? */
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index b32134b093e..91cdc8c509b 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,6 +4,19 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(void, mutex_unlock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1(bool, cpu_feature_enabled, u16, feature);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static DEFINE_STATIC_KEY_FALSE(bhv_integrity_enabled_key);
+#endif
+
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_VAULT_SPACES)
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
@@ -11,21 +24,36 @@ enum insn_type {
 	RET = 3,  /* tramp / site cond-tail-call */
 	JCC = 4,
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
  * that there is no false-positive trampoline identification while also being a
  * speculation stop.
  */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 tramp_ud[] = { 0x0f, 0xb9, 0xcc };
 
 /*
  * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
  */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
 
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_init_static_call(void)
+{
+	if (bhv_integrity_is_enabled())
+		static_branch_enable(&bhv_integrity_enabled_key);
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_init_static_call);
+#endif
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static u8 __is_Jcc(u8 *insn) /* Jcc.d32 */
 {
 	u8 ret = 0;
@@ -49,8 +77,9 @@ asm (".global __static_call_return\n\t"
      "ret; int3\n\t"
      ".size __static_call_return, . - __static_call_return \n\t");
 
-static void __ref __static_call_transform(void *insn, enum insn_type type,
-					  void *func, bool modinit)
+BHV_VAULT_ADD_TO_REF_CODE_REGION(jump_label)
+static void __static_call_transform(void *insn, enum insn_type type,
+				    void *func, bool modinit)
 {
 	const void *emulate = NULL;
 	int size = CALL_INSN_SIZE;
@@ -60,6 +89,13 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	if ((type == JMP || type == RET) && (op = __is_Jcc(insn)))
 		type = JCC;
 
+#if defined(CONFIG_BHV_VAS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func, modinit);
+		return;
+	}
+#endif
+
 	switch (type) {
 	case CALL:
 		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
@@ -79,7 +115,11 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 		break;
 
 	case RET:
+#ifdef CONFIG_BHV_VAULT_SPACES
+		if (bhv_wrapper_cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#else
 		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#endif
 			code = text_gen_insn(JMP32_INSN_OPCODE, insn, x86_return_thunk);
 		else
 			code = &retinsn;
@@ -88,7 +128,11 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	case JCC:
 		if (!func) {
 			func = __static_call_return;
+#ifdef CONFIG_BHV_VAULT_SPACES
+			if (bhv_wrapper_cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#else
 			if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+#endif
 				func = __x86_return_thunk;
 		}
 
@@ -103,12 +147,21 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	if (memcmp(insn, code, size) == 0)
 		return;
 
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	if (static_branch_likely(&bhv_integrity_enabled_key)) {
+		bhv_apply_alternatives(insn, code, size);
+		return;
+	}
+#endif
+
 	if (system_state == SYSTEM_BOOTING || modinit)
 		return text_poke_early(insn, code, size);
 
 	text_poke_bp(insn, code, size, emulate);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_call_validate(u8 *insn, bool tail, bool tramp)
 {
 	u8 opcode = insn[0];
@@ -137,6 +190,7 @@ static void __static_call_validate(u8 *insn, bool tail, bool tramp)
 	BUG();
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline enum insn_type __sc_insn(bool null, bool tail)
 {
 	/*
@@ -152,9 +206,14 @@ static inline enum insn_type __sc_insn(bool null, bool tail)
 	return 2*tail + null;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
 
 	if (tramp) {
 		__static_call_validate(tramp, true, true);
@@ -166,8 +225,13 @@ void arch_static_call_transform(void *site, void *tramp, void *func, bool tail)
 		__static_call_transform(site, __sc_insn(!func, tail), func, false);
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, arch_static_call_transform);
 EXPORT_SYMBOL_GPL(arch_static_call_transform);
 
 #ifdef CONFIG_RETHUNK
@@ -182,6 +246,7 @@ EXPORT_SYMBOL_GPL(arch_static_call_transform);
  * This means that __static_call_transform() above can have overwritten the
  * return trampoline and we now need to fix things up to be consistent.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 bool __static_call_fixup(void *tramp, u8 op, void *dest)
 {
 	unsigned long addr = (unsigned long)tramp;
@@ -202,11 +267,27 @@ bool __static_call_fixup(void *tramp, u8 op, void *dest)
 		return false;
 	}
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&text_mutex);
+#else
 	mutex_lock(&text_mutex);
+#endif
+
 	if (op == RET_INSN_OPCODE || dest == &__x86_return_thunk)
 		__static_call_transform(tramp, RET, NULL, true);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&text_mutex);
+#else
 	mutex_unlock(&text_mutex);
+#endif
 
 	return true;
 }
+/*
+ * XXX: REMOVE ENTRY POINT AS SOON AS WE ADD ALTERNATIVE INSTRUCTIONS INTO THE
+ * VAULT.
+ */
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_fixup);
+
 #endif
diff --git arch/x86/kernel/traps.c arch/x86/kernel/traps.c
index c0a5a4f225d..1a10849da96 100644
--- arch/x86/kernel/traps.c
+++ arch/x86/kernel/traps.c
@@ -30,6 +30,7 @@
 #include <linux/errno.h>
 #include <linux/kexec.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/timer.h>
 #include <linux/init.h>
@@ -89,6 +90,16 @@ static inline void cond_local_irq_disable(struct pt_regs *regs)
 		local_irq_disable();
 }
 
+__always_inline static bool is_vault(unsigned long addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
+
 __always_inline int is_valid_bugaddr(unsigned long addr)
 {
 	if (addr < TASK_SIZE_MAX)
@@ -308,6 +319,11 @@ static noinstr bool handle_bug(struct pt_regs *regs)
 	 * irqentry_enter().
 	 */
 	kmsan_unpoison_entry_regs(regs);
+	if (is_vault(regs->ip)){
+		show_regs(regs);
+		panic("[%s] BUG in vault at %pS\n", __FUNCTION__, (void*)regs->ip);
+	}
+
 	if (!is_valid_bugaddr(regs->ip))
 		return handled;
 
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index 78ccb5ec3c0..6ba9c86d2a4 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -150,8 +150,13 @@ SECTIONS
 		ALIGN_ENTRY_TEXT_END
 		SOFTIRQENTRY_TEXT
 		STATIC_CALL_TEXT
+		BHV_TEXT
 		*(.gnu.warning)
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+		BHV_VAULT_TEXT(jump_label)
+#endif
+
 #ifdef CONFIG_RETPOLINE
 		__indirect_thunk_start = .;
 		*(.text..__x86.indirect_thunk)
@@ -252,8 +257,12 @@ SECTIONS
 	 *
 	 * See static_cpu_has() for an example.
 	 */
+	. = ALIGN(PAGE_SIZE);
 	.altinstr_aux : AT(ADDR(.altinstr_aux) - LOAD_OFFSET) {
+		__altinstr_aux_start = .;
 		*(.altinstr_aux)
+		. = ALIGN(PAGE_SIZE);
+		__altinstr_aux_end = .;
 	}
 
 	INIT_DATA_SECTION(16)
@@ -279,7 +288,7 @@ SECTIONS
 	 * baremetal native ones. Think page table operations.
 	 * Details in paravirt_types.h
 	 */
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.parainstructions : AT(ADDR(.parainstructions) - LOAD_OFFSET) {
 		__parainstructions = .;
 		*(.parainstructions)
@@ -292,18 +301,19 @@ SECTIONS
 	 * __x86_indirect_thunk_*(). These instructions can be patched along
 	 * with alternatives, after which the section can be freed.
 	 */
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.retpoline_sites : AT(ADDR(.retpoline_sites) - LOAD_OFFSET) {
 		__retpoline_sites = .;
 		*(.retpoline_sites)
 		__retpoline_sites_end = .;
 	}
 
-	. = ALIGN(8);
+	. = ALIGN(PAGE_SIZE);
 	.return_sites : AT(ADDR(.return_sites) - LOAD_OFFSET) {
 		__return_sites = .;
 		*(.return_sites)
 		__return_sites_end = .;
+		. = ALIGN(PAGE_SIZE);
 	}
 #endif
 
@@ -323,8 +333,10 @@ SECTIONS
 	 */
 	. = ALIGN(8);
 	.altinstructions : AT(ADDR(.altinstructions) - LOAD_OFFSET) {
+		. = ALIGN(PAGE_SIZE);
 		__alt_instructions = .;
 		*(.altinstructions)
+		. = ALIGN(PAGE_SIZE);
 		__alt_instructions_end = .;
 	}
 
@@ -344,15 +356,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -386,6 +412,34 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#ifdef CONFIG_BHV_VAULT_SPACES
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.comm : AT(ADDR(.bhv.vault.comm) - LOAD_OFFSET) {
+		__bhv_vault_comm_start = .;
+		. += PAGE_SIZE;
+		. = ALIGN(PAGE_SIZE);
+		__bhv_vault_comm_end = .;
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.data : AT(ADDR(.bhv.vault.data) - LOAD_OFFSET) {
+		BHV_VAULT_DATA(jump_label)
+	}
+	. = ALIGN(PAGE_SIZE);
+	.bhv.vault.rodata : AT(ADDR(.bhv.vault.rodata) - LOAD_OFFSET) {
+		BHV_VAULT_RO_DATA(jump_label)
+	}
+#endif
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/lib/inat.c arch/x86/lib/inat.c
index b0f3b2a62ae..7d5351f4f37 100644
--- arch/x86/lib/inat.c
+++ arch/x86/lib/inat.c
@@ -6,15 +6,19 @@
  */
 #include <asm/insn.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 /* Attribute tables are generated from opcode map */
 #include "inat-tables.c"
 
 /* Attribute search APIs */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
 {
 	return inat_primary_table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int inat_get_last_prefix_id(insn_byte_t last_pfx)
 {
 	insn_attr_t lpfx_attr;
@@ -23,6 +27,7 @@ int inat_get_last_prefix_id(insn_byte_t last_pfx)
 	return inat_last_prefix_id(lpfx_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 				      insn_attr_t esc_attr)
 {
@@ -42,6 +47,7 @@ insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
 	return table[opcode];
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 				     insn_attr_t grp_attr)
 {
@@ -62,6 +68,7 @@ insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
 	       inat_group_common_attribute(grp_attr);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
 				   insn_byte_t vex_p)
 {
diff --git arch/x86/lib/insn.c arch/x86/lib/insn.c
index 55e371cc69f..f0439864c4a 100644
--- arch/x86/lib/insn.c
+++ arch/x86/lib/insn.c
@@ -20,6 +20,8 @@
 
 #include <asm/emulate_prefix.h> /* __ignore_sync_check__ */
 
+#include <bhv/vault.h>
+
 #define leXX_to_cpu(t, r)						\
 ({									\
 	__typeof__(t) v;						\
@@ -58,6 +60,7 @@
  * @buf_len:	length of the insn buffer at @kaddr
  * @x86_64:	!0 for 64-bit kernel or 64-bit app
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 {
 	/*
@@ -82,6 +85,7 @@ void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
 static const insn_byte_t xen_prefix[] = { __XEN_EMULATE_PREFIX };
 static const insn_byte_t kvm_prefix[] = { __KVM_EMULATE_PREFIX };
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __insn_get_emulate_prefix(struct insn *insn,
 				     const insn_byte_t *prefix, size_t len)
 {
@@ -101,6 +105,7 @@ static int __insn_get_emulate_prefix(struct insn *insn,
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void insn_get_emulate_prefix(struct insn *insn)
 {
 	if (__insn_get_emulate_prefix(insn, xen_prefix, sizeof(xen_prefix)))
@@ -121,6 +126,7 @@ static void insn_get_emulate_prefix(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_prefixes(struct insn *insn)
 {
 	struct insn_field *prefixes = &insn->prefixes;
@@ -259,6 +265,7 @@ int insn_get_prefixes(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_opcode(struct insn *insn)
 {
 	struct insn_field *opcode = &insn->opcode;
@@ -330,6 +337,7 @@ int insn_get_opcode(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_modrm(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -378,6 +386,7 @@ int insn_get_modrm(struct insn *insn)
  * If necessary, first collects the instruction up to and including the
  * ModRM byte.  No effect if @insn->x86_64 is 0.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_rip_relative(struct insn *insn)
 {
 	struct insn_field *modrm = &insn->modrm;
@@ -409,6 +418,7 @@ int insn_rip_relative(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_sib(struct insn *insn)
 {
 	insn_byte_t modrm;
@@ -452,6 +462,7 @@ int insn_get_sib(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_displacement(struct insn *insn)
 {
 	insn_byte_t mod, rm, base;
@@ -514,6 +525,7 @@ int insn_get_displacement(struct insn *insn)
 }
 
 /* Decode moffset16/32/64. Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_moffset(struct insn *insn)
 {
 	switch (insn->addr_bytes) {
@@ -539,6 +551,7 @@ static int __get_moffset(struct insn *insn)
 }
 
 /* Decode imm v32(Iz). Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv32(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -560,6 +573,7 @@ static int __get_immv32(struct insn *insn)
 }
 
 /* Decode imm v64(Iv/Ov), Return 0 if failed */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immv(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -585,6 +599,7 @@ static int __get_immv(struct insn *insn)
 }
 
 /* Decode ptr16:16/32(Ap) */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int __get_immptr(struct insn *insn)
 {
 	switch (insn->opnd_bytes) {
@@ -621,6 +636,7 @@ static int __get_immptr(struct insn *insn)
  * 0:  on success
  * < 0: on error
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_immediate(struct insn *insn)
 {
 	int ret;
@@ -696,6 +712,7 @@ int insn_get_immediate(struct insn *insn)
  *  - 0 on success
  *  - < 0 on error
 */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_get_length(struct insn *insn)
 {
 	int ret;
@@ -716,6 +733,7 @@ int insn_get_length(struct insn *insn)
 }
 
 /* Ensure this instruction is decoded completely */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static inline int insn_complete(struct insn *insn)
 {
 	return insn->opcode.got && insn->modrm.got && insn->sib.got &&
@@ -733,6 +751,7 @@ static inline int insn_complete(struct insn *insn)
  * 0: if decoding succeeded
  * < 0: otherwise.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int insn_decode(struct insn *insn, const void *kaddr, int buf_len, enum insn_mode m)
 {
 	int ret;
diff --git arch/x86/mm/fault.c arch/x86/mm/fault.c
index 2fc007752ce..1721cc4e559 100644
--- arch/x86/mm/fault.c
+++ arch/x86/mm/fault.c
@@ -34,6 +34,10 @@
 #include <asm/vdso.h>			/* fixup_vdso_exception()	*/
 #include <asm/irq_stack.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
@@ -527,6 +531,31 @@ static void show_ldttss(const struct desc_ptr *gdt, const char *name, u16 index)
 static void
 show_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kaccess_events()) {
+		// is kernel?
+		if (!(error_code & X86_PF_USER)) {
+			// is perm violation?
+			if ((error_code & X86_PF_PROT) &&
+			    !(error_code & X86_PF_RSVD) &&
+			    !(error_code & X86_PF_PK)) {
+				uint8_t type;
+				if (error_code &
+				    X86_PF_INSTR) { // is instr fetch?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE;
+				} else if (error_code &
+					   X86_PF_WRITE) { // is write?
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE;
+				} else {
+					type = GuestConnABI__GuestLog__KernelAccess__AccessType__READ;
+				}
+				bhv_guestlog_log_kaccess((uint64_t)address,
+							 type);
+			}
+		}
+	}
+#endif
+
 	if (!oops_may_print())
 		return;
 
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index 913287b9340..bcb6e0718a2 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -37,6 +37,9 @@
 
 #include "mm_internal.h"
 
+#include <bhv/init/start.h>
+#include <bhv/vault.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -974,12 +977,84 @@ void free_kernel_image_pages(const char *what, void *begin, void *end)
 		set_memory_np_noalias(begin_ul, len_pages);
 }
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __alt_instructions[];
+extern char __alt_instructions_end[];
+extern char __altinstr_aux_start[];
+extern char __altinstr_aux_end[];
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+static void __ref bhv_vault_release_memory(void)
+{
+	int rc;
+	HypABI__Wagner__Delete__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return;
+
+	vault.mem.gpa = bhv_virt_to_phys(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+#ifdef CONFIG_PARAVIRT
+	vault.mem.gpa = bhv_virt_to_phys(__parainstructions);
+	vault.mem.size = (unsigned long)__parainstructions_end - (unsigned long)__parainstructions;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+#endif
+
+#ifdef CONFIG_RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end - (unsigned long)__retpoline_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+
+	vault.mem.gpa = bhv_virt_to_phys(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end - (unsigned long)__return_sites;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+#endif
+
+	vault.mem.gpa = bhv_virt_to_phys(__altinstr_aux_start);
+	vault.mem.size = (unsigned long)__altinstr_aux_end - (unsigned long)__altinstr_aux_start;
+	rc = HypABI__Wagner__Delete__hypercall_noalloc(&vault);
+	if (rc) {
+		bhv_fail("BHV: Cannot release memory range in spaces-based Vault");
+		return;
+	}
+}
+
+#endif
+
 void __ref free_initmem(void)
 {
 	e820__reallocate_tables();
 
 	mem_encrypt_free_decrypted_mem();
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_vault_release_memory();
+#endif
+
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git arch/x86/mm/pat/memtype.c arch/x86/mm/pat/memtype.c
index d6fe9093ea9..aea52fddfb2 100644
--- arch/x86/mm/pat/memtype.c
+++ arch/x86/mm/pat/memtype.c
@@ -313,7 +313,7 @@ void __init init_cache_modes(void)
 		 * NOTE: When WC or WP is used, it is redirected to UC- per
 		 * the default setup in __cachemode2pte_tbl[].
 		 */
-		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, UC_MINUS) | PAT(3, UC) |
+		pat = PAT(0, WB) | PAT(1, WT) | PAT(2, WC) | PAT(3, UC) |
 		      PAT(4, WB) | PAT(5, WT) | PAT(6, UC_MINUS) | PAT(7, UC);
 	} else if (!pat_force_disabled && cpu_feature_enabled(X86_FEATURE_HYPERVISOR)) {
 		/*
diff --git arch/x86/mm/pgtable.c arch/x86/mm/pgtable.c
index 77ee0012f84..442770ed9a9 100644
--- arch/x86/mm/pgtable.c
+++ arch/x86/mm/pgtable.c
@@ -7,6 +7,10 @@
 #include <asm/fixmap.h>
 #include <asm/mtrr.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 #ifdef CONFIG_DYNAMIC_PHYSICAL_MASK
 phys_addr_t physical_mask __ro_after_init = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;
 EXPORT_SYMBOL(physical_mask);
@@ -489,8 +493,12 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 {
 	int changed = !pte_same(*ptep, entry);
 
-	if (changed && dirty)
+	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pte_at(vma->vm_mm, address, ptep, entry);
+#endif
 		set_pte(ptep, entry);
+	}
 
 	return changed;
 }
@@ -505,6 +513,9 @@ int pmdp_set_access_flags(struct vm_area_struct *vma,
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pmd_at(vma->vm_mm, address, pmdp, entry);
+#endif
 		set_pmd(pmdp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pmd
@@ -525,6 +536,9 @@ int pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,
 	VM_BUG_ON(address & ~HPAGE_PUD_MASK);
 
 	if (changed && dirty) {
+#ifdef CONFIG_MEM_NS
+		bhv_domain_set_pud_at(vma->vm_mm, address, pudp, entry);
+#endif
 		set_pud(pudp, entry);
 		/*
 		 * We had a write-protection fault here and changed the pud
diff --git arch/x86/mm/tlb.c arch/x86/mm/tlb.c
index c1e31e9a85d..d6bb8871e47 100644
--- arch/x86/mm/tlb.c
+++ arch/x86/mm/tlb.c
@@ -19,6 +19,8 @@
 #include <asm/apic.h>
 #include <asm/perf_event.h>
 
+#include <bhv/domain.h>
+
 #include "mm_internal.h"
 
 #ifdef CONFIG_PARAVIRT
@@ -323,6 +325,9 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	local_irq_save(flags);
 	switch_mm_irqs_off(prev, next, tsk);
 	local_irq_restore(flags);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(next == NULL ? NULL : next->owner);
+#endif
 }
 
 /*
diff --git arch/x86/net/bpf_jit_comp.c arch/x86/net/bpf_jit_comp.c
index 7913440c0fd..cd9f5455c21 100644
--- arch/x86/net/bpf_jit_comp.c
+++ arch/x86/net/bpf_jit_comp.c
@@ -16,6 +16,9 @@
 #include <asm/nospec-branch.h>
 #include <asm/text-patching.h>
 
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 static u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)
 {
 	if (len == 1)
@@ -230,6 +233,11 @@ static void jit_fill_hole(void *area, unsigned int size)
 
 int bpf_arch_text_invalidate(void *dst, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		return bhv_bpf_invalidate(dst, 0xcc, len);
+	}
+#endif /* CONFIG_BHV_VAS */
 	return IS_ERR_OR_NULL(text_poke_set(dst, 0xcc, len));
 }
 
@@ -2514,6 +2522,13 @@ bool bpf_jit_supports_kfunc_call(void)
 
 void *bpf_arch_text_copy(void *dst, void *src, size_t len)
 {
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		if (bhv_bpf_write(dst, src, len))
+			return ERR_PTR(-EINVAL);
+		return dst;
+	}
+#endif /* CONFIG_BHV_VAS */
 	if (text_poke_copy(dst, src, len) == NULL)
 		return ERR_PTR(-EINVAL);
 	return dst;
diff --git certs/blacklist.c certs/blacklist.c
index 41f10601cc7..3a1e3f976d2 100644
--- certs/blacklist.c
+++ certs/blacklist.c
@@ -32,6 +32,8 @@
 static const char tbs_prefix[] = "tbs";
 static const char bin_prefix[] = "bin";
 
+#include <bhv/keyring.h>
+
 static struct key *blacklist_keyring;
 
 #ifdef CONFIG_SYSTEM_REVOCATION_LIST
@@ -284,7 +286,9 @@ int add_key_to_revocation_list(const char *data, size_t size)
  */
 int is_key_on_revocation_list(struct pkcs7_message *pkcs7)
 {
-	int ret;
+	int ret = bhv_keyring_verify(blacklist_keyring, &blacklist_keyring);
+	if (ret)
+		return -EPERM;
 
 	ret = pkcs7_validate_trust(pkcs7, blacklist_keyring);
 
@@ -347,6 +351,10 @@ static int __init blacklist_init(void)
 	for (bl = blacklist_hashes; *bl; bl++)
 		if (mark_raw_hash_blacklisted(*bl) < 0)
 			pr_err("- blacklisting failed\n");
+
+	if (bhv_keyring_register_system_trusted(&blacklist_keyring))
+		panic("Can't register system blacklist keyring\n");
+
 	return 0;
 }
 
diff --git certs/system_keyring.c certs/system_keyring.c
index 5042cc54fa5..1729eb0ad11 100644
--- certs/system_keyring.c
+++ certs/system_keyring.c
@@ -17,6 +17,8 @@
 #include <keys/system_keyring.h>
 #include <crypto/pkcs7.h>
 
+#include <bhv/keyring.h>
+
 static struct key *builtin_trusted_keys;
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 static struct key *secondary_trusted_keys;
@@ -43,6 +45,11 @@ int restrict_link_by_builtin_trusted(struct key *dest_keyring,
 				     const union key_payload *payload,
 				     struct key *restriction_key)
 {
+	int rc = bhv_keyring_verify_locked(builtin_trusted_keys,
+					   &builtin_trusted_keys);
+	if (rc)
+		return rc;
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  builtin_trusted_keys);
 }
@@ -62,6 +69,8 @@ int restrict_link_by_builtin_and_secondary_trusted(
 	const union key_payload *payload,
 	struct key *restrict_key)
 {
+	int rc = 0;
+
 	/* If we have a secondary trusted keyring, then that contains a link
 	 * through to the builtin keyring and the search will follow that link.
 	 */
@@ -71,6 +80,12 @@ int restrict_link_by_builtin_and_secondary_trusted(
 		/* Allow the builtin keyring to be added to the secondary */
 		return 0;
 
+	rc = bhv_keyring_verify_locked(secondary_trusted_keys,
+				       &secondary_trusted_keys);
+	if (rc)
+		return rc;
+
+
 	return restrict_link_by_signature(dest_keyring, type, payload,
 					  secondary_trusted_keys);
 }
@@ -103,6 +118,9 @@ void __init set_machine_trusted_keys(struct key *keyring)
 
 	if (key_link(secondary_trusted_keys, machine_trusted_keys) < 0)
 		panic("Can't link (machine) trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&machine_trusted_keys))
+		panic("Can't register machine trusted keyring\n");
 }
 
 /**
@@ -150,6 +168,9 @@ static __init int system_trusted_keyring_init(void)
 	if (IS_ERR(builtin_trusted_keys))
 		panic("Can't allocate builtin trusted keyring\n");
 
+	if (bhv_keyring_register_system_trusted(&builtin_trusted_keys))
+		panic("Can't register builtin trusted keyring\n");
+
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
 	secondary_trusted_keys =
 		keyring_alloc(".secondary_trusted_keys",
@@ -165,6 +186,9 @@ static __init int system_trusted_keyring_init(void)
 
 	if (key_link(secondary_trusted_keys, builtin_trusted_keys) < 0)
 		panic("Can't link trusted keyrings\n");
+
+	if (bhv_keyring_register_system_trusted(&secondary_trusted_keys))
+		panic("Can't register secondary trusted keyring\n");
 #endif
 
 	return 0;
@@ -193,6 +217,10 @@ static __init int load_system_certificate_list(void)
 {
 	const u8 *p;
 	unsigned long size;
+	int rc = bhv_keyring_verify(builtin_trusted_keys,
+				    &builtin_trusted_keys);
+	if (rc)
+		return rc;
 
 	pr_notice("Loading compiled-in X.509 certificates\n");
 
@@ -244,15 +272,35 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 		goto error;
 
 	if (!trusted_keys) {
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 	} else if (trusted_keys == VERIFY_USE_SECONDARY_KEYRING) {
 #ifdef CONFIG_SECONDARY_TRUSTED_KEYRING
+		ret = bhv_keyring_verify(secondary_trusted_keys,
+					 &secondary_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = secondary_trusted_keys;
 #else
+		ret = bhv_keyring_verify(builtin_trusted_keys,
+					 &builtin_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = builtin_trusted_keys;
 #endif
 	} else if (trusted_keys == VERIFY_USE_PLATFORM_KEYRING) {
 #ifdef CONFIG_INTEGRITY_PLATFORM_KEYRING
+		ret = bhv_keyring_verify(platform_trusted_keys,
+					 &platform_trusted_keys);
+		if (ret)
+			goto error;
+
 		trusted_keys = platform_trusted_keys;
 #else
 		trusted_keys = NULL;
@@ -269,6 +317,7 @@ int verify_pkcs7_message_sig(const void *data, size_t len,
 			goto error;
 		}
 	}
+
 	ret = pkcs7_validate_trust(pkcs7, trusted_keys);
 	if (ret < 0) {
 		if (ret == -ENOKEY)
@@ -337,5 +386,7 @@ EXPORT_SYMBOL_GPL(verify_pkcs7_signature);
 void __init set_platform_trusted_keys(struct key *keyring)
 {
 	platform_trusted_keys = keyring;
+	if (bhv_keyring_register_system_trusted(&platform_trusted_keys))
+		panic("Can't register platform trusted keyring\n");
 }
 #endif
diff --git drivers/block/drbd/drbd_main.c drivers/block/drbd/drbd_main.c
index 590d1b50ab5..b5744f5276e 100644
--- drivers/block/drbd/drbd_main.c
+++ drivers/block/drbd/drbd_main.c
@@ -97,9 +97,14 @@ module_param_named(proc_details, drbd_proc_details, int, 0644);
 unsigned int drbd_minor_count = DRBD_MINOR_COUNT_DEF;
 /* Module parameter for setting the user mode helper program
  * to run. Default is /sbin/drbdadm */
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+const char drbd_usermode_helper[] __section(".rodata") = "/sbin/drbdadm";
+#else
 char drbd_usermode_helper[80] = "/sbin/drbdadm";
-module_param_named(minor_count, drbd_minor_count, uint, 0444);
 module_param_string(usermode_helper, drbd_usermode_helper, sizeof(drbd_usermode_helper), 0644);
+#endif
+
+module_param_named(minor_count, drbd_minor_count, uint, 0444);
 
 /* in 2.6.x, our device mapping and config info contains our virtual gendisks
  * as member "struct gendisk *vdisk;"
diff --git drivers/block/zram/zram_drv.c drivers/block/zram/zram_drv.c
index 966aab902d1..7812bc6ce28 100644
--- drivers/block/zram/zram_drv.c
+++ drivers/block/zram/zram_drv.c
@@ -947,7 +947,7 @@ static ssize_t read_block_state(struct file *file, char __user *buf,
 	return written;
 }
 
-static const struct file_operations proc_zram_block_state_op = {
+const struct file_operations proc_zram_block_state_op __section(".rodata") = {
 	.open = simple_open,
 	.read = read_block_state,
 	.llseek = default_llseek,
diff --git drivers/char/mem.c drivers/char/mem.c
index 5611d127363..01c8e04b533 100644
--- drivers/char/mem.c
+++ drivers/char/mem.c
@@ -649,7 +649,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define write_iter_zero	write_iter_null
 #define open_mem	open_port
 
-static const struct file_operations __maybe_unused mem_fops = {
+const struct file_operations __maybe_unused mem_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
@@ -661,7 +661,7 @@ static const struct file_operations __maybe_unused mem_fops = {
 #endif
 };
 
-static const struct file_operations null_fops = {
+const struct file_operations null_fops __section(".rodata") = {
 	.llseek		= null_lseek,
 	.read		= read_null,
 	.write		= write_null,
@@ -671,14 +671,14 @@ static const struct file_operations null_fops = {
 	.uring_cmd	= uring_cmd_null,
 };
 
-static const struct file_operations __maybe_unused port_fops = {
+const struct file_operations __maybe_unused port_fops __section(".rodata") = {
 	.llseek		= memory_lseek,
 	.read		= read_port,
 	.write		= write_port,
 	.open		= open_port,
 };
 
-static const struct file_operations zero_fops = {
+const struct file_operations zero_fops __section(".rodata") = {
 	.llseek		= zero_lseek,
 	.write		= write_zero,
 	.read_iter	= read_iter_zero,
@@ -691,7 +691,7 @@ static const struct file_operations zero_fops = {
 #endif
 };
 
-static const struct file_operations full_fops = {
+const struct file_operations full_fops __section(".rodata") = {
 	.llseek		= full_lseek,
 	.read_iter	= read_iter_zero,
 	.write		= write_full,
diff --git drivers/char/random.c drivers/char/random.c
index fd57eb372d4..948fb03d7fd 100644
--- drivers/char/random.c
+++ drivers/char/random.c
@@ -1482,7 +1482,7 @@ static int random_fasync(int fd, struct file *filp, int on)
 	return fasync_helper(fd, filp, on, &fasync);
 }
 
-const struct file_operations random_fops = {
+const struct file_operations random_fops __section(".rodata") = {
 	.read_iter = random_read_iter,
 	.write_iter = random_write_iter,
 	.poll = random_poll,
@@ -1494,7 +1494,7 @@ const struct file_operations random_fops = {
 	.splice_write = iter_file_splice_write,
 };
 
-const struct file_operations urandom_fops = {
+const struct file_operations urandom_fops __section(".rodata") = {
 	.read_iter = urandom_read_iter,
 	.write_iter = random_write_iter,
 	.unlocked_ioctl = random_ioctl,
diff --git drivers/tty/tty_io.c drivers/tty/tty_io.c
index aaf77a5616f..1fa809d18a0 100644
--- drivers/tty/tty_io.c
+++ drivers/tty/tty_io.c
@@ -462,7 +462,7 @@ static void tty_show_fdinfo(struct seq_file *m, struct file *file)
 		tty->ops->show_fdinfo(tty, m);
 }
 
-static const struct file_operations tty_fops = {
+const struct file_operations tty_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= tty_write,
@@ -477,7 +477,7 @@ static const struct file_operations tty_fops = {
 	.show_fdinfo	= tty_show_fdinfo,
 };
 
-static const struct file_operations console_fops = {
+const struct file_operations console_fops __section(".rodata") = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= redirected_tty_write,
@@ -491,7 +491,7 @@ static const struct file_operations console_fops = {
 	.fasync		= tty_fasync,
 };
 
-static const struct file_operations hung_up_tty_fops = {
+const struct file_operations hung_up_tty_fops = {
 	.llseek		= no_llseek,
 	.read_iter	= hung_up_tty_read,
 	.write_iter	= hung_up_tty_write,
diff --git drivers/video/fbdev/uvesafb.c drivers/video/fbdev/uvesafb.c
index a85463db9f9..19f1b8ed92f 100644
--- drivers/video/fbdev/uvesafb.c
+++ drivers/video/fbdev/uvesafb.c
@@ -34,7 +34,12 @@ static struct cb_id uvesafb_cn_id = {
 	.idx = CN_IDX_V86D,
 	.val = CN_VAL_V86D_UVESAFB
 };
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static const char v86d_path[] __section(".rodata") = "/sbin/v86d";
+#else
 static char v86d_path[PATH_MAX] = "/sbin/v86d";
+#endif
 static char v86d_started;	/* has v86d been started by uvesafb? */
 
 static const struct fb_fix_screeninfo uvesafb_fix = {
@@ -118,7 +123,7 @@ static int uvesafb_helper_start(void)
 	};
 
 	char *argv[] = {
-		v86d_path,
+		(char *)v86d_path,
 		NULL,
 	};
 
@@ -1871,6 +1876,9 @@ static ssize_t v86d_show(struct device_driver *dev, char *buf)
 	return snprintf(buf, PAGE_SIZE, "%s\n", v86d_path);
 }
 
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
+static DRIVER_ATTR_RO(v86d);
+#else
 static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 		size_t count)
 {
@@ -1878,6 +1886,7 @@ static ssize_t v86d_store(struct device_driver *dev, const char *buf,
 	return count;
 }
 static DRIVER_ATTR_RW(v86d);
+#endif
 
 static int uvesafb_init(void)
 {
@@ -1998,8 +2007,11 @@ MODULE_PARM_DESC(mode_option,
 module_param(vbemode, ushort, 0);
 MODULE_PARM_DESC(vbemode,
 	"VBE mode number to set, overrides the 'mode' option");
+
+#ifndef CONFIG_BHV_CONST_CALL_USERMODEHELPER_MODULES
 module_param_string(v86d, v86d_path, PATH_MAX, 0660);
 MODULE_PARM_DESC(v86d, "Path to the v86d userspace helper.");
+#endif
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Michal Januszewski <spock@gentoo.org>");
diff --git fs/aio.c fs/aio.c
index 3e3bf6fdc5a..fa75ef11e3a 100644
--- fs/aio.c
+++ fs/aio.c
@@ -163,6 +163,8 @@ struct kioctx {
 	struct page		*internal_pages[AIO_RING_PAGES];
 	struct file		*aio_ring_file;
 
+	struct mm_struct        *owner;
+
 	unsigned		id;
 };
 
@@ -776,6 +778,8 @@ static struct kioctx *ioctx_alloc(unsigned nr_events)
 	if (!ctx)
 		return ERR_PTR(-ENOMEM);
 
+	ctx->owner = mm;
+
 	ctx->max_reqs = max_reqs;
 
 	spin_lock_init(&ctx->ctx_lock);
@@ -1130,6 +1134,9 @@ static void aio_complete(struct aio_kiocb *iocb)
 	struct io_event	*ev_page, *event;
 	unsigned tail, pos, head;
 	unsigned long	flags;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	/*
 	 * Add a completion event to the ring buffer. Must be done holding
@@ -1147,8 +1154,17 @@ static void aio_complete(struct aio_kiocb *iocb)
 	ev_page = kmap_atomic(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 	event = ev_page + pos % AIO_EVENTS_PER_PAGE;
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(ctx->owner->owner);
+#endif
+
 	*event = iocb->ki_res;
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	kunmap_atomic(ev_page);
 	flush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);
 
diff --git fs/attr.c fs/attr.c
index 3172f57f71f..d8f366a59c6 100644
--- fs/attr.c
+++ fs/attr.c
@@ -18,6 +18,8 @@
 #include <linux/evm.h>
 #include <linux/ima.h>
 
+#include <bhv/inode.h>
+
 #include "internal.h"
 
 /**
@@ -504,6 +506,8 @@ int notify_change(struct user_namespace *mnt_userns, struct dentry *dentry,
 		fsnotify_change(dentry, ia_valid);
 		ima_inode_post_setattr(mnt_userns, dentry);
 		evm_inode_post_setattr(dentry, ia_valid);
+		/* XXX: Make an LSM hook out of me! */
+		bhv_inode_post_setattr(dentry, ia_valid, mode);
 	}
 
 	return error;
diff --git fs/coredump.c fs/coredump.c
index 4d332f14713..2a9df4f099a 100644
--- fs/coredump.c
+++ fs/coredump.c
@@ -58,7 +58,11 @@ static void free_vma_snapshot(struct coredump_params *cprm);
 
 static int core_uses_pid;
 static unsigned int core_pipe_limit;
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+static char core_pattern[CORENAME_MAX_SIZE] __section(".rodata") = "core";
+#else
 static char core_pattern[CORENAME_MAX_SIZE] = "core";
+#endif
 static int core_name_size = CORENAME_MAX_SIZE;
 
 struct core_name {
@@ -964,7 +968,11 @@ static struct ctl_table coredump_sysctls[] = {
 		.procname	= "core_pattern",
 		.data		= core_pattern,
 		.maxlen		= CORENAME_MAX_SIZE,
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.mode		= 0444,
+#else
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring_coredump,
 	},
 	{
diff --git fs/debugfs/file.c fs/debugfs/file.c
index b38304b4447..12f525e0de7 100644
--- fs/debugfs/file.c
+++ fs/debugfs/file.c
@@ -38,12 +38,13 @@ static ssize_t default_write_file(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations debugfs_noop_file_operations = {
-	.read =		default_read_file,
-	.write =	default_write_file,
-	.open =		simple_open,
-	.llseek =	noop_llseek,
-};
+const struct file_operations
+	debugfs_noop_file_operations __section(".rodata") = {
+		.read = default_read_file,
+		.write = default_write_file,
+		.open = simple_open,
+		.llseek = noop_llseek,
+	};
 
 #define F_DENTRY(filp) ((filp)->f_path.dentry)
 
@@ -209,28 +210,29 @@ static int open_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_open_proxy_file_operations = {
-	.open = open_proxy_open,
-};
+const struct file_operations
+	debugfs_open_proxy_file_operations __section(".rodata") = {
+		.open = open_proxy_open,
+	};
 
 #define PROTO(args...) args
 #define ARGS(args...) args
 
-#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)		\
-static ret_type full_proxy_ ## name(proto)				\
-{									\
-	struct dentry *dentry = F_DENTRY(filp);			\
-	const struct file_operations *real_fops;			\
-	ret_type r;							\
-									\
-	r = debugfs_file_get(dentry);					\
-	if (unlikely(r))						\
-		return r;						\
-	real_fops = debugfs_real_fops(filp);				\
-	r = real_fops->name(args);					\
-	debugfs_file_put(dentry);					\
-	return r;							\
-}
+#define FULL_PROXY_FUNC(name, ret_type, filp, proto, args)                     \
+	ret_type full_proxy_##name(proto)                                      \
+	{                                                                      \
+		struct dentry *dentry = F_DENTRY(filp);                        \
+		const struct file_operations *real_fops;                       \
+		ret_type r;                                                    \
+                                                                               \
+		r = debugfs_file_get(dentry);                                  \
+		if (unlikely(r))                                               \
+			return r;                                              \
+		real_fops = debugfs_real_fops(filp);                           \
+		r = real_fops->name(args);                                     \
+		debugfs_file_put(dentry);                                      \
+		return r;                                                      \
+	}
 
 FULL_PROXY_FUNC(llseek, loff_t, filp,
 		PROTO(struct file *filp, loff_t offset, int whence),
@@ -250,8 +252,7 @@ FULL_PROXY_FUNC(unlocked_ioctl, long, filp,
 		PROTO(struct file *filp, unsigned int cmd, unsigned long arg),
 		ARGS(filp, cmd, arg));
 
-static __poll_t full_proxy_poll(struct file *filp,
-				struct poll_table_struct *wait)
+__poll_t full_proxy_poll(struct file *filp, struct poll_table_struct *wait)
 {
 	struct dentry *dentry = F_DENTRY(filp);
 	__poll_t r = 0;
@@ -266,7 +267,7 @@ static __poll_t full_proxy_poll(struct file *filp,
 	return r;
 }
 
-static int full_proxy_release(struct inode *inode, struct file *filp)
+int full_proxy_release(struct inode *inode, struct file *filp)
 {
 	const struct dentry *dentry = F_DENTRY(filp);
 	const struct file_operations *real_fops = debugfs_real_fops(filp);
@@ -367,9 +368,10 @@ static int full_proxy_open(struct inode *inode, struct file *filp)
 	return r;
 }
 
-const struct file_operations debugfs_full_proxy_file_operations = {
-	.open = full_proxy_open,
-};
+const struct file_operations
+	debugfs_full_proxy_file_operations __section(".rodata") = {
+		.open = full_proxy_open,
+	};
 
 ssize_t debugfs_attr_read(struct file *file, char __user *buf,
 			size_t len, loff_t *ppos)
diff --git fs/exec.c fs/exec.c
index b01434d6a51..62d60dacff5 100644
--- fs/exec.c
+++ fs/exec.c
@@ -46,6 +46,7 @@
 #include <linux/personality.h>
 #include <linux/binfmts.h>
 #include <linux/utsname.h>
+#include <linux/mem_namespace.h>
 #include <linux/pid_namespace.h>
 #include <linux/module.h>
 #include <linux/namei.h>
@@ -74,6 +75,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -283,6 +286,7 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 		goto err;
 
 	mm->stack_vm = mm->total_vm = 1;
+
 	mmap_write_unlock(mm);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
@@ -619,6 +623,9 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 {
 	int len = strnlen(arg, MAX_ARG_STRLEN) + 1 /* terminating NUL */;
 	unsigned long pos = bprm->p;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (len == 0)
 		return -EFAULT;
@@ -643,8 +650,16 @@ int copy_string_kernel(const char *arg, struct linux_binprm *bprm)
 		page = get_arg_page(bprm, pos, 1);
 		if (!page)
 			return -E2BIG;
+
+#ifdef CONFIG_MEM_NS
+		domain = bhv_get_active_domain();
+		bhv_domain_enter(bprm->mm->owner);
+#endif
 		flush_arg_page(bprm, pos & PAGE_MASK, page);
 		memcpy_to_page(page, offset_in_page(pos), arg, bytes_to_copy);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
 		put_arg_page(page);
 	}
 
@@ -861,8 +876,10 @@ int setup_arg_pages(struct linux_binprm *bprm,
 #endif
 	current->mm->start_stack = bprm->p;
 	ret = expand_stack_locked(vma, stack_base);
-	if (ret)
+	if (ret) {
 		ret = -EFAULT;
+		goto out_unlock;
+	}
 
 out_unlock:
 	mmap_write_unlock(mm);
diff --git fs/ext4/dir.c fs/ext4/dir.c
index 3985f8c33f9..45ea60c0a16 100644
--- fs/ext4/dir.c
+++ fs/ext4/dir.c
@@ -664,7 +664,7 @@ int ext4_check_all_de(struct inode *dir, struct buffer_head *bh, void *buf,
 	return 0;
 }
 
-const struct file_operations ext4_dir_operations = {
+const struct file_operations ext4_dir_operations __section(".rodata") = {
 	.llseek		= ext4_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= ext4_readdir,
diff --git fs/ext4/file.c fs/ext4/file.c
index 18f5fd2a163..82d777abc8a 100644
--- fs/ext4/file.c
+++ fs/ext4/file.c
@@ -910,7 +910,7 @@ loff_t ext4_llseek(struct file *file, loff_t offset, int whence)
 	return vfs_setpos(file, offset, maxbytes);
 }
 
-const struct file_operations ext4_file_operations = {
+const struct file_operations ext4_file_operations __section(".rodata") = {
 	.llseek		= ext4_llseek,
 	.read_iter	= ext4_file_read_iter,
 	.write_iter	= ext4_file_write_iter,
diff --git fs/inode.c fs/inode.c
index 8cfda7a6d59..b18962ffdb1 100644
--- fs/inode.c
+++ fs/inode.c
@@ -23,6 +23,8 @@
 #include <trace/events/writeback.h>
 #include "internal.h"
 
+#include <bhv/inode.h>
+
 /*
  * Inode locking rules:
  *
@@ -1814,6 +1816,7 @@ void iput(struct inode *inode)
 			mark_inode_dirty_sync(inode);
 			goto retry;
 		}
+		bhv_inode_iput_final(inode);
 		iput_final(inode);
 	}
 }
diff --git fs/libfs.c fs/libfs.c
index aada4e7c871..44bc11316e4 100644
--- fs/libfs.c
+++ fs/libfs.c
@@ -229,7 +229,7 @@ ssize_t generic_read_dir(struct file *filp, char __user *buf, size_t siz, loff_t
 }
 EXPORT_SYMBOL(generic_read_dir);
 
-const struct file_operations simple_dir_operations = {
+const struct file_operations simple_dir_operations __section(".rodata") = {
 	.open		= dcache_dir_open,
 	.release	= dcache_dir_close,
 	.llseek		= dcache_dir_lseek,
@@ -1355,7 +1355,7 @@ static int empty_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-static const struct file_operations empty_dir_operations = {
+const struct file_operations empty_dir_operations __section(".rodata") = {
 	.llseek		= empty_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= empty_dir_readdir,
diff --git fs/nfsd/nfs4recover.c fs/nfsd/nfs4recover.c
index 78b8cd9651d..6fc5b2d08e6 100644
--- fs/nfsd/nfs4recover.c
+++ fs/nfsd/nfs4recover.c
@@ -1663,11 +1663,15 @@ static const struct nfsd4_client_tracking_ops nfsd4_cld_tracking_ops_v2 = {
 	.msglen		= sizeof(struct cld_msg_v2),
 };
 
+#ifdef BHV_CONFIG_CONST_CALL_USERMODEHELPER_MODULES
+static const char cltrack_prog[] __section(".rodata") = "/sbin/nfsdcltrack";
+#else
 /* upcall via usermodehelper */
 static char cltrack_prog[PATH_MAX] = "/sbin/nfsdcltrack";
 module_param_string(cltrack_prog, cltrack_prog, sizeof(cltrack_prog),
 			S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(cltrack_prog, "Path to the nfsdcltrack upcall program");
+#endif
 
 static bool cltrack_legacy_disable;
 module_param(cltrack_legacy_disable, bool, S_IRUGO|S_IWUSR);
diff --git fs/proc/array.c fs/proc/array.c
index d210b2f8b7e..e21b3eb5aed 100644
--- fs/proc/array.c
+++ fs/proc/array.c
@@ -803,7 +803,7 @@ static int children_seq_open(struct inode *inode, struct file *file)
 	return seq_open(file, &children_seq_ops);
 }
 
-const struct file_operations proc_tid_children_operations = {
+const struct file_operations proc_tid_children_operations __section(".rodata") = {
 	.open    = children_seq_open,
 	.read    = seq_read,
 	.llseek  = seq_lseek,
diff --git fs/proc/base.c fs/proc/base.c
index 74442e01793..938e44dc6a3 100644
--- fs/proc/base.c
+++ fs/proc/base.c
@@ -372,7 +372,7 @@ static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_pid_cmdline_ops = {
+const struct file_operations proc_pid_cmdline_ops __section(".rodata") = {
 	.read	= proc_pid_cmdline_read,
 	.llseek	= generic_file_llseek,
 };
@@ -536,7 +536,7 @@ static ssize_t lstats_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_lstats_operations = {
+const struct file_operations proc_lstats_operations __section(".rodata") = {
 	.open		= lstats_open,
 	.read		= seq_read,
 	.write		= lstats_write,
@@ -785,7 +785,7 @@ static int proc_single_open(struct inode *inode, struct file *filp)
 	return single_open(filp, proc_single_show, inode);
 }
 
-static const struct file_operations proc_single_file_operations = {
+const struct file_operations proc_single_file_operations __section(".rodata") = {
 	.open		= proc_single_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -925,7 +925,7 @@ static int mem_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_mem_operations = {
+const struct file_operations proc_mem_operations __section(".rodata") = {
 	.llseek		= mem_lseek,
 	.read		= mem_read,
 	.write		= mem_write,
@@ -1001,7 +1001,7 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_environ_operations = {
+const struct file_operations proc_environ_operations __section(".rodata") = {
 	.open		= environ_open,
 	.read		= environ_read,
 	.llseek		= generic_file_llseek,
@@ -1028,7 +1028,7 @@ static ssize_t auxv_read(struct file *file, char __user *buf,
 				       nwords * sizeof(mm->saved_auxv[0]));
 }
 
-static const struct file_operations proc_auxv_operations = {
+const struct file_operations proc_auxv_operations __section(".rodata") = {
 	.open		= auxv_open,
 	.read		= auxv_read,
 	.llseek		= generic_file_llseek,
@@ -1188,7 +1188,7 @@ static ssize_t oom_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_adj_operations = {
+const struct file_operations proc_oom_adj_operations __section(".rodata") = {
 	.read		= oom_adj_read,
 	.write		= oom_adj_write,
 	.llseek		= generic_file_llseek,
@@ -1239,7 +1239,7 @@ static ssize_t oom_score_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_score_adj_operations = {
+const struct file_operations proc_oom_score_adj_operations __section(".rodata") = {
 	.read		= oom_score_adj_read,
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
@@ -1307,7 +1307,7 @@ static ssize_t proc_loginuid_write(struct file * file, const char __user * buf,
 	return count;
 }
 
-static const struct file_operations proc_loginuid_operations = {
+const struct file_operations proc_loginuid_operations __section(".rodata") = {
 	.read		= proc_loginuid_read,
 	.write		= proc_loginuid_write,
 	.llseek		= generic_file_llseek,
@@ -1329,7 +1329,7 @@ static ssize_t proc_sessionid_read(struct file * file, char __user * buf,
 	return simple_read_from_buffer(buf, count, ppos, tmpbuf, length);
 }
 
-static const struct file_operations proc_sessionid_operations = {
+const struct file_operations proc_sessionid_operations __section(".rodata") = {
 	.read		= proc_sessionid_read,
 	.llseek		= generic_file_llseek,
 };
@@ -1384,7 +1384,7 @@ static ssize_t proc_fault_inject_write(struct file * file,
 	return count;
 }
 
-static const struct file_operations proc_fault_inject_operations = {
+const struct file_operations proc_fault_inject_operations __section(".rodata") = {
 	.read		= proc_fault_inject_read,
 	.write		= proc_fault_inject_write,
 	.llseek		= generic_file_llseek,
@@ -1425,7 +1425,7 @@ static ssize_t proc_fail_nth_read(struct file *file, char __user *buf,
 	return simple_read_from_buffer(buf, count, ppos, numbuf, len);
 }
 
-static const struct file_operations proc_fail_nth_operations = {
+const struct file_operations proc_fail_nth_operations __section(".rodata") = {
 	.read		= proc_fail_nth_read,
 	.write		= proc_fail_nth_write,
 };
@@ -1474,7 +1474,7 @@ static int sched_open(struct inode *inode, struct file *filp)
 	return single_open(filp, sched_show, inode);
 }
 
-static const struct file_operations proc_pid_sched_operations = {
+const struct file_operations proc_pid_sched_operations __section(".rodata") = {
 	.open		= sched_open,
 	.read		= seq_read,
 	.write		= sched_write,
@@ -1549,7 +1549,7 @@ static int sched_autogroup_open(struct inode *inode, struct file *filp)
 	return ret;
 }
 
-static const struct file_operations proc_pid_sched_autogroup_operations = {
+const struct file_operations proc_pid_sched_autogroup_operations __section(".rodata") = {
 	.open		= sched_autogroup_open,
 	.read		= seq_read,
 	.write		= sched_autogroup_write,
@@ -1652,7 +1652,7 @@ static int timens_offsets_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timens_offsets_show, inode);
 }
 
-static const struct file_operations proc_timens_offsets_operations = {
+const struct file_operations proc_timens_offsets_operations __section(".rodata") = {
 	.open		= timens_offsets_open,
 	.read		= seq_read,
 	.write		= timens_offsets_write,
@@ -1711,7 +1711,7 @@ static int comm_open(struct inode *inode, struct file *filp)
 	return single_open(filp, comm_show, inode);
 }
 
-static const struct file_operations proc_pid_set_comm_operations = {
+const struct file_operations proc_pid_set_comm_operations __section(".rodata") = {
 	.open		= comm_open,
 	.read		= seq_read,
 	.write		= comm_write,
@@ -2434,7 +2434,7 @@ proc_map_files_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-static const struct file_operations proc_map_files_operations = {
+const struct file_operations proc_map_files_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_map_files_readdir,
 	.llseek		= generic_file_llseek,
@@ -2533,7 +2533,7 @@ static int proc_timers_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_timers_operations = {
+const struct file_operations proc_timers_operations __section(".rodata") = {
 	.open		= proc_timers_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -2625,7 +2625,7 @@ static int timerslack_ns_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timerslack_ns_show, inode);
 }
 
-static const struct file_operations proc_pid_set_timerslack_ns_operations = {
+const struct file_operations proc_pid_set_timerslack_ns_operations __section(".rodata") = {
 	.open		= timerslack_ns_open,
 	.read		= seq_read,
 	.write		= timerslack_ns_write,
@@ -2798,7 +2798,7 @@ static ssize_t proc_pid_attr_write(struct file * file, const char __user * buf,
 	return rv;
 }
 
-static const struct file_operations proc_pid_attr_operations = {
+const struct file_operations proc_pid_attr_operations __section(".rodata") = {
 	.open		= proc_pid_attr_open,
 	.read		= proc_pid_attr_read,
 	.write		= proc_pid_attr_write,
@@ -2874,7 +2874,7 @@ static int proc_attr_dir_readdir(struct file *file, struct dir_context *ctx)
 				   attr_dir_stuff, ARRAY_SIZE(attr_dir_stuff));
 }
 
-static const struct file_operations proc_attr_dir_operations = {
+const struct file_operations proc_attr_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_attr_dir_readdir,
 	.llseek		= generic_file_llseek,
@@ -2966,7 +2966,7 @@ static ssize_t proc_coredump_filter_write(struct file *file,
 	return count;
 }
 
-static const struct file_operations proc_coredump_filter_operations = {
+const struct file_operations proc_coredump_filter_operations __section(".rodata") = {
 	.read		= proc_coredump_filter_read,
 	.write		= proc_coredump_filter_write,
 	.llseek		= generic_file_llseek,
@@ -3089,7 +3089,7 @@ static int proc_projid_map_open(struct inode *inode, struct file *file)
 	return proc_id_map_open(inode, file, &proc_projid_seq_operations);
 }
 
-static const struct file_operations proc_uid_map_operations = {
+const struct file_operations proc_uid_map_operations __section(".rodata") = {
 	.open		= proc_uid_map_open,
 	.write		= proc_uid_map_write,
 	.read		= seq_read,
@@ -3097,7 +3097,7 @@ static const struct file_operations proc_uid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_gid_map_operations = {
+const struct file_operations proc_gid_map_operations __section(".rodata") = {
 	.open		= proc_gid_map_open,
 	.write		= proc_gid_map_write,
 	.read		= seq_read,
@@ -3105,7 +3105,7 @@ static const struct file_operations proc_gid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_projid_map_operations = {
+const struct file_operations proc_projid_map_operations __section(".rodata") = {
 	.open		= proc_projid_map_open,
 	.write		= proc_projid_map_write,
 	.read		= seq_read,
@@ -3156,7 +3156,7 @@ static int proc_setgroups_release(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static const struct file_operations proc_setgroups_operations = {
+const struct file_operations proc_setgroups_operations __section(".rodata") = {
 	.open		= proc_setgroups_open,
 	.write		= proc_setgroups_write,
 	.read		= seq_read,
@@ -3232,7 +3232,7 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 /*
  * Thread groups
  */
-static const struct file_operations proc_task_operations;
+const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
 static const struct pid_entry tgid_base_stuff[] = {
@@ -3357,7 +3357,7 @@ static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
 				   tgid_base_stuff, ARRAY_SIZE(tgid_base_stuff));
 }
 
-static const struct file_operations proc_tgid_base_operations = {
+const struct file_operations proc_tgid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3703,7 +3703,7 @@ static struct dentry *proc_tid_base_lookup(struct inode *dir, struct dentry *den
 				  tid_base_stuff + ARRAY_SIZE(tid_base_stuff));
 }
 
-static const struct file_operations proc_tid_base_operations = {
+const struct file_operations proc_tid_base_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3915,7 +3915,7 @@ static const struct inode_operations proc_task_inode_operations = {
 	.permission	= proc_pid_permission,
 };
 
-static const struct file_operations proc_task_operations = {
+const struct file_operations proc_task_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_task_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/fd.c fs/proc/fd.c
index 913bef0d2a3..16998db13c3 100644
--- fs/proc/fd.c
+++ fs/proc/fd.c
@@ -99,7 +99,7 @@ static int seq_fdinfo_open(struct inode *inode, struct file *file)
 	return single_open(file, seq_show, inode);
 }
 
-static const struct file_operations proc_fdinfo_file_operations = {
+const struct file_operations proc_fdinfo_file_operations __section(".rodata") = {
 	.open		= seq_fdinfo_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -284,7 +284,7 @@ static int proc_readfd(struct file *file, struct dir_context *ctx)
 	return proc_readfd_common(file, ctx, proc_fd_instantiate);
 }
 
-const struct file_operations proc_fd_operations = {
+const struct file_operations proc_fd_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfd,
 	.llseek		= generic_file_llseek,
@@ -373,7 +373,7 @@ const struct inode_operations proc_fdinfo_inode_operations = {
 	.setattr	= proc_setattr,
 };
 
-const struct file_operations proc_fdinfo_operations = {
+const struct file_operations proc_fdinfo_operations __section(".rodata") = {
 	.open		= proc_open_fdinfo,
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_readfdinfo,
diff --git fs/proc/generic.c fs/proc/generic.c
index 587b91d9d99..92bdc16b042 100644
--- fs/proc/generic.c
+++ fs/proc/generic.c
@@ -338,7 +338,7 @@ int proc_readdir(struct file *file, struct dir_context *ctx)
  * use the in-memory "struct proc_dir_entry" tree to parse
  * the /proc directory.
  */
-static const struct file_operations proc_dir_operations = {
+const struct file_operations proc_dir_operations __section(".rodata") = {
 	.llseek			= generic_file_llseek,
 	.read			= generic_read_dir,
 	.iterate_shared		= proc_readdir,
diff --git fs/proc/inode.c fs/proc/inode.c
index f495fdb3915..3768a494d3f 100644
--- fs/proc/inode.c
+++ fs/proc/inode.c
@@ -575,7 +575,7 @@ static int proc_reg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_reg_file_ops = {
+const struct file_operations proc_reg_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -587,7 +587,7 @@ static const struct file_operations proc_reg_file_ops = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops = {
+const struct file_operations proc_iter_file_ops __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.write		= proc_reg_write,
@@ -601,7 +601,7 @@ static const struct file_operations proc_iter_file_ops = {
 };
 
 #ifdef CONFIG_COMPAT
-static const struct file_operations proc_reg_file_ops_compat = {
+const struct file_operations proc_reg_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -614,7 +614,7 @@ static const struct file_operations proc_reg_file_ops_compat = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops_compat = {
+const struct file_operations proc_iter_file_ops_compat __section(".rodata") = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/proc/namespaces.c fs/proc/namespaces.c
index 8e159fc78c0..df908ffc838 100644
--- fs/proc/namespaces.c
+++ fs/proc/namespaces.c
@@ -11,7 +11,6 @@
 #include <linux/user_namespace.h>
 #include "internal.h"
 
-
 static const struct proc_ns_operations *ns_entries[] = {
 #ifdef CONFIG_NET_NS
 	&netns_operations,
@@ -23,8 +22,7 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&ipcns_operations,
 #endif
 #ifdef CONFIG_PID_NS
-	&pidns_operations,
-	&pidns_for_children_operations,
+	&pidns_operations,    &pidns_for_children_operations,
 #endif
 #ifdef CONFIG_USER_NS
 	&userns_operations,
@@ -34,8 +32,10 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&cgroupns_operations,
 #endif
 #ifdef CONFIG_TIME_NS
-	&timens_operations,
-	&timens_for_children_operations,
+	&timens_operations,   &timens_for_children_operations,
+#endif
+#ifdef CONFIG_MEM_NS
+	&memns_operations,
 #endif
 };
 
@@ -142,7 +142,7 @@ static int proc_ns_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-const struct file_operations proc_ns_dir_operations = {
+const struct file_operations proc_ns_dir_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_ns_dir_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/proc_net.c fs/proc/proc_net.c
index 856839b8ae8..ad7cb7f772d 100644
--- fs/proc/proc_net.c
+++ fs/proc/proc_net.c
@@ -337,7 +337,7 @@ static int proc_tgid_net_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-const struct file_operations proc_net_operations = {
+const struct file_operations proc_net_operations __section(".rodata") = {
 	.llseek		= generic_file_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_net_readdir,
diff --git fs/proc/proc_sysctl.c fs/proc/proc_sysctl.c
index 4a4c04a3b1a..a3a58fb847d 100644
--- fs/proc/proc_sysctl.c
+++ fs/proc/proc_sysctl.c
@@ -23,9 +23,9 @@
 	for ((entry) = (table); (entry)->procname; (entry)++)
 
 static const struct dentry_operations proc_sys_dentry_operations;
-static const struct file_operations proc_sys_file_operations;
+const struct file_operations proc_sys_file_operations;
 static const struct inode_operations proc_sys_inode_operations;
-static const struct file_operations proc_sys_dir_file_operations;
+const struct file_operations proc_sys_dir_file_operations;
 static const struct inode_operations proc_sys_dir_operations;
 
 /* Support for permanently empty directories */
@@ -864,7 +864,7 @@ static int proc_sys_getattr(struct user_namespace *mnt_userns,
 	return 0;
 }
 
-static const struct file_operations proc_sys_file_operations = {
+const struct file_operations proc_sys_file_operations __section(".rodata") = {
 	.open		= proc_sys_open,
 	.poll		= proc_sys_poll,
 	.read_iter	= proc_sys_read,
@@ -874,7 +874,7 @@ static const struct file_operations proc_sys_file_operations = {
 	.llseek		= default_llseek,
 };
 
-static const struct file_operations proc_sys_dir_file_operations = {
+const struct file_operations proc_sys_dir_file_operations __section(".rodata") = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_sys_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/root.c fs/proc/root.c
index 3c2ee3eb113..c2cf19a29a1 100644
--- fs/proc/root.c
+++ fs/proc/root.c
@@ -344,7 +344,7 @@ static int proc_root_readdir(struct file *file, struct dir_context *ctx)
  * <pid> directories. Thus we don't use the generic
  * directory handling functions for that..
  */
-static const struct file_operations proc_root_operations = {
+const struct file_operations proc_root_operations __section(".rodata") = {
 	.read		 = generic_read_dir,
 	.iterate_shared	 = proc_root_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/task_mmu.c fs/proc/task_mmu.c
index a954305fbc3..e6379171545 100644
--- fs/proc/task_mmu.c
+++ fs/proc/task_mmu.c
@@ -363,7 +363,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return do_maps_open(inode, file, &proc_pid_maps_op);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1048,14 +1048,14 @@ static int smaps_rollup_release(struct inode *inode, struct file *file)
 	return single_release(inode, file);
 }
 
-const struct file_operations proc_pid_smaps_operations = {
+const struct file_operations proc_pid_smaps_operations __section(".rodata") = {
 	.open		= pid_smaps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
 	.release	= proc_map_release,
 };
 
-const struct file_operations proc_pid_smaps_rollup_operations = {
+const struct file_operations proc_pid_smaps_rollup_operations __section(".rodata") = {
 	.open		= smaps_rollup_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -1317,7 +1317,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-const struct file_operations proc_clear_refs_operations = {
+const struct file_operations proc_clear_refs_operations __section(".rodata") = {
 	.write		= clear_refs_write,
 	.llseek		= noop_llseek,
 };
@@ -1753,7 +1753,7 @@ static int pagemap_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations proc_pagemap_operations = {
+const struct file_operations proc_pagemap_operations __section(".rodata") = {
 	.llseek		= mem_lseek, /* borrow this */
 	.read		= pagemap_read,
 	.open		= pagemap_open,
@@ -2016,7 +2016,7 @@ static int pid_numa_maps_open(struct inode *inode, struct file *file)
 				sizeof(struct numa_maps_private));
 }
 
-const struct file_operations proc_pid_numa_maps_operations = {
+const struct file_operations proc_pid_numa_maps_operations __section(".rodata") = {
 	.open		= pid_numa_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc/task_nommu.c fs/proc/task_nommu.c
index dc05780f93e..f93f9f5486d 100644
--- fs/proc/task_nommu.c
+++ fs/proc/task_nommu.c
@@ -300,7 +300,7 @@ static int pid_maps_open(struct inode *inode, struct file *file)
 	return maps_open(inode, file, &proc_pid_maps_ops);
 }
 
-const struct file_operations proc_pid_maps_operations = {
+const struct file_operations proc_pid_maps_operations __section(".rodata") = {
 	.open		= pid_maps_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc_namespace.c fs/proc_namespace.c
index 846f9455ae2..3ab28d4f085 100644
--- fs/proc_namespace.c
+++ fs/proc_namespace.c
@@ -321,7 +321,7 @@ static int mountstats_open(struct inode *inode, struct file *file)
 	return mounts_open_common(inode, file, show_vfsstat);
 }
 
-const struct file_operations proc_mounts_operations = {
+const struct file_operations proc_mounts_operations __section(".rodata") = {
 	.open		= mounts_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -330,7 +330,7 @@ const struct file_operations proc_mounts_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountinfo_operations = {
+const struct file_operations proc_mountinfo_operations __section(".rodata") = {
 	.open		= mountinfo_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
@@ -339,7 +339,7 @@ const struct file_operations proc_mountinfo_operations = {
 	.poll		= mounts_poll,
 };
 
-const struct file_operations proc_mountstats_operations = {
+const struct file_operations proc_mountstats_operations __section(".rodata") = {
 	.open		= mountstats_open,
 	.read_iter	= seq_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/read_write.c fs/read_write.c
index 7a2ff6157ed..b0b7907a60a 100644
--- fs/read_write.c
+++ fs/read_write.c
@@ -25,6 +25,8 @@
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
 
+#include "bhv/file_protection.h"
+
 const struct file_operations generic_ro_fops = {
 	.llseek		= generic_file_llseek,
 	.read_iter	= generic_file_read_iter,
@@ -584,6 +586,9 @@ ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_
 		ret = new_sync_write(file, buf, count, pos);
 	else
 		ret = -EINVAL;
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_modify(file);
 		add_wchar(current, ret);
@@ -887,6 +892,9 @@ ssize_t vfs_iocb_iter_write(struct file *file, struct kiocb *iocb,
 		return ret;
 
 	ret = call_write_iter(file, iocb, iter);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	if (ret > 0)
 		fsnotify_modify(file);
 
@@ -897,9 +905,14 @@ EXPORT_SYMBOL(vfs_iocb_iter_write);
 ssize_t vfs_iter_write(struct file *file, struct iov_iter *iter, loff_t *ppos,
 		rwf_t flags)
 {
+	ssize_t ret = 0;
 	if (!file->f_op->write_iter)
 		return -EINVAL;
-	return do_iter_write(file, iter, ppos, flags);
+	ret = do_iter_write(file, iter, ppos, flags);
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
+	return ret;
 }
 EXPORT_SYMBOL(vfs_iter_write);
 
@@ -935,6 +948,9 @@ static ssize_t vfs_writev(struct file *file, const struct iovec __user *vec,
 		file_end_write(file);
 		kfree(iov);
 	}
+
+	bhv_check_file_dirty_cred(file, MAY_WRITE);
+
 	return ret;
 }
 
@@ -1540,6 +1556,9 @@ ssize_t vfs_copy_file_range(struct file *file_in, loff_t pos_in,
 				      flags);
 
 done:
+
+	bhv_check_file_dirty_cred(file_out, MAY_WRITE);
+
 	if (ret > 0) {
 		fsnotify_access(file_in);
 		add_rchar(current, ret);
diff --git fs/xfs/xfs_file.c fs/xfs/xfs_file.c
index 595a5bcf46b..5d73f2023dc 100644
--- fs/xfs/xfs_file.c
+++ fs/xfs/xfs_file.c
@@ -1433,7 +1433,7 @@ xfs_file_mmap(
 	return 0;
 }
 
-const struct file_operations xfs_file_operations = {
+const struct file_operations xfs_file_operations __section(".rodata") = {
 	.llseek		= xfs_file_llseek,
 	.read_iter	= xfs_file_read_iter,
 	.write_iter	= xfs_file_write_iter,
@@ -1455,7 +1455,7 @@ const struct file_operations xfs_file_operations = {
 	.remap_file_range = xfs_file_remap_range,
 };
 
-const struct file_operations xfs_dir_file_operations = {
+const struct file_operations xfs_dir_file_operations __section(".rodata") = {
 	.open		= xfs_dir_open,
 	.read		= generic_read_dir,
 	.iterate_shared	= xfs_file_readdir,
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index db13bb620f5..33aae23e8fa 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifdef CONFIG_HAVE_FUNCTION_DESCRIPTORS
 void *dereference_function_descriptor(void *ptr);
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 1d1f480a5e9..c1cdcc1f7a3 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -356,9 +356,13 @@
 	*(.data.once)							\
 	__end_once = .;							\
 	STRUCT_ALIGN();							\
+	. = ALIGN(PAGE_SIZE);						\
+	__start_bhv_tp_vault = .;					\
 	*(__tracepoints)						\
+	__end_bhv_tp_vault = .;						\
+	. = ALIGN(PAGE_SIZE);						\
 	/* implement dynamic printk debug */				\
-	. = ALIGN(8);							\
+	/*. = ALIGN(8);	*/						\
 	__start___dyndbg_classes = .;					\
 	KEEP(*(__dyndbg_classes))					\
 	__stop___dyndbg_classes = .;					\
@@ -406,6 +410,7 @@
 	__end_init_task = .;
 
 #define JUMP_TABLE_DATA							\
+	. = ALIGN(PAGE_SIZE);           				\
 	. = ALIGN(8);							\
 	__start___jump_table = .;					\
 	KEEP(*(__jump_table))						\
@@ -419,6 +424,7 @@
 	__stop_static_call_sites = .;					\
 	__start_static_call_tramp_key = .;				\
 	KEEP(*(.static_call_tramp_key))					\
+	. = ALIGN(PAGE_SIZE); 				            	\
 	__stop_static_call_tramp_key = .;
 #else
 #define STATIC_CALL_DATA
@@ -570,14 +576,96 @@
 	. = ALIGN((align));						\
 	__end_rodata = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_start = .;						\
+	*(.bhv.text)							\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_text_end = .;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define BHV_VAULT_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_start = .;			\
+	*(.bhv.vault.text.##vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_text_##vault_name##_end = .;
+
+#define BHV_VAULT_REF_TEXT(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_start = .;			\
+	*(.ref.text.bhv.vault.text.##vault_name)			\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ref_text_##vault_name##_end = .;
+
+#define BHV_VAULT_SHARED_TEXT(vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_start = .;		\
+	*(.bhv.vault.shared.text.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_shared_text_##vault_name##_end = .;
+
+#define BHV_VAULT_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_data_##vault_name##_start = .; 			\
+	*(.bhv.vault.data.##vault_name)					\
+	. = ALIGN(PAGE_SIZE); 						\
+	__bhv_vault_data_##vault_name##_end = .;
+
+#define BHV_VAULT_RO_DATA(vault_name)					\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_start = .;			\
+	*(.bhv.vault.rodata.##vault_name)				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_##vault_name##_eps_start = .;			\
+	*(.bhv.vault.##vault_name##.eps)				\
+	__bhv_vault_##vault_name##_eps_end = .;				\
+	. = ALIGN(PAGE_SIZE);						\
+	__bhv_vault_ro_data_##vault_name##_end = .;
+
+#define BHV_VAULT_ENTRIES()	. = ALIGN(8);				\
+			__start_vault_entries = .;			\
+			KEEP(*(.vault_entries))				\
+			__stop_vault_entries = .;			\
+			__start_vault_return_sites = .;			\
+			KEEP(*(.vault_sites))				\
+			__stop_vault_return_sites = .;			\
+			__start_vault_rethunk_sites = .;		\
+			KEEP(*(.vault_rethunks))			\
+			__stop_vault_rethunk_sites = .;
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define BHV_VAULT_TEXT(vault_name)
+#define BHV_VAULT_DATA(vault_name)
+#define BHV_VAULT_RO_DATA(vault_name)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else /* !CONFIG_BHV_VAS */
+
+#define BHV_TEXT
+
+#endif /* CONFIG_BHV_VAS */
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+#define BHV_VAULT_ENTRIES()
+#define BHV_VAULT_REF_TEXT(vault_name)
+#define BHV_VAULT_SHARED_TEXT(vault_name)
+
+#endif
+
 
 /*
  * Non-instrumentable text section
  */
 #define NOINSTR_TEXT							\
-		ALIGN_FUNCTION();					\
+		. = ALIGN(PAGE_SIZE);					\
 		__noinstr_text_start = .;				\
 		*(.noinstr.text)					\
+		. = ALIGN(PAGE_SIZE);                                   \
 		__noinstr_text_end = .;
 
 /*
@@ -597,7 +685,9 @@
 		NOINSTR_TEXT						\
 		*(.text..refcount)					\
 		*(.ref.text)						\
+		BHV_VAULT_REF_TEXT(jump_label)				\
 		*(.text.asan.* .text.tsan.*)				\
+		BHV_VAULT_SHARED_TEXT(jump_label)			\
 	MEM_KEEP(init.text*)						\
 
 
@@ -718,6 +808,7 @@
 	MEM_DISCARD(init.data*)						\
 	KERNEL_CTORS()							\
 	MCOUNT_REC()							\
+	BHV_VAULT_ENTRIES()                             \
 	*(.init.rodata .init.rodata.*)					\
 	FTRACE_EVENTS()							\
 	TRACE_SYSCALLS()						\
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 00000000000..01f3c8a1460
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#if defined CONFIG_BHV_VAS && !defined VASKM
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_acl(void);
+/************************************************************/
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_PROC_ACL, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_DRIVER_ACL, bhv_configuration_bitmap);
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else // defined CONFIG_BHV_VAS && !defined VASKM
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_block_driver(const char *target)
+{
+	return false;
+}
+
+static inline bool bhv_block_process(const char *target)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS && !defined VASKM
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 00000000000..c26a22d412c
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,92 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define __bhv_text
+#else
+#define __bhv_text __section(".bhv.text") noinline
+#endif
+
+#ifndef VASKM // inside kernel tree
+#define __bhv_data __section(".bhv.data") noinline
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+#define __init_km
+#else // out of tree
+#define __init_km __init
+#endif // VASKM
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+extern bool bhv_initialized __ro_after_init;
+extern unsigned long *bhv_configuration_bitmap __ro_after_init;
+
+static inline bool is_bhv_initialized(void)
+{
+	BUG_ON(bhv_initialized && bhv_configuration_bitmap == NULL);
+	return bhv_initialized;
+}
+
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr);
+
+extern bool __bhv_init_done;
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys(void *address)
+{
+	BUG_ON(!address);
+
+	rcu_read_lock_sched_notrace();
+	if (__bhv_init_done && is_vmalloc_or_module_addr(address)) {
+		struct page *p = bhv_vmalloc_to_page(address);
+		uint64_t offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		rcu_read_unlock_sched_notrace();
+
+		BUG_ON(!p);
+		BUG_ON(PageCompound(p));
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	} else {
+		rcu_read_unlock_sched_notrace();
+		return virt_to_phys(address);
+	}
+}
+#else /* CONFIG_BHV_VAS */
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+
+static inline phys_addr_t bhv_virt_to_phys(volatile void *address)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 00000000000..5c58285aa6f
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BHV_VAS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/bhv_trace.h include/bhv/bhv_trace.h
new file mode 100644
index 00000000000..bee38e49503
--- /dev/null
+++ include/bhv/bhv_trace.h
@@ -0,0 +1,50 @@
+#ifdef CONFIG_BHV_TRACEPOINTS
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM bhv
+
+#if !defined(__BHV_TRACE_H__) || defined(TRACE_HEADER_MULTI_READ)
+#define __BHV_TRACE_H__
+
+#include <linux/tracepoint.h>
+
+DECLARE_EVENT_CLASS(bhv_trace_class,
+	TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+	TP_ARGS(bhv_target, bhv_backend, op),
+
+	TP_STRUCT__entry(
+		__field(uint32_t, bhv_target)
+		__field(uint32_t, bhv_backend)
+		__field(uint32_t, op)
+	),
+
+	TP_fast_assign(
+		__entry->bhv_target = bhv_target;
+		__entry->bhv_backend = bhv_backend;
+		__entry->op = op;
+	),
+
+	TP_printk("BHV hypercall: target=%u backend=%u op=%u",
+		  __entry->bhv_target, __entry->bhv_backend, __entry->op)
+);
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_start,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+DEFINE_EVENT(bhv_trace_class, bhv_hypercall_end,
+	     TP_PROTO(uint32_t bhv_target, uint32_t bhv_backend, uint32_t op),
+             TP_ARGS(bhv_target, bhv_backend, op));
+
+#endif /* __BHV_TRACE_H__ */
+
+/* This part must be outside protection */
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+
+#define TRACE_INCLUDE_PATH ../../include/bhv
+#define TRACE_INCLUDE_FILE bhv_trace
+
+#include <trace/define_trace.h>
+
+#endif /* CONFIG_BHV_TRACEPOINTS */
diff --git include/bhv/container_integrity.h include/bhv/container_integrity.h
new file mode 100644
index 00000000000..1d93756bf03
--- /dev/null
+++ include/bhv/container_integrity.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_CCONTAINER_INTEGRITY_H__
+#define __BHV_CCONTAINER_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h>
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+static inline bool bhv_container_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return false;
+}
+
+void __init bhv_mm_init_container_integrity(void);
+
+int bhv_container_integrity_file_permission(struct file *file, int mask);
+int bhv_container_integrity_bprm_check_security(struct linux_binprm *bprm);
+int bhv_container_integrity_inode_setxattr(const char *name);
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_CCONTAINER_INTEGRITY_H__ */
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 00000000000..16574f4022e
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_CREDS, bhv_configuration_bitmap);
+}
+
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+#ifdef VASKM // out of tree
+int bhv_cred_assign_init(struct task_struct *t);
+#endif // VASKM
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+/******************************************************************
+ * init
+ ******************************************************************/
+int __init bhv_init_cred(void);
+/******************************************************************/
+
+/******************************************************************
+ * mm_init
+ ******************************************************************/
+void __init bhv_mm_init_cred(void);
+/******************************************************************/
+
+#else /* CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+#ifdef VASKM // out of tree
+static inline int bhv_cred_assign_init(struct task_struct *t) {
+	return 0;
+}
+#endif // VASKM
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+#endif // defined CONFIG_BHV_VAS
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/domain.h include/bhv/domain.h
new file mode 100644
index 00000000000..646d04362c6
--- /dev/null
+++ include/bhv/domain.h
@@ -0,0 +1,214 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_H__
+#define __BHV_DOMAIN_H__
+
+#include <linux/mem_namespace.h>
+#include <linux/sched.h>
+#include <linux/sched/task.h>
+#include <linux/mm_types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain_pt.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#include <asm/bhv/domain.h>
+#endif
+
+#define BHV_INIT_DOMAIN 0UL
+#define BHV_INVALID_DOMAIN (-1UL)
+
+#ifdef CONFIG_MEM_NS
+
+#if defined(CONFIG_X86_64)
+// #define BHV_VAS_DOMAIN_DEBUG 1
+#endif
+
+DECLARE_PER_CPU(uint64_t, bhv_domain_current_domain);
+extern bool bhv_domain_initialized;
+
+/*
+ * As long as the memory namespaces are not part of the official Linux mainline
+ * sources, they re-purpose the PID namespace instantiation request for their
+ * own creation. Once user space tools become aware of Linux memory namespaces,
+ * we will have to remove this function; otherwise, flags requesting both memory
+ * and PID namespaces would create two memory namespaces.
+ */
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	if (flags & (CLONE_NEWMEM | CLONE_NEWPID))
+		return true;
+
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_STRONG_ISOLATION,
+			  bhv_configuration_bitmap);
+}
+
+// Initialized and enabled
+static inline bool bhv_domain_is_active(void)
+{
+	if (!bhv_domain_initialized)
+		return false;
+
+	return bhv_domain_is_enabled();
+}
+
+int bhv_domain_mm_init(void);
+
+int bhv_domain_create(uint64_t *domid);
+void bhv_domain_destroy(uint64_t domid);
+int bhv_domain_switch(uint64_t domid);
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags);
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+				     bool foreign);
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte);
+
+static inline uint64_t bhv_get_active_domain(void)
+{
+	return this_cpu_read(bhv_domain_current_domain);
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return memns_of_task(task)->domain;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+	uint64_t domain_next = bhv_get_domain(next);
+	bhv_domain_switch(domain_next);
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns);
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec);
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline pgd_t *bhv_domain_get_user_pgd(const pgd_t *pgd)
+{
+	pgd_t *pgd_normalized = (pgd_t *)pgd;
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	/*
+	 * We need to ensure that the kernel was not configured to disable KPTI,
+	 * despite CONFIG_PAGE_TABLE_ISOLATION being set. Only then, we can
+	 * normalize the PGD pointer. The normalized PGD pointer ensures that
+	 * BRASS becomes able to always associate both user and kernel memory
+	 * accesses with the virtual address space, and the domain it belongs
+	 * to.
+	 */
+	if (static_cpu_has(X86_FEATURE_PTI))
+		pgd_normalized = bhv_domain_arch_get_user_pgd(pgd_normalized);
+#endif
+	return pgd_normalized;
+}
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+				  struct mm_struct *mm);
+#else
+static inline void bhv_domain_debug_destroy_pgd(struct task_struct *tsk,
+						struct mm_struct *mm)
+{
+}
+#endif
+
+#else /* !CONFIG_MEM_NS */
+
+static inline bool bhv_check_memns_enable_flags(unsigned long flags)
+{
+	return false;
+}
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_domain_create(uint64_t *domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_destroy(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_switch(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_report(const struct task_struct *t,
+				    const struct mm_struct *mm_target,
+				    const struct vm_area_struct *vma,
+				    unsigned int gup_flags)
+{
+	return 0;
+}
+
+static inline bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma,
+						   bool write, bool foreign)
+{
+	return true;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next)
+{
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return 0;
+}
+
+static inline void bhv_domain_set_pte_at_kernel(struct mm_struct *mm,
+						unsigned long addr, pte_t *ptep,
+						pte_t pte)
+{
+}
+#endif /* CONFIG_MEM_NS */
+
+#if !defined CONFIG_MEM_NS || !BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline void bhv_domain_destroy_pgd(struct task_struct *tsk,
+					  struct mm_struct *mm)
+{
+}
+
+static inline int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn,
+					uint64_t nr_pages, bool read,
+					bool write, bool exec)
+{
+	return 0;
+}
+
+static inline int bhv_domain_transfer_mm(struct mm_struct *const mm,
+					 struct nsproxy *const old_ns,
+					 struct nsproxy *const new_ns)
+{
+	return 0;
+}
+
+#endif /* !defined CONFIG_MEM_NS || !BHV_VAS_DOMAIN_SPACES_BASED */
+
+#endif /* __BHV_DOMAIN_H__ */
diff --git include/bhv/domain_pt.h include/bhv/domain_pt.h
new file mode 100644
index 00000000000..f873fab640a
--- /dev/null
+++ include/bhv/domain_pt.h
@@ -0,0 +1,66 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_PT_H__
+#define __BHV_DOMAIN_PT_H__
+
+#define BHV_VAS_DOMAIN_SPACES_BASED 0
+
+#include <asm/page.h>
+
+#if defined(CONFIG_MEM_NS) && BHV_VAS_DOMAIN_SPACES_BASED
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte);
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd);
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud);
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte);
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd);
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud);
+
+#else // !defined(CONFIG_MEM_NS) || !BHV_VAS_DOMAIN_SPACES_BASED
+
+static inline void bhv_domain_set_pte_at(struct mm_struct *mm,
+					 unsigned long addr, pte_t *ptep,
+					 pte_t pte)
+{
+}
+static inline void bhv_domain_set_pmd_at(struct mm_struct *mm,
+					 unsigned long addr, pmd_t *pmdp,
+					 pmd_t pmd)
+{
+}
+static inline void bhv_domain_set_pud_at(struct mm_struct *mm,
+					 unsigned long addr, pud_t *pudp,
+					 pud_t pud)
+{
+}
+
+static inline void bhv_domain_clear_pte(struct mm_struct *mm,
+					unsigned long addr, pte_t *ptep,
+					pte_t pte)
+{
+}
+static inline void bhv_domain_clear_pmd(struct mm_struct *mm,
+					unsigned long addr, pmd_t *pmdp,
+					pmd_t pmd)
+{
+}
+static inline void bhv_domain_clear_pud(struct mm_struct *mm,
+					unsigned long addr, pud_t *pudp,
+					pud_t pud)
+{
+}
+
+#endif // defined(CONFIG_MEM_NS) && BHV_VAS_DOMAIN_SPACES_BASED
+
+#endif // __BHV_DOMAIN_PT_H__
diff --git include/bhv/event.h include/bhv/event.h
new file mode 100644
index 00000000000..dcec17c4c65
--- /dev/null
+++ include/bhv/event.h
@@ -0,0 +1,179 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_EVENT_H__
+#define __BHV_EVENT_H__
+
+#include <linux/cgroup.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/time_namespace.h>
+#include <linux/types.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+static inline int get_file_path(struct path *f_path, char *buf, size_t max_size)
+{
+	char *tmp_buf = NULL;
+	char *path = NULL;
+
+	BUG_ON(f_path == NULL);
+	BUG_ON(buf == NULL);
+
+	/* Avoid allocating this buffer on the stack. */
+	tmp_buf = kmalloc(HypABI__Context__MAX_PATH_SZ, GFP_KERNEL);
+	if (tmp_buf == NULL)
+		return -ENOMEM;
+
+	path = d_path(f_path, tmp_buf, HypABI__Context__MAX_PATH_SZ);
+	if (IS_ERR(path)) {
+		kfree(tmp_buf);
+		return PTR_ERR(path);
+	}
+
+	strncpy(buf, path, max_size);
+	buf[max_size - 1] = '\0';
+
+	kfree(tmp_buf);
+
+	return 0;
+}
+
+static inline int get_path_from_task(struct task_struct *task, char *buf,
+				     bool get_full_path)
+{
+	BUG_ON(task == NULL);
+	BUG_ON(buf == NULL);
+
+	if (!get_full_path || task->active_mm == NULL ||
+	    task->active_mm->exe_file == NULL) {
+		strncpy(buf, task->comm, HypABI__Context__MAX_PATH_SZ);
+		buf[HypABI__Context__MAX_PATH_SZ - 1] = '\0';
+		return 0;
+	}
+
+	return get_file_path(&task->active_mm->exe_file->f_path, buf,
+			     HypABI__Context__MAX_PATH_SZ);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 3, 0)
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	u64 val = current_cred_xxx(cap_effective).val;
+	memcpy(&context->cap_effective, &val, sizeof(context->cap_effective));
+	val = current_cred_xxx(cap_permitted).val;
+	memcpy(&context->cap_permitted, &val, sizeof(context->cap_permitted));
+}
+#else
+static inline void _copy_cap(HypABI__Context__T *context)
+{
+	memcpy(&context->cap_effective, &current_cred_xxx(cap_effective).cap[0],
+	       sizeof(context->cap_effective));
+	memcpy(&context->cap_permitted, &current_cred_xxx(cap_permitted).cap[0],
+	       sizeof(context->cap_permitted));
+}
+#endif
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _bhv_get_ns_inums(struct task_struct *tsk,
+			      HypABI__Context__Inums__T *inums)
+{
+	if (tsk->nsproxy == NULL) {
+		memset(inums, 0, sizeof(HypABI__Context__Inums__T));
+	} else {
+		inums->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		inums->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+		inums->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		rcu_read_lock();
+		inums->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+		inums->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+	}
+}
+#undef NS_DEREF
+
+static int _bhv_get_cgroup_info(struct task_struct *tsk,
+				HypABI__Context__CGroupInfo__T *cgroup)
+{
+	int r;
+	struct cgroup *cgrp;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	cgroup->cgroup_id = cgroup_id(cgrp);
+	r = cgroup_name(cgrp, cgroup->cgroup_name,
+			HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+	rcu_read_unlock();
+	return r;
+}
+
+static inline int populate_event_context(HypABI__Context__T *context,
+					 bool get_full_path)
+{
+	int rv;
+
+	BUG_ON(context == NULL);
+
+	context->vcpu_id = raw_smp_processor_id();
+	context->uid = current_uid().val;
+	context->euid = current_euid().val;
+	context->gid = current_gid().val;
+	context->egid = current_egid().val;
+	_copy_cap(context);
+	context->pid = current->tgid;
+
+	rv = get_path_from_task(current, context->comm, get_full_path);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	if (current->real_parent != NULL) {
+		context->parent_pid = current->real_parent->tgid;
+
+		rv = get_path_from_task(current->real_parent,
+					context->parent_comm, get_full_path);
+		if (rv != 0) {
+			context->valid = false;
+			return rv;
+		}
+	} else {
+		context->parent_pid = 0;
+		context->parent_comm[0] = '\0';
+	}
+
+	rv = _bhv_get_cgroup_info(current, &context->cgroup);
+	if (rv != 0)
+		context->cgroup.cgroup_name[0] = '\0';
+
+	_bhv_get_ns_inums(current, &context->inums);
+
+	context->valid = true;
+	return 0;
+}
+
+#endif /* __BHV_EVENT_H__ */
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 00000000000..f479f2046ac
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void);
+/***********************************************************************/
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILE_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_READ_ONLY(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_fileops_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_FILE_OPS(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_dirtycred_file_protection_is_enabled(void)
+{
+	if (!bhv_file_protection_is_enabled())
+		return false;
+
+	if (!HypABI__FileProtection__Init__Config__has_DIRTY_CRED(
+		    &bhv_file_protection_config))
+		return false;
+
+	return true;
+}
+
+bool bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+	const char *target);
+bool bhv_block_read_only_file_write_ViolationDirtyCredWrite(const char *target);
+
+bool bhv_block_read_only_file_write(const char *target, bool dirtycred);
+void bhv_check_file_dirty_cred(struct file *file, int mask);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+static void bhv_check_file_dirty_cred(struct file *, int)
+{
+	return;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_internal.h include/bhv/fileops_internal.h
new file mode 100644
index 00000000000..d58a0be5238
--- /dev/null
+++ include/bhv/fileops_internal.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS__
+#define __BHV_FILEOPS__
+
+// used by security/bhv.c
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h> // simple_dir_operations
+#include <linux/ramfs.h> // ramfs_file_operations
+#include <linux/printk.h> // kmsg_fops
+#include <linux/mnt_namespace.h> // proc_mount{s,stats,info}_operations
+
+#define FOPS(sym) extern const struct file_operations sym;
+#include <bhv/fileops_internal_symlist.h>
+
+typedef const struct file_operations *fops_t[2];
+
+// basic regular file + directory file ops
+#ifndef VASKM // inside kernel tree
+extern const fops_t fileops_map[];
+#else // out of tree
+extern fops_t *fileops_map;
+#endif // VASKM
+
+// additional /proc/ file operations
+extern struct file_operations const *proc_fops[] __ro_after_init;
+
+/******************************************************************
+ * init
+ ******************************************************************/
+void __init bhv_init_fileops(void);
+/******************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **);
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr);
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS__ */
diff --git include/bhv/fileops_internal_fopsmap.h include/bhv/fileops_internal_fopsmap.h
new file mode 100644
index 00000000000..56d27e0f677
--- /dev/null
+++ include/bhv/fileops_internal_fopsmap.h
@@ -0,0 +1,49 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+
+#if defined CONFIG_EXT4_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // ext4 files and dirs
+FOPS_MAP(ext4, FT(EXT4), ext4_file_operations, ext4_dir_operations)
+#endif
+        // tmpfs files and dirs
+FOPS_MAP(tmpfs, FT(TMPFS), shmem_file_operations, simple_dir_operations)
+        // sockets
+FOPS_MAP_DIRNULL(sockfs, FT(SOCKFS), socket_file_ops)
+        // pipes
+FOPS_MAP_DIRNULL(pipefs, FT(PIPEFS), pipefifo_fops)
+        // special chardevs
+#if defined CONFIG_DEVMEM || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+FOPS_MAP_DIRNULL(mem, FT(DEV_MEM), mem_fops)
+#endif
+FOPS_MAP_DIRNULL(null, FT(DEV_NULL), null_fops)
+FOPS_MAP_DIRNULL(port, FT(DEV_PORT), port_fops)
+FOPS_MAP_DIRNULL(zero, FT(DEV_ZERO), zero_fops)
+FOPS_MAP_DIRNULL(full, FT(DEV_FULL), full_fops)
+FOPS_MAP_DIRNULL(random, FT(DEV_RANDOM), random_fops)
+FOPS_MAP_DIRNULL(urandom, FT(DEV_URANDOM), urandom_fops)
+FOPS_MAP_DIRNULL(kmsg, FT(DEV_KMSG), kmsg_fops)
+FOPS_MAP_DIRNULL(tty, FT(DEV_TTY), tty_fops)
+FOPS_MAP_DIRNULL(console, FT(DEV_CONSOLE), console_fops)
+        // proc basic
+FOPS_MAP(proc, FT(PROC), proc_reg_file_ops, proc_root_operations)
+#if defined CONFIG_XFS_FS || defined FILEOPS_INTERNAL_FOPSMAP_ALL
+        // xfs files and dirs
+FOPS_MAP(xfs, FT(XFS), xfs_file_operations, xfs_dir_file_operations)
+#endif
+        // sys fs
+FOPS_MAP(sysfs, FT(SYSFS), kernfs_file_fops, kernfs_dir_fops)
+
+#undef FT
+#undef FOPS_MAP
+#undef FOPS_MAP_DIRNULL
+#ifdef FILEOPS_INTERNAL_FOPSMAP_ALL
+#undef FILEOPS_INTERNAL_FOPSMAP_ALL
+#endif
\ No newline at end of file
diff --git include/bhv/fileops_internal_symlist.h include/bhv/fileops_internal_symlist.h
new file mode 100644
index 00000000000..044aad3e4a5
--- /dev/null
+++ include/bhv/fileops_internal_symlist.h
@@ -0,0 +1,170 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// no pragma once on purpose
+
+#ifndef FOPS_PROC
+#define FOPS_PROC FOPS
+#endif
+
+// fs/char_dev.c
+FOPS(def_chr_fops)
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+FOPS(ext4_dir_operations)
+FOPS(ext4_file_operations)
+#endif
+// mm/shmem.c
+#ifdef CONFIG_SHMEM
+FOPS(shmem_file_operations)
+#else
+FOPS(ramfs_file_operations)
+#define shmem_file_operations ramfs_file_operations
+#endif
+// fs/libfs.c
+FOPS(simple_dir_operations)
+// drivers/char/mem.c
+#ifdef CONFIG_DEVMEM
+FOPS(mem_fops)
+#endif
+FOPS(null_fops)
+FOPS(port_fops)
+FOPS(zero_fops)
+FOPS(full_fops)
+// drivers/char/random.c
+FOPS(random_fops)
+FOPS(urandom_fops)
+// kernel/printk/printk.c
+FOPS(kmsg_fops)
+// drivers/tty/tty_io.c
+FOPS(tty_fops)
+FOPS(console_fops)
+FOPS(hung_up_tty_fops)
+
+#ifdef CONFIG_XFS_FS
+// fs/xfs/xfs_iops.h
+FOPS(xfs_dir_file_operations)
+FOPS(xfs_file_operations)
+#endif
+
+// net/sockets.c
+FOPS(socket_file_ops)
+// fs/pipe.c
+FOPS(pipefifo_fops)
+
+// fs/kernfs/file.c
+FOPS(kernfs_file_fops)
+// fs/kernfs/dir.c
+FOPS(kernfs_dir_fops)
+
+// DEBUGFS
+// sys/kernel/debug
+FOPS(debugfs_noop_file_operations)
+FOPS(debugfs_open_proxy_file_operations)
+FOPS(debugfs_full_proxy_file_operations)
+
+// PROC:
+// fs/proc/inode.c
+FOPS_PROC(proc_reg_file_ops)
+FOPS_PROC(proc_iter_file_ops)
+#ifdef CONFIG_COMPAT
+FOPS_PROC(proc_reg_file_ops_compat)
+FOPS_PROC(proc_iter_file_ops_compat)
+#endif
+// fs/proc/root.c
+FOPS_PROC(proc_root_operations)
+// fs/proc/proc_sysctl.c
+FOPS_PROC(proc_sys_file_operations)
+FOPS_PROC(proc_sys_dir_file_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fd_operations)
+// fs/proc/base.c
+FOPS_PROC(proc_oom_score_adj_operations)
+FOPS_PROC(proc_pid_cmdline_ops)
+#ifdef CONFIG_LATENCYTOP
+FOPS_PROC(proc_lstats_operations)
+#endif
+FOPS_PROC(proc_mem_operations)
+FOPS_PROC(proc_environ_operations)
+FOPS_PROC(proc_auxv_operations)
+FOPS_PROC(proc_oom_adj_operations)
+FOPS_PROC(proc_loginuid_operations)
+#ifdef CONFIG_AUDIT
+FOPS_PROC(proc_sessionid_operations)
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+FOPS_PROC(proc_fault_inject_operations)
+FOPS_PROC(proc_fail_nth_operations)
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+FOPS_PROC(proc_pid_sched_operations)
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+FOPS_PROC(proc_pid_sched_autogroup_operations)
+#endif
+#ifdef CONFIG_TIME_NS
+FOPS_PROC(proc_timens_offsets_operations)
+#endif
+FOPS_PROC(proc_pid_set_comm_operations)
+FOPS_PROC(proc_map_files_operations)
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+FOPS_PROC(proc_timers_operations)
+#endif
+FOPS_PROC(proc_pid_set_timerslack_ns_operations)
+#ifdef CONFIG_SECURITY
+FOPS_PROC(proc_pid_attr_operations)
+FOPS_PROC(proc_attr_dir_operations)
+#endif
+#ifdef CONFIG_ELF_CORE
+FOPS_PROC(proc_coredump_filter_operations)
+#endif
+#ifdef CONFIG_USER_NS
+FOPS_PROC(proc_uid_map_operations)
+FOPS_PROC(proc_gid_map_operations)
+FOPS_PROC(proc_projid_map_operations)
+FOPS_PROC(proc_setgroups_operations)
+#endif
+FOPS_PROC(proc_tgid_base_operations)
+FOPS_PROC(proc_tid_base_operations)
+FOPS_PROC(proc_task_operations)
+FOPS_PROC(proc_single_file_operations)
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+FOPS_PROC(proc_zram_block_state_op)
+#endif
+#ifdef CONFIG_PAGE_OWNER
+// mm/page_owner.c
+FOPS_PROC(proc_page_owner_operations)
+#endif
+// fs/proc/internal.h
+FOPS_PROC(proc_ns_dir_operations)
+FOPS_PROC(proc_net_operations)
+FOPS_PROC(proc_pid_maps_operations)
+#ifdef CONFIG_NUMA
+FOPS_PROC(proc_pid_numa_maps_operations)
+#endif
+FOPS_PROC(proc_pid_smaps_operations)
+FOPS_PROC(proc_pid_smaps_rollup_operations)
+FOPS_PROC(proc_clear_refs_operations)
+FOPS_PROC(proc_pagemap_operations)
+#ifdef CONFIG_PROC_CHILDREN
+FOPS_PROC(proc_tid_children_operations)
+#endif
+// fs/proc/generic.c
+FOPS_PROC(proc_dir_operations)
+// fs/proc/fd.h
+FOPS_PROC(proc_fdinfo_operations)
+// fs/proc/fd.c
+FOPS_PROC(proc_fdinfo_file_operations)
+// fs/proc_namespace.c
+FOPS_PROC(proc_mounts_operations)
+FOPS_PROC(proc_mountinfo_operations)
+FOPS_PROC(proc_mountstats_operations)
+
+FOPS_PROC(empty_dir_operations)
+
+#undef FOPS
+#undef FOPS_PROC
\ No newline at end of file
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 00000000000..e659ac5f585
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <linux/init.h>
+#include <bhv/interface/common.h>
+
+bool bhv_strict_fileops_enforced(void);
+bool bhv_block_fileops(const char *, u8, bool, const void *);
+u8 bhv_fileops_type(u32 fs_magic);
+bool bhv_fileops_is_ro(u64 f_op);
+
+#endif // CONFIG_BHV_VAS
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 00000000000..91b081d20f6
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_H__
+#define __BHV_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_base_autogen.h>
+
+#define BHV_GUESTCONN_MAX_PAYLOAD_SZ \
+	GuestConnABI__MAX_MSG_SZ - GuestConnABI__Header__SZ
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port);
+/*********************************************************/
+
+/*********************************************************
+ * start
+ *********************************************************/
+void bhv_start_guestconn(void);
+/*********************************************************/
+
+/*********************************************************
+ * mm_init
+ *********************************************************/
+void __init bhv_mm_init_guestconn(void);
+/*********************************************************/
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void);
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg);
+
+int bhv_guestconn_send(uint16_t type, GuestConnABI__Header__T *data,
+		       size_t size);
+
+#endif /* __BHV_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 00000000000..0cf0334c33a
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,88 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#include <linux/cgroup-defs.h>
+#include <linux/types.h>
+#include <linux/binfmts.h>
+#include <linux/nsproxy.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+extern HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init;
+
+/*********************************************************
+ * init
+ *********************************************************/
+int __init bhv_init_guestlog(void);
+/*********************************************************/
+
+static inline bool bhv_guestlog_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(
+		bhv_configuration_bitmap);
+}
+
+#define BHV_GUESTLOG_EVENT_ENABLED(EVT)                         \
+	(bhv_guestlog_enabled() && bhv_guestlog_config.valid && \
+	 HypABI__Guestlog__Init__GuestlogFlags__has_##EVT(      \
+		 (HypABI__Guestlog__Init__GuestlogFlags__T      \
+			  *)&bhv_guestlog_config.log_bitmap))
+
+static inline bool bhv_guestlog_log_process_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(PROCESS_EVENTS);
+}
+static inline bool bhv_guestlog_log_driver_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(DRIVER_EVENTS);
+}
+static inline bool bhv_guestlog_log_kaccess_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_ACCESS);
+}
+static inline bool bhv_guestlog_log_unknown_fileops(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(UNKNOWN_FILEOPS);
+}
+static inline bool bhv_guestlog_log_kernel_exec_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(KERNEL_EXEC_EVENTS);
+}
+static inline bool bhv_guestlog_log_container_events(void)
+{
+	return BHV_GUESTLOG_EVENT_ENABLED(CONTAINER_EVENTS);
+}
+
+#undef BHV_GUESTLOG_EVENT_ENABLED
+
+int bhv_guestlog_log_str(char *fmt, ...);
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm);
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm);
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_driver_load(const char *name);
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t event_id);
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint8_t type, uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr);
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp);
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp);
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp);
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset);
+
+#endif /* __BHV_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 00000000000..cc86d614bc0
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_GUEST_POLICY,
+			      bhv_configuration_bitmap);
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init/init.h include/bhv/init/init.h
new file mode 100644
index 00000000000..0ccd25f1ccd
--- /dev/null
+++ include/bhv/init/init.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_INIT_H__
+#define __BHV_INIT_INIT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/integrity.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+#define BHV_INIT_MAX_REGIONS 7
+
+void __init bhv_init_platform(void);
+void __init bhv_init_arch(void);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_init_platform(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_INIT_H__ */
diff --git include/bhv/init/late_start.h include/bhv/init/late_start.h
new file mode 100644
index 00000000000..a0171c2fd23
--- /dev/null
+++ include/bhv/init/late_start.h
@@ -0,0 +1,17 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_LATE_START_H__
+#define __BHV_INIT_LATE_START_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_late_start(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_late_start(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_LATE_START_H__ */
diff --git include/bhv/init/mm_init.h include/bhv/init/mm_init.h
new file mode 100644
index 00000000000..292df21f2f7
--- /dev/null
+++ include/bhv/init/mm_init.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_MM_INIT_H__
+#define __BHV_INIT_MM_INIT_H__
+#ifdef CONFIG_BHV_VAS
+void __init bhv_mm_init(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_mm_init(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_INIT_MM_INIT_H__ */
diff --git include/bhv/init/start.h include/bhv/init/start.h
new file mode 100644
index 00000000000..6f5daf853b8
--- /dev/null
+++ include/bhv/init/start.h
@@ -0,0 +1,20 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_START_H__
+#define __BHV_INIT_START_H__
+
+#ifdef CONFIG_BHV_VAS
+bool bhv_start(void);
+int bhv_start_arch(void);
+#else /* CONFIG_BHV_VAS */
+static inline bool bhv_start(void)
+{
+	return true;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INIT_START_H__ */
diff --git include/bhv/inode.h include/bhv/inode.h
new file mode 100644
index 00000000000..2e88680466a
--- /dev/null
+++ include/bhv/inode.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INODE_H__
+#define __BHV_INODE_H__
+
+#include <linux/binfmts.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_inode_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_INODE, bhv_configuration_bitmap);
+}
+
+int __init bhv_inode_init(void);
+
+/* LSM Hooks */
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm,
+				   struct file *file);
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old,
+			      int flags);
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old,
+			      int flags);
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode);
+
+void bhv_inode_iput_final(struct inode *inode);
+
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid, umode_t mode);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_inode_iput_final(i)			do { } while(0)
+#define bhv_inode_post_setattr(d,v,m)		do { } while(0)
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INODE_H__ */
+
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 00000000000..0e2202d58be
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,281 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+typedef union {
+	HypABI__Integrity__Create__Mem_Region__T create;
+	HypABI__Integrity__Update__Mem_Region__T update;
+	HypABI__Integrity__Remove__Mem_Region__T remove;
+} bhv_mem_region_t;
+
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * start
+ ************************************************************/
+int bhv_start_integrity_arch(void);
+void __init_km bhv_start_ptpg(void);
+void __init_km bhv_start_get_pt_protect_pgd_data(uint64_t *pgd_offset,
+						 uint64_t *pgd_value);
+/************************************************************/
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void);
+/************************************************************/
+
+/************************************************************
+ * late_start
+ ************************************************************/
+int bhv_late_start_init_ptpg(void);
+void bhv_late_start_get_pt_protect_data(
+	HypABI__Integrity__PtpgInit__arg__T *init_ptpg_arg);
+/************************************************************/
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(
+		bhv_configuration_bitmap);
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	if (!bhv_integrity_is_enabled())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(
+		bhv_configuration_bitmap);
+}
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
+extern bool bhv_integrity_freeze_create_currently_frozen;
+extern bool bhv_integrity_freeze_update_currently_frozen;
+extern bool bhv_integrity_freeze_remove_currently_frozen;
+extern bool bhv_integrity_freeze_patch_currently_frozen;
+int bhv_integrity_freeze_events(uint64_t flags);
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks);
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head);
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner);
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm);
+bool bhv_pt_protect_check_pgd_arch(struct mm_struct *mm, uint64_t pgd_offset,
+				   uint64_t pgd_value);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline void bhv_mem_region_create_ctor(bhv_mem_region_t *curr_item,
+					      bhv_mem_region_t *prev_item,
+					      uint64_t addr, uint64_t size,
+					      uint32_t type, uint64_t flags,
+					      const char *label)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_mem_region_t){
+		.create =
+			(HypABI__Integrity__Create__Mem_Region__T){
+				.start_addr = addr,
+				.size = size,
+				.type = type,
+				.flags = flags,
+				.next = BHV_INVALID_PHYS_ADDR,
+			}
+	};
+	strncpy(curr_item->create.label, label,
+		HypABI__Integrity__MAX_LABEL_SIZE);
+	curr_item->create.label[HypABI__Integrity__MAX_LABEL_SIZE - 1] = '\0';
+
+	if (prev_item)
+		prev_item->create.next = bhv_virt_to_phys(curr_item);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags, const char *label)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	bhv_mem_region_create_ctor(&n->region, NULL, addr, size, type, flags,
+				   label);
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.create.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.update.start_addr = addr;
+	n->region.update.type = type;
+	n->region.update.flags = flags;
+	n->region.update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.update.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.remove.start_addr = addr;
+	n->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.remove.next = bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_integrity_pt_prot_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_integrity_freeze_events(uint64_t)
+{
+	return 0;
+}
+
+static inline int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return 0;
+}
+
+static inline void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+}
+
+#define bhv_allow_kmod_loads true
+#define bhv_allow_patch true
+#define bhv_integrity_freeze_create_currently_frozen false
+#define bhv_integrity_freeze_update_currently_frozen false
+#define bhv_integrity_freeze_remove_currently_frozen false
+#define bhv_integrity_freeze_patch_currently_frozen false
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/interface/abi_base_autogen.h include/bhv/interface/abi_base_autogen.h
new file mode 100644
index 00000000000..320f3940590
--- /dev/null
+++ include/bhv/interface/abi_base_autogen.h
@@ -0,0 +1,1664 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-08-23T11:38:28).
+ */
+
+#pragma once
+
+#include <linux/types.h>
+
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#define BHV__HypABI__VAS__TARGET_ID 1
+
+_Static_assert(sizeof(char) == sizeof(uint8_t), "Unexpected char size");
+
+typedef uint64_t HypABI__MemoryRegionOwner;
+
+#define HypABI__Context__MAX_PATH_SZ 256ULL
+
+
+// start of HypABI__Context__Inums
+
+struct __attribute__((packed)) HypABI__Context__Inums {
+        /** This is the cgroup namespace identifier. */
+        uint32_t cgroup_ns_inum;
+        /** This is the ipc namespace identifier. */
+        uint32_t ipc_ns_inum;
+        /** This is the mount namespace identifier. */
+        uint32_t mnt_ns_inum;
+        /** This is the network namespace identifier. */
+        uint32_t net_ns_inum;
+        /** This is the pid namespace identifier. */
+        uint32_t pid_ns_inum;
+        /** This is the pid namespace identifier granted to child processes. */
+        uint32_t pid_for_children_ns_inum;
+        /** This is the time namespace identifier. */
+        uint32_t time_ns_inum;
+        /** This is the time namespace identifier granted to child processes. */
+        uint32_t time_for_children_ns_inum;
+        /** This is the user namespace identifier. */
+        uint32_t user_ns_inum;
+        /** This is the uts namespace identifier. */
+        uint32_t uts_ns_inum;
+};
+typedef struct HypABI__Context__Inums HypABI__Context__Inums__T;
+#define HypABI__Context__Inums__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Context__Inums) == HypABI__Context__Inums__SZ, "Unexpected size for HypABI__Context__Inums");
+
+#define HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ 128ULL
+
+
+// start of HypABI__Context__CGroupInfo
+
+struct __attribute__((packed)) HypABI__Context__CGroupInfo {
+        /** This is the cgroup identifier. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct HypABI__Context__CGroupInfo HypABI__Context__CGroupInfo__T;
+#define HypABI__Context__CGroupInfo__SZ 136ULL
+_Static_assert(sizeof(struct HypABI__Context__CGroupInfo) == HypABI__Context__CGroupInfo__SZ, "Unexpected size for HypABI__Context__CGroupInfo");
+
+
+// start of HypABI__Context
+
+struct __attribute__((packed)) HypABI__Context {
+        /** This field indicates to the host whether the event context is valid. */
+        uint8_t valid;
+        uint8_t padding[3];
+        /** The ID of the vCPU that the event occurred on. */
+        uint32_t vcpu_id;
+        /** The user ID that was in use when the event occurred. This helps us to tie user information to an event. For example, it may allow us to answer the question which user account caused a certain security violation. Note that not this information is not always reliable. Just because an attack happened in a certain context does not necessarily mean that it was caused by the given user. */
+        uint32_t uid;
+        /** The effective user ID that was in use when the event occurred. */
+        uint32_t euid;
+        /** The group ID that was in use when the event occurred. */
+        uint32_t gid;
+        /** The and effective group ID that was in use when the event occurred. */
+        uint32_t egid;
+        /** The effective capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_effective;
+        /** The permitted capabilities of the process that was in context when the event occurred. */
+        uint64_t cap_permitted;
+        /** The PID of the process that was in context when the event occurred. */
+        uint32_t pid;
+        /** The parent PID of the process that was in context when the event occurred. */
+        uint32_t parent_pid;
+        /** The PATH of the process that was in context when the event occurred. */
+        char comm[HypABI__Context__MAX_PATH_SZ];
+        /** The PATH of the parent binary of the process that was in context when the event occurred. */
+        char parent_comm[HypABI__Context__MAX_PATH_SZ];
+        /** The namespace identifiers for the current process. */
+        struct HypABI__Context__Inums inums;
+        /** The information about the cgroup for the current process. */
+        struct HypABI__Context__CGroupInfo cgroup;
+};
+typedef struct HypABI__Context HypABI__Context__T;
+#define HypABI__Context__SZ 736ULL
+_Static_assert(sizeof(struct HypABI__Context) == HypABI__Context__SZ, "Unexpected size for HypABI__Context");
+
+#define HypABI__Init__BACKEND_ID 1
+
+#define HypABI__Init__Init__OP_ID 0
+
+enum HypABI__Init__Init__BHVSectionRun__BHVSectionRunType {
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT = 42,
+        HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA = 43,
+};
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__COUNT 2
+#define HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__LABELS \
+        OP(RICHARD_VAULT) OP(DATA)
+
+// start of HypABI__Init__Init__BHVSectionRun
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVSectionRun {
+        /** Guest-physical start address of this run. Must be page-aligned. */
+        uint64_t gpa_start;
+        /** Size in bytes. Must be a non-zero multiple of the page size. */
+        uint64_t size;
+        /** The guest-physical address of the next item in the linked list, or `INVALID_GPA` if this item is the last one. */
+        uint64_t next;
+        /** Type of the run */
+        uint8_t type;
+};
+typedef struct HypABI__Init__Init__BHVSectionRun HypABI__Init__Init__BHVSectionRun__T;
+#define HypABI__Init__Init__BHVSectionRun__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVSectionRun) == HypABI__Init__Init__BHVSectionRun__SZ, "Unexpected size for HypABI__Init__Init__BHVSectionRun");
+
+
+// start of HypABI__Init__Init__BHVData__HypABI__Init__Init__BHVData__BHVConfigBitmap
+
+typedef unsigned long HypABI__Init__Init__BHVData__BHVConfigBitmap__T;
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__NONE 0UL
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT 0
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY (1UL<<0)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT 1
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL (1UL<<1)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT 2
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL (1UL<<2)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT 3
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING (1UL<<3)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT 4
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY (1UL<<4)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT 5
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION (1UL<<5)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT 6
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY (1UL<<6)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT 7
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION (1UL<<7)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT 8
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION (1UL<<8)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT 9
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT (1UL<<9)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT 10
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY (1UL<<10)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT 11
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY (1UL<<11)
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT__BIT 12
+#define HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT (1UL<<12)
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr);
+
+
+// start of HypABI__Init__Init__BHVData
+
+struct __attribute__((packed)) HypABI__Init__Init__BHVData {
+        /** This is a bitmap that is used to communicate which mechanisms the host has enabled such that the guest can leverage them. */
+        uint64_t config_bitmap;
+        /** This field contains the cid of the vsocket endpoint the guest should can connect to. If this value is `0`, this indicates the host is not listening for a vsocket connection. */
+        uint32_t vsocket_cid;
+        /** This field contains the port of the vsocket endpoint the guest should can connect to. If the `vsocket_cid` value is `0`, this value is to be ignored. */
+        uint32_t vsocket_port;
+};
+typedef struct HypABI__Init__Init__BHVData HypABI__Init__Init__BHVData__T;
+#define HypABI__Init__Init__BHVData__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__BHVData) == HypABI__Init__Init__BHVData__SZ, "Unexpected size for HypABI__Init__Init__BHVData");
+
+
+// start of HypABI__Init__Init__arg
+
+struct __attribute__((packed)) HypABI__Init__Init__arg {
+        /** The size of the buffer passed in `modprobe_path`. This field is ignored if `modprobe_path == INVALID_GPA`. */
+        uint64_t modprobe_path_sz;
+        /** The physical address of the guest's modprobe path. In certain cases, this can be set from the host. This field can be set to `INVALID_GPA` and it will be ignored by the host. */
+        uint64_t modprobe_path;
+        /** This establishes the owner id of the guest kernel. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0`. */
+        uint64_t owner;
+        /** The physical address of the head of a list of BHV-specific memory regions to be protected. */
+        uint64_t bhv_region_head;
+        /** The physical address of the head of a list of memory regions to be protected. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Init__Init__arg HypABI__Init__Init__arg__T;
+#define HypABI__Init__Init__arg__SZ 40ULL
+_Static_assert(sizeof(struct HypABI__Init__Init__arg) == HypABI__Init__Init__arg__SZ, "Unexpected size for HypABI__Init__Init__arg");
+
+#define HypABI__Init__Start__OP_ID 1
+
+
+// start of HypABI__Init__Start__arg
+
+struct __attribute__((packed)) HypABI__Init__Start__arg {
+        /** This field is used to indicate whether the `data_sz` and `data` fields are valid. When this field is set to `false`, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        uint8_t padding[1];
+        /** The guest is expected to provide the size of the buffer (in 4k pages) that the `bhv_init_start_config_t` struct resides in. The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire guest policy into the `data` field of the `bhv_init_start_config_t` struct. */
+        uint16_t num_pages;
+        /** If the `valid` field is true, this field contains the size of buffer in bytes stored in the `data` field. */
+        uint32_t data_sz;
+        /** If the `valid` field is true and the `data_sz` field is greater than 0, this field contains the guest policy buffer. This can be used to load an arbitrary guest policy into the guest from the host. For example, this is used by the BHV Linux kernel to load SELinux policies into the guest. */
+        uint8_t data[];
+};
+typedef struct HypABI__Init__Start__arg HypABI__Init__Start__arg__T;
+#define HypABI__Init__Start__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Init__Start__arg) == HypABI__Init__Start__arg__SZ, "Unexpected size for HypABI__Init__Start__arg");
+
+#define HypABI__Integrity__BACKEND_ID 2
+
+enum HypABI__Integrity__MemType {
+        HypABI__Integrity__MemType__UNKNOWN = 0,
+        HypABI__Integrity__MemType__CODE = 1,
+        HypABI__Integrity__MemType__CODE_WRITABLE = 2,
+        HypABI__Integrity__MemType__CODE_PATCHABLE = 3,
+        HypABI__Integrity__MemType__DATA = 4,
+        HypABI__Integrity__MemType__DATA_READ_ONLY = 5,
+        HypABI__Integrity__MemType__VDSO = 6,
+        HypABI__Integrity__MemType__VVAR = 7,
+};
+#define HypABI__Integrity__MemType__COUNT 8
+#define HypABI__Integrity__MemType__LABELS \
+        OP(UNKNOWN) OP(CODE) OP(CODE_WRITABLE) OP(CODE_PATCHABLE) OP(DATA) OP(DATA_READ_ONLY) OP(VDSO) OP(VVAR)
+
+// start of HypABI__Integrity__HypABI__Integrity__MemFlags
+
+typedef unsigned long HypABI__Integrity__MemFlags__T;
+#define HypABI__Integrity__MemFlags__NONE 0UL
+#define HypABI__Integrity__MemFlags__TRANSIENT__BIT 0
+#define HypABI__Integrity__MemFlags__TRANSIENT (1UL<<0)
+#define HypABI__Integrity__MemFlags__MUTABLE__BIT 1
+#define HypABI__Integrity__MemFlags__MUTABLE (1UL<<1)
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr);
+
+#define HypABI__Integrity__MAX_LABEL_SIZE 32ULL
+
+#define HypABI__Integrity__Create__OP_ID 0
+
+
+// start of HypABI__Integrity__Create__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Create__Mem_Region {
+        /** The start address of the memory range to create. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The size of the memory region to create. This must be greater than 0 and must be a multiple of 4K. */
+        uint64_t size;
+        /** The type of the memory region to create. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The flags associated with the memory region to create. */
+        uint64_t flags;
+        /** A custom label for this section. The purpose of this label is to easier identify a memory range if an event is observed. */
+        char label[HypABI__Integrity__MAX_LABEL_SIZE];
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Create__Mem_Region HypABI__Integrity__Create__Mem_Region__T;
+#define HypABI__Integrity__Create__Mem_Region__SZ 72ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__Mem_Region) == HypABI__Integrity__Create__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Create__Mem_Region");
+
+
+// start of HypABI__Integrity__Create__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Create__arg {
+        /** This establishes the owner id of this memory region. This is used for organization and is simply a unique `uint64_t`. The general convention is that the kernel itself uses `0` and kernel modules/drivers use unique non-zero ids. This allows the user to remove regions by owner later. */
+        uint64_t owner;
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Create__arg HypABI__Integrity__Create__arg__T;
+#define HypABI__Integrity__Create__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Create__arg) == HypABI__Integrity__Create__arg__SZ, "Unexpected size for HypABI__Integrity__Create__arg");
+
+#define HypABI__Integrity__Update__OP_ID 1
+
+
+// start of HypABI__Integrity__Update__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Update__Mem_Region {
+        /** The start address of the memory range to update. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** The type of the memory region to update to. */
+        uint32_t type;
+        uint8_t padding[4];
+        /** The new flags to associate with the region. */
+        uint64_t flags;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Update__Mem_Region HypABI__Integrity__Update__Mem_Region__T;
+#define HypABI__Integrity__Update__Mem_Region__SZ 32ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__Mem_Region) == HypABI__Integrity__Update__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Update__Mem_Region");
+
+
+// start of HypABI__Integrity__Update__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Update__arg {
+        /** This is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+        uint64_t region_head;
+};
+typedef struct HypABI__Integrity__Update__arg HypABI__Integrity__Update__arg__T;
+#define HypABI__Integrity__Update__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Update__arg) == HypABI__Integrity__Update__arg__SZ, "Unexpected size for HypABI__Integrity__Update__arg");
+
+#define HypABI__Integrity__Remove__OP_ID 2
+
+
+// start of HypABI__Integrity__Remove__Mem_Region
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__Mem_Region {
+        /** The start address of the memory range to remove. This must be a 4K page aligned physical address. */
+        uint64_t start_addr;
+        /** This points to the next memory region in a list. If this is the last memory region, it must contain `INVALID_GPA`. */
+        uint64_t next;
+};
+typedef struct HypABI__Integrity__Remove__Mem_Region HypABI__Integrity__Remove__Mem_Region__T;
+#define HypABI__Integrity__Remove__Mem_Region__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__Mem_Region) == HypABI__Integrity__Remove__Mem_Region__SZ, "Unexpected size for HypABI__Integrity__Remove__Mem_Region");
+
+
+// start of HypABI__Integrity__Remove__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Remove__arg {
+        /** This parameter determines whether the call is used remove multiple sections by an owner id or whether to explicitly remove defined sections. */
+        uint8_t rm_by_owner;
+        uint8_t padding[7];
+        union {
+                /** This field is only considered if `rm_by_owner` is true. When considered, this field contains the owner id of one or more regions to be removed. */
+                uint64_t owner;
+                /** This field is only considered if `rm_by_owner` is false. When considered, this field is a pointer to a guest physical address that contains a `Mem_Region` struct. */
+                uint64_t region_head;
+        };
+};
+typedef struct HypABI__Integrity__Remove__arg HypABI__Integrity__Remove__arg__T;
+#define HypABI__Integrity__Remove__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Remove__arg) == HypABI__Integrity__Remove__arg__SZ, "Unexpected size for HypABI__Integrity__Remove__arg");
+
+#define HypABI__Integrity__Freeze__OP_ID 3
+
+
+// start of HypABI__Integrity__Freeze__HypABI__Integrity__Freeze__FreezeFlags
+
+typedef unsigned long HypABI__Integrity__Freeze__FreezeFlags__T;
+#define HypABI__Integrity__Freeze__FreezeFlags__NONE 0UL
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT 0
+#define HypABI__Integrity__Freeze__FreezeFlags__CREATE (1UL<<0)
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT 1
+#define HypABI__Integrity__Freeze__FreezeFlags__UPDATE (1UL<<1)
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT 2
+#define HypABI__Integrity__Freeze__FreezeFlags__REMOVE (1UL<<2)
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT 3
+#define HypABI__Integrity__Freeze__FreezeFlags__PATCH (1UL<<3)
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr);
+
+
+// start of HypABI__Integrity__Freeze__arg
+
+struct __attribute__((packed)) HypABI__Integrity__Freeze__arg {
+        /** This field contains a bitfield that specifies which operations should be frozen. */
+        uint64_t flags;
+};
+typedef struct HypABI__Integrity__Freeze__arg HypABI__Integrity__Freeze__arg__T;
+#define HypABI__Integrity__Freeze__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Integrity__Freeze__arg) == HypABI__Integrity__Freeze__arg__SZ, "Unexpected size for HypABI__Integrity__Freeze__arg");
+
+#define HypABI__Integrity__PtpgInit__OP_ID 4
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES 8ULL
+
+#define HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO 16ULL
+
+
+// start of HypABI__Integrity__PtpgInit__arg
+
+struct __attribute__((packed)) HypABI__Integrity__PtpgInit__arg {
+        /** This must contain the physical address of the kernel's page table. In Linux this is the `pgd` stored in `init_mm`. */
+        uint64_t init_pgd;
+        /** This must contain the number of paging levels the kernel is using.  Currently only 4 and 5 level paging is supported. */
+        uint8_t pt_levels;
+        uint8_t padding[3];
+        /** This must contain the number of ranges in the succeeding `ranges` array. This must be a number >= 0 and <= `MAX_RANGES`. */
+        uint32_t num_ranges;
+        /** This is an array of pairs of `uint64_t` values in which the first value represents the start GVA of a range and the second value of the pair represents the end GVA of a range. */
+        uint64_t ranges[HypABI__Integrity__PtpgInit__MAX_RANGES_TIMES_TWO];
+};
+typedef struct HypABI__Integrity__PtpgInit__arg HypABI__Integrity__PtpgInit__arg__T;
+#define HypABI__Integrity__PtpgInit__arg__SZ 144ULL
+_Static_assert(sizeof(struct HypABI__Integrity__PtpgInit__arg) == HypABI__Integrity__PtpgInit__arg__SZ, "Unexpected size for HypABI__Integrity__PtpgInit__arg");
+
+#define HypABI__Integrity__PtpgReport__OP_ID 5
+
+#define HypABI__Patch__BACKEND_ID 3
+
+#define HypABI__Patch__MAX_PATCH_SZ 32ULL
+
+#define HypABI__Patch__Patch__OP_ID 0
+
+
+// start of HypABI__Patch__Patch__arg
+
+struct __attribute__((packed)) HypABI__Patch__Patch__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__Patch__arg HypABI__Patch__Patch__arg__T;
+#define HypABI__Patch__Patch__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__Patch__arg) == HypABI__Patch__Patch__arg__SZ, "Unexpected size for HypABI__Patch__Patch__arg");
+
+#define HypABI__Patch__PatchNoClose__OP_ID 1
+
+
+// start of HypABI__Patch__PatchNoClose__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchNoClose__arg {
+        /** This must contain the guest physical address to be patched. This address must be within a patch-able code region. */
+        uint64_t dest_phys_addr;
+        /** This is a buffer of size `MAX_PATCH_SZ`. */
+        uint8_t src_value[HypABI__Patch__MAX_PATCH_SZ];
+        /** This is the size of the instruction to patch. This value must be less than `MAX_PATCH_SZ`. */
+        uint64_t size;
+};
+typedef struct HypABI__Patch__PatchNoClose__arg HypABI__Patch__PatchNoClose__arg__T;
+#define HypABI__Patch__PatchNoClose__arg__SZ 48ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchNoClose__arg) == HypABI__Patch__PatchNoClose__arg__SZ, "Unexpected size for HypABI__Patch__PatchNoClose__arg");
+
+#define HypABI__Patch__PatchViolation__OP_ID 2
+
+#define HypABI__Patch__PatchViolation__MAX_MSG_SZ 256ULL
+
+
+// start of HypABI__Patch__PatchViolation__arg
+
+struct __attribute__((packed)) HypABI__Patch__PatchViolation__arg {
+        /** The virtual address that the guest attempted to patch. */
+        uint64_t dest_virt_addr;
+        /** The physical address that the guest attempted to patch. This is the translated `dest_virt_addr`. */
+        uint64_t dest_phys_addr;
+        /** A custom message that provides additional information about the violation. This allows each subsystem (e.g. jump labels) to provide detailed information about the violation. */
+        char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+        /** This field is set by the host and will be processed by the guest after the hypercall. It tells the guest whether to block the violation or whether to allow the patch. */
+        uint8_t block;
+};
+typedef struct HypABI__Patch__PatchViolation__arg HypABI__Patch__PatchViolation__arg__T;
+#define HypABI__Patch__PatchViolation__arg__SZ 273ULL
+_Static_assert(sizeof(struct HypABI__Patch__PatchViolation__arg) == HypABI__Patch__PatchViolation__arg__SZ, "Unexpected size for HypABI__Patch__PatchViolation__arg");
+
+#define HypABI__Richard__BACKEND_ID 4
+
+#define HypABI__Richard__Open__OP_ID 0
+
+#define HypABI__Richard__Close__OP_ID 1
+
+#define HypABI__Acl__BACKEND_ID 5
+
+#define HypABI__Acl__ProcessInit__OP_ID 0
+
+
+// start of HypABI__Acl__ProcessInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__ProcessInit__arg HypABI__Acl__ProcessInit__arg__T;
+#define HypABI__Acl__ProcessInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessInit__arg) == HypABI__Acl__ProcessInit__arg__SZ, "Unexpected size for HypABI__Acl__ProcessInit__arg");
+
+#define HypABI__Acl__DriverInit__OP_ID 1
+
+
+// start of HypABI__Acl__DriverInit__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverInit__arg {
+        /** This field is set by the **host** and is used to indicate whether the `is_allow`, `list_len`, and `list` fields are valid. This field is interpreted as a boolean. When this field is set to false, the guest is expected to re-try the hypercall with a larger buffer. The total size of the shared buffer required to write the guest policy is written by the host in the `num_pages` field. */
+        uint8_t valid;
+        /** This field is set by the **host** when `valid` is set to true. This field is interpreted as a boolean and indicates whether the provided list should be enforced as an allow list (`is_allow = true`) or as a deny list (`is_allow = false`).<br/>When an allow list is indicated, the guest is obligated to allow only the paths in the `list` field to load/execute.<br/>When a deny list is indicated, the guest is obligated to not allow any paths in the list field to load/execute */
+        uint8_t is_allow;
+        /** This field is set by both the **host** and the **guest**.<br/>The guest is expected to provide the size of the buffer (in 4k pages) that the struct resides in.<br/>The host will update this value if the `valid` field is false. In this case, the host uses this field to indicate the size of buffer (in 4k pages) required for writing the entire acl list into the `list` field of the `bhv_acl_config_t` struct. */
+        uint16_t num_pages;
+        /** This field is set by the **host** when `valid` is set to true. This parameter holds the number of strings stored in the `list` field. */
+        uint16_t list_len;
+        uint8_t padding[2];
+        /** This field is set by the **host** when valid is set to true. This field contains `list_len` strings that contain the paths for the ACL list. This buffer is organized as follows:<br/>The first part of this buffer contains an array of `list_len` `uint64_t` values. Each of these values represent the offset from the start of the `bhv_acl_config_t` struct that a null-terminated ascii string lies.<br/>Beyond this initial array, lie the strings pointed to in the initial array of this buffer. These strings are to be addressed by using the offsets in the initial part of the buffer. */
+        uint64_t list[];
+};
+typedef struct HypABI__Acl__DriverInit__arg HypABI__Acl__DriverInit__arg__T;
+#define HypABI__Acl__DriverInit__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverInit__arg) == HypABI__Acl__DriverInit__arg__SZ, "Unexpected size for HypABI__Acl__DriverInit__arg");
+
+#define HypABI__Acl__ProcessViolation__OP_ID 2
+
+
+// start of HypABI__Acl__ProcessViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__ProcessViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__ProcessViolation__arg HypABI__Acl__ProcessViolation__arg__T;
+#define HypABI__Acl__ProcessViolation__arg__SZ 747ULL
+_Static_assert(sizeof(struct HypABI__Acl__ProcessViolation__arg) == HypABI__Acl__ProcessViolation__arg__SZ, "Unexpected size for HypABI__Acl__ProcessViolation__arg");
+
+#define HypABI__Acl__DriverViolation__OP_ID 3
+
+
+// start of HypABI__Acl__DriverViolation__arg
+
+struct __attribute__((packed)) HypABI__Acl__DriverViolation__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This field must contain the guest physical address of a string. This string must contain the path of the process or driver that caused the violation. */
+        uint64_t name;
+        /** This field must contain the length of the string pointed to by the `name` field. */
+        uint16_t name_len;
+        /** This field is treated as a boolean value. This field should indicate whether the guest will block the violation or not. */
+        uint8_t block;
+};
+typedef struct HypABI__Acl__DriverViolation__arg HypABI__Acl__DriverViolation__arg__T;
+#define HypABI__Acl__DriverViolation__arg__SZ 747ULL
+_Static_assert(sizeof(struct HypABI__Acl__DriverViolation__arg) == HypABI__Acl__DriverViolation__arg__SZ, "Unexpected size for HypABI__Acl__DriverViolation__arg");
+
+#define HypABI__Guestlog__BACKEND_ID 6
+
+#define HypABI__Guestlog__Init__OP_ID 0
+
+
+// start of HypABI__Guestlog__Init__HypABI__Guestlog__Init__GuestlogFlags
+
+typedef unsigned long HypABI__Guestlog__Init__GuestlogFlags__T;
+#define HypABI__Guestlog__Init__GuestlogFlags__NONE 0UL
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT 0
+#define HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS (1UL<<0)
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT 1
+#define HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS (1UL<<1)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT 2
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS (1UL<<2)
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT 3
+#define HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS (1UL<<3)
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT 4
+#define HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS (1UL<<4)
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT 5
+#define HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS (1UL<<5)
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr);
+
+
+// start of HypABI__Guestlog__Init__arg
+
+struct __attribute__((packed)) HypABI__Guestlog__Init__arg {
+        /** This contains a bit map that is valid if the `valid` field is true and is written by the **host**. This bit map is used to indicate to the guest which logging information is to be sent to the host. */
+        uint64_t log_bitmap;
+        /** This field is written by the **host**. If this is false, no logging events are expected by the host. If this field is true, the `log_bitmap` field is valid and should be respected by the guest. */
+        uint8_t valid;
+};
+typedef struct HypABI__Guestlog__Init__arg HypABI__Guestlog__Init__arg__T;
+#define HypABI__Guestlog__Init__arg__SZ 9ULL
+_Static_assert(sizeof(struct HypABI__Guestlog__Init__arg) == HypABI__Guestlog__Init__arg__SZ, "Unexpected size for HypABI__Guestlog__Init__arg");
+
+#define HypABI__Creds__BACKEND_ID 7
+
+enum HypABI__Creds__EventType {
+        HypABI__Creds__EventType__EVENT_NONE = 0,
+        HypABI__Creds__EventType__CORRUPTION = 1,
+        HypABI__Creds__EventType__INVALID_ASSIGNMENT = 2,
+        HypABI__Creds__EventType__DOUBLE_ASSIGNMENT = 3,
+        HypABI__Creds__EventType__INVALID_COMMIT = 4,
+        HypABI__Creds__EventType__DOUBLE_COMMIT = 5,
+};
+#define HypABI__Creds__EventType__COUNT 6
+#define HypABI__Creds__EventType__LABELS \
+        OP(EVENT_NONE) OP(CORRUPTION) OP(INVALID_ASSIGNMENT) OP(DOUBLE_ASSIGNMENT) OP(INVALID_COMMIT) OP(DOUBLE_COMMIT)
+
+// start of HypABI__Creds__TaskCred
+
+struct __attribute__((packed)) HypABI__Creds__TaskCred {
+        /** This is the guest virtual address of the `task_struct` associated with this cThis is the guest virtual address of the `task_struct` associated with this call. */
+        uint64_t addr;
+        /** This is the guest virtual address of the `cred` struct associated with task whose `task_struct` address is passed in the `addr` parameter. */
+        uint64_t cred;
+        /** This is the computed HMAC for the `task_struct` and `cred` structures passed in the previous two values. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Creds__TaskCred HypABI__Creds__TaskCred__T;
+#define HypABI__Creds__TaskCred__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__TaskCred) == HypABI__Creds__TaskCred__SZ, "Unexpected size for HypABI__Creds__TaskCred");
+
+#define HypABI__Creds__Configure__OP_ID 0
+
+
+// start of HypABI__Creds__Configure__arg
+
+struct __attribute__((packed)) HypABI__Creds__Configure__arg {
+        /** The SipHash key. */
+        uint64_t key[2];
+};
+typedef struct HypABI__Creds__Configure__arg HypABI__Creds__Configure__arg__T;
+#define HypABI__Creds__Configure__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Creds__Configure__arg) == HypABI__Creds__Configure__arg__SZ, "Unexpected size for HypABI__Creds__Configure__arg");
+
+#define HypABI__Creds__RegisterInitTask__OP_ID 1
+
+
+// start of HypABI__Creds__RegisterInitTask__arg
+
+struct __attribute__((packed)) HypABI__Creds__RegisterInitTask__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the first kernel thread (`swapper`). */
+        struct HypABI__Creds__TaskCred init_task;
+};
+typedef struct HypABI__Creds__RegisterInitTask__arg HypABI__Creds__RegisterInitTask__arg__T;
+#define HypABI__Creds__RegisterInitTask__arg__SZ 24ULL
+_Static_assert(sizeof(struct HypABI__Creds__RegisterInitTask__arg) == HypABI__Creds__RegisterInitTask__arg__SZ, "Unexpected size for HypABI__Creds__RegisterInitTask__arg");
+
+#define HypABI__Creds__Assign__OP_ID 2
+
+
+// start of HypABI__Creds__Assign__arg
+
+struct __attribute__((packed)) HypABI__Creds__Assign__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the new task for which credentials are being allocated. */
+        struct HypABI__Creds__TaskCred new_task;
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the parent task of the task specified in the `new_task` parameter. The parent task is the task from which the privileges will be copied. */
+        struct HypABI__Creds__TaskCred parent;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Assign__arg HypABI__Creds__Assign__arg__T;
+#define HypABI__Creds__Assign__arg__SZ 49ULL
+_Static_assert(sizeof(struct HypABI__Creds__Assign__arg) == HypABI__Creds__Assign__arg__SZ, "Unexpected size for HypABI__Creds__Assign__arg");
+
+#define HypABI__Creds__AssignPriv__OP_ID 3
+
+
+// start of HypABI__Creds__AssignPriv__arg
+
+struct __attribute__((packed)) HypABI__Creds__AssignPriv__arg {
+        /** This is the guest virtual address of the `cred` struct that is being copied into. */
+        uint64_t cred;
+        /** This parameter expects the guest virtual address of the `task_struct` for the process from which credentials are being copied. If this parameter is `NULL`, it is assumed that the `swapper` process is the process from which credentials are being copied. */
+        uint64_t daemon;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__AssignPriv__arg HypABI__Creds__AssignPriv__arg__T;
+#define HypABI__Creds__AssignPriv__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Creds__AssignPriv__arg) == HypABI__Creds__AssignPriv__arg__SZ, "Unexpected size for HypABI__Creds__AssignPriv__arg");
+
+#define HypABI__Creds__Commit__OP_ID 4
+
+
+// start of HypABI__Creds__Commit__arg
+
+struct __attribute__((packed)) HypABI__Creds__Commit__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the current task that is committing its credentials. */
+        struct HypABI__Creds__TaskCred currnt;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Commit__arg HypABI__Creds__Commit__arg__T;
+#define HypABI__Creds__Commit__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Commit__arg) == HypABI__Creds__Commit__arg__SZ, "Unexpected size for HypABI__Creds__Commit__arg");
+
+#define HypABI__Creds__Release__OP_ID 5
+
+
+// start of HypABI__Creds__Release__arg
+
+struct __attribute__((packed)) HypABI__Creds__Release__arg {
+        /** This is the guest virtual address of the `cred` struct that is being released. */
+        uint64_t cred;
+};
+typedef struct HypABI__Creds__Release__arg HypABI__Creds__Release__arg__T;
+#define HypABI__Creds__Release__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Creds__Release__arg) == HypABI__Creds__Release__arg__SZ, "Unexpected size for HypABI__Creds__Release__arg");
+
+#define HypABI__Creds__Verification__OP_ID 6
+
+
+// start of HypABI__Creds__Verification__arg
+
+struct __attribute__((packed)) HypABI__Creds__Verification__arg {
+        /** This is the `TaskCred` structure containing the `task_struct`, `cred`, and HMAC for the task whose credentials must be verified. */
+        struct HypABI__Creds__TaskCred task;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Creds__Verification__arg HypABI__Creds__Verification__arg__T;
+#define HypABI__Creds__Verification__arg__SZ 25ULL
+_Static_assert(sizeof(struct HypABI__Creds__Verification__arg) == HypABI__Creds__Verification__arg__SZ, "Unexpected size for HypABI__Creds__Verification__arg");
+
+#define HypABI__Creds__Log__OP_ID 7
+
+#define HypABI__Creds__Log__MAX_TASK_NAME_LEN 16ULL
+
+
+// start of HypABI__Creds__Log__arg
+
+struct __attribute__((packed)) HypABI__Creds__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter interpreted as a boolean. It is written by the **host** and indicates whether the guest to block the event or not. */
+        uint8_t block;
+        uint8_t padding[2];
+        /** This parameter contains the process ID of the process for which the event is being sent. */
+        uint32_t task_pid;
+        /** This parameter contains the guest virtual address of the `task_struct` of the process for which the event is being sent. */
+        uint64_t task_addr;
+        /** This parameter contains the guest virtual address of the `cred` struct of the process for which the event is being sent. */
+        uint64_t task_cred;
+        /** This parameter contains a NUL- terminated ASCII string that contains the name of the process for which the event is being sent. */
+        char task_name[HypABI__Creds__Log__MAX_TASK_NAME_LEN];
+};
+typedef struct HypABI__Creds__Log__arg HypABI__Creds__Log__arg__T;
+#define HypABI__Creds__Log__arg__SZ 776ULL
+_Static_assert(sizeof(struct HypABI__Creds__Log__arg) == HypABI__Creds__Log__arg__SZ, "Unexpected size for HypABI__Creds__Log__arg");
+
+#define HypABI__FileProtection__BACKEND_ID 8
+
+#define HypABI__FileProtection__Init__OP_ID 0
+
+
+// start of HypABI__FileProtection__Init__HypABI__FileProtection__Init__Config
+
+typedef unsigned long HypABI__FileProtection__Init__Config__T;
+#define HypABI__FileProtection__Init__Config__NONE 0UL
+#define HypABI__FileProtection__Init__Config__READ_ONLY__BIT 0
+#define HypABI__FileProtection__Init__Config__READ_ONLY (1UL<<0)
+#define HypABI__FileProtection__Init__Config__FILE_OPS__BIT 1
+#define HypABI__FileProtection__Init__Config__FILE_OPS (1UL<<1)
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT 2
+#define HypABI__FileProtection__Init__Config__DIRTY_CRED (1UL<<2)
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr);
+
+
+// start of HypABI__FileProtection__Init__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__Init__arg {
+        /** This bitmap is written by the **host** and is updated to indicate which file protection violations the host is expecting from the guest. */
+        uint64_t feature_bitmap;
+};
+typedef struct HypABI__FileProtection__Init__arg HypABI__FileProtection__Init__arg__T;
+#define HypABI__FileProtection__Init__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__Init__arg) == HypABI__FileProtection__Init__arg__SZ, "Unexpected size for HypABI__FileProtection__Init__arg");
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID 1
+
+
+// start of HypABI__FileProtection__ViolationWriteReadOnlyFile__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationWriteReadOnlyFile__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ 747ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationWriteReadOnlyFile__arg) == HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationWriteReadOnlyFile__arg");
+
+#define HypABI__FileProtection__ViolationFileOps__OP_ID 2
+
+enum HypABI__FileProtection__ViolationFileOps__FopsType {
+        HypABI__FileProtection__ViolationFileOps__FopsType__EXT4 = 0,
+        HypABI__FileProtection__ViolationFileOps__FopsType__TMPFS = 1,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_MEM = 2,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_NULL = 3,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_PORT = 4,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_ZERO = 5,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_FULL = 6,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_RANDOM = 7,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_URANDOM = 8,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_KMSG = 9,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_TTY = 10,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEV_CONSOLE = 11,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PROC = 12,
+        HypABI__FileProtection__ViolationFileOps__FopsType__XFS = 13,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SOCKFS = 14,
+        HypABI__FileProtection__ViolationFileOps__FopsType__PIPEFS = 15,
+        HypABI__FileProtection__ViolationFileOps__FopsType__SYSFS = 16,
+        HypABI__FileProtection__ViolationFileOps__FopsType__DEBUGFS = 17,
+        HypABI__FileProtection__ViolationFileOps__FopsType__UNSUPPORTED = 255,
+};
+#define HypABI__FileProtection__ViolationFileOps__FopsType__COUNT 19
+#define HypABI__FileProtection__ViolationFileOps__FopsType__LABELS \
+        OP(EXT4) OP(TMPFS) OP(DEV_MEM) OP(DEV_NULL) OP(DEV_PORT) OP(DEV_ZERO) OP(DEV_FULL) OP(DEV_RANDOM) OP(DEV_URANDOM) OP(DEV_KMSG) OP(DEV_TTY) OP(DEV_CONSOLE) OP(PROC) OP(XFS) OP(SOCKFS) OP(PIPEFS) OP(SYSFS) OP(DEBUGFS) OP(UNSUPPORTED)
+#define HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ 1024ULL
+
+
+// start of HypABI__FileProtection__ViolationFileOps__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationFileOps__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a byte value set by the guest to indicate the file system type on which a violation occurred. */
+        uint8_t fops_type;
+        /** This parameter denotes whether the violating access was on a file (`false`) or directory (`true`). */
+        uint8_t is_dir;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+        uint8_t padding[5];
+        /** This parameter contains the address of the `file_ops` struct that was corrupted. */
+        uint64_t fops_ptr;
+        /** The path whose access generated the violation. This char array is at most 1024 bytes long. */
+        char path_name[HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ];
+};
+typedef struct HypABI__FileProtection__ViolationFileOps__arg HypABI__FileProtection__ViolationFileOps__arg__T;
+#define HypABI__FileProtection__ViolationFileOps__arg__SZ 1776ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationFileOps__arg) == HypABI__FileProtection__ViolationFileOps__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationFileOps__arg");
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID 3
+
+
+// start of HypABI__FileProtection__ViolationDirtyCredWrite__arg
+
+struct __attribute__((packed)) HypABI__FileProtection__ViolationDirtyCredWrite__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is a guest physical address that points to a string. This string represents the name of file that was the target of the write operation. */
+        uint64_t name;
+        /** This parameter contains the length of the string pointed to by the `name`parameter. */
+        uint16_t name_len;
+        /** This parameter is written by the **host** and determines whether the guest should block the write or not. */
+        uint8_t block;
+};
+typedef struct HypABI__FileProtection__ViolationDirtyCredWrite__arg HypABI__FileProtection__ViolationDirtyCredWrite__arg__T;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ 747ULL
+_Static_assert(sizeof(struct HypABI__FileProtection__ViolationDirtyCredWrite__arg) == HypABI__FileProtection__ViolationDirtyCredWrite__arg__SZ, "Unexpected size for HypABI__FileProtection__ViolationDirtyCredWrite__arg");
+
+#define HypABI__RegisterProtection__BACKEND_ID 9
+
+#define HypABI__RegisterProtection__Freeze__OP_ID 0
+
+#ifdef CONFIG_X86_64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR3 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__CR4 = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__EFER = 8,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 4
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(CR0) OP(CR3) OP(CR4) OP(EFER)
+
+#elif defined CONFIG_ARM64
+enum HypABI__RegisterProtection__Freeze__RegisterSelector {
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR0 = 1,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TTBR1 = 2,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__TCR = 4,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__SCTLR = 8,
+        HypABI__RegisterProtection__Freeze__RegisterSelector__VBAR = 16,
+};
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT 5
+#define HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS \
+        OP(TTBR0) OP(TTBR1) OP(TCR) OP(SCTLR) OP(VBAR)
+
+#else
+#error Unsupported architecture
+#endif
+
+// start of HypABI__RegisterProtection__Freeze__arg
+
+struct __attribute__((packed)) HypABI__RegisterProtection__Freeze__arg {
+        /** This parameter selects which register this freeze request applies to. */
+        uint64_t register_selector;
+        /** Any bit which is set in this bitfield will be frozen. As an example, a value of `ffffffffffffffff` will completely freeze a register. */
+        uint64_t freeze_bitfield;
+};
+typedef struct HypABI__RegisterProtection__Freeze__arg HypABI__RegisterProtection__Freeze__arg__T;
+#define HypABI__RegisterProtection__Freeze__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__RegisterProtection__Freeze__arg) == HypABI__RegisterProtection__Freeze__arg__SZ, "Unexpected size for HypABI__RegisterProtection__Freeze__arg");
+
+#define HypABI__Domain__BACKEND_ID 10
+
+
+// start of HypABI__Domain__Domain
+
+struct __attribute__((packed)) HypABI__Domain__Domain {
+        /** The ID of the domain that the operation refers to. */
+        uint64_t id;
+        /** The page global directory (process) that the operation refers to. */
+        uint64_t pgd;
+};
+typedef struct HypABI__Domain__Domain HypABI__Domain__Domain__T;
+#define HypABI__Domain__Domain__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Domain__Domain) == HypABI__Domain__Domain__SZ, "Unexpected size for HypABI__Domain__Domain");
+
+#define HypABI__Domain__Configure__OP_ID 0
+
+
+// start of HypABI__Domain__Configure__arg
+
+struct __attribute__((packed)) HypABI__Domain__Configure__arg {
+        uint8_t padding0[16];
+        /** Whether or not the guest is using page table isolation. */
+        uint8_t pti;
+        uint8_t padding1[7];
+        /** The GPA of the physical region that is used for the indirect communication channel. */
+        uint64_t batched_region;
+        /** Differentiates between the heavy- and light-weight strong isolation version. If `true`, the heavy-weight strong isolation version is in use. */
+        uint8_t isolate;
+        /** If `true`, forced memory access (e.g., through debuggers) is allowed. */
+        uint8_t allow_forced_mem_access;
+};
+typedef struct HypABI__Domain__Configure__arg HypABI__Domain__Configure__arg__T;
+#define HypABI__Domain__Configure__arg__SZ 34ULL
+_Static_assert(sizeof(struct HypABI__Domain__Configure__arg) == HypABI__Domain__Configure__arg__SZ, "Unexpected size for HypABI__Domain__Configure__arg");
+
+#define HypABI__Domain__Report__OP_ID 10
+
+
+// start of HypABI__Domain__Report__arg
+
+struct __attribute__((packed)) HypABI__Domain__Report__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__Report__arg HypABI__Domain__Report__arg__T;
+#define HypABI__Domain__Report__arg__SZ 802ULL
+_Static_assert(sizeof(struct HypABI__Domain__Report__arg) == HypABI__Domain__Report__arg__SZ, "Unexpected size for HypABI__Domain__Report__arg");
+
+#define HypABI__Domain__ReportForcedMemAccess__OP_ID 11
+
+
+// start of HypABI__Domain__ReportForcedMemAccess__arg
+
+struct __attribute__((packed)) HypABI__Domain__ReportForcedMemAccess__arg {
+        uint8_t padding[16];
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** The domain that intends to request access permissions. */
+        struct HypABI__Domain__Domain domain_src;
+        /** The domain that was requested to be accessed. */
+        struct HypABI__Domain__Domain domain_target;
+        /** The start of the GVA range that was requested to be accessed. */
+        uint64_t gva_start;
+        /** The end of the GVA range that was requested to be accessed. */
+        uint64_t gva_end;
+        /** Differentiates the requested access permission between read and read/write. */
+        uint8_t write;
+        /** This value is written by BRASS to inform the kernel about whether not to block the reported event. */
+        uint8_t block;
+};
+typedef struct HypABI__Domain__ReportForcedMemAccess__arg HypABI__Domain__ReportForcedMemAccess__arg__T;
+#define HypABI__Domain__ReportForcedMemAccess__arg__SZ 802ULL
+_Static_assert(sizeof(struct HypABI__Domain__ReportForcedMemAccess__arg) == HypABI__Domain__ReportForcedMemAccess__arg__SZ, "Unexpected size for HypABI__Domain__ReportForcedMemAccess__arg");
+
+#define HypABI__Inode__BACKEND_ID 11
+
+enum HypABI__Inode__EventType {
+        HypABI__Inode__EventType__EVENT_NONE = 0,
+        HypABI__Inode__EventType__NO_INODE = 1,
+        HypABI__Inode__EventType__CORRUPTION = 2,
+};
+#define HypABI__Inode__EventType__COUNT 3
+#define HypABI__Inode__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_INODE) OP(CORRUPTION)
+
+// start of HypABI__Inode__TaskCred
+
+struct __attribute__((packed)) HypABI__Inode__TaskCred {
+        /** This is the guest virtual address of the `struct inode` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct inode` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Inode__TaskCred HypABI__Inode__TaskCred__T;
+#define HypABI__Inode__TaskCred__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__TaskCred) == HypABI__Inode__TaskCred__SZ, "Unexpected size for HypABI__Inode__TaskCred");
+
+#define HypABI__Inode__Register__OP_ID 0
+
+
+// start of HypABI__Inode__Register__arg
+
+struct __attribute__((packed)) HypABI__Inode__Register__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Inode__TaskCred inode;
+};
+typedef struct HypABI__Inode__Register__arg HypABI__Inode__Register__arg__T;
+#define HypABI__Inode__Register__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Register__arg) == HypABI__Inode__Register__arg__SZ, "Unexpected size for HypABI__Inode__Register__arg");
+
+#define HypABI__Inode__Update__OP_ID 1
+
+
+// start of HypABI__Inode__Update__arg
+
+struct __attribute__((packed)) HypABI__Inode__Update__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be updated by BRASS. */
+        struct HypABI__Inode__TaskCred inode;
+};
+typedef struct HypABI__Inode__Update__arg HypABI__Inode__Update__arg__T;
+#define HypABI__Inode__Update__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Update__arg) == HypABI__Inode__Update__arg__SZ, "Unexpected size for HypABI__Inode__Update__arg");
+
+#define HypABI__Inode__Release__OP_ID 2
+
+
+// start of HypABI__Inode__Release__arg
+
+struct __attribute__((packed)) HypABI__Inode__Release__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` that is to be released by BRASS. */
+        struct HypABI__Inode__TaskCred inode;
+};
+typedef struct HypABI__Inode__Release__arg HypABI__Inode__Release__arg__T;
+#define HypABI__Inode__Release__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Inode__Release__arg) == HypABI__Inode__Release__arg__SZ, "Unexpected size for HypABI__Inode__Release__arg");
+
+#define HypABI__Inode__Verify__OP_ID 3
+
+
+// start of HypABI__Inode__Verify__arg
+
+struct __attribute__((packed)) HypABI__Inode__Verify__arg {
+        /** This is the inode structure containing the GVA of the `struct inode` and its corresponding HMAC that is to be verified by BRASS. */
+        struct HypABI__Inode__TaskCred inode;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Inode__Verify__arg HypABI__Inode__Verify__arg__T;
+#define HypABI__Inode__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Inode__Verify__arg) == HypABI__Inode__Verify__arg__SZ, "Unexpected size for HypABI__Inode__Verify__arg");
+
+#define HypABI__Inode__Log__OP_ID 4
+
+
+// start of HypABI__Inode__Log__arg
+
+struct __attribute__((packed)) HypABI__Inode__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the inode in question. */
+        uint64_t inode_addr;
+        /** This parameter contains the uid field of the inode in question. */
+        uint32_t inode_uid;
+        /** This parameter contains the gid field of the inode in question. */
+        uint32_t inode_gid;
+        /** This parameter contains the mode field of the inode in question. */
+        uint16_t inode_mode;
+        uint8_t padding[4];
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /**  */
+        uint8_t block;
+        /** This parameter contains the file path to the binary that is represented by the inode in question. */
+        char file_path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Inode__Log__arg HypABI__Inode__Log__arg__T;
+#define HypABI__Inode__Log__arg__SZ 1016ULL
+_Static_assert(sizeof(struct HypABI__Inode__Log__arg) == HypABI__Inode__Log__arg__SZ, "Unexpected size for HypABI__Inode__Log__arg");
+
+#define HypABI__Keyring__BACKEND_ID 12
+
+enum HypABI__Keyring__EventType {
+        HypABI__Keyring__EventType__EVENT_NONE = 0,
+        HypABI__Keyring__EventType__NO_KEYRING = 1,
+        HypABI__Keyring__EventType__CORRUPTION = 2,
+};
+#define HypABI__Keyring__EventType__COUNT 3
+#define HypABI__Keyring__EventType__LABELS \
+        OP(EVENT_NONE) OP(NO_KEYRING) OP(CORRUPTION)
+
+// start of HypABI__Keyring__Keyring
+
+struct __attribute__((packed)) HypABI__Keyring__Keyring {
+        /** This is the guest virtual address of the `struct key` associated with this call. */
+        uint64_t addr;
+        /** This is the computed HMAC for the `struct key` structure of the previous value. */
+        uint64_t hmac;
+};
+typedef struct HypABI__Keyring__Keyring HypABI__Keyring__Keyring__T;
+#define HypABI__Keyring__Keyring__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Keyring) == HypABI__Keyring__Keyring__SZ, "Unexpected size for HypABI__Keyring__Keyring");
+
+#define HypABI__Keyring__Register__OP_ID 0
+
+
+// start of HypABI__Keyring__Register__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Register__arg {
+        /** This is the keyring structure containing the GVA of the `struct key` and its corresponding HMAC that is to be registered by BRASS. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        uint8_t block;
+};
+typedef struct HypABI__Keyring__Register__arg HypABI__Keyring__Register__arg__T;
+#define HypABI__Keyring__Register__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Register__arg) == HypABI__Keyring__Register__arg__SZ, "Unexpected size for HypABI__Keyring__Register__arg");
+
+#define HypABI__Keyring__Verify__OP_ID 2
+
+
+// start of HypABI__Keyring__Verify__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Verify__arg {
+        /** This field is set by the host if the given keyring was already registered and will be processed by the guest after the hypercall. It tells the guest whether to block the action. */
+        struct HypABI__Keyring__Keyring keyring;
+        /** This parameter is written by the host and will return the `event_type` of the operation. */
+        uint8_t ret;
+};
+typedef struct HypABI__Keyring__Verify__arg HypABI__Keyring__Verify__arg__T;
+#define HypABI__Keyring__Verify__arg__SZ 17ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Verify__arg) == HypABI__Keyring__Verify__arg__SZ, "Unexpected size for HypABI__Keyring__Verify__arg");
+
+#define HypABI__Keyring__Log__OP_ID 3
+
+
+// start of HypABI__Keyring__Log__arg
+
+struct __attribute__((packed)) HypABI__Keyring__Log__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter contains the guest-virtual address of the keyring in question. */
+        uint64_t keyring_addr;
+        /** This parameter contains the uid field of the keyring in question. */
+        uint32_t keyring_uid;
+        /** This parameter contains the gid field of the keyring in question. */
+        uint32_t keyring_gid;
+        /** This parameter contains the permissions of the keyring in question. */
+        uint32_t keyring_perm;
+        /** This parameter contains the ID of the keyring in question. */
+        uint32_t keyring_serial;
+        /** This parameter contains the `event_type` of the event to log. */
+        uint8_t event_type;
+        /** This parameter tells the guest whether to block the event. */
+        uint8_t block;
+        /** This parameter contains the keyrings name/description. */
+        char keyring_desc[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__Keyring__Log__arg HypABI__Keyring__Log__arg__T;
+#define HypABI__Keyring__Log__arg__SZ 1018ULL
+_Static_assert(sizeof(struct HypABI__Keyring__Log__arg) == HypABI__Keyring__Log__arg__SZ, "Unexpected size for HypABI__Keyring__Log__arg");
+
+#define HypABI__Confserver__BACKEND_ID 13
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID 0
+
+
+// start of HypABI__Confserver__FreezeMemoryAfterBoot__arg
+
+struct __attribute__((packed)) HypABI__Confserver__FreezeMemoryAfterBoot__arg {
+        /** This field will be set to `true` if the host wants the guest to freeze its memory protections after boot. */
+        uint8_t freeze_memory_after_boot;
+};
+typedef struct HypABI__Confserver__FreezeMemoryAfterBoot__arg HypABI__Confserver__FreezeMemoryAfterBoot__arg__T;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__FreezeMemoryAfterBoot__arg) == HypABI__Confserver__FreezeMemoryAfterBoot__arg__SZ, "Unexpected size for HypABI__Confserver__FreezeMemoryAfterBoot__arg");
+
+#define HypABI__Confserver__StrictFileops__OP_ID 1
+
+
+// start of HypABI__Confserver__StrictFileops__arg
+
+struct __attribute__((packed)) HypABI__Confserver__StrictFileops__arg {
+        /** This field will be set to `true` if the host wants the guest to enforce strict file operation checks. */
+        uint8_t strict_fileops;
+};
+typedef struct HypABI__Confserver__StrictFileops__arg HypABI__Confserver__StrictFileops__arg__T;
+#define HypABI__Confserver__StrictFileops__arg__SZ 1ULL
+_Static_assert(sizeof(struct HypABI__Confserver__StrictFileops__arg) == HypABI__Confserver__StrictFileops__arg__SZ, "Unexpected size for HypABI__Confserver__StrictFileops__arg");
+
+#define HypABI__ContainerIntegrity__BACKEND_ID 14
+
+#define HypABI__ContainerIntegrity__Init__OP_ID 0
+
+
+// start of HypABI__ContainerIntegrity__Init__HypABI__ContainerIntegrity__Init__Config
+
+typedef unsigned long HypABI__ContainerIntegrity__Init__Config__T;
+#define HypABI__ContainerIntegrity__Init__Config__NONE 0UL
+#define HypABI__ContainerIntegrity__Init__Config__NEW_FILE_EXECUTION__BIT 0
+#define HypABI__ContainerIntegrity__Init__Config__NEW_FILE_EXECUTION (1UL<<0)
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_NEW_FILE__BIT 1
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_NEW_FILE (1UL<<1)
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_CMD__BIT 2
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_CMD (1UL<<2)
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_BINDS__BIT 3
+#define HypABI__ContainerIntegrity__Init__Config__INTERPRETER_BINDS (1UL<<3)
+void HypABI__ContainerIntegrity__Init__Config__dump(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr);
+
+
+// start of HypABI__ContainerIntegrity__Init__arg
+
+struct __attribute__((packed)) HypABI__ContainerIntegrity__Init__arg {
+        /** This bitmap is written by the **host** and is updated to indicate which containe integrity violations the host is expecting from the guest. */
+        uint64_t feature_bitmap;
+};
+typedef struct HypABI__ContainerIntegrity__Init__arg HypABI__ContainerIntegrity__Init__arg__T;
+#define HypABI__ContainerIntegrity__Init__arg__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__ContainerIntegrity__Init__arg) == HypABI__ContainerIntegrity__Init__arg__SZ, "Unexpected size for HypABI__ContainerIntegrity__Init__arg");
+
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__OP_ID 1
+
+
+// start of HypABI__ContainerIntegrity__ViolationNewFileExecution__arg
+
+struct __attribute__((packed)) HypABI__ContainerIntegrity__ViolationNewFileExecution__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is written by the **host** and determines whether the guest should block the execution or not. */
+        uint8_t block;
+        /** The path of the new file being executed. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__ContainerIntegrity__ViolationNewFileExecution__arg HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T;
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__SZ 993ULL
+_Static_assert(sizeof(struct HypABI__ContainerIntegrity__ViolationNewFileExecution__arg) == HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__SZ, "Unexpected size for HypABI__ContainerIntegrity__ViolationNewFileExecution__arg");
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__OP_ID 2
+
+
+// start of HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg
+
+struct __attribute__((packed)) HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is written by the **host** and determines whether the guest should block the execution or not. */
+        uint8_t block;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The path of the new file being executed. */
+        char path[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T;
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__SZ 1249ULL
+_Static_assert(sizeof(struct HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg) == HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__SZ, "Unexpected size for HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg");
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__OP_ID 3
+
+
+// start of HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg
+
+struct __attribute__((packed)) HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is written by the **host** and determines whether the guest should block the execution or not. */
+        uint8_t block;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+        /** The command being executed. */
+        char command[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T;
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__SZ 1249ULL
+_Static_assert(sizeof(struct HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg) == HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__SZ, "Unexpected size for HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg");
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__OP_ID 4
+
+
+// start of HypABI__ContainerIntegrity__ViolationInterpreterBound__arg
+
+struct __attribute__((packed)) HypABI__ContainerIntegrity__ViolationInterpreterBound__arg {
+        /** The BHV event context. */
+        struct HypABI__Context context;
+        /** This parameter is written by the **host** and determines whether the guest should block the execution or not. */
+        uint8_t block;
+        /** The path of the interpreter. */
+        char interpreter_name[HypABI__Context__MAX_PATH_SZ];
+};
+typedef struct HypABI__ContainerIntegrity__ViolationInterpreterBound__arg HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T;
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__SZ 993ULL
+_Static_assert(sizeof(struct HypABI__ContainerIntegrity__ViolationInterpreterBound__arg) == HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__SZ, "Unexpected size for HypABI__ContainerIntegrity__ViolationInterpreterBound__arg");
+
+#define HypABI__Wagner__BACKEND_ID 15
+
+
+// start of HypABI__Wagner__MemRegion
+
+struct __attribute__((packed)) HypABI__Wagner__MemRegion {
+        /** This is the guest-physical start address of the memory region. */
+        uint64_t gpa;
+        /** This is the size of the memory region (in Bytes). */
+        uint64_t size;
+};
+typedef struct HypABI__Wagner__MemRegion HypABI__Wagner__MemRegion__T;
+#define HypABI__Wagner__MemRegion__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemRegion) == HypABI__Wagner__MemRegion__SZ, "Unexpected size for HypABI__Wagner__MemRegion");
+
+
+// start of HypABI__Wagner__MemSegment
+
+struct __attribute__((packed)) HypABI__Wagner__MemSegment {
+        /** - */
+        uint64_t start;
+        /** - */
+        uint64_t end;
+};
+typedef struct HypABI__Wagner__MemSegment HypABI__Wagner__MemSegment__T;
+#define HypABI__Wagner__MemSegment__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__MemSegment) == HypABI__Wagner__MemSegment__SZ, "Unexpected size for HypABI__Wagner__MemSegment");
+
+
+// start of HypABI__Wagner__TransitPoint
+
+struct __attribute__((packed)) HypABI__Wagner__TransitPoint {
+        /** This is the guest-physical address of a transit point, that can be an entry point or a return point, to the vault. An entry point is a guest-physical address, the execution of which allows to enter the vault; a return point is a guest-physical address, to which the vault can safely return, e.g., after a function call. */
+        uint64_t gpa;
+};
+typedef struct HypABI__Wagner__TransitPoint HypABI__Wagner__TransitPoint__T;
+#define HypABI__Wagner__TransitPoint__SZ 8ULL
+_Static_assert(sizeof(struct HypABI__Wagner__TransitPoint) == HypABI__Wagner__TransitPoint__SZ, "Unexpected size for HypABI__Wagner__TransitPoint");
+
+#define HypABI__Wagner__Create__OP_ID 0
+
+
+// start of HypABI__Wagner__Create__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Create__arg {
+        /** The vault code region. */
+        struct HypABI__Wagner__MemRegion code;
+        /** The vault ref code region. */
+        struct HypABI__Wagner__MemRegion ref_code;
+        /** The shared code region. */
+        struct HypABI__Wagner__MemRegion shared_code;
+        /** The vault alternative instruction region. */
+        struct HypABI__Wagner__MemRegion altinstr_aux;
+        /** The vault noinstr region. */
+        struct HypABI__Wagner__MemRegion noinstr_text;
+        /** The vault thunk region. */
+        struct HypABI__Wagner__MemRegion thunks;
+        /** The vault data region. */
+        struct HypABI__Wagner__MemRegion data;
+        /** The vault read-only data region. */
+        struct HypABI__Wagner__MemRegion ro_data;
+        /** The vault entry text region. */
+        struct HypABI__Wagner__MemSegment entry_text;
+        /** This is the total number of entry points into the vault. */
+        uint32_t nr_entry_points;
+        /** This is the total number of return points into the vault. */
+        uint32_t nr_return_points;
+        /** This buffer contains `nr_entry_points` + `nr_return_points` entries with transit points. */
+        uint64_t transit_points;
+};
+typedef struct HypABI__Wagner__Create__arg HypABI__Wagner__Create__arg__T;
+#define HypABI__Wagner__Create__arg__SZ 160ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Create__arg) == HypABI__Wagner__Create__arg__SZ, "Unexpected size for HypABI__Wagner__Create__arg");
+
+#define HypABI__Wagner__Extend__OP_ID 1
+
+
+// start of HypABI__Wagner__Extend__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Extend__arg {
+        /** This entry contains the memory holding data that should be added to the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Extend__arg HypABI__Wagner__Extend__arg__T;
+#define HypABI__Wagner__Extend__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Extend__arg) == HypABI__Wagner__Extend__arg__SZ, "Unexpected size for HypABI__Wagner__Extend__arg");
+
+#define HypABI__Wagner__Delete__OP_ID 2
+
+
+// start of HypABI__Wagner__Delete__arg
+
+struct __attribute__((packed)) HypABI__Wagner__Delete__arg {
+        /** This entry contains the memory holding data that should be removed from the vault. */
+        struct HypABI__Wagner__MemRegion mem;
+};
+typedef struct HypABI__Wagner__Delete__arg HypABI__Wagner__Delete__arg__T;
+#define HypABI__Wagner__Delete__arg__SZ 16ULL
+_Static_assert(sizeof(struct HypABI__Wagner__Delete__arg) == HypABI__Wagner__Delete__arg__SZ, "Unexpected size for HypABI__Wagner__Delete__arg");
+
+
+
+// GuestConnABI
+
+#define GuestConnABI__MAX_MSG_SZ 4096ULL
+
+
+// start of GuestConnABI__Header
+
+struct __attribute__((packed)) GuestConnABI__Header {
+        uint16_t backend;
+        uint16_t sz;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__Header GuestConnABI__Header__T;
+#define GuestConnABI__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__Header) == GuestConnABI__Header__SZ, "Unexpected size for GuestConnABI__Header");
+
+#define GuestConnABI__GuestLog__BACKEND_ID 0
+
+#define GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ 32ULL
+
+#define GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ 1024ULL
+
+#define GuestConnABI__GuestLog__PROC_COMM_SZ 16ULL
+
+
+// start of GuestConnABI__GuestLog__Header
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Header {
+        uint16_t evt;
+        uint16_t sz;
+};
+typedef struct GuestConnABI__GuestLog__Header GuestConnABI__GuestLog__Header__T;
+#define GuestConnABI__GuestLog__Header__SZ 4ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Header) == GuestConnABI__GuestLog__Header__SZ, "Unexpected size for GuestConnABI__GuestLog__Header");
+
+
+// start of GuestConnABI__GuestLog__Message
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__Message {
+        struct GuestConnABI__GuestLog__Header header;
+        struct HypABI__Context context;
+        uint8_t payload[];
+};
+typedef struct GuestConnABI__GuestLog__Message GuestConnABI__GuestLog__Message__T;
+#define GuestConnABI__GuestLog__Message__SZ 740ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__Message) == GuestConnABI__GuestLog__Message__SZ, "Unexpected size for GuestConnABI__GuestLog__Message");
+
+#define GuestConnABI__GuestLog__StringMsg__EVT_ID 0
+
+
+// start of GuestConnABI__GuestLog__StringMsg
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__StringMsg {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated string that will be printed to the host logs. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__StringMsg GuestConnABI__GuestLog__StringMsg__T;
+#define GuestConnABI__GuestLog__StringMsg__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__StringMsg) == GuestConnABI__GuestLog__StringMsg__SZ, "Unexpected size for GuestConnABI__GuestLog__StringMsg");
+
+#define GuestConnABI__GuestLog__ProcessFork__EVT_ID 1
+
+
+// start of GuestConnABI__GuestLog__ProcessFork
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessFork {
+        /** The process ID of the newly forked process. */
+        uint32_t child_pid;
+        /** The process ID of the parent that forked the process. */
+        uint32_t parent_pid;
+        /** The offset from the start of the `buf` field where the command string for the child process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t child_comm_offset;
+        /** The offset from the start of the `buf` field where the command string for the parent process is stored. This string should contain a NUL-terminated ASCII string. */
+        uint32_t parent_comm_offset;
+        /** A buffer to store `child_comm_offset` and `parent_comm_offset`. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessFork GuestConnABI__GuestLog__ProcessFork__T;
+#define GuestConnABI__GuestLog__ProcessFork__SZ 16ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessFork) == GuestConnABI__GuestLog__ProcessFork__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessFork");
+
+#define GuestConnABI__GuestLog__ProcessExec__EVT_ID 2
+
+
+// start of GuestConnABI__GuestLog__ProcessExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExec {
+        /** The process ID of the process that executed the binary. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** A NUL-terminated ASCII string that contains the path of the binary to be executed. */
+        char name[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExec GuestConnABI__GuestLog__ProcessExec__T;
+#define GuestConnABI__GuestLog__ProcessExec__SZ 2088ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExec) == GuestConnABI__GuestLog__ProcessExec__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExec");
+
+#define GuestConnABI__GuestLog__ProcessExit__EVT_ID 3
+
+
+// start of GuestConnABI__GuestLog__ProcessExit
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__ProcessExit {
+        /** The process ID of the process that terminated. */
+        uint32_t pid;
+        /** The process ID of the parent of the calling process. */
+        uint32_t parent_pid;
+        /** A NUL-terminated ASCII string that contains the path of the binary that was terminated. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__ProcessExit GuestConnABI__GuestLog__ProcessExit__T;
+#define GuestConnABI__GuestLog__ProcessExit__SZ 8ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__ProcessExit) == GuestConnABI__GuestLog__ProcessExit__SZ, "Unexpected size for GuestConnABI__GuestLog__ProcessExit");
+
+#define GuestConnABI__GuestLog__DriverLoad__EVT_ID 4
+
+
+// start of GuestConnABI__GuestLog__DriverLoad
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__DriverLoad {
+        /** Make sure that the unbound array is not the only field */
+        uint8_t _[0];
+        /** A NUL-terminated ASCII string that contains the path of the binary that was loaded as a driver. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__DriverLoad GuestConnABI__GuestLog__DriverLoad__T;
+#define GuestConnABI__GuestLog__DriverLoad__SZ 0ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__DriverLoad) == GuestConnABI__GuestLog__DriverLoad__SZ, "Unexpected size for GuestConnABI__GuestLog__DriverLoad");
+
+#define GuestConnABI__GuestLog__KernelAccess__EVT_ID 5
+
+enum GuestConnABI__GuestLog__KernelAccess__AccessType {
+        GuestConnABI__GuestLog__KernelAccess__AccessType__READ = 0,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__WRITE = 1,
+        GuestConnABI__GuestLog__KernelAccess__AccessType__EXECUTE = 2,
+};
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__COUNT 3
+#define GuestConnABI__GuestLog__KernelAccess__AccessType__LABELS \
+        OP(READ) OP(WRITE) OP(EXECUTE)
+
+// start of GuestConnABI__GuestLog__KernelAccess
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelAccess {
+        uint64_t address;
+        uint8_t type;
+};
+typedef struct GuestConnABI__GuestLog__KernelAccess GuestConnABI__GuestLog__KernelAccess__T;
+#define GuestConnABI__GuestLog__KernelAccess__SZ 9ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelAccess) == GuestConnABI__GuestLog__KernelAccess__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelAccess");
+
+#define GuestConnABI__GuestLog__FopsUnknown__EVT_ID 6
+
+enum GuestConnABI__GuestLog__FopsUnknown__FileStructType {
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__UNKNOWN = 0,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE = 1,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY = 2,
+        GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL = 3,
+};
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__COUNT 4
+#define GuestConnABI__GuestLog__FopsUnknown__FileStructType__LABELS \
+        OP(UNKNOWN) OP(FILE) OP(DIRECTORY) OP(SPECIAL)
+
+// start of GuestConnABI__GuestLog__FopsUnknown
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__FopsUnknown {
+        /** The file-system magic number as defined in [Linux](https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/magic.h). */
+        uint64_t magic;
+        /** The type of the file struct using the unknown file operation instance. */
+        uint8_t struct_type;
+        uint8_t padding[3];
+        /** Major device number for the unknown file operation instance (only relevant for special files). */
+        uint32_t special_major;
+        /** Minor device number for the unknown file operation instance (only relevant for special files). */
+        uint64_t special_minor;
+        /** This is the address of the unknown operations instance used. */
+        uint64_t address;
+        /** The name of the accessed file or directory which used the unknown file operations instance. */
+        char buf[];
+};
+typedef struct GuestConnABI__GuestLog__FopsUnknown GuestConnABI__GuestLog__FopsUnknown__T;
+#define GuestConnABI__GuestLog__FopsUnknown__SZ 32ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__FopsUnknown) == GuestConnABI__GuestLog__FopsUnknown__SZ, "Unexpected size for GuestConnABI__GuestLog__FopsUnknown");
+
+#define GuestConnABI__GuestLog__KernelExec__EVT_ID 7
+
+
+// start of GuestConnABI__GuestLog__KernelExec
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__KernelExec {
+        /** The path to the program that is being executed. */
+        char path[GuestConnABI__GuestLog__PROC_EXEC_NAME_SZ];
+        /** The arguments passed to the program that is being executed. */
+        char args[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+        /** The environment passed to the program that is being executed. */
+        char env[GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ];
+};
+typedef struct GuestConnABI__GuestLog__KernelExec GuestConnABI__GuestLog__KernelExec__T;
+#define GuestConnABI__GuestLog__KernelExec__SZ 2080ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__KernelExec) == GuestConnABI__GuestLog__KernelExec__SZ, "Unexpected size for GuestConnABI__GuestLog__KernelExec");
+
+#define GuestConnABI__GuestLog__CgroupCreate__EVT_ID 8
+
+
+// start of GuestConnABI__GuestLog__CgroupCreate
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupCreate {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupCreate GuestConnABI__GuestLog__CgroupCreate__T;
+#define GuestConnABI__GuestLog__CgroupCreate__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupCreate) == GuestConnABI__GuestLog__CgroupCreate__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupCreate");
+
+#define GuestConnABI__GuestLog__CgroupDestroy__EVT_ID 9
+
+
+// start of GuestConnABI__GuestLog__CgroupDestroy
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__CgroupDestroy {
+        /** This is the cgroup identifier for the current process. */
+        uint64_t cgroup_id;
+        /** This is the cgroup name for the current process. */
+        char cgroup_name[HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ];
+};
+typedef struct GuestConnABI__GuestLog__CgroupDestroy GuestConnABI__GuestLog__CgroupDestroy__T;
+#define GuestConnABI__GuestLog__CgroupDestroy__SZ 136ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__CgroupDestroy) == GuestConnABI__GuestLog__CgroupDestroy__SZ, "Unexpected size for GuestConnABI__GuestLog__CgroupDestroy");
+
+#define GuestConnABI__GuestLog__NamespaceChange__EVT_ID 10
+
+
+// start of GuestConnABI__GuestLog__NamespaceChange
+
+struct __attribute__((packed)) GuestConnABI__GuestLog__NamespaceChange {
+        /** The process ID of the process that updated. */
+        uint32_t target_task_pid;
+        uint8_t padding[4];
+        /** The process name of the process that updated. */
+        char target_task_name[GuestConnABI__GuestLog__PROC_COMM_SZ];
+        /** The incoming namespace identifiers for the target process. */
+        struct HypABI__Context__Inums incoming_inums;
+};
+typedef struct GuestConnABI__GuestLog__NamespaceChange GuestConnABI__GuestLog__NamespaceChange__T;
+#define GuestConnABI__GuestLog__NamespaceChange__SZ 64ULL
+_Static_assert(sizeof(struct GuestConnABI__GuestLog__NamespaceChange) == GuestConnABI__GuestLog__NamespaceChange__SZ, "Unexpected size for GuestConnABI__GuestLog__NamespaceChange");
+
diff --git include/bhv/interface/abi_hl_autogen.h include/bhv/interface/abi_hl_autogen.h
new file mode 100644
index 00000000000..0817126cbcc
--- /dev/null
+++ include/bhv/interface/abi_hl_autogen.h
@@ -0,0 +1,1724 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-08-23T11:38:28).
+ */
+
+#pragma once
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_ml_autogen.h>
+
+void HypABI__init_slabs(void);
+
+extern struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab;
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__Mem_Region__T *)kmem_cache_alloc(HypABI__Integrity__Create__Mem_Region__slab, gfp)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__Mem_Region__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__ALLOC() \
+        HypABI__Integrity__Create__Mem_Region__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__Mem_Region__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__Mem_Region__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__Mem_Region__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__Mem_Region__T *__arg = &static; \
+        if (HypABI__Integrity__Create__Mem_Region__slab) \
+                __arg = HypABI__Integrity__Create__Mem_Region__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__Mem_Region__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__Mem_Region__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Integrity__Create__arg__slab;
+#define HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Create__arg__T *)kmem_cache_alloc(HypABI__Integrity__Create__arg__slab, gfp)
+#define HypABI__Integrity__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__ALLOC() \
+        HypABI__Integrity__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Create__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Create__arg__slab) \
+                __arg = HypABI__Integrity__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall(const HypABI__Integrity__Create__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Create__arg__T *bhv_arg = HypABI__Integrity__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Create__arg__T));
+        rc = HypABI__Integrity__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Create__HYPERCALL(...) HypABI__Integrity__Create__hypercall((HypABI__Integrity__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Update__arg__slab;
+#define HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Update__arg__T *)kmem_cache_alloc(HypABI__Integrity__Update__arg__slab, gfp)
+#define HypABI__Integrity__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__ALLOC() \
+        HypABI__Integrity__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Update__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Update__arg__slab) \
+                __arg = HypABI__Integrity__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall(const HypABI__Integrity__Update__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Update__arg__T *bhv_arg = HypABI__Integrity__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Update__arg__T));
+        rc = HypABI__Integrity__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Update__HYPERCALL(...) HypABI__Integrity__Update__hypercall((HypABI__Integrity__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Remove__arg__slab;
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Remove__arg__T *)kmem_cache_alloc(HypABI__Integrity__Remove__arg__slab, gfp)
+#define HypABI__Integrity__Remove__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Remove__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__ALLOC() \
+        HypABI__Integrity__Remove__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Remove__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Remove__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Remove__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Remove__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Remove__arg__slab) \
+                __arg = HypABI__Integrity__Remove__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Remove__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Remove__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall(const HypABI__Integrity__Remove__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Remove__arg__T *bhv_arg = HypABI__Integrity__Remove__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Remove__arg__T));
+        rc = HypABI__Integrity__Remove__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Remove__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Remove__HYPERCALL(...) HypABI__Integrity__Remove__hypercall((HypABI__Integrity__Remove__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__Freeze__arg__slab;
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__Freeze__arg__T *)kmem_cache_alloc(HypABI__Integrity__Freeze__arg__slab, gfp)
+#define HypABI__Integrity__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__ALLOC() \
+        HypABI__Integrity__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__Freeze__arg__T *__arg = &static; \
+        if (HypABI__Integrity__Freeze__arg__slab) \
+                __arg = HypABI__Integrity__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall(const HypABI__Integrity__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__Freeze__arg__T *bhv_arg = HypABI__Integrity__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__Freeze__arg__T));
+        rc = HypABI__Integrity__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__Freeze__HYPERCALL(...) HypABI__Integrity__Freeze__hypercall((HypABI__Integrity__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab;
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Integrity__PtpgInit__arg__T *)kmem_cache_alloc(HypABI__Integrity__PtpgInit__arg__slab, gfp)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Integrity__PtpgInit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_NOCHECK() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__ALLOC() \
+        HypABI__Integrity__PtpgInit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Integrity__PtpgInit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Integrity__PtpgInit__arg__slab, name); \
+        name = NULL
+#define HypABI__Integrity__PtpgInit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Integrity__PtpgInit__arg__T *__arg = &static; \
+        if (HypABI__Integrity__PtpgInit__arg__slab) \
+                __arg = HypABI__Integrity__PtpgInit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Integrity__PtpgInit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Integrity__PtpgInit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall(const HypABI__Integrity__PtpgInit__arg__T s)
+{
+        int rc;
+        HypABI__Integrity__PtpgInit__arg__T *bhv_arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Integrity__PtpgInit__arg__T));
+        rc = HypABI__Integrity__PtpgInit__hypercall_noalloc(bhv_arg);
+        HypABI__Integrity__PtpgInit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Integrity__PtpgInit__HYPERCALL(...) HypABI__Integrity__PtpgInit__hypercall((HypABI__Integrity__PtpgInit__arg__T){__VA_ARGS__})
+
+#define HypABI__Integrity__PtpgReport__hypercall() \
+        HypABI__Integrity__PtpgReport__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Patch__Patch__arg__slab;
+#define HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__Patch__arg__T *)kmem_cache_alloc(HypABI__Patch__Patch__arg__slab, gfp)
+#define HypABI__Patch__Patch__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__Patch__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__ALLOC() \
+        HypABI__Patch__Patch__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__Patch__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__Patch__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__Patch__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__Patch__arg__T *__arg = &static; \
+        if (HypABI__Patch__Patch__arg__slab) \
+                __arg = HypABI__Patch__Patch__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__Patch__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__Patch__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall(const HypABI__Patch__Patch__arg__T s)
+{
+        int rc;
+        HypABI__Patch__Patch__arg__T *bhv_arg = HypABI__Patch__Patch__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__Patch__arg__T));
+        rc = HypABI__Patch__Patch__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__Patch__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__Patch__HYPERCALL(...) HypABI__Patch__Patch__hypercall((HypABI__Patch__Patch__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab;
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchNoClose__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchNoClose__arg__slab, gfp)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchNoClose__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__ALLOC() \
+        HypABI__Patch__PatchNoClose__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchNoClose__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchNoClose__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchNoClose__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchNoClose__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchNoClose__arg__slab) \
+                __arg = HypABI__Patch__PatchNoClose__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchNoClose__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchNoClose__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall(const HypABI__Patch__PatchNoClose__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchNoClose__arg__T *bhv_arg = HypABI__Patch__PatchNoClose__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchNoClose__arg__T));
+        rc = HypABI__Patch__PatchNoClose__hypercall_noalloc(bhv_arg);
+        HypABI__Patch__PatchNoClose__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchNoClose__HYPERCALL(...) HypABI__Patch__PatchNoClose__hypercall((HypABI__Patch__PatchNoClose__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab;
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Patch__PatchViolation__arg__T *)kmem_cache_alloc(HypABI__Patch__PatchViolation__arg__slab, gfp)
+#define HypABI__Patch__PatchViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Patch__PatchViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__ALLOC() \
+        HypABI__Patch__PatchViolation__arg__ALLOC_GFP(GFP_ATOMIC)
+#define HypABI__Patch__PatchViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Patch__PatchViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Patch__PatchViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Patch__PatchViolation__arg__T *__arg = &static; \
+        if (HypABI__Patch__PatchViolation__arg__slab) \
+                __arg = HypABI__Patch__PatchViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Patch__PatchViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Patch__PatchViolation__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall(uint8_t *block_out, const HypABI__Patch__PatchViolation__arg__T s)
+{
+        int rc;
+        HypABI__Patch__PatchViolation__arg__T *bhv_arg = HypABI__Patch__PatchViolation__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Patch__PatchViolation__arg__T));
+        rc = HypABI__Patch__PatchViolation__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Patch__PatchViolation__arg__T*)bhv_arg)->block;
+        HypABI__Patch__PatchViolation__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Patch__PatchViolation__HYPERCALL(BLOCK_OUT, ...) HypABI__Patch__PatchViolation__hypercall(BLOCK_OUT, (HypABI__Patch__PatchViolation__arg__T){__VA_ARGS__})
+
+#define HypABI__Richard__Open__hypercall() \
+        HypABI__Richard__Open__hypercall_noalloc()
+
+#define HypABI__Richard__Close__hypercall() \
+        HypABI__Richard__Close__hypercall_noalloc()
+
+extern struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab;
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__ProcessViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__ProcessViolation__arg__slab, gfp)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__ProcessViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__ALLOC() \
+        HypABI__Acl__ProcessViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__ProcessViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__ProcessViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__ProcessViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__ProcessViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__ProcessViolation__arg__slab) \
+                __arg = HypABI__Acl__ProcessViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__ProcessViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__ProcessViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab;
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Acl__DriverViolation__arg__T *)kmem_cache_alloc(HypABI__Acl__DriverViolation__arg__slab, gfp)
+#define HypABI__Acl__DriverViolation__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Acl__DriverViolation__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__ALLOC_NOCHECK() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__ALLOC() \
+        HypABI__Acl__DriverViolation__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Acl__DriverViolation__arg__FREE(name) \
+        kmem_cache_free(HypABI__Acl__DriverViolation__arg__slab, name); \
+        name = NULL
+#define HypABI__Acl__DriverViolation__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Acl__DriverViolation__arg__T *__arg = &static; \
+        if (HypABI__Acl__DriverViolation__arg__slab) \
+                __arg = HypABI__Acl__DriverViolation__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Acl__DriverViolation__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Acl__DriverViolation__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Guestlog__Init__arg__slab;
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Guestlog__Init__arg__T *)kmem_cache_alloc(HypABI__Guestlog__Init__arg__slab, gfp)
+#define HypABI__Guestlog__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Guestlog__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__ALLOC_NOCHECK() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__ALLOC() \
+        HypABI__Guestlog__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Guestlog__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__Guestlog__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__Guestlog__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Guestlog__Init__arg__T *__arg = &static; \
+        if (HypABI__Guestlog__Init__arg__slab) \
+                __arg = HypABI__Guestlog__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Guestlog__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Guestlog__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall(uint64_t *log_bitmap_out, uint8_t *valid_out, const HypABI__Guestlog__Init__arg__T s)
+{
+        int rc;
+        HypABI__Guestlog__Init__arg__T *bhv_arg = HypABI__Guestlog__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Guestlog__Init__arg__T));
+        rc = HypABI__Guestlog__Init__hypercall_noalloc(bhv_arg);
+        *log_bitmap_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->log_bitmap;
+        *valid_out = ((volatile HypABI__Guestlog__Init__arg__T*)bhv_arg)->valid;
+        HypABI__Guestlog__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Guestlog__Init__HYPERCALL(LOG_BITMAP_OUT, VALID_OUT, ...) HypABI__Guestlog__Init__hypercall(LOG_BITMAP_OUT, VALID_OUT, (HypABI__Guestlog__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Configure__arg__slab;
+#define HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Configure__arg__T *)kmem_cache_alloc(HypABI__Creds__Configure__arg__slab, gfp)
+#define HypABI__Creds__Configure__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Configure__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__ALLOC() \
+        HypABI__Creds__Configure__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Configure__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Configure__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Configure__arg__T *__arg = &static; \
+        if (HypABI__Creds__Configure__arg__slab) \
+                __arg = HypABI__Creds__Configure__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Configure__arg__FREE(name); \
+        }
+
+
+extern struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab;
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__RegisterInitTask__arg__T *)kmem_cache_alloc(HypABI__Creds__RegisterInitTask__arg__slab, gfp)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__RegisterInitTask__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC() \
+        HypABI__Creds__RegisterInitTask__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__RegisterInitTask__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__RegisterInitTask__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__RegisterInitTask__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__RegisterInitTask__arg__T *__arg = &static; \
+        if (HypABI__Creds__RegisterInitTask__arg__slab) \
+                __arg = HypABI__Creds__RegisterInitTask__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__RegisterInitTask__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__RegisterInitTask__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall(const HypABI__Creds__RegisterInitTask__arg__T s)
+{
+        int rc;
+        HypABI__Creds__RegisterInitTask__arg__T *bhv_arg = HypABI__Creds__RegisterInitTask__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__RegisterInitTask__arg__T));
+        rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__RegisterInitTask__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__RegisterInitTask__HYPERCALL(...) HypABI__Creds__RegisterInitTask__hypercall((HypABI__Creds__RegisterInitTask__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Assign__arg__slab;
+#define HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Assign__arg__T *)kmem_cache_alloc(HypABI__Creds__Assign__arg__slab, gfp)
+#define HypABI__Creds__Assign__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Assign__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__ALLOC() \
+        HypABI__Creds__Assign__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Assign__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Assign__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Assign__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Assign__arg__T *__arg = &static; \
+        if (HypABI__Creds__Assign__arg__slab) \
+                __arg = HypABI__Creds__Assign__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Assign__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Assign__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall(uint8_t *ret_out, const HypABI__Creds__Assign__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Assign__arg__T *bhv_arg = HypABI__Creds__Assign__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Assign__arg__T));
+        rc = HypABI__Creds__Assign__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Assign__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Assign__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Assign__HYPERCALL(RET_OUT, ...) HypABI__Creds__Assign__hypercall(RET_OUT, (HypABI__Creds__Assign__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab;
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__AssignPriv__arg__T *)kmem_cache_alloc(HypABI__Creds__AssignPriv__arg__slab, gfp)
+#define HypABI__Creds__AssignPriv__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__AssignPriv__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__ALLOC() \
+        HypABI__Creds__AssignPriv__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__AssignPriv__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__AssignPriv__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__AssignPriv__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__AssignPriv__arg__T *__arg = &static; \
+        if (HypABI__Creds__AssignPriv__arg__slab) \
+                __arg = HypABI__Creds__AssignPriv__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__AssignPriv__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__AssignPriv__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall(uint8_t *ret_out, const HypABI__Creds__AssignPriv__arg__T s)
+{
+        int rc;
+        HypABI__Creds__AssignPriv__arg__T *bhv_arg = HypABI__Creds__AssignPriv__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__AssignPriv__arg__T));
+        rc = HypABI__Creds__AssignPriv__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__AssignPriv__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__AssignPriv__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__AssignPriv__HYPERCALL(RET_OUT, ...) HypABI__Creds__AssignPriv__hypercall(RET_OUT, (HypABI__Creds__AssignPriv__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Commit__arg__slab;
+#define HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Commit__arg__T *)kmem_cache_alloc(HypABI__Creds__Commit__arg__slab, gfp)
+#define HypABI__Creds__Commit__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Commit__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__ALLOC() \
+        HypABI__Creds__Commit__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Commit__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Commit__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Commit__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Commit__arg__T *__arg = &static; \
+        if (HypABI__Creds__Commit__arg__slab) \
+                __arg = HypABI__Creds__Commit__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Commit__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Commit__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall(uint8_t *ret_out, const HypABI__Creds__Commit__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Commit__arg__T *bhv_arg = HypABI__Creds__Commit__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Commit__arg__T));
+        rc = HypABI__Creds__Commit__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Commit__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Commit__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Commit__HYPERCALL(RET_OUT, ...) HypABI__Creds__Commit__hypercall(RET_OUT, (HypABI__Creds__Commit__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Release__arg__slab;
+#define HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Release__arg__T *)kmem_cache_alloc(HypABI__Creds__Release__arg__slab, gfp)
+#define HypABI__Creds__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Release__arg__T *__arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__ALLOC() \
+        HypABI__Creds__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Release__arg__T *__arg = &static; \
+        if (HypABI__Creds__Release__arg__slab) \
+                __arg = HypABI__Creds__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall(const HypABI__Creds__Release__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Release__arg__T *bhv_arg = HypABI__Creds__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Release__arg__T));
+        rc = HypABI__Creds__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Creds__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Release__HYPERCALL(...) HypABI__Creds__Release__hypercall((HypABI__Creds__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Verification__arg__slab;
+#define HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Verification__arg__T *)kmem_cache_alloc(HypABI__Creds__Verification__arg__slab, gfp)
+#define HypABI__Creds__Verification__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Verification__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__ALLOC() \
+        HypABI__Creds__Verification__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Verification__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Verification__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Verification__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Verification__arg__T *__arg = &static; \
+        if (HypABI__Creds__Verification__arg__slab) \
+                __arg = HypABI__Creds__Verification__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Verification__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Verification__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall(uint8_t *ret_out, const HypABI__Creds__Verification__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Verification__arg__T *bhv_arg = HypABI__Creds__Verification__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Verification__arg__T));
+        rc = HypABI__Creds__Verification__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Creds__Verification__arg__T*)bhv_arg)->ret;
+        HypABI__Creds__Verification__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Verification__HYPERCALL(RET_OUT, ...) HypABI__Creds__Verification__hypercall(RET_OUT, (HypABI__Creds__Verification__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Creds__Log__arg__slab;
+#define HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Creds__Log__arg__T *)kmem_cache_alloc(HypABI__Creds__Log__arg__slab, gfp)
+#define HypABI__Creds__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Creds__Log__arg__T *__arg = HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Creds__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Creds__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__ALLOC() \
+        HypABI__Creds__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Creds__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Creds__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Creds__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Creds__Log__arg__T *__arg = &static; \
+        if (HypABI__Creds__Log__arg__slab) \
+                __arg = HypABI__Creds__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Creds__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Creds__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall(uint8_t *block_out, const HypABI__Creds__Log__arg__T s)
+{
+        int rc;
+        HypABI__Creds__Log__arg__T *bhv_arg = HypABI__Creds__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Creds__Log__arg__T));
+        rc = HypABI__Creds__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Creds__Log__arg__T*)bhv_arg)->block;
+        HypABI__Creds__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Creds__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Creds__Log__hypercall(BLOCK_OUT, (HypABI__Creds__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__Init__arg__slab;
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__Init__arg__T *)kmem_cache_alloc(HypABI__FileProtection__Init__arg__slab, gfp)
+#define HypABI__FileProtection__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__ALLOC() \
+        HypABI__FileProtection__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__Init__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__Init__arg__slab) \
+                __arg = HypABI__FileProtection__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall(uint64_t *feature_bitmap_out, const HypABI__FileProtection__Init__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__Init__arg__T *bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__Init__arg__T));
+        rc = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+        *feature_bitmap_out = ((volatile HypABI__FileProtection__Init__arg__T*)bhv_arg)->feature_bitmap;
+        HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__Init__HYPERCALL(FEATURE_BITMAP_OUT, ...) HypABI__FileProtection__Init__hypercall(FEATURE_BITMAP_OUT, (HypABI__FileProtection__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab;
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC() \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *bhv_arg = HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T));
+        rc = HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationWriteReadOnlyFile__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab;
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationFileOps__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationFileOps__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationFileOps__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC() \
+        HypABI__FileProtection__ViolationFileOps__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationFileOps__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationFileOps__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationFileOps__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationFileOps__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationFileOps__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationFileOps__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationFileOps__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationFileOps__arg__T *bhv_arg = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationFileOps__arg__T));
+        rc = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationFileOps__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationFileOps__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationFileOps__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationFileOps__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationFileOps__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab;
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *)kmem_cache_alloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, gfp)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__FileProtection__ViolationDirtyCredWrite__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_NOCHECK() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC() \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name) \
+        kmem_cache_free(HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab, name); \
+        name = NULL
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *__arg = &static; \
+        if (HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab) \
+                __arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(uint8_t *block_out, const HypABI__FileProtection__ViolationDirtyCredWrite__arg__T s)
+{
+        int rc;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *bhv_arg = HypABI__FileProtection__ViolationDirtyCredWrite__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T));
+        rc = HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__FileProtection__ViolationDirtyCredWrite__arg__T*)bhv_arg)->block;
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__FileProtection__ViolationDirtyCredWrite__HYPERCALL(BLOCK_OUT, ...) HypABI__FileProtection__ViolationDirtyCredWrite__hypercall(BLOCK_OUT, (HypABI__FileProtection__ViolationDirtyCredWrite__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab;
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__RegisterProtection__Freeze__arg__T *)kmem_cache_alloc(HypABI__RegisterProtection__Freeze__arg__slab, gfp)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__RegisterProtection__Freeze__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_NOCHECK() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC() \
+        HypABI__RegisterProtection__Freeze__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__RegisterProtection__Freeze__arg__FREE(name) \
+        kmem_cache_free(HypABI__RegisterProtection__Freeze__arg__slab, name); \
+        name = NULL
+#define HypABI__RegisterProtection__Freeze__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__RegisterProtection__Freeze__arg__T *__arg = &static; \
+        if (HypABI__RegisterProtection__Freeze__arg__slab) \
+                __arg = HypABI__RegisterProtection__Freeze__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__RegisterProtection__Freeze__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__RegisterProtection__Freeze__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall(const HypABI__RegisterProtection__Freeze__arg__T s)
+{
+        int rc;
+        HypABI__RegisterProtection__Freeze__arg__T *bhv_arg = HypABI__RegisterProtection__Freeze__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__RegisterProtection__Freeze__arg__T));
+        rc = HypABI__RegisterProtection__Freeze__hypercall_noalloc(bhv_arg);
+        HypABI__RegisterProtection__Freeze__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__RegisterProtection__Freeze__HYPERCALL(...) HypABI__RegisterProtection__Freeze__hypercall((HypABI__RegisterProtection__Freeze__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Register__arg__slab;
+#define HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Register__arg__T *)kmem_cache_alloc(HypABI__Inode__Register__arg__slab, gfp)
+#define HypABI__Inode__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Register__arg__T *__arg = HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__ALLOC() \
+        HypABI__Inode__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Register__arg__T *__arg = &static; \
+        if (HypABI__Inode__Register__arg__slab) \
+                __arg = HypABI__Inode__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall(const HypABI__Inode__Register__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Register__arg__T *bhv_arg = HypABI__Inode__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Register__arg__T));
+        rc = HypABI__Inode__Register__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Register__HYPERCALL(...) HypABI__Inode__Register__hypercall((HypABI__Inode__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Update__arg__slab;
+#define HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Update__arg__T *)kmem_cache_alloc(HypABI__Inode__Update__arg__slab, gfp)
+#define HypABI__Inode__Update__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Update__arg__T *__arg = HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Update__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Update__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__ALLOC() \
+        HypABI__Inode__Update__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Update__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Update__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Update__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Update__arg__T *__arg = &static; \
+        if (HypABI__Inode__Update__arg__slab) \
+                __arg = HypABI__Inode__Update__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Update__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Update__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall(const HypABI__Inode__Update__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Update__arg__T *bhv_arg = HypABI__Inode__Update__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Update__arg__T));
+        rc = HypABI__Inode__Update__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Update__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Update__HYPERCALL(...) HypABI__Inode__Update__hypercall((HypABI__Inode__Update__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Release__arg__slab;
+#define HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Release__arg__T *)kmem_cache_alloc(HypABI__Inode__Release__arg__slab, gfp)
+#define HypABI__Inode__Release__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Release__arg__T *__arg = HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Release__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Release__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__ALLOC() \
+        HypABI__Inode__Release__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Release__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Release__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Release__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Release__arg__T *__arg = &static; \
+        if (HypABI__Inode__Release__arg__slab) \
+                __arg = HypABI__Inode__Release__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Release__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Release__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall(const HypABI__Inode__Release__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Release__arg__T *bhv_arg = HypABI__Inode__Release__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Release__arg__T));
+        rc = HypABI__Inode__Release__hypercall_noalloc(bhv_arg);
+        HypABI__Inode__Release__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Release__HYPERCALL(...) HypABI__Inode__Release__hypercall((HypABI__Inode__Release__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Verify__arg__slab;
+#define HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Verify__arg__T *)kmem_cache_alloc(HypABI__Inode__Verify__arg__slab, gfp)
+#define HypABI__Inode__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__ALLOC() \
+        HypABI__Inode__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Verify__arg__T *__arg = &static; \
+        if (HypABI__Inode__Verify__arg__slab) \
+                __arg = HypABI__Inode__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall(uint8_t *ret_out, const HypABI__Inode__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Verify__arg__T *bhv_arg = HypABI__Inode__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Verify__arg__T));
+        rc = HypABI__Inode__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Inode__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Inode__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Verify__HYPERCALL(RET_OUT, ...) HypABI__Inode__Verify__hypercall(RET_OUT, (HypABI__Inode__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Inode__Log__arg__slab;
+#define HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Inode__Log__arg__T *)kmem_cache_alloc(HypABI__Inode__Log__arg__slab, gfp)
+#define HypABI__Inode__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Inode__Log__arg__T *__arg = HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Inode__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Inode__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__ALLOC() \
+        HypABI__Inode__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Inode__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Inode__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Inode__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Inode__Log__arg__T *__arg = &static; \
+        if (HypABI__Inode__Log__arg__slab) \
+                __arg = HypABI__Inode__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Inode__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Inode__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall(uint8_t *block_out, const HypABI__Inode__Log__arg__T s)
+{
+        int rc;
+        HypABI__Inode__Log__arg__T *bhv_arg = HypABI__Inode__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Inode__Log__arg__T));
+        rc = HypABI__Inode__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Inode__Log__arg__T*)bhv_arg)->block;
+        HypABI__Inode__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Inode__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Inode__Log__hypercall(BLOCK_OUT, (HypABI__Inode__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Register__arg__slab;
+#define HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Register__arg__T *)kmem_cache_alloc(HypABI__Keyring__Register__arg__slab, gfp)
+#define HypABI__Keyring__Register__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Register__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__ALLOC() \
+        HypABI__Keyring__Register__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Register__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Register__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Register__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Register__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Register__arg__slab) \
+                __arg = HypABI__Keyring__Register__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Register__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Register__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall(uint8_t *block_out, const HypABI__Keyring__Register__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Register__arg__T *bhv_arg = HypABI__Keyring__Register__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Register__arg__T));
+        rc = HypABI__Keyring__Register__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Register__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Register__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Register__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Register__hypercall(BLOCK_OUT, (HypABI__Keyring__Register__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Verify__arg__slab;
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Verify__arg__T *)kmem_cache_alloc(HypABI__Keyring__Verify__arg__slab, gfp)
+#define HypABI__Keyring__Verify__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Verify__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__ALLOC() \
+        HypABI__Keyring__Verify__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Verify__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Verify__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Verify__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Verify__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Verify__arg__slab) \
+                __arg = HypABI__Keyring__Verify__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Verify__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Verify__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall(uint8_t *ret_out, const HypABI__Keyring__Verify__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Verify__arg__T *bhv_arg = HypABI__Keyring__Verify__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Verify__arg__T));
+        rc = HypABI__Keyring__Verify__hypercall_noalloc(bhv_arg);
+        *ret_out = ((volatile HypABI__Keyring__Verify__arg__T*)bhv_arg)->ret;
+        HypABI__Keyring__Verify__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Verify__HYPERCALL(RET_OUT, ...) HypABI__Keyring__Verify__hypercall(RET_OUT, (HypABI__Keyring__Verify__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Keyring__Log__arg__slab;
+#define HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Keyring__Log__arg__T *)kmem_cache_alloc(HypABI__Keyring__Log__arg__slab, gfp)
+#define HypABI__Keyring__Log__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Keyring__Log__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__ALLOC_NOCHECK() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__ALLOC() \
+        HypABI__Keyring__Log__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Keyring__Log__arg__FREE(name) \
+        kmem_cache_free(HypABI__Keyring__Log__arg__slab, name); \
+        name = NULL
+#define HypABI__Keyring__Log__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Keyring__Log__arg__T *__arg = &static; \
+        if (HypABI__Keyring__Log__arg__slab) \
+                __arg = HypABI__Keyring__Log__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Keyring__Log__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Keyring__Log__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall(uint8_t *block_out, const HypABI__Keyring__Log__arg__T s)
+{
+        int rc;
+        HypABI__Keyring__Log__arg__T *bhv_arg = HypABI__Keyring__Log__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Keyring__Log__arg__T));
+        rc = HypABI__Keyring__Log__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__Keyring__Log__arg__T*)bhv_arg)->block;
+        HypABI__Keyring__Log__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Keyring__Log__HYPERCALL(BLOCK_OUT, ...) HypABI__Keyring__Log__hypercall(BLOCK_OUT, (HypABI__Keyring__Log__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab;
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *)kmem_cache_alloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, gfp)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__FreezeMemoryAfterBoot__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC() \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *__arg = &static; \
+        if (HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab) \
+                __arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(uint8_t *freeze_memory_after_boot_out, const HypABI__Confserver__FreezeMemoryAfterBoot__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *bhv_arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T));
+        rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(bhv_arg);
+        *freeze_memory_after_boot_out = ((volatile HypABI__Confserver__FreezeMemoryAfterBoot__arg__T*)bhv_arg)->freeze_memory_after_boot;
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__FreezeMemoryAfterBoot__HYPERCALL(FREEZE_MEMORY_AFTER_BOOT_OUT, ...) HypABI__Confserver__FreezeMemoryAfterBoot__hypercall(FREEZE_MEMORY_AFTER_BOOT_OUT, (HypABI__Confserver__FreezeMemoryAfterBoot__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab;
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Confserver__StrictFileops__arg__T *)kmem_cache_alloc(HypABI__Confserver__StrictFileops__arg__slab, gfp)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Confserver__StrictFileops__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_NOCHECK() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__ALLOC() \
+        HypABI__Confserver__StrictFileops__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Confserver__StrictFileops__arg__FREE(name) \
+        kmem_cache_free(HypABI__Confserver__StrictFileops__arg__slab, name); \
+        name = NULL
+#define HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Confserver__StrictFileops__arg__T *__arg = &static; \
+        if (HypABI__Confserver__StrictFileops__arg__slab) \
+                __arg = HypABI__Confserver__StrictFileops__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Confserver__StrictFileops__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall(uint8_t *strict_fileops_out, const HypABI__Confserver__StrictFileops__arg__T s)
+{
+        int rc;
+        HypABI__Confserver__StrictFileops__arg__T *bhv_arg = HypABI__Confserver__StrictFileops__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Confserver__StrictFileops__arg__T));
+        rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+        *strict_fileops_out = ((volatile HypABI__Confserver__StrictFileops__arg__T*)bhv_arg)->strict_fileops;
+        HypABI__Confserver__StrictFileops__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Confserver__StrictFileops__HYPERCALL(STRICT_FILEOPS_OUT, ...) HypABI__Confserver__StrictFileops__hypercall(STRICT_FILEOPS_OUT, (HypABI__Confserver__StrictFileops__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__ContainerIntegrity__Init__arg__slab;
+#define HypABI__ContainerIntegrity__Init__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__ContainerIntegrity__Init__arg__T *)kmem_cache_alloc(HypABI__ContainerIntegrity__Init__arg__slab, gfp)
+#define HypABI__ContainerIntegrity__Init__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__ContainerIntegrity__Init__arg__T *__arg = HypABI__ContainerIntegrity__Init__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__ContainerIntegrity__Init__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__Init__arg__ALLOC_NOCHECK() \
+        HypABI__ContainerIntegrity__Init__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__Init__arg__ALLOC() \
+        HypABI__ContainerIntegrity__Init__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__Init__arg__FREE(name) \
+        kmem_cache_free(HypABI__ContainerIntegrity__Init__arg__slab, name); \
+        name = NULL
+#define HypABI__ContainerIntegrity__Init__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__ContainerIntegrity__Init__arg__T *__arg = &static; \
+        if (HypABI__ContainerIntegrity__Init__arg__slab) \
+                __arg = HypABI__ContainerIntegrity__Init__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__Init__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__ContainerIntegrity__Init__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__Init__hypercall(uint64_t *feature_bitmap_out, const HypABI__ContainerIntegrity__Init__arg__T s)
+{
+        int rc;
+        HypABI__ContainerIntegrity__Init__arg__T *bhv_arg = HypABI__ContainerIntegrity__Init__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__ContainerIntegrity__Init__arg__T));
+        rc = HypABI__ContainerIntegrity__Init__hypercall_noalloc(bhv_arg);
+        *feature_bitmap_out = ((volatile HypABI__ContainerIntegrity__Init__arg__T*)bhv_arg)->feature_bitmap;
+        HypABI__ContainerIntegrity__Init__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__ContainerIntegrity__Init__HYPERCALL(FEATURE_BITMAP_OUT, ...) HypABI__ContainerIntegrity__Init__hypercall(FEATURE_BITMAP_OUT, (HypABI__ContainerIntegrity__Init__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab;
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T *)kmem_cache_alloc(HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab, gfp)
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T *__arg = HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_NOCHECK() \
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC() \
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(name) \
+        kmem_cache_free(HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab, name); \
+        name = NULL
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T *__arg = &static; \
+        if (HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab) \
+                __arg = HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationNewFileExecution__hypercall(uint8_t *block_out, const HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T s)
+{
+        int rc;
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T *bhv_arg = HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T));
+        rc = HypABI__ContainerIntegrity__ViolationNewFileExecution__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T*)bhv_arg)->block;
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__ContainerIntegrity__ViolationNewFileExecution__HYPERCALL(BLOCK_OUT, ...) HypABI__ContainerIntegrity__ViolationNewFileExecution__hypercall(BLOCK_OUT, (HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab;
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T *)kmem_cache_alloc(HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab, gfp)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T *__arg = HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_NOCHECK() \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC() \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(name) \
+        kmem_cache_free(HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab, name); \
+        name = NULL
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T *__arg = &static; \
+        if (HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab) \
+                __arg = HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__hypercall(uint8_t *block_out, const HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T s)
+{
+        int rc;
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T *bhv_arg = HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T));
+        rc = HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T*)bhv_arg)->block;
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__HYPERCALL(BLOCK_OUT, ...) HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__hypercall(BLOCK_OUT, (HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab;
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T *)kmem_cache_alloc(HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab, gfp)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T *__arg = HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_NOCHECK() \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC() \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(name) \
+        kmem_cache_free(HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab, name); \
+        name = NULL
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T *__arg = &static; \
+        if (HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab) \
+                __arg = HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__hypercall(uint8_t *block_out, const HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T s)
+{
+        int rc;
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T *bhv_arg = HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T));
+        rc = HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T*)bhv_arg)->block;
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__HYPERCALL(BLOCK_OUT, ...) HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__hypercall(BLOCK_OUT, (HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab;
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T *)kmem_cache_alloc(HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab, gfp)
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T *__arg = HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_NOCHECK() \
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC() \
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(name) \
+        kmem_cache_free(HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab, name); \
+        name = NULL
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T *__arg = &static; \
+        if (HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab) \
+                __arg = HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterBound__hypercall(uint8_t *block_out, const HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T s)
+{
+        int rc;
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T *bhv_arg = HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T));
+        rc = HypABI__ContainerIntegrity__ViolationInterpreterBound__hypercall_noalloc(bhv_arg);
+        *block_out = ((volatile HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T*)bhv_arg)->block;
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__ContainerIntegrity__ViolationInterpreterBound__HYPERCALL(BLOCK_OUT, ...) HypABI__ContainerIntegrity__ViolationInterpreterBound__hypercall(BLOCK_OUT, (HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Create__arg__slab;
+#define HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Create__arg__T *)kmem_cache_alloc(HypABI__Wagner__Create__arg__slab, gfp)
+#define HypABI__Wagner__Create__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Create__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__ALLOC() \
+        HypABI__Wagner__Create__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Create__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Create__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Create__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Create__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Create__arg__slab) \
+                __arg = HypABI__Wagner__Create__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Create__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Create__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall(const HypABI__Wagner__Create__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Create__arg__T *bhv_arg = HypABI__Wagner__Create__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Create__arg__T));
+        rc = HypABI__Wagner__Create__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Create__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Create__HYPERCALL(...) HypABI__Wagner__Create__hypercall((HypABI__Wagner__Create__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Extend__arg__slab;
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Extend__arg__T *)kmem_cache_alloc(HypABI__Wagner__Extend__arg__slab, gfp)
+#define HypABI__Wagner__Extend__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Extend__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__ALLOC() \
+        HypABI__Wagner__Extend__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Extend__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Extend__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Extend__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Extend__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Extend__arg__slab) \
+                __arg = HypABI__Wagner__Extend__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Extend__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Extend__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall(const HypABI__Wagner__Extend__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Extend__arg__T *bhv_arg = HypABI__Wagner__Extend__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Extend__arg__T));
+        rc = HypABI__Wagner__Extend__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Extend__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Extend__HYPERCALL(...) HypABI__Wagner__Extend__hypercall((HypABI__Wagner__Extend__arg__T){__VA_ARGS__})
+
+extern struct kmem_cache *HypABI__Wagner__Delete__arg__slab;
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp) \
+        (HypABI__Wagner__Delete__arg__T *)kmem_cache_alloc(HypABI__Wagner__Delete__arg__slab, gfp)
+#define HypABI__Wagner__Delete__arg__ALLOC_GFP(gfp) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(gfp); \
+        if (!__arg) { \
+                panic("BHV: failed to allocate HypABI__Wagner__Delete__arg__T."); \
+        } \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__ALLOC_NOCHECK() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP_NOCHECK(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__ALLOC() \
+        HypABI__Wagner__Delete__arg__ALLOC_GFP(GFP_KERNEL)
+#define HypABI__Wagner__Delete__arg__FREE(name) \
+        kmem_cache_free(HypABI__Wagner__Delete__arg__slab, name); \
+        name = NULL
+#define HypABI__Wagner__Delete__arg__ALLOC_STATICFALLBACK(static) ({ \
+        HypABI__Wagner__Delete__arg__T *__arg = &static; \
+        if (HypABI__Wagner__Delete__arg__slab) \
+                __arg = HypABI__Wagner__Delete__arg__ALLOC(); \
+        __arg; \
+})
+#define HypABI__Wagner__Delete__arg__FREE_STATICFALLBACK(name, static) \
+        if (name != &static) { \
+                 HypABI__Wagner__Delete__arg__FREE(name); \
+        }
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall(const HypABI__Wagner__Delete__arg__T s)
+{
+        int rc;
+        HypABI__Wagner__Delete__arg__T *bhv_arg = HypABI__Wagner__Delete__arg__ALLOC();
+        memcpy(bhv_arg, &s, sizeof(HypABI__Wagner__Delete__arg__T));
+        rc = HypABI__Wagner__Delete__hypercall_noalloc(bhv_arg);
+        HypABI__Wagner__Delete__arg__FREE(bhv_arg);
+        return rc;
+}
+
+#define HypABI__Wagner__Delete__HYPERCALL(...) HypABI__Wagner__Delete__hypercall((HypABI__Wagner__Delete__arg__T){__VA_ARGS__})
+
diff --git include/bhv/interface/abi_ml_autogen.h include/bhv/interface/abi_ml_autogen.h
new file mode 100644
index 00000000000..b2088f13bfb
--- /dev/null
+++ include/bhv/interface/abi_ml_autogen.h
@@ -0,0 +1,403 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-08-23T11:38:28).
+ */
+
+#pragma once
+
+#include <asm/bitops.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__PROC_ACL__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__DRIVER_ACL__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__LOGGING__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__CREDS_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__FILE_PROTECTION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__GUEST_POLICY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__REGISTER_PROTECTION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__STRONG_ISOLATION__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KERNEL_INTEGRITY_PT_PROT__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__INODE_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__KEYRING_INTEGRITY__BIT, addr);
+}
+static inline bool HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        return test_bit(HypABI__Init__Init__BHVData__BHVConfigBitmap__VAULT__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Init__Init__hypercall_noalloc(HypABI__Init__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Init__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Init__Start__hypercall_noalloc(HypABI__Init__Start__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Init__BACKEND_ID, HypABI__Init__Start__OP_ID, arg);
+}
+
+static inline bool HypABI__Integrity__MemFlags__has_TRANSIENT(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__TRANSIENT__BIT, addr);
+}
+static inline bool HypABI__Integrity__MemFlags__has_MUTABLE(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__MemFlags__MUTABLE__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Create__hypercall_noalloc(HypABI__Integrity__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Create__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Update__hypercall_noalloc(HypABI__Integrity__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Update__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__Remove__hypercall_noalloc(HypABI__Integrity__Remove__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Remove__OP_ID, arg);
+}
+
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__CREATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__UPDATE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__REMOVE__BIT, addr);
+}
+static inline bool HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        return test_bit(HypABI__Integrity__Freeze__FreezeFlags__PATCH__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Integrity__Freeze__hypercall_noalloc(HypABI__Integrity__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__Freeze__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgInit__hypercall_noalloc(HypABI__Integrity__PtpgInit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgInit__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Integrity__PtpgReport__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Integrity__BACKEND_ID, HypABI__Integrity__PtpgReport__OP_ID, NULL);
+}
+
+static __always_inline __must_check int HypABI__Patch__Patch__hypercall_noalloc(HypABI__Patch__Patch__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__Patch__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchNoClose__hypercall_noalloc(HypABI__Patch__PatchNoClose__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchNoClose__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Patch__PatchViolation__hypercall_noalloc(HypABI__Patch__PatchViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Patch__BACKEND_ID, HypABI__Patch__PatchViolation__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Richard__Open__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Open__OP_ID, NULL);
+}
+
+static __always_inline __must_check int HypABI__Richard__Close__hypercall_noalloc(void)
+{
+        return bhv_hypercall_vas(HypABI__Richard__BACKEND_ID, HypABI__Richard__Close__OP_ID, NULL);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessInit__hypercall_noalloc(HypABI__Acl__ProcessInit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessInit__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverInit__hypercall_noalloc(HypABI__Acl__DriverInit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverInit__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Acl__ProcessViolation__hypercall_noalloc(HypABI__Acl__ProcessViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__ProcessViolation__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Acl__DriverViolation__hypercall_noalloc(HypABI__Acl__DriverViolation__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Acl__BACKEND_ID, HypABI__Acl__DriverViolation__OP_ID, arg);
+}
+
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__PROCESS_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__DRIVER_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_ACCESS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__UNKNOWN_FILEOPS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__KERNEL_EXEC_EVENTS__BIT, addr);
+}
+static inline bool HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        return test_bit(HypABI__Guestlog__Init__GuestlogFlags__CONTAINER_EVENTS__BIT, addr);
+}
+static __always_inline __must_check int HypABI__Guestlog__Init__hypercall_noalloc(HypABI__Guestlog__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Guestlog__BACKEND_ID, HypABI__Guestlog__Init__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Configure__hypercall_noalloc(HypABI__Creds__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Configure__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__RegisterInitTask__hypercall_noalloc(HypABI__Creds__RegisterInitTask__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__RegisterInitTask__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Assign__hypercall_noalloc(HypABI__Creds__Assign__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Assign__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__AssignPriv__hypercall_noalloc(HypABI__Creds__AssignPriv__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__AssignPriv__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Commit__hypercall_noalloc(HypABI__Creds__Commit__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Commit__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Release__hypercall_noalloc(HypABI__Creds__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Release__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Verification__hypercall_noalloc(HypABI__Creds__Verification__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Verification__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Creds__Log__hypercall_noalloc(HypABI__Creds__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Creds__BACKEND_ID, HypABI__Creds__Log__OP_ID, arg);
+}
+
+static inline bool HypABI__FileProtection__Init__Config__has_READ_ONLY(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__READ_ONLY__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_FILE_OPS(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__FILE_OPS__BIT, addr);
+}
+static inline bool HypABI__FileProtection__Init__Config__has_DIRTY_CRED(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        return test_bit(HypABI__FileProtection__Init__Config__DIRTY_CRED__BIT, addr);
+}
+static __always_inline __must_check int HypABI__FileProtection__Init__hypercall_noalloc(HypABI__FileProtection__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__Init__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationWriteReadOnlyFile__hypercall_noalloc(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationWriteReadOnlyFile__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(HypABI__FileProtection__ViolationFileOps__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationFileOps__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__FileProtection__ViolationDirtyCredWrite__hypercall_noalloc(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__FileProtection__BACKEND_ID, HypABI__FileProtection__ViolationDirtyCredWrite__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__RegisterProtection__Freeze__hypercall_noalloc(HypABI__RegisterProtection__Freeze__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__RegisterProtection__BACKEND_ID, HypABI__RegisterProtection__Freeze__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Domain__Configure__hypercall_noalloc(HypABI__Domain__Configure__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Configure__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Domain__Report__hypercall_noalloc(HypABI__Domain__Report__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__Report__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Domain__ReportForcedMemAccess__hypercall_noalloc(HypABI__Domain__ReportForcedMemAccess__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Domain__BACKEND_ID, HypABI__Domain__ReportForcedMemAccess__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Inode__Register__hypercall_noalloc(HypABI__Inode__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Register__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Inode__Update__hypercall_noalloc(HypABI__Inode__Update__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Update__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Inode__Release__hypercall_noalloc(HypABI__Inode__Release__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Release__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Inode__Verify__hypercall_noalloc(HypABI__Inode__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Verify__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Inode__Log__hypercall_noalloc(HypABI__Inode__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Inode__BACKEND_ID, HypABI__Inode__Log__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Register__hypercall_noalloc(HypABI__Keyring__Register__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Register__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Verify__hypercall_noalloc(HypABI__Keyring__Verify__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Verify__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Keyring__Log__hypercall_noalloc(HypABI__Keyring__Log__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Keyring__BACKEND_ID, HypABI__Keyring__Log__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__FreezeMemoryAfterBoot__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Confserver__StrictFileops__hypercall_noalloc(HypABI__Confserver__StrictFileops__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Confserver__BACKEND_ID, HypABI__Confserver__StrictFileops__OP_ID, arg);
+}
+
+static inline bool HypABI__ContainerIntegrity__Init__Config__has_NEW_FILE_EXECUTION(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr)
+{
+        return test_bit(HypABI__ContainerIntegrity__Init__Config__NEW_FILE_EXECUTION__BIT, addr);
+}
+static inline bool HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_NEW_FILE(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr)
+{
+        return test_bit(HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_NEW_FILE__BIT, addr);
+}
+static inline bool HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_CMD(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr)
+{
+        return test_bit(HypABI__ContainerIntegrity__Init__Config__INTERPRETER_ARG_CMD__BIT, addr);
+}
+static inline bool HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_BINDS(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr)
+{
+        return test_bit(HypABI__ContainerIntegrity__Init__Config__INTERPRETER_BINDS__BIT, addr);
+}
+static __always_inline __must_check int HypABI__ContainerIntegrity__Init__hypercall_noalloc(HypABI__ContainerIntegrity__Init__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__ContainerIntegrity__BACKEND_ID, HypABI__ContainerIntegrity__Init__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationNewFileExecution__hypercall_noalloc(HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__ContainerIntegrity__BACKEND_ID, HypABI__ContainerIntegrity__ViolationNewFileExecution__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__hypercall_noalloc(HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__ContainerIntegrity__BACKEND_ID, HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__hypercall_noalloc(HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__ContainerIntegrity__BACKEND_ID, HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__ContainerIntegrity__ViolationInterpreterBound__hypercall_noalloc(HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__ContainerIntegrity__BACKEND_ID, HypABI__ContainerIntegrity__ViolationInterpreterBound__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Create__hypercall_noalloc(HypABI__Wagner__Create__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Create__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Extend__hypercall_noalloc(HypABI__Wagner__Extend__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Extend__OP_ID, arg);
+}
+
+static __always_inline __must_check int HypABI__Wagner__Delete__hypercall_noalloc(HypABI__Wagner__Delete__arg__T *arg)
+{
+        return bhv_hypercall_vas(HypABI__Wagner__BACKEND_ID, HypABI__Wagner__Delete__OP_ID, arg);
+}
+
diff --git include/bhv/interface/abi_version_autogen.h include/bhv/interface/abi_version_autogen.h
new file mode 100644
index 00000000000..cd9fb418d05
--- /dev/null
+++ include/bhv/interface/abi_version_autogen.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-08-23T11:38:28).
+ */
+
+#pragma once
+
+#define HypABI__ABI_VERSION __BHV_VAS_ABI_VERSION(24, 35, 0, 619)
+
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 00000000000..eb89156a928
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,64 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define __BHV_VAS_ABI_VERSION(rel_year, rel_week, rel_extra, internal_info)    \
+	({                                                                     \
+		static_assert((unsigned long)(rel_year) <= 99);                \
+		static_assert((unsigned long)(rel_year) >= 23);                \
+		static_assert((unsigned long)(rel_week) < 53);                 \
+		static_assert((unsigned long)(rel_extra) < 0xff);              \
+		static_assert((unsigned long)(internal_info) > 0x00000);       \
+		static_assert((unsigned long)(internal_info) < 0xfffff);       \
+		(unsigned long)0xbedUL << (13 * 4) |                           \
+			(unsigned long)(rel_year) << (11 * 4) |                \
+			(unsigned long)(rel_week) << (9 * 4) |                 \
+			(unsigned long)(rel_extra) << (7 * 4) |                \
+			(unsigned long)(0x00UL) << (5 * 4) |                   \
+			(unsigned long)(internal_info) << (0 * 4);             \
+	})
+
+#include <bhv/version.h>
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_REGISTER_PROTECTION 9
+#define BHV_VAS_BACKEND_DOMAIN 10
+#define BHV_VAS_BACKEND_INODE 11
+#define BHV_VAS_BACKEND_KEYRING 12
+#define BHV_VAS_BACKEND_CONFSERVER 13
+#define BHV_VAS_BACKEND_CONTAINER_INTEGRITY 14
+
+/* BHV CONFIGURATION BITS */
+#define BHV_CONFIG_PROC_ACL 1
+#define BHV_CONFIG_DRIVER_ACL 2
+#define BHV_CONFIG_LOGGING 3
+#define BHV_CONFIG_CREDS 4
+#define BHV_CONFIG_FILE_PROTECTION 5
+#define BHV_CONFIG_GUEST_POLICY 6
+#define BHV_CONFIG_REGISTER_PROTECTION 7
+#define BHV_CONFIG_STRONG_ISOLATION 8
+#define BHV_CONFIG_INODE 10
+#define BHV_CONFIG_KEYRING 11
+#define BHV_CONFIG_CONTAINER_INTEGRITY 12
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 00000000000..cb620b43400
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,55 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static __always_inline int bhv_hypercall_vas(uint32_t backend, uint32_t op,
+					     void *arg)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	if (arg != NULL)
+		phys_addr = bhv_virt_to_phys(arg);
+
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_start(TARGET_BHV_VAS, backend, op);
+#endif
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			   phys_addr);
+#if defined(CONFIG_BHV_TRACEPOINTS) && !defined(CONFIG_BHV_VAULT_SPACES)
+	trace_bhv_hypercall_end(TARGET_BHV_VAS, backend, op);
+#endif
+
+	if (rv) {
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION, arg,
+		      phys_addr);
+#else
+		pr_warn("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %px %llx)",
+			rv, TARGET_BHV_VAS, backend, op, HypABI__ABI_VERSION,
+			arg, phys_addr);
+		dump_stack();
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 00000000000..7fbcc27527b
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,208 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_PATCH_H
+#define _ASM_INTERFACE_BHV_PATCH_H
+
+#include <linux/kernel.h>
+
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define PATCH_BODY()                                                          \
+	int rc;                                                                    \
+	HypABI__Patch__Patch__arg__T bhv_arg;                                      \
+                                                                              \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                                \
+                                                                              \
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys(dest_virt_addr);                 \
+	memcpy(bhv_arg.src_value, src, size);                                      \
+	bhv_arg.size = size;                                                       \
+                                                                              \
+	rc = HypABI__Patch__Patch__hypercall_noalloc(&bhv_arg);                    \
+                                                                              \
+	return rc;
+
+static __always_inline int
+__bhv_patch_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				    uint64_t size)
+{
+	PATCH_BODY();
+}
+
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+#define PATCH_BODY(TYP)                                                       \
+	int rc;                                                               \
+	static HypABI__Patch__##TYP##__arg__T early_arg;                      \
+	HypABI__Patch__##TYP##__arg__T *bhv_arg =                             \
+		HypABI__Patch__##TYP##__arg__ALLOC_STATICFALLBACK(early_arg); \
+                                                                              \
+	BUG_ON(size > HypABI__Patch__MAX_PATCH_SZ);                           \
+                                                                              \
+	bhv_arg->dest_phys_addr = bhv_virt_to_phys(dest_virt_addr);           \
+	memcpy(bhv_arg->src_value, src, size);                                \
+	bhv_arg->size = size;                                                 \
+                                                                              \
+	rc = HypABI__Patch__##TYP##__hypercall_noalloc(bhv_arg);              \
+                                                                              \
+	HypABI__Patch__##TYP##__arg__FREE_STATICFALLBACK(bhv_arg, early_arg); \
+                                                                              \
+	return rc;
+
+static __always_inline int
+__bhv_patchnoclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+				    uint64_t size)
+{
+	PATCH_BODY(PatchNoClose);
+}
+
+static __always_inline int
+__bhv_patchclose_hypercall_single(void *dest_virt_addr, const uint8_t *src,
+                                 uint64_t size)
+{
+       PATCH_BODY(Patch);
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#undef PATCH_BODY
+
+static __always_inline uint64_t __bhv_patch_round(void **const dest_virt_addr,
+						  const uint8_t *const src,
+						  uint64_t *const size,
+#ifndef CONFIG_BHV_VAULT_SPACES
+						  const bool close_vault,
+#endif
+						  unsigned long *const rc)
+{
+	unsigned long r;
+	uint64_t bytes_until_page_boundary =
+		PAGE_SIZE - ((uint64_t)*dest_virt_addr % PAGE_SIZE);
+	uint64_t this_patch_size = min3(*size, bytes_until_page_boundary,
+					(uint64_t)HypABI__Patch__MAX_PATCH_SZ);
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	r = __bhv_patch_hypercall_single(*dest_virt_addr, src, this_patch_size);
+#else
+	if (close_vault && *size == this_patch_size) {
+		r = __bhv_patchclose_hypercall_single(*dest_virt_addr, src,
+						      this_patch_size);
+	} else {
+		r = __bhv_patchnoclose_hypercall_single(*dest_virt_addr, src,
+							this_patch_size);
+	}
+#endif
+	if (r)
+		*rc = r;
+
+	*dest_virt_addr += this_patch_size;
+	*size -= this_patch_size;
+	return this_patch_size;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ */
+static __always_inline int bhv_patch_hypercall(void *dest_virt_addr,
+					       const uint8_t *src,
+					       uint64_t size
+#ifndef CONFIG_BHV_VAULT_SPACES
+					       , const bool close_vault
+#endif
+					       )
+{
+	unsigned long rc = 0;
+
+	BUG_ON(!src);
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, &rc);
+#else
+		src += __bhv_patch_round(&dest_virt_addr, src, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+ * Handle arbitrarily-sized patch requests.
+ * Automatically splits requests so they don't cross page boundaries.
+ * Sets memory using a one-byte pattern rather than just copying (like memset)
+ */
+static __always_inline int bhv_patch_hypercall_memset(void *dest_virt_addr,
+						      uint64_t size,
+                             uint8_t pattern
+#ifndef CONFIG_BHV_VAULT_SPACES
+                             , bool close_vault
+#endif
+					       	      )
+{
+#ifdef CONFIG_BHV_VAULT_SPACES
+	size_t i = 0;
+#endif
+	unsigned long rc = 0;
+	uint8_t buf[HypABI__Patch__MAX_PATCH_SZ];
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	for (i = 0; i < HypABI__Patch__MAX_PATCH_SZ; i++)
+		buf[i] = pattern;
+#else
+	memset(buf, pattern, HypABI__Patch__MAX_PATCH_SZ);
+#endif
+
+	while (size) {
+#ifdef CONFIG_BHV_VAULT_SPACES
+		__bhv_patch_round(&dest_virt_addr, buf, &size, &rc);
+#else
+		__bhv_patch_round(&dest_virt_addr, buf, &size, close_vault, &rc);
+#endif
+	}
+
+	return rc;
+}
+
+/**
+* Handle patch violation hypercalls
+*
+* This function sends a patch violation hypercall and determines whether the
+* patch should be blocked.
+*
+* \returns True if the patch should be blocked, false otherwise.
+*/
+static __always_inline bool bhv_patch_violation_hypercall(void *dest_virt_addr,
+							  const char *message)
+{
+	unsigned long r;
+	bool rc;
+
+	HypABI__Patch__PatchViolation__arg__T bhv_arg;
+
+	// Setup arguments. We block by default
+	bhv_arg.dest_virt_addr = (uint64_t)dest_virt_addr;
+	bhv_arg.dest_phys_addr = bhv_virt_to_phys(dest_virt_addr);
+	bhv_arg.block = true;
+
+	if (message != NULL)
+		strncpy(bhv_arg.message, message,
+			HypABI__Patch__PatchViolation__MAX_MSG_SZ);
+	bhv_arg.message[HypABI__Patch__PatchViolation__MAX_MSG_SZ - 1] = '\0';
+
+	r = HypABI__Patch__PatchViolation__hypercall_noalloc(&bhv_arg);
+	rc = r || bhv_arg.block;
+
+	// Block in case of error or if block is set
+	return rc;
+}
+
+#endif /* _ASM_INTERFACE_BHV_PATCH_H */
diff --git include/bhv/kernel-kln.h include/bhv/kernel-kln.h
new file mode 100644
index 00000000000..d73de9ce668
--- /dev/null
+++ include/bhv/kernel-kln.h
@@ -0,0 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#define KLN_SYM(sym) ((unsigned long) sym)
+#define KLN_SYMBOL(ty, sym) ((ty)sym)
+#define KLN_SYMBOL_P(ty, sym) ((ty)&sym)
diff --git include/bhv/keyring.h include/bhv/keyring.h
new file mode 100644
index 00000000000..6eba70a7c73
--- /dev/null
+++ include/bhv/keyring.h
@@ -0,0 +1,40 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_KEYRING_H__
+#define __BHV_KEYRING_H__
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_KEYS
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_keyring_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_KEYRING, bhv_configuration_bitmap);
+}
+
+int __init bhv_init_keyring(void);
+
+int bhv_keyring_register_system_trusted(struct key **k);
+int bhv_keyring_verify(struct key *keyring, void *anchor);
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor);
+
+#else /* CONFIG_BHV_VAS */
+
+#define bhv_keyring_register_system_trusted(k) 0
+#define bhv_keyring_verify(k, a) 0
+#define bhv_keyring_verify_locked(k, a) 0
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* CONFIG_KEYS */
+
+#endif /* __BHV_KEYRING_H__ */
+
diff --git include/bhv/kversion.h include/bhv/kversion.h
new file mode 100644
index 00000000000..d109fd40c25
--- /dev/null
+++ include/bhv/kversion.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/version.h>
+
+#ifndef VASKM // inside kernel tree
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 186) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 11, 0)
+#define BHV_KVERS_5_10
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 72) && LINUX_VERSION_CODE < KERNEL_VERSION(5, 16, 0)
+#define BHV_KVERS_5_15
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 31) && LINUX_VERSION_CODE < KERNEL_VERSION(6, 2, 0)
+#define BHV_KVERS_6_1
+#else
+#error Unsupported linux version
+#endif
+
+#endif // VASKM
+
+#undef LINUX_VERSION_CODE
+#undef KERNEL_VERSION
+#undef LINUX_VERSION_MAJOR
+#undef LINUX_VERSION_PATCHLEVEL
+#undef LINUX_VERSION_SUBLEVEL
diff --git include/bhv/memory_freeze.h include/bhv/memory_freeze.h
new file mode 100644
index 00000000000..fbea9c9e0c2
--- /dev/null
+++ include/bhv/memory_freeze.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#ifdef CONFIG_BHV_VAS
+
+void bhv_memory_freeze_init(void);
+
+#else // !CONFIG_BHV_VAS
+
+static inline void bhv_memory_freeze_init(void)
+{
+}
+
+#endif // CONFIG_BHV_VAS
\ No newline at end of file
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 00000000000..1835ae267f2
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description);
+#endif //VASKM
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+#if 0
+static inline void bhv_protect_generic_memory(uint64_t owner, const void *base,
+					      uint64_t size, uint32_t type,
+					      uint64_t flags, char *description)
+{
+}
+#endif
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 00000000000..d4a50328d57
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,149 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+#include <linux/version.h>
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+extern struct mutex bhv_alternatives_mutex;
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_init_alternatives(void);
+#ifdef CONFIG_JUMP_LABEL
+void bhv_init_jump_label(void);
+#else
+static inline void bhv_init_jump_label(void) {}
+#endif
+void bhv_init_static_call(void);
+#endif
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void);
+/**************************************************/
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+int bhv_bpf_write(void *dst, void *src, size_t sz);
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz);
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages);
+void bhv_rm_bpf_code_range(uint64_t pfn);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+#ifndef CONFIG_BHV_VAULT_SPACES
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#endif /* CONFIG_JUMP_LABEL */
+
+static void __always_inline bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static void __always_inline bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len);
+#else /* !CONFIG_BHV_VAULT_SPACES */
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#ifndef VASKM // inside kernel tree
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+void __init_or_module bhv_apply_retpolines(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#if defined CONFIG_PARAVIRT && defined CONFIG_X86
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* defined CONFIG_PARAVIRT && defined CONFIG_X86 */
+
+#ifdef CONFIG_X86_KERNEL_IBT
+void __init_or_module bhv_apply_ibt_endbr(s32 *s);
+#endif
+#endif // VASKM
+
+#else // CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif // CONFIG_BHV_VAS
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/reg_protect.h include/bhv/reg_protect.h
new file mode 100644
index 00000000000..9d19bf3a1b5
--- /dev/null
+++ include/bhv/reg_protect.h
@@ -0,0 +1,42 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void);
+void bhv_start_reg_protect_arch(void);
+/***************************************************/
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield);
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs.h include/bhv/sysfs.h
new file mode 100644
index 00000000000..8231447a781
--- /dev/null
+++ include/bhv/sysfs.h
@@ -0,0 +1,11 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_sysfs(void);
+/**********************************************************/
diff --git include/bhv/sysfs_fops.h include/bhv/sysfs_fops.h
new file mode 100644
index 00000000000..3c48c20da85
--- /dev/null
+++ include/bhv/sysfs_fops.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_fileops_protection(struct kobject *fops,
+					struct kobject *status);
+/***************************************************/
+
+#ifdef VASKM // out of tree
+int bhv_freeze_fops_map(void);
+extern bool bhv_allow_update_fileops_map;
+#endif // VASKM
+#endif /* CONFIG_BHV_VAS */
+
+
+
diff --git include/bhv/sysfs_integrity_freeze.h include/bhv/sysfs_integrity_freeze.h
new file mode 100644
index 00000000000..62ab213f179
--- /dev/null
+++ include/bhv/sysfs_integrity_freeze.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_integrity_freeze(struct kobject *kobj);
+/***************************************************/
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
\ No newline at end of file
diff --git include/bhv/sysfs_reg_protect.h include/bhv/sysfs_reg_protect.h
new file mode 100644
index 00000000000..c25c19d3ef1
--- /dev/null
+++ include/bhv/sysfs_reg_protect.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_reg_protect(struct kobject *kobj);
+/***************************************************/
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/sysfs_version.h include/bhv/sysfs_version.h
new file mode 100644
index 00000000000..4b2915c3fc7
--- /dev/null
+++ include/bhv/sysfs_version.h
@@ -0,0 +1,16 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+
+#ifdef CONFIG_BHV_VAS
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_sysfs_version(struct kobject *kobj);
+/***************************************************/
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/vault.h include/bhv/vault.h
new file mode 100644
index 00000000000..1a3dce5edf4
--- /dev/null
+++ include/bhv/vault.h
@@ -0,0 +1,234 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VAULT_H__
+#define __BHV_VAULT_H__
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_vault_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(bhv_configuration_bitmap);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+typedef struct {
+	int type;
+	unsigned long start;
+	unsigned long end;
+} bhv_vault_memory_region_helper_t;
+
+typedef struct {
+	void *ep;
+} bhv_vault_entry_point_helper_t;
+
+typedef struct {
+	void *rp;
+} bhv_vault_return_point_helper_t;
+
+#define STRINGIFY(x) #x
+#define ARG_DECL(t, a) t a
+#define RET(t) STRINGIFY(t)
+
+#define void_fn(fn, ...) fn(__VA_ARGS__)
+#define nonvoid_fn(fn, ...) return fn(__VA_ARGS__)
+
+/* Check out system call macros. */
+
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)                               \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(void)                                         \
+	{                                                                      \
+		void_fn(fn);                                                   \
+	}
+
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)                                     \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(void)                                         \
+	{                                                                      \
+		nonvoid_fn(fn);                                                \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)                       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		void_fn(fn, a1);                                               \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)                             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1);                                            \
+	}
+
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)                           \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label")))  \
+		bhv_wrapper_##fn##_##a1(void)                                  \
+	{                                                                      \
+		return fn(a1);                                                 \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)               \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		void_fn(fn, a1, a2);                                           \
+	}
+
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)                     \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2))           \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2);                                        \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)       \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		void_fn(fn, a1, a2, a3);                                       \
+	}
+
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)             \
+	static rettype noinline notrace                                        \
+		__attribute__((section(".bhv.vault.shared.text.jump_label"),   \
+			       __optimize__("no-optimize-sibling-calls")))     \
+		bhv_wrapper_##fn(ARG_DECL(t1, a1), ARG_DECL(t2, a2),           \
+				 ARG_DECL(t3, a3))                             \
+	{                                                                      \
+		nonvoid_fn(fn, a1, a2, a3);                                    \
+	}
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.data." #vault)))
+
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)                                 \
+	__attribute__((section(".bhv.vault.rodata." #vault)))
+
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)                                    \
+	__attribute__((section(".bhv.vault.text." #vault),                     \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       function_return("keep"), no_instrument_function))
+
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault)                                \
+	__attribute__((section(".ref.text.bhv.vault.text." #vault),            \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       function_return("keep"), no_instrument_function))
+
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)                             \
+	__attribute__((section(".bhv.vault.shared.text." #vault),              \
+		       __optimize__("no-optimize-sibling-calls"),              \
+		       function_return("keep"), no_instrument_function))
+
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)                                 \
+	static bhv_vault_entry_point_helper_t bhv_vault_##vault##_ep_##func    \
+		__attribute__((used,                                           \
+			       section(".bhv.vault." #vault ".eps"))) = {      \
+			.ep = func,                                            \
+		}
+
+#define BHV_VAULT_FOR_EACH(section_name, type_t, elem)                         \
+		/*type_t *elem;	*/					       \
+		for (elem = ({                                         	       \
+		     	extern type_t section_name##_start;                    \
+		     	&section_name##_start;                                 \
+	     	});                                                            \
+	     	elem != ({                                                     \
+		     	extern type_t section_name##_end;                      \
+		     	&section_name##_end;                                   \
+	     	});                                                            \
+	     	++elem)
+
+#define BHV_VAULT_FOR_EACH_SECTION(vault, elem_name)                           \
+	BHV_VAULT_FOR_EACH(bhv_vault_##vault##_regions,                        \
+			   bhv_vault_memory_region_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, elem_name)                       \
+	BHV_VAULT_FOR_EACH(__bhv_vault_##vault##_eps,                          \
+			   bhv_vault_entry_point_helper_t, elem_name)
+
+#define BHV_VAULT_FOR_EACH_RETURN_POINT(vault, elem)                           \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_return_sites;                       \
+		     &__start_vault_return_sites;                              \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_return_sites;                        \
+		     &__stop_vault_return_sites;                               \
+	     });                                                               \
+	     ++elem)
+
+#define BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, elem)                          \
+	for (/*bhv_vault_return_point_helper_t * */elem = ({                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __start_vault_rethunk_sites;                      \
+		     &__start_vault_rethunk_sites;                             \
+	     });                                                               \
+	     elem != ({                                                        \
+		     extern bhv_vault_return_point_helper_t                    \
+			     __stop_vault_rethunk_sites;                       \
+		     &__stop_vault_rethunk_sites;                              \
+	     });                                                               \
+	     ++elem)
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+#else /* !CONFIG_BHV_VAS */
+
+#define bhv_vault_is_enabled() 0
+
+#endif /* CONFIG_BHV_VAS */
+
+#if !defined(CONFIG_BHV_VAS) || !defined(CONFIG_BHV_VAULT_SPACES)
+#define BHV_VAULT_FN_WRAPPER0_NORET(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER0(rettype, fn)
+#define BHV_VAULT_FN_WRAPPER1_NORET(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1(rettype, fn, t1, a1)
+#define BHV_VAULT_FN_WRAPPER1_MACRO(rettype, fn, a1)
+#define BHV_VAULT_FN_WRAPPER2_NORET(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER2(rettype, fn, t1, a1, t2, a2)
+#define BHV_VAULT_FN_WRAPPER3_NORET(rettype, fn, t1, a1, t2, a2, t3, a3)
+#define BHV_VAULT_FN_WRAPPER3(rettype, fn, t1, a1, t2, a2, t3, a3)
+
+#define BHV_VAULT_ADD_TO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_RO_DATA_REGION(vault)
+#define BHV_VAULT_ADD_TO_CODE_REGION(vault)
+#define BHV_VAULT_ADD_TO_REF_CODE_REGION(vault) __ref
+#define BHV_VAULT_ADD_TO_SHARED_CODE_REGION(vault)
+#define BHV_VAULT_ADD_ENTRY_POINT(vault, func)
+#endif /* !CONFIG_BHV_VAS || !CONFIG_BHV_VAULT_SPACES */
+
+#endif /* __BHV_VAULT_H__ */
diff --git include/bhv/version.h include/bhv/version.h
new file mode 100644
index 00000000000..7e6d926948a
--- /dev/null
+++ include/bhv/version.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VERSION_H__
+#define __BHV_VERSION_H__
+
+#define BHV_VERSION __BHV_VERSION(24, 35, 0)
+
+#include <bhv/interface/abi_version_autogen.h>
+
+#endif //__BHV_VERSION_H__
diff --git include/keys/system_keyring.h include/keys/system_keyring.h
index 91e080efb91..7d30e82f6e9 100644
--- include/keys/system_keyring.h
+++ include/keys/system_keyring.h
@@ -98,6 +98,9 @@ extern struct key *ima_blacklist_keyring;
 
 static inline struct key *get_ima_blacklist_keyring(void)
 {
+	if (bhv_keyring_verify(ima_blacklist_keyring, &ima_blacklist_keyring))
+		return NULL;
+
 	return ima_blacklist_keyring;
 }
 #else
diff --git include/linux/filter.h include/linux/filter.h
index face590b24e..bc8071ce65b 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -28,6 +28,9 @@
 #include <asm/byteorder.h>
 #include <uapi/linux/filter.h>
 
+#include <bhv/module.h>
+#include <bhv/integrity.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -856,6 +859,10 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 	if (!fp->jited) {
 		set_vm_flush_reset_perms(fp);
 		set_memory_ro((unsigned long)fp, fp->pages);
+		if (!bhv_integrity_freeze_create_currently_frozen &&
+		    !bhv_integrity_freeze_update_currently_frozen &&
+		    !bhv_integrity_freeze_patch_currently_frozen)
+			bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
 	}
 #endif
 }
@@ -865,6 +872,7 @@ static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->size >> PAGE_SHIFT);
 	set_memory_x((unsigned long)hdr, hdr->size >> PAGE_SHIFT);
+	bhv_bpf_protect_x(hdr, hdr->size);
 }
 
 int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);
@@ -892,6 +900,8 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited && !bhv_integrity_freeze_remove_currently_frozen)
+		bhv_bpf_unprotect(fp);
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/highmem.h include/linux/highmem.h
index 44242268f53..207ca4a12c3 100644
--- include/linux/highmem.h
+++ include/linux/highmem.h
@@ -11,6 +11,8 @@
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
 
+#include <bhv/domain.h>
+
 #include "highmem-internal.h"
 
 /**
@@ -309,12 +311,20 @@ static inline void copy_user_highpage(struct page *to, struct page *from,
 {
 	char *vfrom, *vto;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
 	vfrom = kmap_local_page(from);
 	vto = kmap_local_page(to);
 	copy_user_page(vto, vfrom, vaddr, to);
 	kmsan_unpoison_memory(page_address(to), PAGE_SIZE);
 	kunmap_local(vto);
 	kunmap_local(vfrom);
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 #endif
diff --git include/linux/jump_label.h include/linux/jump_label.h
index 570831ca995..5bf1f729e0e 100644
--- include/linux/jump_label.h
+++ include/linux/jump_label.h
@@ -235,6 +235,27 @@ extern void static_key_enable_cpuslocked(struct static_key *key);
 extern void static_key_disable_cpuslocked(struct static_key *key);
 extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);
 
+#ifdef CONFIG_MODULES
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+struct static_key_mod {
+        struct static_key_mod *next;
+        struct jump_entry *entries;
+        struct module *mod;
+};
+
+struct jump_label_patch {
+        const void *code;
+        int size;
+};
+
+inline struct static_key_mod *static_key_mod(struct static_key *key);
+#endif
+
+#endif /* CONFIG_MODULES */
+
+inline bool static_key_linked(struct static_key *key);
+
 /*
  * We should be using ATOMIC_INIT() for initializing .enabled, but
  * the inclusion of atomic.h is problematic for inclusion of jump_label.h
diff --git include/linux/kmod.h include/linux/kmod.h
index 68f69362d42..a54d81beea5 100644
--- include/linux/kmod.h
+++ include/linux/kmod.h
@@ -17,7 +17,11 @@
 #define KMOD_PATH_LEN 256
 
 #ifdef CONFIG_MODULES
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+extern const char modprobe_path[] __section(".rodata"); /* for sysctl */
+#else
 extern char modprobe_path[]; /* for sysctl */
+#endif
 /* modprobe exit status on success, -ve on error.  Return value
  * usually useless though. */
 extern __printf(2, 3)
diff --git include/linux/lsm_hook_defs.h include/linux/lsm_hook_defs.h
index 6239a378c0e..9f56d3cce6d 100644
--- include/linux/lsm_hook_defs.h
+++ include/linux/lsm_hook_defs.h
@@ -413,3 +413,9 @@ LSM_HOOK(int, 0, uring_override_creds, const struct cred *new)
 LSM_HOOK(int, 0, uring_sqpoll, void)
 LSM_HOOK(int, 0, uring_cmd, struct io_uring_cmd *ioucmd)
 #endif /* CONFIG_IO_URING */
+
+LSM_HOOK(void, LSM_RET_VOID, module_loaded, struct module *mod)
+LSM_HOOK(int, 0, unshare, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, setns, struct task_struct *tsk, struct nsset *nsset)
+LSM_HOOK(int, 0, cgroup_mkdir, struct cgroup *cgrp)
+LSM_HOOK(void, LSM_RET_VOID, cgroup_rmdir, struct cgroup *cgrp)
diff --git include/linux/lsm_hooks.h include/linux/lsm_hooks.h
index 4ec80b96c22..377d244084c 100644
--- include/linux/lsm_hooks.h
+++ include/linux/lsm_hooks.h
@@ -1589,6 +1589,23 @@
  * @uring_cmd:
  *      Check whether the file_operations uring_cmd is allowed to run.
  *
+ * Security hooks for BlueRock Security Module
+ *
+ * @module_loaded:
+ * 	A module has successfully loaded.
+ *
+ * @unshare:
+ * 	A process unshares a part of its context.
+ *
+ * @setns:
+ * 	A setns system call is made.
+ *
+ * @cgroup_mkdir:
+ * 	A cgroup is created.
+ *
+ * @cgroup_rmdir:
+ * 	A cgroup is destroyed.
+ *
  */
 union security_list_options {
 	#define LSM_HOOK(RET, DEFAULT, NAME, ...) RET (*NAME)(__VA_ARGS__);
diff --git include/linux/mem_namespace.h include/linux/mem_namespace.h
new file mode 100644
index 00000000000..58a84fd776f
--- /dev/null
+++ include/linux/mem_namespace.h
@@ -0,0 +1,89 @@
+#ifndef _LINUX_MEM_NS_H
+#define _LINUX_MEM_NS_H
+
+#include <linux/kref.h>
+#include <linux/nsproxy.h>
+#include <linux/ns_common.h>
+
+#ifdef CONFIG_MEM_NS
+#define bhv_pr_info(msg, ...)   pr_info("[-BHV-] %s: " msg "\n", __FUNCTION__, ##__VA_ARGS__)
+#else
+#define bhv_pr_info(msg, ...)
+#endif
+
+struct mem_namespace {
+	struct kref kref;
+	struct user_namespace *user_ns;
+	struct ucounts *ucounts;
+	struct ns_common ns;
+	struct mem_namespace *parent;
+	unsigned int level;
+	uint64_t domain;
+} __randomize_layout;
+
+extern struct mem_namespace init_mem_ns;
+
+#ifdef CONFIG_MEM_NS
+static inline struct mem_namespace *get_mem_ns(struct mem_namespace *ns)
+{
+	if (ns != &init_mem_ns)
+		kref_get(&ns->kref);
+	return ns;
+}
+
+extern void free_mem_ns(struct kref *kref);
+
+static inline void put_mem_ns(struct mem_namespace *ns)
+{
+	struct mem_namespace *parent = NULL;
+
+	while (ns != &init_mem_ns) {
+		parent = ns->parent;
+		if (!kref_put(&ns->kref, free_mem_ns))
+			break;
+		ns = parent;
+	}
+}
+
+extern struct mem_namespace *copy_mem_ns(unsigned long flags,
+					 struct user_namespace *user_ns,
+					 struct mem_namespace *old_ns);
+
+extern struct mem_namespace *memns_of_task(const struct task_struct *task);
+
+extern bool current_in_same_mem_ns(const struct task_struct *task);
+
+extern bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns);
+
+static inline unsigned int task_memns_level(struct task_struct *task) {
+	return memns_of_task(task)->level;
+}
+
+#else /* CONFIG_MEM_NS */
+
+static inline void get_mem_ns(struct mem_namespace *ns) {}
+static inline void put_mem_ns(struct mem_namespace *ns) {}
+
+static inline struct mem_namespace *copy_mem_ns(unsigned long flags,
+						struct user_namespace *user_ns,
+						struct mem_namespace *old_ns)
+{
+	if (flags & CLONE_NEWMEM)
+		return ERR_PTR(-EINVAL);
+
+	return old_ns;
+}
+
+static inline bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return true;
+}
+
+static inline unsigned int task_memns_level(struct task_struct *task)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* _LINUX_MEM_NS_H */
diff --git include/linux/module.h include/linux/module.h
index 35876e89eb9..8acd63e053d 100644
--- include/linux/module.h
+++ include/linux/module.h
@@ -161,6 +161,13 @@ extern void cleanup_module(void);
 #define __INITRODATA_OR_MODULE __INITRODATA
 #endif /*CONFIG_MODULES*/
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+#define __bhv_init_or_module
+#else
+#define __bhv_init_or_module __init_or_module
+#endif
+
+
 /* Generic info of form tag = "info" */
 #define MODULE_INFO(tag, info) __MODULE_INFO(tag, tag, info)
 
diff --git include/linux/nsproxy.h include/linux/nsproxy.h
index cdb171efc7c..499668ef15a 100644
--- include/linux/nsproxy.h
+++ include/linux/nsproxy.h
@@ -11,6 +11,7 @@ struct ipc_namespace;
 struct pid_namespace;
 struct cgroup_namespace;
 struct fs_struct;
+struct mem_namespace;
 
 /*
  * A structure to contain pointers to all per-process
@@ -38,6 +39,7 @@ struct nsproxy {
 	struct time_namespace *time_ns;
 	struct time_namespace *time_ns_for_children;
 	struct cgroup_namespace *cgroup_ns;
+	struct mem_namespace *mem_ns;
 };
 extern struct nsproxy init_nsproxy;
 
diff --git include/linux/proc_ns.h include/linux/proc_ns.h
index 75807ecef88..ff1046249a6 100644
--- include/linux/proc_ns.h
+++ include/linux/proc_ns.h
@@ -34,6 +34,7 @@ extern const struct proc_ns_operations mntns_operations;
 extern const struct proc_ns_operations cgroupns_operations;
 extern const struct proc_ns_operations timens_operations;
 extern const struct proc_ns_operations timens_for_children_operations;
+extern const struct proc_ns_operations memns_operations;
 
 /*
  * We always define these enumerators
@@ -46,6 +47,7 @@ enum {
 	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
 	PROC_CGROUP_INIT_INO	= 0xEFFFFFFBU,
 	PROC_TIME_INIT_INO	= 0xEFFFFFFAU,
+	PROC_MEM_INIT_INO	= 0xEFFFFFF9U,
 };
 
 #ifdef CONFIG_PROC_FS
diff --git include/linux/security.h include/linux/security.h
index 2772f6375f1..d93b87cfe0d 100644
--- include/linux/security.h
+++ include/linux/security.h
@@ -31,6 +31,7 @@
 #include <linux/err.h>
 #include <linux/string.h>
 #include <linux/mm.h>
+#include <linux/nsproxy.h>
 
 struct linux_binprm;
 struct cred;
@@ -481,6 +482,11 @@ int security_inode_notifysecctx(struct inode *inode, void *ctx, u32 ctxlen);
 int security_inode_setsecctx(struct dentry *dentry, void *ctx, u32 ctxlen);
 int security_inode_getsecctx(struct inode *inode, void **ctx, u32 *ctxlen);
 int security_locked_down(enum lockdown_reason what);
+void security_module_loaded(struct module *mod);
+int security_unshare(struct task_struct *tsk, struct nsset *nsset);
+int security_setns(struct task_struct *tsk, struct nsset *nsset);
+int security_cgroup_mkdir(struct cgroup *cgrp);
+void security_cgroup_rmdir(struct cgroup *cgrp);
 #else /* CONFIG_SECURITY */
 
 static inline int call_blocking_lsm_notifier(enum lsm_event event, void *data)
@@ -1381,6 +1387,24 @@ static inline int security_locked_down(enum lockdown_reason what)
 {
 	return 0;
 }
+static inline void security_module_loaded(struct module *)
+{
+}
+static inline int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return 0;
+}
+static inline int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return 0;
+}
+static inline void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+}
 #endif	/* CONFIG_SECURITY */
 
 #if defined(CONFIG_SECURITY) && defined(CONFIG_WATCH_QUEUE)
diff --git include/linux/static_call.h include/linux/static_call.h
index df53bed9d71..1df693d69bf 100644
--- include/linux/static_call.h
+++ include/linux/static_call.h
@@ -135,6 +135,8 @@
 #include <linux/cpu.h>
 #include <linux/static_call_types.h>
 
+#include <bhv/vault.h>
+
 #ifdef CONFIG_HAVE_STATIC_CALL
 #include <asm/static_call.h>
 
@@ -248,7 +250,13 @@ static inline int static_call_init(void) { return 0; }
 
 #define static_call_cond(name)	(void)__static_call(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#ifdef CONFIG_BHV_VAULT_SPACES
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	cpus_read_lock();
@@ -256,6 +264,7 @@ void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 	arch_static_call_transform(NULL, tramp, func, false);
 	cpus_read_unlock();
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_update);
 
 static inline int static_call_text_reserved(void *start, void *end)
 {
@@ -325,7 +334,13 @@ static inline void __static_call_nop(void) { }
 
 #define static_call_cond(name)	(void)__static_call_cond(name)
 
-static inline
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static
+#ifdef CONFIG_BHV_VAULT_SPACES
+	noinline
+#else
+	inline
+#endif
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	WRITE_ONCE(key->func, func);
diff --git include/linux/user_namespace.h include/linux/user_namespace.h
index 45f09bec02c..6e43f0e2b44 100644
--- include/linux/user_namespace.h
+++ include/linux/user_namespace.h
@@ -54,6 +54,7 @@ enum ucount_type {
 	UCOUNT_FANOTIFY_GROUPS,
 	UCOUNT_FANOTIFY_MARKS,
 #endif
+	UCOUNT_MEM_NAMESPACES,
 	UCOUNT_COUNTS,
 };
 
diff --git include/uapi/linux/perf_event.h include/uapi/linux/perf_event.h
index ccb7f5dad59..4858e49aea1 100644
--- include/uapi/linux/perf_event.h
+++ include/uapi/linux/perf_event.h
@@ -824,6 +824,7 @@ enum {
 	USER_NS_INDEX		= 4,
 	MNT_NS_INDEX		= 5,
 	CGROUP_NS_INDEX		= 6,
+	MEM_NS_INDEX		= 7,
 
 	NR_NAMESPACES,		/* number of available namespaces */
 };
diff --git include/uapi/linux/sched.h include/uapi/linux/sched.h
index 3bac0a8ceab..94224cf517c 100644
--- include/uapi/linux/sched.h
+++ include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
  * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
  * syscalls only:
  */
+#define CLONE_NEWMEM	0x00000040	/* New memory namespace */
 #define CLONE_NEWTIME	0x00000080	/* New time namespace */
 
 #ifndef __ASSEMBLY__
diff --git init/main.c init/main.c
index e46aa00b3c9..4839cfb1240 100644
--- init/main.c
+++ init/main.c
@@ -113,6 +113,10 @@
 
 #include <kunit/test.h>
 
+#include <bhv/init/mm_init.h>
+#include <bhv/init/late_start.h>
+#include <bhv/memory_freeze.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -863,6 +867,8 @@ static void __init mm_init(void)
 	pti_init();
 	kmsan_init_runtime();
 	mm_cache_init();
+
+	bhv_mm_init();
 }
 
 #ifdef CONFIG_RANDOMIZE_KSTACK_OFFSET
@@ -999,6 +1005,7 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	trap_init();
 	mm_init();
 	poking_init();
+
 	ftrace_init();
 
 	/* trace_printk can be enabled here */
@@ -1418,6 +1425,7 @@ static void __init do_pre_smp_initcalls(void)
 static int run_init_process(const char *init_filename)
 {
 	const char *const *p;
+	int ret;
 
 	argv_init[0] = init_filename;
 	pr_info("Run %s as init process\n", init_filename);
@@ -1427,7 +1435,10 @@ static int run_init_process(const char *init_filename)
 	pr_debug("  with environment:\n");
 	for (p = envp_init; *p; p++)
 		pr_debug("    %s\n", *p);
-	return kernel_execve(init_filename, argv_init, envp_init);
+	ret = kernel_execve(init_filename, argv_init, envp_init);
+	if (!ret)
+		bhv_memory_freeze_init();
+	return ret;
 }
 
 static int try_to_run_init_process(const char *init_filename)
@@ -1523,6 +1534,8 @@ static int __ref kernel_init(void *unused)
 	free_initmem();
 	mark_readonly();
 
+	bhv_late_start();
+
 	/*
 	 * Kernel mappings are now finalized - update the userspace page-table
 	 * to finalize PTI.
diff --git kernel/Makefile kernel/Makefile
index c90ee75eb80..035c366492d 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -78,6 +78,7 @@ obj-$(CONFIG_CGROUPS) += cgroup/
 obj-$(CONFIG_UTS_NS) += utsname.o
 obj-$(CONFIG_USER_NS) += user_namespace.o
 obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_MEM_NS) += mem_namespace.o
 obj-$(CONFIG_IKCONFIG) += configs.o
 obj-$(CONFIG_IKHEADERS) += kheaders.o
 obj-$(CONFIG_SMP) += stop_machine.o
diff --git kernel/bpf/Kconfig kernel/bpf/Kconfig
index 2dfe1079f77..88b0c3cd594 100644
--- kernel/bpf/Kconfig
+++ kernel/bpf/Kconfig
@@ -42,6 +42,7 @@ config BPF_JIT
 	depends on BPF
 	depends on HAVE_CBPF_JIT || HAVE_EBPF_JIT
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	help
 	  BPF programs are normally handled by a BPF interpreter. This option
 	  allows the kernel to generate native code when a program is loaded
diff --git kernel/bpf/bpf_struct_ops.c kernel/bpf/bpf_struct_ops.c
index 84b2d9dba79..9c6ea35d27f 100644
--- kernel/bpf/bpf_struct_ops.c
+++ kernel/bpf/bpf_struct_ops.c
@@ -496,6 +496,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 
 	set_memory_ro((long)st_map->image, 1);
 	set_memory_x((long)st_map->image, 1);
+	bhv_bpf_protect_x(st_map->image, PAGE_SIZE);
 	err = st_ops->reg(kdata);
 	if (likely(!err)) {
 		/* Pair with smp_load_acquire() during lookup_elem().
@@ -513,6 +514,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 	 */
 	set_memory_nx((long)st_map->image, 1);
 	set_memory_rw((long)st_map->image, 1);
+	bhv_bpf_unprotect(st_map->image);
 	bpf_map_put(map);
 
 reset_unlock:
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 44abf88e1bb..314783ba571 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -39,6 +39,9 @@
 #include <asm/barrier.h>
 #include <asm/unaligned.h>
 
+#include <bhv/integrity.h>
+#include <bhv/vault.h>
+
 /* Registers */
 #define BPF_R0	regs[BPF_REG_0]
 #define BPF_R1	regs[BPF_REG_1]
@@ -691,6 +694,7 @@ void bpf_prog_kallsyms_del(struct bpf_prog *fp)
 	bpf_ksym_del(&fp->aux->ksym);
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static struct bpf_ksym *bpf_ksym_find(unsigned long addr)
 {
 	struct latch_tree_node *n;
@@ -724,6 +728,7 @@ const char *__bpf_address_lookup(unsigned long addr, unsigned long *size,
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_bpf_text_address(unsigned long addr)
 {
 	bool ret;
@@ -889,6 +894,7 @@ static struct bpf_prog_pack *alloc_new_pack(bpf_jit_fill_hole_t bpf_fill_ill_ins
 	set_vm_flush_reset_perms(pack->ptr);
 	set_memory_ro((unsigned long)pack->ptr, BPF_PROG_PACK_SIZE / PAGE_SIZE);
 	set_memory_x((unsigned long)pack->ptr, BPF_PROG_PACK_SIZE / PAGE_SIZE);
+	bhv_bpf_protect_x(pack->ptr, BPF_PROG_PACK_SIZE);
 	return pack;
 }
 
@@ -908,6 +914,7 @@ void *bpf_prog_pack_alloc(u32 size, bpf_jit_fill_hole_t bpf_fill_ill_insns)
 			set_vm_flush_reset_perms(ptr);
 			set_memory_ro((unsigned long)ptr, size / PAGE_SIZE);
 			set_memory_x((unsigned long)ptr, size / PAGE_SIZE);
+			bhv_bpf_protect_x(ptr, size);
 		}
 		goto out;
 	}
@@ -942,6 +949,7 @@ void bpf_prog_pack_free(struct bpf_binary_header *hdr)
 	mutex_lock(&pack_mutex);
 	if (hdr->size > BPF_PROG_PACK_SIZE) {
 		module_memfree(hdr);
+		bhv_bpf_unprotect(hdr);
 		goto out;
 	}
 
@@ -966,6 +974,7 @@ void bpf_prog_pack_free(struct bpf_binary_header *hdr)
 				       BPF_PROG_CHUNK_COUNT, 0) == 0) {
 		list_del(&pack->list);
 		module_memfree(pack->ptr);
+		bhv_bpf_unprotect(pack->ptr);
 		kfree(pack);
 	}
 out:
@@ -1067,6 +1076,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 size = hdr->size;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(size);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index 748ac861699..d762b4fa4c5 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -282,6 +282,7 @@ bpf_trampoline_get_progs(const struct bpf_trampoline *tr, int *total, bool *ip_a
 static void bpf_tramp_image_free(struct bpf_tramp_image *im)
 {
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 	bpf_jit_uncharge_modmem(PAGE_SIZE);
 	percpu_ref_exit(&im->pcref);
@@ -409,6 +410,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 out_uncharge:
 	bpf_jit_uncharge_modmem(PAGE_SIZE);
@@ -474,6 +476,7 @@ static int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mut
 
 	set_memory_ro((long)im->image, 1);
 	set_memory_x((long)im->image, 1);
+	bhv_bpf_protect_x(im->image, PAGE_SIZE);
 
 	WARN_ON(tr->cur_image && total == 0);
 	if (tr->cur_image)
@@ -496,6 +499,7 @@ static int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mut
 		/* reset im->image memory attr for arch_prepare_bpf_trampoline */
 		set_memory_nx((long)im->image, 1);
 		set_memory_rw((long)im->image, 1);
+		bhv_bpf_unprotect(im->image);
 		goto again;
 	}
 #endif
diff --git kernel/cgroup/cgroup.c kernel/cgroup/cgroup.c
index 97ecca43386..d5028fb3a20 100644
--- kernel/cgroup/cgroup.c
+++ kernel/cgroup/cgroup.c
@@ -5759,6 +5759,10 @@ int cgroup_mkdir(struct kernfs_node *parent_kn, const char *name, umode_t mode)
 	if (ret)
 		goto out_destroy;
 
+	ret = security_cgroup_mkdir(cgrp);
+	if (ret)
+		goto out_destroy;
+
 	TRACE_CGROUP_PATH(mkdir, cgrp);
 
 	/* let's create and online css's */
@@ -5957,8 +5961,10 @@ int cgroup_rmdir(struct kernfs_node *kn)
 		return 0;
 
 	ret = cgroup_destroy_locked(cgrp);
-	if (!ret)
+	if (!ret) {
+		security_cgroup_rmdir(cgrp);
 		TRACE_CGROUP_PATH(rmdir, cgrp);
+	}
 
 	cgroup_kn_unlock(kn);
 	return ret;
diff --git kernel/cred.c kernel/cred.c
index d35bc0aa98c..2ca7773d959 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -17,6 +17,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -113,6 +115,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 #endif
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -461,6 +464,8 @@ int commit_creds(struct cred *new)
 #endif
 	BUG_ON(atomic_long_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -720,6 +725,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	if (daemon)
 		old = get_task_cred(daemon);
 	else
diff --git kernel/entry/common.c kernel/entry/common.c
index ccf2b1e1b40..1c3c8b50b7e 100644
--- kernel/entry/common.c
+++ kernel/entry/common.c
@@ -206,6 +206,10 @@ static void exit_to_user_mode_prepare(struct pt_regs *regs)
 	tick_nohz_user_enter_prepare();
 
 	ti_work = read_thread_flags();
+
+	// Make sure we are on the current domain before exiting to userspace
+	bhv_domain_enter(current);
+
 	if (unlikely(ti_work & EXIT_TO_USER_MODE_WORK))
 		ti_work = exit_to_user_mode_loop(regs, ti_work);
 
diff --git kernel/events/core.c kernel/events/core.c
index 872d149b195..3d526aa68bd 100644
--- kernel/events/core.c
+++ kernel/events/core.c
@@ -8280,6 +8280,10 @@ void perf_event_namespaces(struct task_struct *task)
 	perf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],
 			       task, &cgroupns_operations);
 #endif
+#ifdef CONFIG_MEM_NS
+	perf_fill_ns_link_info(&ns_link_info[MEM_NS_INDEX],
+			       task, &memns_operations);
+#endif
 
 	perf_iterate_sb(perf_event_namespaces_output,
 			&namespaces_event,
diff --git kernel/exit.c kernel/exit.c
index bccfa421835..1312139d5f6 100644
--- kernel/exit.c
+++ kernel/exit.c
@@ -73,6 +73,8 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 /*
  * The default value should be high enough to not crash a system that randomly
  * crashes its kernel from time to time, but low enough to at least not permit
diff --git kernel/extable.c kernel/extable.c
index 71f482581ca..dab330d5b6d 100644
--- kernel/extable.c
+++ kernel/extable.c
@@ -16,6 +16,8 @@
 #include <asm/sections.h>
 #include <linux/uaccess.h>
 
+#include <bhv/vault.h>
+
 /*
  * mutex protecting text section modification (dynamic code patching).
  * some users need to sleep (allocating memory...) while they hold this lock.
@@ -63,6 +65,7 @@ const struct exception_table_entry *search_exception_tables(unsigned long addr)
 	return e;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int notrace core_kernel_text(unsigned long addr)
 {
 	if (is_kernel_text(addr))
@@ -74,6 +77,7 @@ int notrace core_kernel_text(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int __kernel_text_address(unsigned long addr)
 {
 	if (kernel_text_address(addr))
@@ -91,6 +95,7 @@ int __kernel_text_address(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int kernel_text_address(unsigned long addr)
 {
 	bool no_rcu;
diff --git kernel/fork.c kernel/fork.c
index 7e9a5919299..5cae601a519 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -97,6 +97,9 @@
 #include <linux/scs.h>
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
+#include <linux/mem_namespace.h>
+
+#include <bhv/creds.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -703,6 +706,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	}
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
+
 loop_out:
 	mas_destroy(&mas);
 	if (!retval)
@@ -1192,6 +1196,11 @@ static inline void __mmput(struct mm_struct *mm)
 {
 	VM_BUG_ON(atomic_read(&mm->mm_users));
 
+#ifdef CONFIG_MEM_NS
+	bhv_domain_destroy_pgd(current, mm);
+	bhv_domain_debug_destroy_pgd(current, mm);
+#endif
+
 	uprobe_clear_state(mm);
 	exit_aio(mm);
 	ksm_exit(mm);
@@ -1207,6 +1216,10 @@ static inline void __mmput(struct mm_struct *mm)
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
 	lru_gen_del_mm(mm);
+#ifdef CONFIG_MEM_NS
+	bhv_domain_destroy_pgd(current, mm);
+	bhv_domain_debug_destroy_pgd(current, mm);
+#endif
 	mmdrop(mm);
 }
 
@@ -2050,6 +2063,8 @@ static __latent_entropy struct task_struct *copy_process(
 	/*
 	 * If the new process will be in a different pid or user namespace
 	 * do not allow it to share a thread group with the forking task.
+	 *
+	 * XXX: Consider adding additional constraints for memory namespaces.
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
@@ -2249,9 +2264,12 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2264,15 +2282,15 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_signal(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_sighand;
-	retval = copy_mm(clone_flags, p);
+	retval = copy_namespaces(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_signal;
-	retval = copy_namespaces(clone_flags, p);
+	retval = copy_mm(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_mm;
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_io(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_mm;
 	retval = copy_thread(p, args);
 	if (retval)
 		goto bad_fork_cleanup_io;
@@ -2526,13 +2544,13 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);
-bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm) {
 		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
 	}
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2544,6 +2562,7 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
@@ -3078,7 +3097,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
 				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+				CLONE_NEWTIME|CLONE_NEWMEM))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -3181,6 +3200,11 @@ int ksys_unshare(unsigned long unshare_flags)
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
 
+	/*
+	 * XXX: Consider CLONE_NEWMEM! Do we need to unshare the thread group
+	 * via CLONE_THREAD?
+	 */
+
 	err = check_unshare_flags(unshare_flags);
 	if (err)
 		goto bad_unshare_out;
@@ -3212,6 +3236,15 @@ int ksys_unshare(unsigned long unshare_flags)
 	}
 
 	if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
+		struct nsset nsset = { .flags = unshare_flags,
+				       .nsproxy = new_nsproxy,
+				       .fs = new_fs,
+				       .cred = new_cred };
+
+		err = security_unshare(current, &nsset);
+		if (err)
+			goto bad_unshare_cleanup_cred;
+
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
diff --git kernel/jump_label.c kernel/jump_label.c
index 714ac4c3b55..cd5b443790f 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,9 +19,16 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
+#include <bhv/vault.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
+static int jump_label_cmp(const void *a, const void *b);
+static void jump_label_swap(void *a, void *b, int size);
+
 void jump_label_lock(void)
 {
 	mutex_lock(&jump_label_mutex);
@@ -32,6 +39,17 @@ void jump_label_unlock(void)
 	mutex_unlock(&jump_label_mutex);
 }
 
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_lock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, cpus_read_unlock)
+BHV_VAULT_FN_WRAPPER0_NORET(void, lockdep_assert_cpus_held)
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER2(int, jump_label_cmp, const void *, a, const void *, b);
+BHV_VAULT_FN_WRAPPER3_NORET(void, jump_label_swap, void *, a, void *, b, int, s);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_cmp(const void *a, const void *b)
 {
 	const struct jump_entry *jea = a;
@@ -59,7 +77,9 @@ static int jump_label_cmp(const void *a, const void *b)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_cmp);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_swap(void *a, void *b, int size)
 {
 	long delta = (unsigned long)a - (unsigned long)b;
@@ -75,7 +95,9 @@ static void jump_label_swap(void *a, void *b, int size)
 	jeb->target	= tmp.target + delta;
 	jeb->key	= tmp.key + delta;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_swap);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void
 jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 {
@@ -83,13 +105,23 @@ jump_label_sort_entries(struct jump_entry *start, struct jump_entry *stop)
 	void *swapfn = NULL;
 
 	if (IS_ENABLED(CONFIG_HAVE_ARCH_JUMP_LABEL_RELATIVE))
+#ifdef CONFIG_BHV_VAULT_SPACES
+		swapfn = bhv_wrapper_jump_label_swap;
+#else
 		swapfn = jump_label_swap;
+#endif
 
 	size = (((unsigned long)stop - (unsigned long)start)
 					/ sizeof(struct jump_entry));
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	sort(start, size, sizeof(struct jump_entry), bhv_wrapper_jump_label_cmp, swapfn);
+#else
 	sort(start, size, sizeof(struct jump_entry), jump_label_cmp, swapfn);
+#endif
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key);
 
 /*
@@ -101,6 +133,7 @@ static void jump_label_update(struct static_key *key);
  * 'static_key_disable()', which require bug.h. This should allow jump_label.h
  * to be included from most/all places for CONFIG_JUMP_LABEL.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int static_key_count(struct static_key *key)
 {
 	/*
@@ -111,14 +144,20 @@ int static_key_count(struct static_key *key)
 
 	return n >= 0 ? n : 1;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_count);
 EXPORT_SYMBOL_GPL(static_key_count);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_slow_inc_cpuslocked(struct static_key *key)
 {
 	int v, v1;
 
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	/*
 	 * Careful if we get concurrent static_key_slow_inc() calls;
@@ -138,7 +177,11 @@ void static_key_slow_inc_cpuslocked(struct static_key *key)
 			return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_read(&key->enabled) == 0) {
 		atomic_set(&key->enabled, -1);
 		jump_label_update(key);
@@ -150,8 +193,13 @@ void static_key_slow_inc_cpuslocked(struct static_key *key)
 	} else {
 		atomic_inc(&key->enabled);
 	}
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_slow_inc_cpuslocked);
 
 void static_key_slow_inc(struct static_key *key)
 {
@@ -161,17 +209,26 @@ void static_key_slow_inc(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_slow_inc);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_enable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) > 0) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 1);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_read(&key->enabled) == 0) {
 		atomic_set(&key->enabled, -1);
 		jump_label_update(key);
@@ -180,8 +237,13 @@ void static_key_enable_cpuslocked(struct static_key *key)
 		 */
 		atomic_set_release(&key->enabled, 1);
 	}
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_enable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_enable_cpuslocked);
 
 void static_key_enable(struct static_key *key)
@@ -192,21 +254,35 @@ void static_key_enable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_enable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void static_key_disable_cpuslocked(struct static_key *key)
 {
 	STATIC_KEY_CHECK_USE(key);
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (atomic_read(&key->enabled) != 1) {
 		WARN_ON_ONCE(atomic_read(&key->enabled) != 0);
 		return;
 	}
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_cmpxchg(&key->enabled, 1, 0))
 		jump_label_update(key);
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_key_disable_cpuslocked);
 EXPORT_SYMBOL_GPL(static_key_disable_cpuslocked);
 
 void static_key_disable(struct static_key *key)
@@ -217,6 +293,7 @@ void static_key_disable(struct static_key *key)
 }
 EXPORT_SYMBOL_GPL(static_key_disable);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool static_key_slow_try_dec(struct static_key *key)
 {
 	int val;
@@ -236,18 +313,32 @@ static bool static_key_slow_try_dec(struct static_key *key)
 	return true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __static_key_slow_dec_cpuslocked(struct static_key *key)
 {
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_lockdep_assert_cpus_held();
+#else
 	lockdep_assert_cpus_held();
+#endif
 
 	if (static_key_slow_try_dec(key))
 		return;
 
-	jump_label_lock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
+	mutex_lock(&jump_label_mutex);
+#endif
 	if (atomic_dec_and_test(&key->enabled))
 		jump_label_update(key);
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+#else
+	mutex_unlock(&jump_label_mutex);
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_key_slow_dec_cpuslocked);
 
 static void __static_key_slow_dec(struct static_key *key)
 {
@@ -306,6 +397,7 @@ void jump_label_rate_limit(struct static_key_deferred *key,
 }
 EXPORT_SYMBOL_GPL(jump_label_rate_limit);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 {
 	if (jump_entry_code(entry) <= (unsigned long)end &&
@@ -315,6 +407,7 @@ static int addr_conflict(struct jump_entry *entry, void *start, void *end)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_text_reserved(struct jump_entry *iter_start,
 		struct jump_entry *iter_stop, void *start, void *end, bool init)
 {
@@ -340,27 +433,32 @@ static void arch_jump_label_transform_static(struct jump_entry *entry,
 }
 #endif
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct jump_entry *static_key_entries(struct static_key *key)
 {
 	WARN_ON_ONCE(key->type & JUMP_TYPE_LINKED);
 	return (struct jump_entry *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_key_type(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_TRUE;
 }
 
-static inline bool static_key_linked(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline bool static_key_linked(struct static_key *key)
 {
 	return key->type & JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_clear_linked(struct static_key *key)
 {
 	key->type &= ~JUMP_TYPE_LINKED;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_key_set_linked(struct static_key *key)
 {
 	key->type |= JUMP_TYPE_LINKED;
@@ -375,6 +473,7 @@ static inline void static_key_set_linked(struct static_key *key)
  * type is in use and to store the initial branch direction, we use an access
  * function which preserves these bits.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_entries(struct static_key *key,
 				   struct jump_entry *entries)
 {
@@ -386,6 +485,7 @@ static void static_key_set_entries(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum jump_label_type jump_label_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -396,6 +496,7 @@ static enum jump_label_type jump_label_type(struct jump_entry *entry)
 	return enabled ^ branch;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static bool jump_label_can_update(struct jump_entry *entry, bool init)
 {
 	/*
@@ -423,6 +524,7 @@ static bool jump_label_can_update(struct jump_entry *entry, bool init)
 }
 
 #ifndef HAVE_JUMP_LABEL_BATCH
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -434,6 +536,7 @@ static void __jump_label_update(struct static_key *key,
 	}
 }
 #else
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_update(struct static_key *key,
 				struct jump_entry *entry,
 				struct jump_entry *stop,
@@ -455,8 +558,10 @@ static void __jump_label_update(struct static_key *key,
 	arch_jump_label_transform_apply();
 }
 #endif
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_update);
 
-void __init jump_label_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static void __jump_label_init(void)
 {
 	struct jump_entry *iter_start = __start___jump_table;
 	struct jump_entry *iter_stop = __stop___jump_table;
@@ -475,8 +580,13 @@ void __init jump_label_init(void)
 	if (static_key_initialized)
 		return;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 	jump_label_sort_entries(iter_start, iter_stop);
 
 	for (iter = iter_start; iter < iter_stop; iter++) {
@@ -498,12 +608,24 @@ void __init jump_label_init(void)
 		static_key_set_entries(key, iter);
 	}
 	static_key_initialized = true;
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __jump_label_init);
+
+void __init jump_label_init(void) {
+	__jump_label_init();
+}
+
 
 #ifdef CONFIG_MODULES
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 {
 	struct static_key *key = jump_entry_key(entry);
@@ -514,24 +636,29 @@ enum jump_label_type jump_label_init_type(struct jump_entry *entry)
 	return type ^ branch;
 }
 
+#ifndef CONFIG_BHV_VAULT_SPACES
 struct static_key_mod {
 	struct static_key_mod *next;
 	struct jump_entry *entries;
 	struct module *mod;
 };
+#endif
 
-static inline struct static_key_mod *static_key_mod(struct static_key *key)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+inline struct static_key_mod *static_key_mod(struct static_key *key)
 {
 	WARN_ON_ONCE(!static_key_linked(key));
 	return (struct static_key_mod *)(key->type & ~JUMP_TYPE_MASK);
 }
 
+
 /***
  * key->type and key->next are the same via union.
  * This sets key->next and preserves the type bits.
  *
  * See additional comments above static_key_set_entries().
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_key_set_mod(struct static_key *key,
 			       struct static_key_mod *mod)
 {
@@ -543,6 +670,7 @@ static void static_key_set_mod(struct static_key *key,
 	key->type |= type;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __jump_label_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
@@ -567,6 +695,7 @@ static int __jump_label_mod_text_reserved(void *start, void *end)
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void __jump_label_mod_update(struct static_key *key)
 {
 	struct static_key_mod *mod;
@@ -592,6 +721,7 @@ static void __jump_label_mod_update(struct static_key *key)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int jump_label_add_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -599,6 +729,9 @@ static int jump_label_add_module(struct module *mod)
 	struct jump_entry *iter;
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, *jlm2;
+#ifndef CONFIG_BHV_VAULT_SPACES
+	int rc;
+#endif
 
 	/* if the module doesn't have jump label entries, just return */
 	if (iter_start == iter_stop)
@@ -606,6 +739,11 @@ static int jump_label_add_module(struct module *mod)
 
 	jump_label_sort_entries(iter_start, iter_stop);
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if ((rc = bhv_jump_label_add_module(mod)))
+		return rc;
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		struct static_key *iterk;
 		bool in_init;
@@ -654,6 +792,7 @@ static int jump_label_add_module(struct module *mod)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_del_module(struct module *mod)
 {
 	struct jump_entry *iter_start = mod->jump_entries;
@@ -662,6 +801,10 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+#ifndef CONFIG_BHV_VAULT_SPACES
+	bhv_jump_label_del_module(mod);
+#endif
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
@@ -704,6 +847,7 @@ static void jump_label_del_module(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int
 jump_label_module_notify(struct notifier_block *self, unsigned long val,
 			 void *data)
@@ -711,8 +855,13 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 	struct module *mod = data;
 	int ret = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&jump_label_mutex);
+#else
 	cpus_read_lock();
-	jump_label_lock();
+	mutex_lock(&jump_label_mutex);
+#endif
 
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -727,11 +876,17 @@ jump_label_module_notify(struct notifier_block *self, unsigned long val,
 		break;
 	}
 
-	jump_label_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&jump_label_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&jump_label_mutex);
 	cpus_read_unlock();
+#endif
 
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_module_notify);
 
 static struct notifier_block jump_label_module_nb = {
 	.notifier_call = jump_label_module_notify,
@@ -759,6 +914,7 @@ early_initcall(jump_label_init_module);
  *
  * returns 1 if there is an overlap, 0 otherwise
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int jump_label_text_reserved(void *start, void *end)
 {
 	bool init = system_state < SYSTEM_RUNNING;
@@ -773,7 +929,9 @@ int jump_label_text_reserved(void *start, void *end)
 #endif
 	return ret;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_text_reserved);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void jump_label_update(struct static_key *key)
 {
 	struct jump_entry *stop = __stop___jump_table;
@@ -800,6 +958,7 @@ static void jump_label_update(struct static_key *key)
 	if (entry)
 		__jump_label_update(key, entry, stop, init);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, jump_label_update);
 
 #ifdef CONFIG_STATIC_KEYS_SELFTEST
 static DEFINE_STATIC_KEY_TRUE(sk_true);
diff --git kernel/kmod.c kernel/kmod.c
index b717134ebe1..82e2514271d 100644
--- kernel/kmod.c
+++ kernel/kmod.c
@@ -58,7 +58,11 @@ static DECLARE_WAIT_QUEUE_HEAD(kmod_wq);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+const char modprobe_path[] __section(".rodata") = CONFIG_MODPROBE_PATH;
+#else
 char modprobe_path[KMOD_PATH_LEN] = CONFIG_MODPROBE_PATH;
+#endif
 
 static void free_modprobe_argv(struct subprocess_info *info)
 {
@@ -84,7 +88,7 @@ static int call_modprobe(char *module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)modprobe_path;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
diff --git kernel/kthread.c kernel/kthread.c
index f97fd01a293..d912a1a48a1 100644
--- kernel/kthread.c
+++ kernel/kthread.c
@@ -30,6 +30,7 @@
 #include <linux/sched/isolation.h>
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
@@ -1422,6 +1423,9 @@ void kthread_use_mm(struct mm_struct *mm)
 	membarrier_update_current_mm(mm);
 	switch_mm_irqs_off(active_mm, mm, tsk);
 	local_irq_enable();
+#ifdef CONFIG_MEM_NS
+	bhv_domain_enter(mm == NULL ? NULL : mm->owner);
+#endif
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
@@ -1470,6 +1474,7 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	/* active_mm is still 'mm' */
 	enter_lazy_tlb(mm, tsk);
 	local_irq_enable();
+	bhv_domain_enter(NULL);
 	task_unlock(tsk);
 }
 EXPORT_SYMBOL_GPL(kthread_unuse_mm);
diff --git kernel/mem_namespace.c kernel/mem_namespace.c
new file mode 100644
index 00000000000..2d993160a64
--- /dev/null
+++ kernel/mem_namespace.c
@@ -0,0 +1,255 @@
+#include <linux/user_namespace.h>
+#include <linux/mem_namespace.h>
+#include <linux/proc_ns.h>
+#include <linux/cred.h>
+#include <linux/sched/task.h>
+#include <linux/slab.h>
+
+#include <bhv/domain.h>
+
+uint64_t get_free_domain(void)
+{
+	uint64_t domain = BHV_INVALID_DOMAIN;
+	bhv_domain_create(&domain);
+	return domain;
+}
+
+void put_domain(uint64_t domain)
+{
+	/*
+	 * XXX: Do we need to destroy nested, higher-level domains that belong
+	 * to the acestor tree of this domain if they are still around?
+	 */
+
+	if (domain == BHV_INVALID_DOMAIN)
+		return;
+
+	/*
+	 * We assume that the caller takes the necessary steps to switch to
+	 * another, valid domain before putting/releasing the given domain.
+	 */
+
+	BUG_ON(bhv_get_domain(current) == domain);
+
+	bhv_domain_destroy(domain);
+}
+
+static struct kmem_cache *mem_ns_cache;
+
+struct mem_namespace init_mem_ns = {
+	.kref = KREF_INIT(2),
+	.user_ns = &init_user_ns,
+	.domain = BHV_INIT_DOMAIN,
+	.ns.inum = PROC_MEM_INIT_INO,
+	.level = 0,
+	.parent = NULL,
+#ifdef CONFIG_MEM_NS
+	.ns.ops = &memns_operations,
+#endif
+};
+EXPORT_SYMBOL_GPL(init_mem_ns);
+
+struct mem_namespace *memns_of_task(const struct task_struct *task)
+{
+	/*
+	 * Kernel threads, and threads that do not act on behalf of a user space
+	 * task, do not have a valid nsproxy. These threads shall switch to the
+	 * default memory namespace that we use for the init_task.
+	 * Alternatively, we can define a dedicated memory namespace, which all
+	 * kernel threads will enter if they do not execute on behalf of a user
+	 * space task.
+	 */
+	if (task == NULL || task->nsproxy == NULL)
+		return init_task.nsproxy->mem_ns;
+
+	return task->nsproxy->mem_ns;
+}
+
+bool current_in_same_mem_ns(const struct task_struct *task)
+{
+	return memns_of_task(current) == memns_of_task(task);
+}
+
+static struct ucounts *inc_mem_namespaces(struct user_namespace *ns)
+{
+	return inc_ucount(ns, current_euid(), UCOUNT_MEM_NAMESPACES);
+}
+
+static void dec_mem_namespaces(struct ucounts *ucounts)
+{
+	dec_ucount(ucounts, UCOUNT_MEM_NAMESPACES);
+}
+
+static struct mem_namespace *create_mem_namespace(struct user_namespace *user_ns,
+						  struct mem_namespace *parent_ns)
+{
+	struct mem_namespace *ns = NULL;
+	unsigned int level = parent_ns->level + 1;
+	struct ucounts *ucounts;
+	uint64_t domain = 0;
+	int err = -EINVAL;
+
+	if (!in_userns(parent_ns->user_ns, user_ns))
+		goto out;
+
+	/* XXX: Consider limiting the number of nested memory namespaces. */
+
+	domain = get_free_domain();
+	if (domain == BHV_INVALID_DOMAIN && bhv_domain_is_active())
+		goto out;
+
+	ucounts = inc_mem_namespaces(user_ns);
+	if (!ucounts)
+		goto out_domain;
+
+	err = -ENOMEM;
+	ns = kmem_cache_zalloc(mem_ns_cache, GFP_KERNEL);
+	if (ns == NULL)
+		goto out_dec;
+
+	err = ns_alloc_inum(&ns->ns);
+	if (err)
+		goto out_free;
+
+	kref_init(&ns->kref);
+	ns->level = level;
+	ns->parent = get_mem_ns(parent_ns);
+	ns->ns.ops = &memns_operations;
+	ns->domain = domain;
+	ns->user_ns = get_user_ns(user_ns);
+	ns->ucounts = ucounts;
+
+	return ns;
+
+out_free:
+	kmem_cache_free(mem_ns_cache, ns);
+out_dec:
+	dec_mem_namespaces(ucounts);
+out_domain:
+	put_domain(domain);
+out:
+	return ERR_PTR(err);
+}
+
+struct mem_namespace *copy_mem_ns(unsigned long flags,
+				  struct user_namespace *user_ns,
+				  struct mem_namespace *old_ns)
+{
+	BUG_ON(!old_ns);
+
+	if (!(flags & CLONE_NEWMEM)) {
+		return get_mem_ns(old_ns);
+	}
+
+	/*
+	 * XXX: Consider performing additional checks (see pid_namespaces.c); we
+	 * shall proceed only if the old_ns corresponds to the namespace, in
+	 * which the current task resides.
+	 */
+
+	return create_mem_namespace(user_ns, old_ns);
+}
+
+static void destroy_mem_namespace(struct mem_namespace *ns)
+{
+	put_domain(ns->domain);
+	ns_free_inum(&ns->ns);
+
+	/*
+	 * XXX: Make the namespace leverage RCU (see pid_namespace.c)!
+	 */
+
+	dec_mem_namespaces(ns->ucounts);
+	put_user_ns(ns->user_ns);
+
+	kmem_cache_free(mem_ns_cache, ns);
+}
+
+void free_mem_ns(struct kref *kref)
+{
+	struct mem_namespace *ns = container_of(kref, struct mem_namespace, kref);
+	destroy_mem_namespace(ns);
+}
+
+static inline struct mem_namespace *to_mem_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct mem_namespace, ns);
+}
+
+static struct ns_common *memns_get(struct task_struct *task)
+{
+	struct mem_namespace *ns = NULL;
+	struct nsproxy *nsproxy;
+
+	task_lock(task);
+	nsproxy = task->nsproxy;
+	if (nsproxy) {
+		ns = nsproxy->mem_ns;
+		get_mem_ns(ns);
+	}
+	task_unlock(task);
+
+	return ns ? &ns->ns : NULL;
+}
+
+static void memns_put(struct ns_common *ns)
+{
+	put_mem_ns(to_mem_ns(ns));
+}
+
+bool task_in_ancestor_memns(struct task_struct *task, struct mem_namespace *ns)
+{
+	struct mem_namespace *task_ns = memns_of_task(task);
+	struct mem_namespace *ancestor = ns;
+
+	if (ancestor->level < task_ns->level)
+		return false;
+
+	while (ancestor->level > task_ns->level) {
+		ancestor = ancestor->parent;
+	}
+
+	return (ancestor == task_ns);
+}
+
+static int memns_install(struct nsset *nsset, struct ns_common *ns)
+{
+	struct nsproxy *nsproxy = nsset->nsproxy;
+	struct mem_namespace *new = to_mem_ns(ns);
+
+	if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
+	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!task_in_ancestor_memns(current, new))
+		return -EINVAL;
+
+	put_mem_ns(nsproxy->mem_ns);
+	nsproxy->mem_ns = get_mem_ns(new);
+
+	/* XXX: Do we need an explicit mem_ns_for_children? */
+
+	return 0;
+}
+
+static struct user_namespace *memns_owner(struct ns_common *ns)
+{
+	return to_mem_ns(ns)->user_ns;
+}
+
+const struct proc_ns_operations memns_operations = {
+	.name		= "mem",
+	.type		= CLONE_NEWMEM,
+	.get		= memns_get,
+	.put		= memns_put,
+	.install	= memns_install,
+	.owner		= memns_owner,
+};
+
+static int __init mem_ns_init(void)
+{
+	mem_ns_cache = KMEM_CACHE(mem_namespace, SLAB_PANIC);
+	return 0;
+}
+
+__initcall(mem_ns_init);
diff --git kernel/module/main.c kernel/module/main.c
index 554aba47ab6..4034c58608f 100644
--- kernel/module/main.c
+++ kernel/module/main.c
@@ -57,6 +57,9 @@
 #include <uapi/linux/module.h>
 #include "internal.h"
 
+#include <bhv/module.h>
+#include <bhv/vault.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -1189,6 +1192,8 @@ static void free_module(struct module *mod)
 		       mod->name);
 	mutex_unlock(&module_mutex);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
@@ -2355,6 +2360,7 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 #ifdef CONFIG_ARCH_WANTS_MODULES_DATA_IN_VMALLOC
 	vfree(mod->data_layout.base);
 #endif
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -2510,6 +2516,7 @@ static noinline int do_init_module(struct module *mod)
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	mod->init_layout.base = NULL;
 	mod->init_layout.size = 0;
 	mod->init_layout.ro_size = 0;
@@ -2840,10 +2847,14 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
 
+	security_module_loaded(mod);
+
 	mod->async_probe_requested = async_probe;
 
 	/* Module is ready to execute: parsing args may do that. */
@@ -3062,6 +3073,7 @@ bool is_module_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_address(unsigned long addr)
 {
 	struct module *mod;
@@ -3095,6 +3107,7 @@ struct module *__module_address(unsigned long addr)
  * anywhere in a module.  See kernel_text_address() for testing if an
  * address corresponds to kernel or module code.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_module_text_address(unsigned long addr)
 {
 	bool ret;
@@ -3113,6 +3126,7 @@ bool is_module_text_address(unsigned long addr)
  * Must be called with preempt disabled or module mutex held so that
  * module doesn't get freed during this.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *__module_text_address(unsigned long addr)
 {
 	struct module *mod = __module_address(addr);
diff --git kernel/module/signing.c kernel/module/signing.c
index a2ff4242e62..c96ea726303 100644
--- kernel/module/signing.c
+++ kernel/module/signing.c
@@ -19,9 +19,23 @@
 #undef MODULE_PARAM_PREFIX
 #define MODULE_PARAM_PREFIX "module."
 
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+#define sig_enforce true
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
+void set_module_sig_enforced(void)
+{
+	sig_enforce = true;
+}
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
+
 /*
  * Export sig_enforce kernel cmdline parameter to allow other subsystems rely
  * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.
@@ -32,11 +46,6 @@ bool is_module_sig_enforced(void)
 }
 EXPORT_SYMBOL(is_module_sig_enforced);
 
-void set_module_sig_enforced(void)
-{
-	sig_enforce = true;
-}
-
 /*
  * Verify the signature on a module.
  */
diff --git kernel/module/tree_lookup.c kernel/module/tree_lookup.c
index 8ec5cfd6049..99f22d1e883 100644
--- kernel/module/tree_lookup.c
+++ kernel/module/tree_lookup.c
@@ -8,6 +8,7 @@
 
 #include <linux/module.h>
 #include <linux/rbtree_latch.h>
+#include <bhv/vault.h>
 #include "internal.h"
 
 /*
@@ -105,6 +106,7 @@ void mod_tree_remove(struct module *mod)
 #endif
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct module *mod_find(unsigned long addr, struct mod_tree_root *tree)
 {
 	struct latch_tree_node *ltn;
diff --git kernel/nsproxy.c kernel/nsproxy.c
index eec72ca962e..d54f241d99b 100644
--- kernel/nsproxy.c
+++ kernel/nsproxy.c
@@ -16,6 +16,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/utsname.h>
 #include <linux/pid_namespace.h>
+#include <linux/mem_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
 #include <linux/time_namespace.h>
@@ -27,6 +28,8 @@
 #include <linux/cgroup.h>
 #include <linux/perf_event.h>
 
+#include <bhv/domain.h>
+
 static struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = {
@@ -47,6 +50,9 @@ struct nsproxy init_nsproxy = {
 	.time_ns		= &init_time_ns,
 	.time_ns_for_children	= &init_time_ns,
 #endif
+#ifdef CONFIG_MEM_NS
+	.mem_ns			= &init_mem_ns,
+#endif
 };
 
 static inline struct nsproxy *create_nsproxy(void)
@@ -75,6 +81,10 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	if (!new_nsp)
 		return ERR_PTR(-ENOMEM);
 
+	if (bhv_check_memns_enable_flags(flags)) {
+		flags |= CLONE_NEWMEM;
+	}
+
 	new_nsp->mnt_ns = copy_mnt_ns(flags, tsk->nsproxy->mnt_ns, user_ns, new_fs);
 	if (IS_ERR(new_nsp->mnt_ns)) {
 		err = PTR_ERR(new_nsp->mnt_ns);
@@ -121,8 +131,19 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	}
 	new_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);
 
+	new_nsp->mem_ns = copy_mem_ns(flags, user_ns, tsk->nsproxy->mem_ns);
+	if (IS_ERR(new_nsp->mem_ns)) {
+		err = PTR_ERR(new_nsp->mem_ns);
+		goto out_mem;
+	}
+
 	return new_nsp;
 
+out_mem:
+	if (new_nsp->time_ns)
+		put_time_ns(new_nsp->time_ns);
+	if (new_nsp->time_ns_for_children)
+		put_time_ns(new_nsp->time_ns_for_children);
 out_time:
 	put_net(new_nsp->net_ns);
 out_net:
@@ -156,7 +177,7 @@ int copy_namespaces(unsigned long flags, struct task_struct *tsk)
 
 	if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			      CLONE_NEWPID | CLONE_NEWNET |
-			      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {
+			      CLONE_NEWCGROUP | CLONE_NEWTIME | CLONE_NEWMEM)))) {
 		if (likely(old_ns->time_ns_for_children == old_ns->time_ns)) {
 			get_nsproxy(old_ns);
 			return 0;
@@ -199,6 +220,8 @@ void free_nsproxy(struct nsproxy *ns)
 		put_time_ns(ns->time_ns);
 	if (ns->time_ns_for_children)
 		put_time_ns(ns->time_ns_for_children);
+	if (ns->mem_ns)
+		put_mem_ns(ns->mem_ns);
 	put_cgroup_ns(ns->cgroup_ns);
 	put_net(ns->net_ns);
 	kmem_cache_free(nsproxy_cachep, ns);
@@ -216,7 +239,7 @@ int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 
 	if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |
-			       CLONE_NEWTIME)))
+			       CLONE_NEWTIME | CLONE_NEWMEM)))
 		return 0;
 
 	user_ns = new_cred ? new_cred->user_ns : current_user_ns();
@@ -245,6 +268,31 @@ void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
 	p->nsproxy = new;
 	task_unlock(p);
 
+	/*
+	 * Move the task's address space to the given domain only if we do not
+	 * destroy the nsproxy that the task is about to switch to is valid.
+	 * Switching to an invalid nsproxy (nsproxy == NULL) means that the task
+	 * is about to be destroyed.
+	 */
+	if (new != NULL) {
+		bhv_domain_transfer_mm(p->mm, ns, new);
+
+		// If we change the domain of the current process, we need to switch.
+		if (current == p) {
+			bhv_domain_enter(p);
+		}
+	} else {
+		/*
+		* Note that  bhv_domain_enter will automatically determine which domain
+		* to switch to (i.e., to the task's domain maintained by its nsproxy or
+		* to the default domain of init_task).
+		*
+		* XXX: If the task switches to an invalid nsproxy, we should consider
+		* switching to the parent's domain.
+		*/
+		bhv_domain_enter(p);
+	}
+
 	if (ns)
 		put_nsproxy(ns);
 }
@@ -258,7 +306,7 @@ static int check_setns_flags(unsigned long flags)
 {
 	if (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 				 CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |
-				 CLONE_NEWPID | CLONE_NEWCGROUP)))
+				 CLONE_NEWPID | CLONE_NEWCGROUP | CLONE_NEWMEM)))
 		return -EINVAL;
 
 #ifndef CONFIG_USER_NS
@@ -289,6 +337,10 @@ static int check_setns_flags(unsigned long flags)
 	if (flags & CLONE_NEWTIME)
 		return -EINVAL;
 #endif
+#ifndef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM)
+		return -EINVAL;
+#endif
 
 	return 0;
 }
@@ -471,6 +523,14 @@ static int validate_nsset(struct nsset *nsset, struct pid *pid)
 	}
 #endif
 
+#ifdef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM) {
+		ret = validate_ns(nsset, &nsp->mem_ns->ns);
+		if (ret)
+			goto out;
+	}
+#endif
+
 out:
 	if (pid_ns)
 		put_pid_ns(pid_ns);
@@ -541,6 +601,10 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 			err = -EINVAL;
 		flags = ns->ops->type;
 	} else if (!IS_ERR(pidfd_pid(file))) {
+		if (bhv_check_memns_enable_flags(flags)) {
+			flags |= CLONE_NEWMEM;
+		}
+
 		err = check_setns_flags(flags);
 	} else {
 		err = -EINVAL;
@@ -553,12 +617,26 @@ SYSCALL_DEFINE2(setns, int, fd, int, flags)
 		goto out;
 
 	if (proc_ns_file(file))
+#ifdef CONFIG_MEM_NS
+		/*
+		 * XXX: Note that we cannot piggy-back memory namespaces onto
+		 * pid namespaces during nsset/nsenter if no pidfd is given.
+		 * This is because we cannot identify the associated memory
+		 * namespace without any additional links between the
+		 * pid_namespace and the mem_namespace data structures. Consider
+		 * adding links or using pid namespaces alone for creating
+		 * memory isolation domains.
+		 */
+#endif
 		err = validate_ns(&nsset, ns);
 	else
 		err = validate_nsset(&nsset, file->private_data);
 	if (!err) {
-		commit_nsset(&nsset);
-		perf_event_namespaces(current);
+		err = security_setns(current, &nsset);
+		if (!err) {
+			commit_nsset(&nsset);
+			perf_event_namespaces(current);
+		}
 	}
 	put_nsset(&nsset);
 out:
diff --git kernel/printk/printk.c kernel/printk/printk.c
index 0ae06d5046b..8592fa5a67e 100644
--- kernel/printk/printk.c
+++ kernel/printk/printk.c
@@ -892,7 +892,7 @@ static int devkmsg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-const struct file_operations kmsg_fops = {
+const struct file_operations kmsg_fops __section(".rodata") = {
 	.open = devkmsg_open,
 	.read = devkmsg_read,
 	.write_iter = devkmsg_write,
diff --git kernel/rcu/tree.c kernel/rcu/tree.c
index 61f9503a5fe..dbf0fd54d89 100644
--- kernel/rcu/tree.c
+++ kernel/rcu/tree.c
@@ -66,6 +66,8 @@
 #include <linux/context_tracking.h>
 #include "../time/tick-internal.h"
 
+#include <bhv/vault.h>
+
 #include "tree.h"
 #include "rcu.h"
 
@@ -713,6 +715,7 @@ static void rcu_disable_urgency_upon_qs(struct rcu_data *rdp)
  * Make notrace because it can be called by the internal functions of
  * ftrace, and making this notrace removes unnecessary recursion calls.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 notrace bool rcu_is_watching(void)
 {
 	bool ret;
diff --git kernel/reboot.c kernel/reboot.c
index 6ebef11c887..ee98c8e1075 100644
--- kernel/reboot.c
+++ kernel/reboot.c
@@ -807,8 +807,14 @@ void ctrl_alt_del(void)
 }
 
 #define POWEROFF_CMD_PATH_LEN  256
+
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+static const char poweroff_cmd[] __section(".rodata") = "/sbin/poweroff";
+static const char reboot_cmd[] __section(".rodata") = "/sbin/reboot";
+#else
 static char poweroff_cmd[POWEROFF_CMD_PATH_LEN] = "/sbin/poweroff";
 static const char reboot_cmd[] = "/sbin/reboot";
+#endif
 
 static int run_cmd(const char *cmd)
 {
@@ -1261,9 +1267,15 @@ static struct attribute *reboot_attrs[] = {
 static struct ctl_table kern_reboot_table[] = {
 	{
 		.procname       = "poweroff_cmd",
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.data           = (char *)&poweroff_cmd,
+		.maxlen         = POWEROFF_CMD_PATH_LEN,
+		.mode           = 0444,
+#else
 		.data           = &poweroff_cmd,
 		.maxlen         = POWEROFF_CMD_PATH_LEN,
 		.mode           = 0644,
+#endif
 		.proc_handler   = proc_dostring,
 	},
 	{
diff --git kernel/sched/core.c kernel/sched/core.c
index d71234729ed..af9f4e1cc30 100644
--- kernel/sched/core.c
+++ kernel/sched/core.c
@@ -91,6 +91,10 @@
 #include "smp.h"
 #include "stats.h"
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+#include <bhv/integrity.h>
+
 #include "../workqueue_internal.h"
 #include "../../io_uring/io-wq.h"
 #include "../smpboot.h"
@@ -5217,6 +5221,8 @@ context_switch(struct rq *rq, struct task_struct *prev,
 			mmgrab(prev->active_mm);
 		else
 			prev->active_mm = NULL;
+
+		bhv_pt_protect_check_pgd(next->active_mm);
 	} else {                                        // to user
 		membarrier_switch_mm(rq, prev->active_mm, next->mm);
 		/*
diff --git kernel/static_call_inline.c kernel/static_call_inline.c
index dc5665b6281..88696723a69 100644
--- kernel/static_call_inline.c
+++ kernel/static_call_inline.c
@@ -10,59 +10,66 @@
 #include <linux/processor.h>
 #include <asm/sections.h>
 
+#include <bhv/vault.h>
+
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_lock, struct mutex *, m);
+BHV_VAULT_FN_WRAPPER1_NORET(void, mutex_unlock, struct mutex *, m);
+
+BHV_VAULT_FN_WRAPPER0(void, cpus_read_lock)
+BHV_VAULT_FN_WRAPPER0(void, cpus_read_unlock)
+
 extern struct static_call_site __start_static_call_sites[],
 			       __stop_static_call_sites[];
 extern struct static_call_tramp_key __start_static_call_tramp_key[],
 				    __stop_static_call_tramp_key[];
 
+/* XXX: CAN WE MOVE THIS INTO RO AFTER INIT? */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static bool static_call_initialized;
 
 /* mutex to protect key modules/sites */
 static DEFINE_MUTEX(static_call_mutex);
 
-static void static_call_lock(void)
-{
-	mutex_lock(&static_call_mutex);
-}
-
-static void static_call_unlock(void)
-{
-	mutex_unlock(&static_call_mutex);
-}
-
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void *static_call_addr(struct static_call_site *site)
 {
 	return (void *)((long)site->addr + (long)&site->addr);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline unsigned long __static_call_key(const struct static_call_site *site)
 {
 	return (long)site->key + (long)&site->key;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_key *static_call_key(const struct static_call_site *site)
 {
 	return (void *)(__static_call_key(site) & ~STATIC_CALL_SITE_FLAGS);
 }
 
 /* These assume the key is word-aligned. */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_is_init(struct static_call_site *site)
 {
 	return __static_call_key(site) & STATIC_CALL_SITE_INIT;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_is_tail(struct static_call_site *site)
 {
 	return __static_call_key(site) & STATIC_CALL_SITE_TAIL;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void static_call_set_init(struct static_call_site *site)
 {
 	site->key = (__static_call_key(site) | STATIC_CALL_SITE_INIT) -
 		    (long)&site->key;
 }
 
-static int static_call_site_cmp(const void *_a, const void *_b)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline int static_call_site_cmp(const void *_a, const void *_b)
 {
 	const struct static_call_site *a = _a;
 	const struct static_call_site *b = _b;
@@ -77,8 +84,10 @@ static int static_call_site_cmp(const void *_a, const void *_b)
 
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_site_cmp);
 
-static void static_call_site_swap(void *_a, void *_b, int size)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static noinline void static_call_site_swap(void *_a, void *_b, int size)
 {
 	long delta = (unsigned long)_a - (unsigned long)_b;
 	struct static_call_site *a = _a;
@@ -91,19 +100,22 @@ static void static_call_site_swap(void *_a, void *_b, int size)
 	b->addr = tmp.addr + delta;
 	b->key  = tmp.key  + delta;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_site_swap);
 
-static inline void static_call_sort_entries(struct static_call_site *start,
-					    struct static_call_site *stop)
+static noinline void static_call_sort_entries(struct static_call_site *start,
+					    	                   struct static_call_site *stop)
 {
 	sort(start, stop - start, sizeof(struct static_call_site),
 	     static_call_site_cmp, static_call_site_swap);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline bool static_call_key_has_mods(struct static_call_key *key)
 {
 	return !(key->type & 1);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_mod *static_call_key_next(struct static_call_key *key)
 {
 	if (!static_call_key_has_mods(key))
@@ -112,6 +124,7 @@ static inline struct static_call_mod *static_call_key_next(struct static_call_ke
 	return key->mods;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline struct static_call_site *static_call_key_sites(struct static_call_key *key)
 {
 	if (static_call_key_has_mods(key))
@@ -120,13 +133,19 @@ static inline struct static_call_site *static_call_key_sites(struct static_call_
 	return (struct static_call_site *)(key->type & ~1);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 {
 	struct static_call_site *site, *stop;
 	struct static_call_mod *site_mod, first;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 
 	if (key->func == func)
 		goto done;
@@ -201,11 +220,18 @@ void __static_call_update(struct static_call_key *key, void *tramp, void *func)
 	}
 
 done:
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, __static_call_update);
 EXPORT_SYMBOL_GPL(__static_call_update);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_init(struct module *mod,
 			      struct static_call_site *start,
 			      struct static_call_site *stop)
@@ -279,6 +305,7 @@ static int __static_call_init(struct module *mod,
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int addr_conflict(struct static_call_site *site, void *start, void *end)
 {
 	unsigned long addr = (unsigned long)static_call_addr(site);
@@ -290,6 +317,7 @@ static int addr_conflict(struct static_call_site *site, void *start, void *end)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_text_reserved(struct static_call_site *iter_start,
 				       struct static_call_site *iter_stop,
 				       void *start, void *end, bool init)
@@ -309,6 +337,7 @@ static int __static_call_text_reserved(struct static_call_site *iter_start,
 
 #ifdef CONFIG_MODULES
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int __static_call_mod_text_reserved(void *start, void *end)
 {
 	struct module *mod;
@@ -333,6 +362,7 @@ static int __static_call_mod_text_reserved(void *start, void *end)
 	return ret;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static unsigned long tramp_key_lookup(unsigned long addr)
 {
 	struct static_call_tramp_key *start = __start_static_call_tramp_key;
@@ -350,6 +380,7 @@ static unsigned long tramp_key_lookup(unsigned long addr)
 	return 0;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int static_call_add_module(struct module *mod)
 {
 	struct static_call_site *start = mod->static_call_sites;
@@ -389,6 +420,7 @@ static int static_call_add_module(struct module *mod)
 	return __static_call_init(mod, start, stop);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void static_call_del_module(struct module *mod)
 {
 	struct static_call_site *start = mod->static_call_sites;
@@ -418,14 +450,20 @@ static void static_call_del_module(struct module *mod)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int static_call_module_notify(struct notifier_block *nb,
 				     unsigned long val, void *data)
 {
 	struct module *mod = data;
 	int ret = 0;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -440,11 +478,17 @@ static int static_call_module_notify(struct notifier_block *nb,
 		break;
 	}
 
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_module_notify);
 
 static struct notifier_block static_call_module_nb = {
 	.notifier_call = static_call_module_notify,
@@ -452,6 +496,7 @@ static struct notifier_block static_call_module_nb = {
 
 #else
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline int __static_call_mod_text_reserved(void *start, void *end)
 {
 	return 0;
@@ -459,6 +504,7 @@ static inline int __static_call_mod_text_reserved(void *start, void *end)
 
 #endif /* CONFIG_MODULES */
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 int static_call_text_reserved(void *start, void *end)
 {
 	bool init = system_state < SYSTEM_RUNNING;
@@ -470,20 +516,32 @@ int static_call_text_reserved(void *start, void *end)
 
 	return __static_call_mod_text_reserved(start, end);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, static_call_text_reserved);
 
-int __init static_call_init(void)
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+int _static_call_init(void)
 {
 	int ret;
 
 	if (static_call_initialized)
-		return 0;
+		return -EEXIST;
 
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_cpus_read_lock();
+	bhv_wrapper_mutex_lock(&static_call_mutex);
+#else
 	cpus_read_lock();
-	static_call_lock();
+	mutex_lock(&static_call_mutex);
+#endif
 	ret = __static_call_init(NULL, __start_static_call_sites,
 				 __stop_static_call_sites);
-	static_call_unlock();
+#ifdef CONFIG_BHV_VAULT_SPACES
+	bhv_wrapper_mutex_unlock(&static_call_mutex);
+	bhv_wrapper_cpus_read_unlock();
+#else
+	mutex_unlock(&static_call_mutex);
 	cpus_read_unlock();
+#endif
 
 	if (ret) {
 		pr_err("Failed to allocate memory for static_call!\n");
@@ -492,6 +550,16 @@ int __init static_call_init(void)
 
 	static_call_initialized = true;
 
+	return ret;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, _static_call_init);
+
+int __init static_call_init(void)
+{
+	int ret = _static_call_init();
+	if (ret)
+		return (ret == -EEXIST) ? 0 : ret;
+
 #ifdef CONFIG_MODULES
 	register_module_notifier(&static_call_module_nb);
 #endif
diff --git kernel/sysctl.c kernel/sysctl.c
index c6d9dec11b7..079a02141a4 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -1768,9 +1768,15 @@ static struct ctl_table kern_table[] = {
 #ifdef CONFIG_MODULES
 	{
 		.procname	= "modprobe",
+#ifdef CONFIG_BHV_CONST_CALL_USERMODEHELPER_KERNEL
+		.data		= (char *)&modprobe_path,
+		.maxlen		= KMOD_PATH_LEN,
+		.mode		= 0444,
+#else
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring,
 	},
 	{
diff --git kernel/trace/Kconfig kernel/trace/Kconfig
index 93d72499628..bad400571b5 100644
--- kernel/trace/Kconfig
+++ kernel/trace/Kconfig
@@ -170,6 +170,7 @@ menuconfig FTRACE
 	bool "Tracers"
 	depends on TRACING_SUPPORT
 	default y if DEBUG_KERNEL
+	depends on !BHV_LOCKDOWN
 	help
 	  Enable the kernel tracing infrastructure.
 
@@ -487,6 +488,7 @@ config MMIOTRACE
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
+	depends on !BHV_LOCKDOWN
 	select TRACING
 	help
 	  This tracer hooks to various trace points in the kernel,
diff --git kernel/trace/ftrace.c kernel/trace/ftrace.c
index e9ce45dce31..191c53e1173 100644
--- kernel/trace/ftrace.c
+++ kernel/trace/ftrace.c
@@ -36,6 +36,8 @@
 #include <linux/rcupdate.h>
 #include <linux/kprobes.h>
 
+#include <bhv/vault.h>
+
 #include <trace/events/sched.h>
 
 #include <asm/sections.h>
@@ -1042,6 +1044,7 @@ struct ftrace_ops global_ops = {
 /*
  * Used by the stack unwinder to know about dynamic ftrace trampolines.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
 {
 	struct ftrace_ops *op = NULL;
@@ -1076,6 +1079,7 @@ struct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)
  * not return true for either core_kernel_text() or
  * is_module_text_address().
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_ftrace_trampoline(unsigned long addr)
 {
 	return ftrace_ops_trampoline(addr) != NULL;
diff --git kernel/tracepoint.c kernel/tracepoint.c
index f23144af574..70f8b969c0f 100644
--- kernel/tracepoint.c
+++ kernel/tracepoint.c
@@ -15,6 +15,8 @@
 #include <linux/sched/task.h>
 #include <linux/static_key.h>
 
+#include <bhv/vault.h>
+
 enum tp_func_state {
 	TP_FUNC_0,
 	TP_FUNC_1,
@@ -42,8 +44,10 @@ struct tp_transition_snapshot {
 };
 
 /* Protected by tracepoints_mutex */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static struct tp_transition_snapshot tp_transition_snapshot[_NR_TP_TRANSITION_SYNC];
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_get_state(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -54,6 +58,7 @@ static void tp_rcu_get_state(enum tp_transition_sync sync)
 	snapshot->ongoing = true;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 {
 	struct tp_transition_snapshot *snapshot = &tp_transition_snapshot[sync];
@@ -67,6 +72,7 @@ static void tp_rcu_cond_sync(enum tp_transition_sync sync)
 }
 
 /* Set to 1 to enable tracepoint debug output */
+BHV_VAULT_ADD_TO_DATA_REGION(jump_label)
 static const int tracepoint_debug;
 
 #ifdef CONFIG_MODULES
@@ -104,6 +110,7 @@ static void tp_stub_func(void)
 	return;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void *allocate_probes(int count)
 {
 	struct tp_probes *p  = kmalloc(struct_size(p, probes, count),
@@ -139,6 +146,7 @@ static __init int release_early_probes(void)
 /* SRCU is initialized at core_initcall */
 postcore_initcall(release_early_probes);
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static inline void release_probes(struct tracepoint_func *old)
 {
 	if (old) {
@@ -165,6 +173,7 @@ static inline void release_probes(struct tracepoint_func *old)
 	}
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void debug_print_probes(struct tracepoint_func *funcs)
 {
 	int i;
@@ -176,6 +185,7 @@ static void debug_print_probes(struct tracepoint_func *funcs)
 		printk(KERN_DEBUG "Probe %d : %p\n", i, funcs[i].func);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static struct tracepoint_func *
 func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	 int prio)
@@ -229,6 +239,7 @@ func_add(struct tracepoint_func **funcs, struct tracepoint_func *tp_func,
 	return old;
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static void *func_remove(struct tracepoint_func **funcs,
 		struct tracepoint_func *tp_func)
 {
@@ -294,6 +305,7 @@ static void *func_remove(struct tracepoint_func **funcs,
 /*
  * Count the number of functions (enum tp_func_state) in a tp_funcs array.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static enum tp_func_state nr_func_state(const struct tracepoint_func *tp_funcs)
 {
 	if (!tp_funcs)
@@ -320,6 +332,7 @@ static void tracepoint_update_call(struct tracepoint *tp, struct tracepoint_func
 /*
  * Add the probe function to a tracepoint.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_add_func(struct tracepoint *tp,
 			       struct tracepoint_func *func, int prio,
 			       bool warn)
@@ -386,6 +399,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_add_func);
 
 /*
  * Remove a probe function from a tracepoint.
@@ -393,6 +407,7 @@ static int tracepoint_add_func(struct tracepoint *tp,
  * function insures that the original callback is not used anymore. This insured
  * by preempt_disable around the call site.
  */
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_remove_func(struct tracepoint *tp,
 		struct tracepoint_func *func)
 {
@@ -458,6 +473,7 @@ static int tracepoint_remove_func(struct tracepoint *tp,
 	release_probes(old);
 	return 0;
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_remove_func);
 
 /**
  * tracepoint_probe_register_prio_may_exist -  Connect a probe to a tracepoint with priority
@@ -698,6 +714,7 @@ static void tracepoint_module_going(struct module *mod)
 	mutex_unlock(&tracepoint_module_list_mutex);
 }
 
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
 static int tracepoint_module_notify(struct notifier_block *self,
 		unsigned long val, void *data)
 {
@@ -718,6 +735,7 @@ static int tracepoint_module_notify(struct notifier_block *self,
 	}
 	return notifier_from_errno(ret);
 }
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, tracepoint_module_notify);
 
 static struct notifier_block tracepoint_module_nb = {
 	.notifier_call = tracepoint_module_notify,
diff --git kernel/ucount.c kernel/ucount.c
index ee8e57fd6f9..74c6b3b7306 100644
--- kernel/ucount.c
+++ kernel/ucount.c
@@ -87,6 +87,7 @@ static struct ctl_table user_table[] = {
 	UCOUNT_ENTRY("max_fanotify_groups"),
 	UCOUNT_ENTRY("max_fanotify_marks"),
 #endif
+	UCOUNT_ENTRY("max_mem_namespaces"),
 	{ }
 };
 #endif /* CONFIG_SYSCTL */
diff --git kernel/umh.c kernel/umh.c
index fbf872c624c..4fcc06932de 100644
--- kernel/umh.c
+++ kernel/umh.c
@@ -32,6 +32,10 @@
 
 #include <trace/events/module.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestlog.h>
+#endif
+
 #define CAP_BSET	(void *)1
 #define CAP_PI		(void *)2
 
@@ -110,6 +114,13 @@ static int call_usermodehelper_exec_async(void *data)
 	commit_creds(new);
 
 	wait_for_initramfs();
+
+#ifdef CONFIG_BHV_VAS
+	if (bhv_guestlog_log_kernel_exec_events())
+		bhv_guestlog_log_kernel_exec(sub_info->path, sub_info->argv,
+					     sub_info->envp);
+#endif
+
 	retval = kernel_execve(sub_info->path,
 			       (const char *const *)sub_info->argv,
 			       (const char *const *)sub_info->envp);
diff --git lib/Kconfig.debug lib/Kconfig.debug
index b2dff193589..2ce09b87097 100644
--- lib/Kconfig.debug
+++ lib/Kconfig.debug
@@ -210,6 +210,7 @@ endmenu # "printk and dmesg options"
 
 config DEBUG_KERNEL
 	bool "Kernel debugging"
+        depends on !BHV_LOCKDOWN
 	help
 	  Say Y here if you are developing drivers or trying to debug and
 	  identify kernel problems.
diff --git lib/Kconfig.kgdb lib/Kconfig.kgdb
index 3b9a4400843..40ddc18e912 100644
--- lib/Kconfig.kgdb
+++ lib/Kconfig.kgdb
@@ -28,6 +28,7 @@ config KGDB_HONOUR_BLOCKLIST
 	bool "KGDB: use kprobe blocklist to prohibit unsafe breakpoints"
 	depends on HAVE_KPROBES
 	depends on MODULES
+	depends on !BHV_LOCKDOWN
 	select KPROBES
 	default y
 	help
diff --git lib/sort.c lib/sort.c
index b399bf10d67..7e95999c1b4 100644
--- lib/sort.c
+++ lib/sort.c
@@ -16,6 +16,8 @@
 #include <linux/export.h>
 #include <linux/sort.h>
 
+#include <bhv/vault.h>
+
 /**
  * is_aligned - is this pointer & size okay for word-wide copying?
  * @base: pointer to data
@@ -55,6 +57,7 @@ static bool is_aligned(const void *base, size_t size, unsigned char align)
  * subtract (since the intervening mov instructions don't alter the flags).
  * Gcc 8.1.0 doesn't have that problem.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_32(void *a, void *b, size_t n)
 {
 	do {
@@ -80,6 +83,7 @@ static void swap_words_32(void *a, void *b, size_t n)
  * but it's possible to have 64-bit loads without 64-bit pointers (e.g.
  * x32 ABI).  Are there any cases the kernel needs to worry about?
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_words_64(void *a, void *b, size_t n)
 {
 	do {
@@ -108,6 +112,7 @@ static void swap_words_64(void *a, void *b, size_t n)
  *
  * This is the fallback if alignment doesn't allow using larger chunks.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void swap_bytes(void *a, void *b, size_t n)
 {
 	do {
@@ -136,6 +141,15 @@ struct wrapper {
  * The function pointer is last to make tail calls most efficient if the
  * compiler decides not to inline this function.
  */
+
+/*
+ * XXX: Try to remove this!!
+ */
+#ifdef swap
+#undef swap
+#endif
+
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static void do_swap(void *a, void *b, size_t size, swap_r_func_t swap_func, const void *priv)
 {
 	if (swap_func == SWAP_WRAPPER) {
@@ -155,6 +169,7 @@ static void do_swap(void *a, void *b, size_t size, swap_r_func_t swap_func, cons
 
 #define _CMP_WRAPPER ((cmp_r_func_t)0L)
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 static int do_cmp(const void *a, const void *b, cmp_r_func_t cmp, const void *priv)
 {
 	if (cmp == _CMP_WRAPPER)
@@ -207,6 +222,7 @@ static size_t parent(size_t i, unsigned int lsbit, size_t size)
  * O(n*n) worst-case behavior and extra memory requirements that make
  * it less suitable for kernel use.
  */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort_r(void *base, size_t num, size_t size,
 	    cmp_r_func_t cmp_func,
 	    swap_r_func_t swap_func,
@@ -278,6 +294,7 @@ void sort_r(void *base, size_t num, size_t size,
 }
 EXPORT_SYMBOL(sort_r);
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 void sort(void *base, size_t num, size_t size,
 	  cmp_func_t cmp_func,
 	  swap_func_t swap_func)
diff --git mm/gup.c mm/gup.c
index f4911ddd307..1a28ce120a5 100644
--- mm/gup.c
+++ mm/gup.c
@@ -22,6 +22,8 @@
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
 
+#include <linux/mem_namespace.h>
+
 #include "internal.h"
 
 struct follow_page_context {
@@ -1049,6 +1051,11 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 	int write = (gup_flags & FOLL_WRITE);
 	int foreign = (gup_flags & FOLL_REMOTE);
 
+#ifdef CONFIG_MEM_NS
+	struct mm_struct *mm = vma->vm_mm;
+	struct task_struct *t = mm->owner;
+#endif
+
 	if (vm_flags & (VM_IO | VM_PFNMAP))
 		return -EFAULT;
 
@@ -1062,6 +1069,17 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 		return -EFAULT;
 
 	if (write) {
+#ifdef CONFIG_MEM_NS
+		/*
+		 * Grant write access to remote address spaces only if both
+		 * processes are executing inside of the same mem_namespace.
+		 */
+		if (!current_in_same_mem_ns(t)) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
 		if (!(vm_flags & VM_WRITE)) {
 			if (!(gup_flags & FOLL_FORCE))
 				return -EFAULT;
@@ -1079,17 +1097,42 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 			 */
 			if (!is_cow_mapping(vm_flags))
 				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
 		}
-	} else if (!(vm_flags & VM_READ)) {
-		if (!(gup_flags & FOLL_FORCE))
-			return -EFAULT;
+	} else {
+#ifdef CONFIG_MEM_NS
 		/*
-		 * Is there actually any vma we can reach here which does not
-		 * have VM_MAYREAD set?
+		 * Grant read access to remote address spaces only if both
+		 * processes are part of the same ancestor tree branch the
+		 * target mem_namespace.
 		 */
-		if (!(vm_flags & VM_MAYREAD))
-			return -EFAULT;
+		if (!task_in_ancestor_memns(current, memns_of_task(t))) {
+			if(bhv_domain_report(current, mm, vma, gup_flags))
+				return -EPERM;
+		}
+#endif
+
+		if (!(vm_flags & VM_READ)) {
+			if (!(gup_flags & FOLL_FORCE))
+				return -EFAULT;
+			/*
+			 * Is there actually any vma we can reach here which does not
+			 * have VM_MAYREAD set?
+			 */
+			if (!(vm_flags & VM_MAYREAD))
+				return -EFAULT;
+
+#ifdef CONFIG_MEM_NS
+			if (!bhv_forced_mem_access_permitted(vma, write, foreign))
+				return -EPERM;
+#endif
+		}
 	}
+
 	/*
 	 * gups are always data accesses, not instruction
 	 * fetches, so execute=false here
diff --git mm/khugepaged.c mm/khugepaged.c
index 65bd0b10526..e050784d73f 100644
--- mm/khugepaged.c
+++ mm/khugepaged.c
@@ -541,6 +541,11 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 	int none_or_zero = 0, shared = 0, result = SCAN_FAIL, referenced = 0;
 	bool writable = false;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 	     _pte++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -665,9 +670,16 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 		result = SCAN_SUCCEED;
 		trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 						    referenced, writable, result);
+#ifdef CONFIG_MEM_NS
+		bhv_domain_switch(domain);
+#endif
 		return result;
 	}
 out:
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	release_pte_pages(pte, _pte, compound_pagelist);
 	trace_mm_collapse_huge_page_isolate(page, none_or_zero,
 					    referenced, writable, result);
@@ -682,6 +694,12 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 {
 	struct page *src_page, *tmp;
 	pte_t *_pte;
+
+#ifdef CONFIG_MEM_NS
+	uint64_t domain = bhv_get_active_domain();
+	bhv_domain_enter(vma->vm_mm->owner);
+#endif
+
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 				_pte++, page++, address += PAGE_SIZE) {
 		pte_t pteval = *_pte;
@@ -694,6 +712,8 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 				 * ptl mostly unnecessary.
 				 */
 				spin_lock(ptl);
+				bhv_domain_clear_pte(vma->vm_mm, address, _pte,
+						     *_pte);
 				ptep_clear(vma->vm_mm, address, _pte);
 				spin_unlock(ptl);
 			}
@@ -708,6 +728,7 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 			 * inside page_remove_rmap().
 			 */
 			spin_lock(ptl);
+			bhv_domain_clear_pte(vma->vm_mm, address, _pte, *_pte);
 			ptep_clear(vma->vm_mm, address, _pte);
 			page_remove_rmap(src_page, vma, false);
 			spin_unlock(ptl);
@@ -724,6 +745,10 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 		free_swap_cache(src_page);
 		putback_lru_page(src_page);
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
 }
 
 static void khugepaged_alloc_sleep(void)
diff --git mm/maccess.c mm/maccess.c
index 518a2566732..0729ea137a3 100644
--- mm/maccess.c
+++ mm/maccess.c
@@ -6,6 +6,17 @@
 #include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
+#include <asm-generic/sections.h>
+
+__always_inline static bool is_vault(const void *addr)
+{
+#ifdef CONFIG_BHV_VAS
+	return ((char *)addr >= __bhv_text_start &&
+		(char *)addr < __bhv_text_end);
+#else
+	return false;
+#endif
+}
 
 bool __weak copy_from_kernel_nofault_allowed(const void *unsafe_src,
 		size_t size)
@@ -28,6 +39,10 @@ long copy_from_kernel_nofault(void *dst, const void *src, size_t size)
 	if (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))
 		align = (unsigned long)dst | (unsigned long)src;
 
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(src, size))
 		return -ERANGE;
 
@@ -83,6 +98,10 @@ long strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr, long count)
 
 	if (unlikely(count <= 0))
 		return 0;
+	if (is_vault(src)) {
+		pr_err("[%s] copy from vault (%pS)\n", __FUNCTION__, src);
+		return -ERANGE;
+	}
 	if (!copy_from_kernel_nofault_allowed(unsafe_addr, count))
 		return -ERANGE;
 
diff --git mm/memory.c mm/memory.c
index 301c74c4443..fcf7cf5758a 100644
--- mm/memory.c
+++ mm/memory.c
@@ -80,6 +80,8 @@
 
 #include <trace/events/kmem.h>
 
+#include <bhv/domain.h>
+
 #include <asm/io.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
@@ -1867,7 +1869,7 @@ static int insert_page_into_pte_locked(struct vm_area_struct *vma, pte_t *pte,
 	get_page(page);
 	inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
 	page_add_file_rmap(page, vma, false);
-	set_pte_at(vma->vm_mm, addr, pte, mk_pte(page, prot));
+	bhv_domain_set_pte_at_kernel(vma->vm_mm, addr, pte, mk_pte(page, prot));
 	return 0;
 }
 
@@ -2186,7 +2188,10 @@ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	}
 
-	set_pte_at(mm, addr, pte, entry);
+	if (pfn_t_devmap(pfn))
+		set_pte_at(mm, addr, pte, entry);
+	else
+		bhv_domain_set_pte_at_kernel(mm, addr, pte, entry);
 	update_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */
 
 out_unlock:
@@ -5685,6 +5690,9 @@ int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags & FOLL_WRITE;
+#ifdef CONFIG_MEM_NS
+	uint64_t domain;
+#endif
 
 	if (mmap_read_lock_killable(mm))
 		return 0;
@@ -5697,6 +5705,11 @@ int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,
 			return 0;
 	}
 
+#ifdef CONFIG_MEM_NS
+	domain = bhv_get_active_domain();
+	bhv_domain_enter(mm->owner);
+#endif
+
 	/* ignore errors, just check how much was successfully transferred */
 	while (len) {
 		int bytes, ret, offset;
@@ -5745,6 +5758,11 @@ int __access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,
 		buf += bytes;
 		addr += bytes;
 	}
+
+#ifdef CONFIG_MEM_NS
+	bhv_domain_switch(domain);
+#endif
+
 	mmap_read_unlock(mm);
 
 	return buf - old_buf;
@@ -5898,6 +5916,10 @@ static void clear_gigantic_page(struct page *page,
 	struct page *p;
 
 	might_sleep();
+
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
+
 	for (i = 0; i < pages_per_huge_page; i++) {
 		p = nth_page(page, i);
 		cond_resched();
@@ -5923,6 +5945,8 @@ void clear_huge_page(struct page *page,
 		return;
 	}
 
+	bhv_domain_map_kernel(current->mm, page_to_pfn(page),
+			      pages_per_huge_page, true, true, false);
 	process_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);
 }
 
diff --git mm/mmap.c mm/mmap.c
index c0f9575493d..090eb70457a 100644
--- mm/mmap.c
+++ mm/mmap.c
@@ -2128,6 +2128,7 @@ int expand_downwards(struct vm_area_struct *vma, unsigned long address)
 			}
 		}
 	}
+
 	anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma(vma, vma->vm_flags);
 	mas_destroy(&mas);
diff --git mm/page_owner.c mm/page_owner.c
index 2d27f532df4..3156b831e87 100644
--- mm/page_owner.c
+++ mm/page_owner.c
@@ -706,7 +706,7 @@ static void init_early_allocated_pages(void)
 		init_zones_in_node(pgdat);
 }
 
-static const struct file_operations proc_page_owner_operations = {
+const struct file_operations proc_page_owner_operations __section(".rodata") = {
 	.read		= read_page_owner,
 	.llseek		= lseek_page_owner,
 };
diff --git mm/page_poison.c mm/page_poison.c
index 98438985e1e..70d1a3e59cd 100644
--- mm/page_poison.c
+++ mm/page_poison.c
@@ -35,6 +35,10 @@ void __kernel_poison_pages(struct page *page, int n)
 {
 	int i;
 
+	bhv_domain_map_kernel(current->mm != NULL ? current->mm :
+						    current->active_mm,
+			      page_to_pfn(page), n, true, true, false);
+
 	for (i = 0; i < n; i++)
 		poison_page(page + i);
 }
diff --git mm/pgtable-generic.c mm/pgtable-generic.c
index 6a582cc0702..b14dcbba28b 100644
--- mm/pgtable-generic.c
+++ mm/pgtable-generic.c
@@ -13,6 +13,10 @@
 #include <linux/mm_inline.h>
 #include <asm/tlb.h>
 
+#ifdef CONFIG_MEM_NS
+#include <bhv/domain.h>
+#endif
+
 /*
  * If a p?d_bad entry is found while walking page tables, report
  * the error, before resetting entry to p?d_none.  Usually (but
@@ -196,7 +200,11 @@ pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 		     pmd_t *pmdp)
 {
 	VM_WARN_ON_ONCE(!pmd_present(*pmdp));
-	pmd_t old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
+	pmd_t old;
+#ifdef CONFIG_MEM_NS
+	bhv_domain_clear_pmd(vma->vm_mm, address, pmdp, pmd_mkinvalid(*pmdp));
+#endif
+	old = pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return old;
 }
diff --git mm/shmem.c mm/shmem.c
index f7c08e169e4..ba8d847d3a6 100644
--- mm/shmem.c
+++ mm/shmem.c
@@ -232,7 +232,7 @@ static inline void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 
 static const struct super_operations shmem_ops;
 const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -3956,7 +3956,7 @@ const struct address_space_operations shmem_aops = {
 };
 EXPORT_SYMBOL(shmem_aops);
 
-static const struct file_operations shmem_file_operations = {
+const struct file_operations shmem_file_operations __section(".rodata") = {
 	.mmap		= shmem_mmap,
 	.get_unmapped_area = shmem_get_unmapped_area,
 #ifdef CONFIG_TMPFS
diff --git mm/vmalloc.c mm/vmalloc.c
index 67a10a04df0..55b14469e73 100644
--- mm/vmalloc.c
+++ mm/vmalloc.c
@@ -43,6 +43,8 @@
 #include <asm/tlbflush.h>
 #include <asm/shmparam.h>
 
+#include <bhv/vault.h>
+
 #include "internal.h"
 #include "pgalloc-track.h"
 
@@ -72,6 +74,7 @@ early_param("nohugevmalloc", set_nohugevmalloc);
 static const bool vmap_allow_huge = false;
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMALLOC */
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 bool is_vmalloc_addr(const void *x)
 {
 	unsigned long addr = (unsigned long)kasan_reset_tag(x);
@@ -643,6 +646,7 @@ static int vmap_pages_range(unsigned long addr, unsigned long end,
 	return err;
 }
 
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
 int is_vmalloc_or_module_addr(const void *x)
 {
 	/*
diff --git net/socket.c net/socket.c
index 639d76f2038..f99934bea27 100644
--- net/socket.c
+++ net/socket.c
@@ -149,7 +149,7 @@ static void sock_show_fdinfo(struct seq_file *m, struct file *f)
  *	in the operation structures but are done directly via the socketcall() multiplexor.
  */
 
-static const struct file_operations socket_file_ops = {
+const struct file_operations socket_file_ops = {
 	.owner =	THIS_MODULE,
 	.llseek =	no_llseek,
 	.read_iter =	sock_read_iter,
diff --git scripts/Makefile.vmlinux_o scripts/Makefile.vmlinux_o
index 0edfdb40364..09df2c35e29 100644
--- scripts/Makefile.vmlinux_o
+++ scripts/Makefile.vmlinux_o
@@ -33,11 +33,12 @@ endif
 # For LTO and IBT, objtool doesn't run on individual translation units.
 # Run everything on vmlinux instead.
 
-objtool-enabled := $(or $(delay-objtool),$(CONFIG_NOINSTR_VALIDATION))
+objtool-enabled := $(or $(delay-objtool),$(CONFIG_NOINSTR_VALIDATION),$(CONFIG_BHV_VAULT_SPACES))
 
 vmlinux-objtool-args-$(delay-objtool)			+= $(objtool-args-y)
 vmlinux-objtool-args-$(CONFIG_GCOV_KERNEL)		+= --no-unreachable
 vmlinux-objtool-args-$(CONFIG_NOINSTR_VALIDATION)	+= --noinstr $(if $(CONFIG_CPU_UNRET_ENTRY), --unret)
+vmlinux-objtool-args-$(CONFIG_BHV_VAULT_SPACES)	+= --vault
 
 objtool-args = $(vmlinux-objtool-args-y) --link
 
diff --git scripts/link-vmlinux.sh scripts/link-vmlinux.sh
index 458b2948b58..4f17b052edd 100755
--- scripts/link-vmlinux.sh
+++ scripts/link-vmlinux.sh
@@ -60,7 +60,7 @@ vmlinux_link()
 	# skip output file argument
 	shift
 
-	if is_enabled CONFIG_LTO_CLANG || is_enabled CONFIG_X86_KERNEL_IBT; then
+	if is_enabled CONFIG_LTO_CLANG || is_enabled CONFIG_X86_KERNEL_IBT || is_enabled CONFIG_BHV_VAULT_SPACES; then
 		# Use vmlinux.o instead of performing the slow LTO link again.
 		objs=vmlinux.o
 		libs=
diff --git security/Kconfig security/Kconfig
index e6db09a779b..4c281aae3cb 100644
--- security/Kconfig
+++ security/Kconfig
@@ -198,6 +198,15 @@ config STATIC_USERMODEHELPER_PATH
 	  If you wish for all usermode helper programs to be disabled,
 	  specify an empty string here (i.e. "").
 
+config MEM_NS
+	bool "Enable memory namespaces"
+	depends on MEMCG
+	depends on BHV_VAS
+	default y
+	help
+	  Enable memory namespaces.
+
+source "security/bhv/Kconfig"
 source "security/selinux/Kconfig"
 source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
@@ -246,11 +255,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf" if DEFAULT_SECURITY_SMACK
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list will be ignored. This can be
diff --git security/Makefile security/Makefile
index 18121f8f85c..2413ddcf8f1 100644
--- security/Makefile
+++ security/Makefile
@@ -19,6 +19,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_BHV_VAS)			+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 00000000000..a08c3e53a0d
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,84 @@
+config BHV_VAS
+	bool "BHV guest support and VAS LSM"
+	default y
+	depends on (X86_64) || (ARM64 && OF)
+	select VIRTIO_VSOCKETS
+	select VSOCKETS
+	select VIRTIO_VSOCKETS_COMMON
+	select EXT4_FS
+	select XFS_FS
+	select BLOCK
+	help
+	  Say Y if you want to enable the BHV LSM and run Linux in a Virtual
+	  Machine on BHV and benefit from Virtualization-assisted Security.
+
+config BHV_PANIC_ON_FAIL
+	bool "BHV guest panics on Hypercall failure"
+	default y
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	bool "Build BHV guest support with DEBUG information"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_TRACEPOINTS
+	bool "Enable BHV Tracepoints"
+	default n
+	depends on BHV_VAS_DEBUG && TRACEPOINTS
+	help
+	  Say Y if you want to enable BHV tracepoints. Note: do not enable this option in production systems.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	default n
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_CALL_USERMODEHELPER_KERNEL
+	bool "Make all paths passed to call_usermodehelper in the kernel constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in the kernel are constant. This implies
+	  that these paths cannot be updated via sysctl. Paths include
+	  the modprobe path and the poweroff path. This setting is recommended,
+	  since these strings are often updated in exploits.
+
+config BHV_CONST_CALL_USERMODEHELPER_MODULES
+	bool "Make all paths passed to call_usermodehelper in modules constant"
+	default y
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to ensure that all paths that are passed to
+	  call_usermodehelper in drivers constant. This implies that
+	  these paths cannot be updated via sysctl. This setting is recommended,
+	  to harden modules against exploits.
+
+config BHV_LOCKDOWN
+	bool "Enable the most secure BHV settings (Lockdown)"
+	default n
+	depends on BHV_VAS
+	select BHV_PANIC_ON_FAIL
+	select BHV_CONST_MODPROBE_PATH
+	help
+	  Say Y if you want to enable the most secure BHV settings
+
+config BHV_VAULT_SPACES
+    bool "Enable the spaces-based BHV vault to guard code patching"
+    default y
+    depends on BHV_VAS
+    depends on X86_64
+    help
+	  Say Y if you want to enable the spaces-based BHV vault
+
+
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 00000000000..99416cc62aa
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,44 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BlueRock Security Inc.
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+#          Tommaso Frassetto <tommaso@bedrocksystems.com>
+
+ccflags-y+=-DKERNEL_COMMIT_HASH=\"37a6cce40b10f77c5103ff7bf3bc58daee6e458c+63f4322ef83f3a7b0c9c65d5827a53bdee656e92\"
+
+obj-$(CONFIG_BHV_VAS)		:= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= abi_autogen.o
+obj-$(CONFIG_BHV_VAS)		+= init/init.o
+obj-$(CONFIG_BHV_VAS)		+= init/start.o
+obj-$(CONFIG_BHV_VAS)		+= init/mm_init.o
+obj-$(CONFIG_BHV_VAS)		+= init/late_start.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_bpf.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn.o
+obj-$(CONFIG_BHV_VAS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
+obj-$(CONFIG_BHV_VAS)		+= reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_fops.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_integrity_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_version.o
+obj-$(CONFIG_BHV_VAS)		+= vmalloc_to_page.o
+obj-$(CONFIG_BHV_VAS)		+= domain.o
+obj-$(CONFIG_BHV_VAS)		+= container_integrity.o
+obj-$(CONFIG_BHV_VAS)		+= lsm.o
+obj-$(CONFIG_BHV_VAS)		+= memory_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= inode.o
+ifeq ($(CONFIG_KEYS),y)
+obj-$(CONFIG_BHV_VAS)		+= keyring.o
+endif
diff --git security/bhv/abi_autogen.c security/bhv/abi_autogen.c
new file mode 100644
index 00000000000..fc9cffd8a01
--- /dev/null
+++ security/bhv/abi_autogen.c
@@ -0,0 +1,396 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security, Inc.
+ *
+ * This file was auto-generated by gen_abi_c.py (2024-08-23T11:38:28).
+ */
+
+#include <linux/slab.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+
+void HypABI__Init__Init__BHVData__BHVConfigBitmap__dump(const volatile HypABI__Init__Init__BHVData__BHVConfigBitmap__T *addr)
+{
+        pr_info("HypABI__Init__Init__BHVData__BHVConfigBitmap: %s%s%s%s%s%s%s%s%s%s%s%s%s%s",
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY(addr) ? "KERNEL_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_PROC_ACL(addr) ? "PROC_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_DRIVER_ACL(addr) ? "DRIVER_ACL " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_LOGGING(addr) ? "LOGGING " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_CREDS_INTEGRITY(addr) ? "CREDS_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_FILE_PROTECTION(addr) ? "FILE_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_GUEST_POLICY(addr) ? "GUEST_POLICY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_REGISTER_PROTECTION(addr) ? "REGISTER_PROTECTION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_STRONG_ISOLATION(addr) ? "STRONG_ISOLATION " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KERNEL_INTEGRITY_PT_PROT(addr) ? "KERNEL_INTEGRITY_PT_PROT " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_INODE_INTEGRITY(addr) ? "INODE_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_KEYRING_INTEGRITY(addr) ? "KEYRING_INTEGRITY " : "", 
+                HypABI__Init__Init__BHVData__BHVConfigBitmap__has_VAULT(addr) ? "VAULT " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__MemFlags__dump(const volatile HypABI__Integrity__MemFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__MemFlags: %s%s%s",
+                HypABI__Integrity__MemFlags__has_TRANSIENT(addr) ? "TRANSIENT " : "", 
+                HypABI__Integrity__MemFlags__has_MUTABLE(addr) ? "MUTABLE " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Integrity__Freeze__FreezeFlags__dump(const volatile HypABI__Integrity__Freeze__FreezeFlags__T *addr)
+{
+        pr_info("HypABI__Integrity__Freeze__FreezeFlags: %s%s%s%s%s",
+                HypABI__Integrity__Freeze__FreezeFlags__has_CREATE(addr) ? "CREATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_UPDATE(addr) ? "UPDATE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_REMOVE(addr) ? "REMOVE " : "", 
+                HypABI__Integrity__Freeze__FreezeFlags__has_PATCH(addr) ? "PATCH " : "", 
+                ""
+        );
+}
+
+
+void HypABI__Guestlog__Init__GuestlogFlags__dump(const volatile HypABI__Guestlog__Init__GuestlogFlags__T *addr)
+{
+        pr_info("HypABI__Guestlog__Init__GuestlogFlags: %s%s%s%s%s%s%s",
+                HypABI__Guestlog__Init__GuestlogFlags__has_PROCESS_EVENTS(addr) ? "PROCESS_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_DRIVER_EVENTS(addr) ? "DRIVER_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_ACCESS(addr) ? "KERNEL_ACCESS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_UNKNOWN_FILEOPS(addr) ? "UNKNOWN_FILEOPS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_KERNEL_EXEC_EVENTS(addr) ? "KERNEL_EXEC_EVENTS " : "", 
+                HypABI__Guestlog__Init__GuestlogFlags__has_CONTAINER_EVENTS(addr) ? "CONTAINER_EVENTS " : "", 
+                ""
+        );
+}
+
+
+void HypABI__FileProtection__Init__Config__dump(const volatile HypABI__FileProtection__Init__Config__T *addr)
+{
+        pr_info("HypABI__FileProtection__Init__Config: %s%s%s%s",
+                HypABI__FileProtection__Init__Config__has_READ_ONLY(addr) ? "READ_ONLY " : "", 
+                HypABI__FileProtection__Init__Config__has_FILE_OPS(addr) ? "FILE_OPS " : "", 
+                HypABI__FileProtection__Init__Config__has_DIRTY_CRED(addr) ? "DIRTY_CRED " : "", 
+                ""
+        );
+}
+
+
+void HypABI__ContainerIntegrity__Init__Config__dump(const volatile HypABI__ContainerIntegrity__Init__Config__T *addr)
+{
+        pr_info("HypABI__ContainerIntegrity__Init__Config: %s%s%s%s%s",
+                HypABI__ContainerIntegrity__Init__Config__has_NEW_FILE_EXECUTION(addr) ? "NEW_FILE_EXECUTION " : "", 
+                HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_NEW_FILE(addr) ? "INTERPRETER_ARG_NEW_FILE " : "", 
+                HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_CMD(addr) ? "INTERPRETER_ARG_CMD " : "", 
+                HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_BINDS(addr) ? "INTERPRETER_BINDS " : "", 
+                ""
+        );
+}
+
+struct kmem_cache *HypABI__Integrity__Create__Mem_Region__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Remove__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Integrity__PtpgInit__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__Patch__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchNoClose__arg__slab = NULL;
+struct kmem_cache *HypABI__Patch__PatchViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__ProcessViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Acl__DriverViolation__arg__slab = NULL;
+struct kmem_cache *HypABI__Guestlog__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Configure__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__RegisterInitTask__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Assign__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__AssignPriv__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Commit__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Verification__arg__slab = NULL;
+struct kmem_cache *HypABI__Creds__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationFileOps__arg__slab = NULL;
+struct kmem_cache *HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = NULL;
+struct kmem_cache *HypABI__RegisterProtection__Freeze__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Update__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Release__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Inode__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Register__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Verify__arg__slab = NULL;
+struct kmem_cache *HypABI__Keyring__Log__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = NULL;
+struct kmem_cache *HypABI__Confserver__StrictFileops__arg__slab = NULL;
+struct kmem_cache *HypABI__ContainerIntegrity__Init__arg__slab = NULL;
+struct kmem_cache *HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab = NULL;
+struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab = NULL;
+struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab = NULL;
+struct kmem_cache *HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Create__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Extend__arg__slab = NULL;
+struct kmem_cache *HypABI__Wagner__Delete__arg__slab = NULL;
+
+void HypABI__init_slabs(void)
+{
+        HypABI__Integrity__Create__Mem_Region__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__Mem_Region__slab",
+                sizeof(HypABI__Integrity__Create__Mem_Region__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__Mem_Region__slab)
+                panic("Could not create slab HypABI__Integrity__Create__Mem_Region__slab!\n");
+        HypABI__Integrity__Create__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Create__arg__slab",
+                sizeof(HypABI__Integrity__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Create__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Create__arg__slab!\n");
+        HypABI__Integrity__Update__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Update__arg__slab",
+                sizeof(HypABI__Integrity__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Update__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Update__arg__slab!\n");
+        HypABI__Integrity__Remove__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Remove__arg__slab",
+                sizeof(HypABI__Integrity__Remove__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Remove__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Remove__arg__slab!\n");
+        HypABI__Integrity__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__Freeze__arg__slab",
+                sizeof(HypABI__Integrity__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__Freeze__arg__slab)
+                panic("Could not create slab HypABI__Integrity__Freeze__arg__slab!\n");
+        HypABI__Integrity__PtpgInit__arg__slab = kmem_cache_create(
+                "HypABI__Integrity__PtpgInit__arg__slab",
+                sizeof(HypABI__Integrity__PtpgInit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Integrity__PtpgInit__arg__slab)
+                panic("Could not create slab HypABI__Integrity__PtpgInit__arg__slab!\n");
+        HypABI__Patch__Patch__arg__slab = kmem_cache_create(
+                "HypABI__Patch__Patch__arg__slab",
+                sizeof(HypABI__Patch__Patch__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__Patch__arg__slab)
+                panic("Could not create slab HypABI__Patch__Patch__arg__slab!\n");
+        HypABI__Patch__PatchNoClose__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchNoClose__arg__slab",
+                sizeof(HypABI__Patch__PatchNoClose__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchNoClose__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchNoClose__arg__slab!\n");
+        HypABI__Patch__PatchViolation__arg__slab = kmem_cache_create(
+                "HypABI__Patch__PatchViolation__arg__slab",
+                sizeof(HypABI__Patch__PatchViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Patch__PatchViolation__arg__slab)
+                panic("Could not create slab HypABI__Patch__PatchViolation__arg__slab!\n");
+        HypABI__Acl__ProcessViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__ProcessViolation__arg__slab",
+                sizeof(HypABI__Acl__ProcessViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__ProcessViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__ProcessViolation__arg__slab!\n");
+        HypABI__Acl__DriverViolation__arg__slab = kmem_cache_create(
+                "HypABI__Acl__DriverViolation__arg__slab",
+                sizeof(HypABI__Acl__DriverViolation__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Acl__DriverViolation__arg__slab)
+                panic("Could not create slab HypABI__Acl__DriverViolation__arg__slab!\n");
+        HypABI__Guestlog__Init__arg__slab = kmem_cache_create(
+                "HypABI__Guestlog__Init__arg__slab",
+                sizeof(HypABI__Guestlog__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Guestlog__Init__arg__slab)
+                panic("Could not create slab HypABI__Guestlog__Init__arg__slab!\n");
+        HypABI__Creds__Configure__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Configure__arg__slab",
+                sizeof(HypABI__Creds__Configure__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Configure__arg__slab)
+                panic("Could not create slab HypABI__Creds__Configure__arg__slab!\n");
+        HypABI__Creds__RegisterInitTask__arg__slab = kmem_cache_create(
+                "HypABI__Creds__RegisterInitTask__arg__slab",
+                sizeof(HypABI__Creds__RegisterInitTask__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__RegisterInitTask__arg__slab)
+                panic("Could not create slab HypABI__Creds__RegisterInitTask__arg__slab!\n");
+        HypABI__Creds__Assign__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Assign__arg__slab",
+                sizeof(HypABI__Creds__Assign__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Assign__arg__slab)
+                panic("Could not create slab HypABI__Creds__Assign__arg__slab!\n");
+        HypABI__Creds__AssignPriv__arg__slab = kmem_cache_create(
+                "HypABI__Creds__AssignPriv__arg__slab",
+                sizeof(HypABI__Creds__AssignPriv__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__AssignPriv__arg__slab)
+                panic("Could not create slab HypABI__Creds__AssignPriv__arg__slab!\n");
+        HypABI__Creds__Commit__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Commit__arg__slab",
+                sizeof(HypABI__Creds__Commit__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Commit__arg__slab)
+                panic("Could not create slab HypABI__Creds__Commit__arg__slab!\n");
+        HypABI__Creds__Release__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Release__arg__slab",
+                sizeof(HypABI__Creds__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Release__arg__slab)
+                panic("Could not create slab HypABI__Creds__Release__arg__slab!\n");
+        HypABI__Creds__Verification__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Verification__arg__slab",
+                sizeof(HypABI__Creds__Verification__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Verification__arg__slab)
+                panic("Could not create slab HypABI__Creds__Verification__arg__slab!\n");
+        HypABI__Creds__Log__arg__slab = kmem_cache_create(
+                "HypABI__Creds__Log__arg__slab",
+                sizeof(HypABI__Creds__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Creds__Log__arg__slab)
+                panic("Could not create slab HypABI__Creds__Log__arg__slab!\n");
+        HypABI__FileProtection__Init__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__Init__arg__slab",
+                sizeof(HypABI__FileProtection__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__Init__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__Init__arg__slab!\n");
+        HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationWriteReadOnlyFile__arg__slab!\n");
+        HypABI__FileProtection__ViolationFileOps__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationFileOps__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationFileOps__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationFileOps__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationFileOps__arg__slab!\n");
+        HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab = kmem_cache_create(
+                "HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab",
+                sizeof(HypABI__FileProtection__ViolationDirtyCredWrite__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab)
+                panic("Could not create slab HypABI__FileProtection__ViolationDirtyCredWrite__arg__slab!\n");
+        HypABI__RegisterProtection__Freeze__arg__slab = kmem_cache_create(
+                "HypABI__RegisterProtection__Freeze__arg__slab",
+                sizeof(HypABI__RegisterProtection__Freeze__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__RegisterProtection__Freeze__arg__slab)
+                panic("Could not create slab HypABI__RegisterProtection__Freeze__arg__slab!\n");
+        HypABI__Inode__Register__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Register__arg__slab",
+                sizeof(HypABI__Inode__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Register__arg__slab)
+                panic("Could not create slab HypABI__Inode__Register__arg__slab!\n");
+        HypABI__Inode__Update__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Update__arg__slab",
+                sizeof(HypABI__Inode__Update__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Update__arg__slab)
+                panic("Could not create slab HypABI__Inode__Update__arg__slab!\n");
+        HypABI__Inode__Release__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Release__arg__slab",
+                sizeof(HypABI__Inode__Release__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Release__arg__slab)
+                panic("Could not create slab HypABI__Inode__Release__arg__slab!\n");
+        HypABI__Inode__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Verify__arg__slab",
+                sizeof(HypABI__Inode__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Verify__arg__slab)
+                panic("Could not create slab HypABI__Inode__Verify__arg__slab!\n");
+        HypABI__Inode__Log__arg__slab = kmem_cache_create(
+                "HypABI__Inode__Log__arg__slab",
+                sizeof(HypABI__Inode__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Inode__Log__arg__slab)
+                panic("Could not create slab HypABI__Inode__Log__arg__slab!\n");
+        HypABI__Keyring__Register__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Register__arg__slab",
+                sizeof(HypABI__Keyring__Register__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Register__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Register__arg__slab!\n");
+        HypABI__Keyring__Verify__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Verify__arg__slab",
+                sizeof(HypABI__Keyring__Verify__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Verify__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Verify__arg__slab!\n");
+        HypABI__Keyring__Log__arg__slab = kmem_cache_create(
+                "HypABI__Keyring__Log__arg__slab",
+                sizeof(HypABI__Keyring__Log__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Keyring__Log__arg__slab)
+                panic("Could not create slab HypABI__Keyring__Log__arg__slab!\n");
+        HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab",
+                sizeof(HypABI__Confserver__FreezeMemoryAfterBoot__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab)
+                panic("Could not create slab HypABI__Confserver__FreezeMemoryAfterBoot__arg__slab!\n");
+        HypABI__Confserver__StrictFileops__arg__slab = kmem_cache_create(
+                "HypABI__Confserver__StrictFileops__arg__slab",
+                sizeof(HypABI__Confserver__StrictFileops__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Confserver__StrictFileops__arg__slab)
+                panic("Could not create slab HypABI__Confserver__StrictFileops__arg__slab!\n");
+        HypABI__ContainerIntegrity__Init__arg__slab = kmem_cache_create(
+                "HypABI__ContainerIntegrity__Init__arg__slab",
+                sizeof(HypABI__ContainerIntegrity__Init__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__ContainerIntegrity__Init__arg__slab)
+                panic("Could not create slab HypABI__ContainerIntegrity__Init__arg__slab!\n");
+        HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab = kmem_cache_create(
+                "HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab",
+                sizeof(HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab)
+                panic("Could not create slab HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__slab!\n");
+        HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab = kmem_cache_create(
+                "HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab",
+                sizeof(HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab)
+                panic("Could not create slab HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__slab!\n");
+        HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab = kmem_cache_create(
+                "HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab",
+                sizeof(HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab)
+                panic("Could not create slab HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__slab!\n");
+        HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab = kmem_cache_create(
+                "HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab",
+                sizeof(HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab)
+                panic("Could not create slab HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__slab!\n");
+        HypABI__Wagner__Create__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Create__arg__slab",
+                sizeof(HypABI__Wagner__Create__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Create__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Create__arg__slab!\n");
+        HypABI__Wagner__Extend__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Extend__arg__slab",
+                sizeof(HypABI__Wagner__Extend__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Extend__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Extend__arg__slab!\n");
+        HypABI__Wagner__Delete__arg__slab = kmem_cache_create(
+                "HypABI__Wagner__Delete__arg__slab",
+                sizeof(HypABI__Wagner__Delete__arg__T), /* align */ 0,
+                SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+        if (!HypABI__Wagner__Delete__arg__slab)
+                panic("Could not create slab HypABI__Wagner__Delete__arg__slab!\n");
+}
diff --git security/bhv/acl.c security/bhv/acl.c
new file mode 100644
index 00000000000..4b9211d1482
--- /dev/null
+++ security/bhv/acl.c
@@ -0,0 +1,312 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define PATH_DELIMITER '/'
+
+HypABI__Acl__ProcessInit__arg__T *ProcessInit_acl_config __ro_after_init = NULL;
+HypABI__Acl__DriverInit__arg__T *DriverInit_acl_config __ro_after_init = NULL;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+#define ACL_INIT(name)                                                         \
+	unsigned long r;                                                       \
+	HypABI__Acl__##name##Init__arg__T *acl_config =                        \
+		(HypABI__Acl__##name##Init__arg__T *)__get_free_pages(         \
+			GFP_KERNEL, 0);                                        \
+	if (acl_config == NULL) {                                              \
+		bhv_fail("Cannot allocate acl config");                        \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	acl_config->num_pages = 1;                                             \
+	r = HypABI__Acl__##name##Init__hypercall_noalloc(acl_config);          \
+	if (r) {                                                               \
+		pr_err("acl init fail");                                       \
+		return;                                                        \
+	}                                                                      \
+                                                                               \
+	if (!acl_config->valid) {                                              \
+		uint16_t required_pages = acl_config->num_pages;               \
+		free_pages((unsigned long)acl_config, 0);                      \
+                                                                               \
+		acl_config =                                                   \
+			(HypABI__Acl__##name##Init__arg__T *)__get_free_pages( \
+				GFP_KERNEL, order_base_2(required_pages));     \
+                                                                               \
+		if (acl_config == NULL) {                                      \
+			bhv_fail("Cannot allocate acl config");                \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		r = HypABI__Acl__##name##Init__hypercall_noalloc(acl_config);  \
+		if (r) {                                                       \
+			pr_err("acl init fail");                               \
+			return;                                                \
+		}                                                              \
+                                                                               \
+		if (!acl_config->valid) {                                      \
+			bhv_fail("host returned invalid config");              \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	/* Protect memory */                                                   \
+	if (bhv_integrity_is_enabled()) {                                      \
+		HypABI__Integrity__Create__Mem_Region__T *region =             \
+			HypABI__Integrity__Create__Mem_Region__ALLOC();        \
+                                                                               \
+		region->start_addr = virt_to_phys(acl_config);                 \
+		region->size = acl_config->num_pages * PAGE_SIZE;              \
+		region->type = HypABI__Integrity__MemType__DATA_READ_ONLY;     \
+		region->flags = HypABI__Integrity__MemFlags__NONE;             \
+		region->next = BHV_INVALID_PHYS_ADDR;                          \
+		strscpy(region->label, "ACL CONFIG",                           \
+			HypABI__Integrity__MAX_LABEL_SIZE);                    \
+                                                                               \
+		r = bhv_create_kern_phys_mem_region_hyp(0, region);            \
+                                                                               \
+		HypABI__Integrity__Create__Mem_Region__FREE(region);           \
+                                                                               \
+		if (r) {                                                       \
+			pr_err("Cannot protect acl config");                   \
+			return;                                                \
+		}                                                              \
+	}                                                                      \
+                                                                               \
+	name##Init_acl_config = acl_config;
+
+void __init bhv_mm_init_acl(void)
+{
+	if (bhv_acl_is_proc_acl_enabled()) {
+		ACL_INIT(Process);
+	}
+	if (bhv_acl_is_driver_acl_enabled()) {
+		ACL_INIT(Driver);
+	}
+}
+#undef ACL_INIT
+/************************************************************/
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+#define MATCHES(name)                                                         \
+	static bool _matches_##name(const char *target, bool strip_ext)       \
+	{                                                                     \
+		size_t target_len = 0;                                        \
+		size_t target_ext_len = 0;                                    \
+		uint16_t i;                                                   \
+                                                                              \
+		BUG_ON(target[0] != PATH_DELIMITER);                          \
+                                                                              \
+		/* Get target len */                                          \
+		target_len = strnlen(target, PATH_MAX);                       \
+		if (strip_ext) {                                              \
+			target_ext_len = _get_ext_len(target);                \
+			target_len -= target_ext_len;                         \
+		}                                                             \
+                                                                              \
+		if (target_len == 0 || target_len >= PATH_MAX)                \
+			return false;                                         \
+                                                                              \
+		for (i = 0; i < name##Init_acl_config->list_len; i++) {       \
+			const char *cur = ((char *)name##Init_acl_config) +   \
+					  name##Init_acl_config->list[i];     \
+			size_t cur_len = 0;                                   \
+                                                                              \
+			if (cur[0] != PATH_DELIMITER) {                       \
+				if (_match_names(cur, target, target_ext_len, \
+						 strip_ext))                  \
+					return true;                          \
+				else                                          \
+					continue;                             \
+			}                                                     \
+                                                                              \
+			cur_len = strnlen(cur, PATH_MAX);                     \
+			if (strip_ext) {                                      \
+				cur_len -= _get_ext_len(cur);                 \
+			}                                                     \
+                                                                              \
+			if (cur_len == 0 || cur_len >= PATH_MAX)              \
+				continue;                                     \
+                                                                              \
+			if (cur[cur_len - 1] == '*') {                        \
+				cur_len--;                                    \
+                                                                              \
+				if (target_len < cur_len)                     \
+					continue;                             \
+			} else if (target_len != cur_len)                     \
+				continue;                                     \
+                                                                              \
+			if (strncmp(cur, target, cur_len) == 0)               \
+				return true;                                  \
+		}                                                             \
+                                                                              \
+		return false;                                                 \
+	}
+
+MATCHES(Process)
+MATCHES(Driver)
+#undef MATCHES
+
+#define BLOCK_ENTITY(name)                                                     \
+	static bool _block_entity_##name(const char *target, bool strip_ext)   \
+	{                                                                      \
+		bool rv;                                                       \
+		unsigned long r;                                               \
+		size_t target_len = strlen(target);                            \
+		bool m;                                                        \
+		HypABI__Acl__##name##Violation__arg__T *violation;             \
+                                                                               \
+		if (name##Init_acl_config == NULL ||                           \
+		    !name##Init_acl_config->valid) {                           \
+			bhv_fail(                                              \
+				"unable to resolve entity due to init error"); \
+			return false;                                          \
+		}                                                              \
+                                                                               \
+		m = _matches_##name(target, strip_ext);                        \
+                                                                               \
+		/* Is this entity part of the allow list? */                   \
+		if (m && name##Init_acl_config->is_allow)                      \
+			return false;                                          \
+		/* Is this entity _NOT_ in the deny list? */                   \
+		if (!m && !name##Init_acl_config->is_allow)                    \
+			return false;                                          \
+                                                                               \
+		BUG_ON(target_len >= PAGE_SIZE);                               \
+                                                                               \
+		violation = HypABI__Acl__##name##Violation__arg__ALLOC();      \
+                                                                               \
+		/* Get Context */                                              \
+		r = populate_event_context(&violation->context, true);         \
+		if (r) {                                                       \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			bhv_fail("%s: BHV cannot retrieve event context",      \
+				 __FUNCTION__);                                \
+		}                                                              \
+                                                                               \
+		violation->name_len = target_len,                              \
+		violation->na##me = virt_to_phys((volatile void *)target);     \
+                                                                               \
+		/* Hypercall */                                                \
+		/* We cannot use the HL interface due to stack constraints */  \
+		r = HypABI__Acl__##name##Violation__hypercall_noalloc(         \
+			violation);                                            \
+		if (r) {                                                       \
+			pr_err("entity hypercall failed");                     \
+			HypABI__Acl__##name##Violation__arg__FREE(violation);  \
+			return true;                                           \
+		}                                                              \
+                                                                               \
+		rv = (bool)violation->block;                                   \
+		HypABI__Acl__##name##Violation__arg__FREE(violation);          \
+                                                                               \
+		return (bool)rv;                                               \
+	}
+
+BLOCK_ENTITY(Process)
+BLOCK_ENTITY(Driver)
+#undef BLOCK_ENTITY
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity_Driver(target, true);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity_Process(target, false);
+}
\ No newline at end of file
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 00000000000..a5e1b17bc6a
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,15 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_BHV_TRACEPOINTS
+#define CREATE_TRACE_POINTS
+#endif
+#include <bhv/bhv_trace.h>
+
+bool bhv_initialized __ro_after_init = false;
+unsigned long *bhv_configuration_bitmap __ro_after_init = NULL;
diff --git security/bhv/container_integrity.c security/bhv/container_integrity.c
new file mode 100644
index 00000000000..61f977fd14e
--- /dev/null
+++ security/bhv/container_integrity.c
@@ -0,0 +1,816 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors:  Sebastian Vogl <sebastian@bedrocksystems.com>
+ * 
+ */
+#include <uapi/linux/magic.h>
+#include <linux/dcache.h>
+#include <linux/xattr.h>
+#include <linux/cgroup.h>
+#include <linux/version.h>
+#include <linux/namei.h>
+#include <linux/binfmts.h>
+#include <linux/file.h>
+#include <linux/pipe_fs_i.h>
+
+#include <bhv/container_integrity.h>
+#include <bhv/event.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+/***********************************************************************
+ * CONFIG
+ ***********************************************************************/
+#define BHV_XATTR_PREFIX "security.bhv."
+#define BHV_XATTR_INFO BHV_XATTR_PREFIX "info"
+/***********************************************************************/
+
+/***********************************************************************
+ * DEBUG
+ ***********************************************************************/
+#define DEBUG 0
+#define ci_pr_info(fmt, ...)                                                   \
+	do {                                                                   \
+		if (DEBUG)                                                     \
+			pr_info("[BHV - CI] " fmt "\n", ##__VA_ARGS__);        \
+	} while (0)
+
+#define ci_pr_debug(fmt, ...) ci_pr_info("[DEBUG] " fmt, ##__VA_ARGS__)
+/***********************************************************************/
+
+/***********************************************************************
+ * CONFIG
+ ***********************************************************************/
+HypABI__ContainerIntegrity__Init__Config__T ci_config __ro_after_init =
+	HypABI__ContainerIntegrity__Init__Config__NONE;
+
+static inline bool ci_new_file_execution_detection_is_enabled(void)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return false;
+
+	if (!HypABI__ContainerIntegrity__Init__Config__has_NEW_FILE_EXECUTION(
+		    &ci_config))
+		return false;
+
+	return true;
+}
+
+static inline bool
+ci_interpreter_with_new_file_execution_detection_is_enabled(void)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return false;
+
+	if (!HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_NEW_FILE(
+		    &ci_config))
+		return false;
+
+	return true;
+}
+
+static inline bool
+ci_interpreter_with_command_execution_detection_is_enabled(void)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return false;
+
+	if (!HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_ARG_CMD(
+		    &ci_config))
+		return false;
+
+	return true;
+}
+
+static inline bool ci_interpreter_binds_detection_is_enabled(void)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return false;
+
+	if (!HypABI__ContainerIntegrity__Init__Config__has_INTERPRETER_BINDS(
+		    &ci_config))
+		return false;
+
+	return true;
+}
+
+/***********************************************************************/
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_container_integrity(void)
+{
+	int r;
+
+	HypABI__ContainerIntegrity__Init__arg__T *bhv_arg;
+
+	if (!bhv_container_integrity_is_enabled())
+		return;
+
+	bhv_arg = HypABI__ContainerIntegrity__Init__arg__ALLOC();
+
+	r = HypABI__ContainerIntegrity__Init__hypercall_noalloc(bhv_arg);
+	if (r) {
+		pr_err("Container integrity init failed");
+	} else {
+		ci_config = bhv_arg->feature_bitmap;
+	}
+
+	HypABI__ContainerIntegrity__Init__arg__FREE(bhv_arg);
+}
+/***********************************************************************/
+
+static inline const char *ci_get_filename(struct dentry *dentry, char *buf,
+					  size_t buf_sz)
+{
+	const char *r = dentry_path(dentry, buf, buf_sz);
+	if (IS_ERR(r)) {
+		return "UNKNOWN (ERROR)";
+	}
+
+	return r;
+}
+
+/***********************************************************************
+ * Hypercalls
+ ***********************************************************************/
+bool ci_hyp_block_new_file(struct file *file)
+{
+	int err;
+	bool retval;
+	HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__T
+		*volatile violation;
+	const char *path;
+	size_t path_sz;
+	int rv;
+
+	if (!ci_new_file_execution_detection_is_enabled())
+		return false;
+
+	violation =
+		HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(
+			violation);
+		return true;
+	}
+
+	path = ci_get_filename(file->f_path.dentry, violation->path,
+			       sizeof(violation->path));
+	path_sz = strlen(path) + 1;
+
+	if (path_sz > sizeof(violation->path)) {
+		pr_err("container integrity: Invalid path size");
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+			violation);
+		return true;
+	}
+
+	memmove(violation->path, path, path_sz);
+	violation->path[path_sz] = '\0';
+
+	err = HypABI__ContainerIntegrity__ViolationNewFileExecution__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("container integrity new file detection hypercall failed");
+		HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__ContainerIntegrity__ViolationNewFileExecution__arg__FREE(
+		violation);
+
+	return retval;
+}
+
+bool ci_hyp_block_interpreter_arg_new_file(struct file *file,
+					   const char *filename,
+					   size_t filename_sz)
+{
+	int err;
+	bool retval;
+	HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__T
+		*volatile violation;
+	const char *path;
+	size_t path_sz;
+	int rv;
+
+	if (!ci_interpreter_with_new_file_execution_detection_is_enabled())
+		return false;
+
+	violation =
+		HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// Lookup interpreter.
+	path = ci_get_filename(file->f_path.dentry, violation->interpreter_name,
+			       sizeof(violation->interpreter_name));
+	path_sz = strlen(path) + 1;
+
+	if (path_sz > sizeof(violation->interpreter_name)) {
+		pr_err("container integrity: Invalid path size");
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+			violation);
+		return true;
+	}
+
+	memmove(violation->interpreter_name, path, path_sz);
+	violation->interpreter_name[path_sz] = '\0';
+
+	// Copy file name
+	path_sz = filename_sz;
+	if (path_sz > sizeof(violation->path)) {
+		path_sz = sizeof(violation->path);
+	}
+	memcpy(violation->path, filename, path_sz);
+	violation->path[path_sz - 1] = '\0';
+
+	err = HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("container integrity new file detection hypercall failed");
+		HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__ContainerIntegrity__ViolationInterpreterArgNewFile__arg__FREE(
+		violation);
+
+	return retval;
+}
+
+bool ci_hyp_block_interpreter_arg_cmd(struct file *file, const char *command,
+				      size_t command_sz)
+{
+	int err;
+	bool retval;
+	HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__T
+		*volatile violation;
+	const char *path;
+	size_t path_sz;
+	int rv;
+
+	if (!ci_interpreter_with_command_execution_detection_is_enabled())
+		return false;
+
+	violation =
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// Lookup interpreter.
+	path = ci_get_filename(file->f_path.dentry, violation->interpreter_name,
+			       sizeof(violation->interpreter_name));
+	path_sz = strlen(path) + 1;
+
+	if (path_sz > sizeof(violation->interpreter_name)) {
+		pr_err("container integrity: Invalid path size");
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+			violation);
+		return true;
+	}
+
+	memmove(violation->interpreter_name, path, path_sz);
+	violation->interpreter_name[path_sz] = '\0';
+
+	// Copy file name
+	path_sz = command_sz;
+	if (path_sz > sizeof(violation->command)) {
+		path_sz = sizeof(violation->command);
+	}
+	memcpy(violation->command, command, path_sz);
+	violation->command[path_sz - 1] = '\0';
+
+	err = HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("container integrity new file detection hypercall failed");
+		HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__ContainerIntegrity__ViolationInterpreterArgCmd__arg__FREE(
+		violation);
+
+	return retval;
+}
+
+bool ci_hyp_block_interpreter_bound(struct file *file)
+{
+	int err;
+	bool retval;
+	HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__T
+		*volatile violation;
+	const char *path;
+	size_t path_sz;
+	int rv;
+
+	if (!ci_interpreter_binds_detection_is_enabled())
+		return false;
+
+	violation =
+		HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(
+			violation);
+		return true;
+	}
+
+	path = ci_get_filename(file->f_path.dentry, violation->interpreter_name,
+			       sizeof(violation->interpreter_name));
+	path_sz = strlen(path) + 1;
+
+	if (path_sz > sizeof(violation->interpreter_name)) {
+		pr_err("container integrity: Invalid path size");
+		HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(
+			violation);
+		return true;
+	}
+
+	memmove(violation->interpreter_name, path, path_sz);
+	violation->interpreter_name[path_sz] = '\0';
+
+	err = HypABI__ContainerIntegrity__ViolationInterpreterBound__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("container integrity new file detection hypercall failed");
+		HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(
+			violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__ContainerIntegrity__ViolationInterpreterBound__arg__FREE(
+		violation);
+
+	return retval;
+}
+/***********************************************************************/
+
+static inline void ci_debug_print_context(struct dentry *dentry)
+{
+#if DEBUG
+	char buf[128];
+	const char *path;
+
+	if (dentry == NULL)
+		return;
+
+	path = ci_get_filename(dentry, buf, sizeof(buf));
+	ci_pr_debug("-> Dentry: %s (%p), Task: %s", path, dentry,
+		    current->comm);
+
+	path = ci_get_filename(d_real(dentry, NULL), buf, sizeof(buf));
+	ci_pr_debug("-> Underlying path: %s (%p)", path, d_real(dentry, NULL));
+#endif
+}
+
+static inline void ci_debug_print_file(struct file *file)
+{
+#if DEBUG
+	char buf[128];
+	const char *path;
+
+	path = ci_get_filename(file->f_path.dentry, buf, sizeof(buf));
+	ci_pr_debug("-> Path: %s", path);
+#endif
+}
+
+static inline void ci_debug_print_dentry(struct dentry *dentry)
+{
+#if DEBUG
+	char buf[128];
+	const char *path;
+
+	path = ci_get_filename(dentry, buf, sizeof(buf));
+	ci_pr_debug("-> Path: %s", path);
+#endif
+}
+
+struct bhv_xattr_info {
+	bool new_file;
+};
+
+static inline int ci_set_xattr(struct dentry *d, struct bhv_xattr_info *info)
+{
+	int rv;
+
+	inode_lock(d->d_inode);
+#if defined(VASKM_IS_UBUNTU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0))
+	rv = __vfs_setxattr_noperm(&nop_mnt_idmap, d, BHV_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	rv = __vfs_setxattr_noperm(&init_user_ns, d, BHV_XATTR_INFO, info,
+				   sizeof(*info), 0);
+#else
+	rv = __vfs_setxattr_noperm(d, BHV_XATTR_INFO, info, sizeof(*info), 0);
+#endif
+	inode_unlock(d->d_inode);
+
+	return rv;
+}
+
+static inline ssize_t ci_get_xattr(struct dentry *d,
+				   struct bhv_xattr_info *info)
+{
+#if defined(VASKM_IS_UBUNTU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0))
+	return vfs_getxattr(&nop_mnt_idmap, d, BHV_XATTR_INFO, info,
+			    sizeof(*info));
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+	return vfs_getxattr(&init_user_ns, d, BHV_XATTR_INFO, info,
+			    sizeof(*info));
+#else
+	return vfs_getxattr(d, BHV_XATTR_INFO, info, sizeof(*info));
+#endif
+}
+
+static inline bool is_new_file_by_dentry(struct dentry *dentry)
+{
+	struct bhv_xattr_info info = { .new_file = false };
+	ssize_t sz = ci_get_xattr(d_real(dentry, NULL), &info);
+
+	if (sz != sizeof(info) && sz != 0) {
+		return false;
+	}
+
+	if (info.new_file) {
+		return true;
+	}
+
+	return false;
+}
+
+static inline bool is_new_file_by_file(struct file *file)
+{
+	return is_new_file_by_dentry(file->f_path.dentry);
+}
+
+static inline bool is_new_file_by_filename(const char *filename)
+{
+	struct path path;
+
+	if (kern_path(filename, LOOKUP_FOLLOW, &path)) {
+		// Could not lookup path
+		ci_pr_info("ERROR: Failed to lookup path '%s'", filename);
+		return false;
+	}
+
+	return is_new_file_by_dentry(path.dentry);
+}
+
+static bool ci_is_interpreter(struct file *file)
+{
+	// Static list of interpreters.
+	// Note that we check for arg "-c" below to identify commands passed to
+	// the interpreter. If an interpreter is different, we need to account for
+	// that.
+	static const char *INTERPRETER[] = {
+		"/bash", "/python", "/ipython", "/sh",	"/dash",
+		"/perl", "/ksh",    "/csh",	"/zsh",
+	};
+	static const size_t INTERPRETER_SZ = (ARRAY_SIZE(INTERPRETER));
+	int i;
+	char buf[256];
+	const char *path =
+		ci_get_filename(file->f_path.dentry, buf, sizeof(buf));
+
+	ci_pr_debug("Checking if '%s' is interpreter", path);
+
+	for (i = 0; i < INTERPRETER_SZ; i++) {
+		if (strstr(path, INTERPRETER[i]) != NULL)
+			return true;
+	}
+
+	return false;
+}
+
+static bool ci_task_in_container(struct task_struct *tsk)
+{
+#define CGRP_NAME_SZ 128
+#define CONTAINER_ID_LEN 64
+	static const char *CONTAINER_CGRP_PREFIX[] = { "libpod-conmon-",
+						       "docker-" };
+	static const size_t CONTAINER_CGRP_PREFIX_SZ =
+		(ARRAY_SIZE(CONTAINER_CGRP_PREFIX));
+
+	int r = 0;
+	struct cgroup *cgrp;
+	char cgrp_name[CGRP_NAME_SZ];
+	int i;
+
+	rcu_read_lock();
+	cgrp = task_dfl_cgroup(tsk);
+	while (cgrp != NULL) {
+		r = cgroup_name(cgrp, cgrp_name, CGRP_NAME_SZ);
+		if (r < 0) {
+			goto out;
+		}
+
+		for (i = 0; i < CONTAINER_CGRP_PREFIX_SZ; i++) {
+			size_t prefix_len = strlen(CONTAINER_CGRP_PREFIX[i]);
+			if (strncmp(cgrp_name, CONTAINER_CGRP_PREFIX[i],
+				    prefix_len) == 0) {
+				rcu_read_unlock();
+				return true;
+			}
+		}
+
+		// Try next
+		cgrp = cgroup_parent(cgrp);
+	}
+
+out:
+	rcu_read_unlock();
+	return false;
+}
+
+static bool ci_consider_file(struct file *file)
+{
+	if (!d_is_reg(file->f_path.dentry)) {
+		return false;
+	}
+
+	// Filer special file systems
+	switch (file->f_inode->i_sb->s_magic) {
+	case PROC_SUPER_MAGIC:
+		return false;
+	case SYSFS_MAGIC:
+		return false;
+	default:
+		break;
+	}
+
+	return true;
+}
+
+int bhv_container_integrity_file_permission(struct file *file, int mask)
+{
+	if (!(ci_interpreter_with_new_file_execution_detection_is_enabled() ||
+	      ci_interpreter_with_command_execution_detection_is_enabled()))
+		return 0;
+
+	if ((mask & MAY_WRITE) == MAY_WRITE && ci_task_in_container(current) &&
+	    ci_consider_file(file)) {
+		int rv;
+		struct bhv_xattr_info info = { .new_file = true };
+
+		struct dentry *d = d_real(file->f_path.dentry, NULL);
+
+		// Do not consider files not in the overlayfs
+		if (d == file->f_path.dentry)
+			return 0;
+
+		rv = ci_set_xattr(d, &info);
+		if (rv) {
+			pr_err("Could not set xattr on file (%d)!\n", rv);
+			ci_debug_print_context(file->f_path.dentry);
+		} else {
+			ci_pr_debug("Set xattr on:");
+			ci_debug_print_context(file->f_path.dentry);
+		}
+	}
+
+	return 0;
+}
+
+static inline bool ci_is_bound(void)
+{
+	int i;
+	struct file *fd;
+	uint8_t stdin = 0;
+	uint8_t stdout = 0;
+	uint8_t stderr = 0;
+	uint8_t *cur = NULL;
+
+#define SOCKET (1 << 0)
+#define PIPE (1 << 1)
+
+	// Print stdin
+	for (i = 0; i < 3; i++) {
+		// We could use __fdget for a more lightweight version
+		// Since executions in containers happen rarely atm,
+		// we use fget.
+		fd = fget(i);
+		if (fd != NULL) {
+			switch (i) {
+			case 0:
+				cur = &stdin;
+				break;
+			case 1:
+				cur = &stdout;
+				break;
+			case 2:
+				cur = &stderr;
+				break;
+			default:
+				pr_err("Unknown fd %d", i);
+				BUG();
+			}
+
+			if (S_ISSOCK(fd->f_path.dentry->d_inode->i_mode)) {
+				(*cur) |= SOCKET;
+			}
+
+			/*
+			This would detect shells bound to pipes. This is likely to produce FPs and is disabled for now.
+			if (S_ISFIFO(fd->f_path.dentry->d_inode->i_mode)) {
+				(*cur) |= PIPE;
+			}
+			*/
+
+			fput(fd);
+
+			ci_pr_debug("FD %d | socket: %s, pipe: %s", i,
+				    (*cur & SOCKET) == SOCKET ? "true" :
+								"false",
+				    (*cur & PIPE) == PIPE ? "true" : "false");
+		}
+	}
+
+	if (stdin || stdout || stderr) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_container_integrity_bprm_check_security(struct linux_binprm *bprm)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return 0;
+
+	if (!ci_task_in_container(current))
+		return 0;
+
+	// Check whether this is a new file
+	if (ci_new_file_execution_detection_is_enabled() &&
+	    is_new_file_by_file(bprm->file)) {
+		ci_pr_info("DETECTED execution of '%s'", bprm->filename);
+		if (ci_hyp_block_new_file(bprm->file))
+			return -EPERM;
+	}
+
+	if ((ci_interpreter_with_new_file_execution_detection_is_enabled() ||
+	     ci_interpreter_with_command_execution_detection_is_enabled() ||
+	     ci_interpreter_binds_detection_is_enabled()) &&
+	    ci_is_interpreter(bprm->file)) {
+		int argc, i;
+		unsigned long pos;
+		char *arg_buf = NULL;
+
+		ci_pr_debug("Processing binary: '%s'", bprm->filename);
+		ci_debug_print_file(bprm->file);
+
+		// Check whether this is an interpreter and it binds to a socket
+		if (ci_interpreter_binds_detection_is_enabled() &&
+		    ci_is_bound()) {
+			ci_pr_info(
+				"DETECTED execution of '%s' that binds to a socket",
+				bprm->filename);
+
+			if (ci_hyp_block_interpreter_bound(bprm->file))
+				return -EPERM;
+		}
+
+		if (ci_interpreter_with_new_file_execution_detection_is_enabled() ||
+		    ci_interpreter_with_command_execution_detection_is_enabled()) {
+			// Check whether a new file is passed as argument or interpreter launched with -c
+			argc = bprm->argc;
+			pos = bprm->p;
+			i = 1;
+			arg_buf = vmalloc(MAX_ARG_STRLEN + 1);
+			if (arg_buf == NULL) {
+				pr_err("Could not allocate memory");
+				// Failing open for now.
+				return 0;
+			}
+
+			while (argc > 0) {
+				int len = 0;
+				int cur_buf_pos = 0;
+
+				ci_pr_debug("%d args left. Reading...", argc);
+
+				// We need to read from the new mm
+				len = access_remote_vm(bprm->mm, pos, arg_buf,
+						       MAX_ARG_STRLEN + 1,
+#if defined(VASKM_IS_UBUNTU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0))
+						       FOLL_FORCE);
+#else
+						       FOLL_REMOTE |
+							       FOLL_FORCE);
+#endif
+
+				ci_pr_debug("Read %d bytes...", len);
+
+				// Process all strings in the buffer
+				while (len > 0 && argc > 0) {
+					char *cur = arg_buf + cur_buf_pos;
+					size_t cur_len = strnlen(cur, len);
+
+					if (cur_len == len &&
+					    arg_buf[cur_buf_pos + len - 1] !=
+						    '\0') {
+						// Incomplete arg. Trigger reread.
+						break;
+					}
+
+					ci_pr_debug(
+						"Processing argv[%d] = '%s' (%lu, %d)",
+						i, cur, cur_len, len);
+
+					// Expects all interprets to use option "-c" to pass arguments.
+					if (ci_interpreter_with_command_execution_detection_is_enabled() &&
+					    strncmp("-c", cur, 3) == 0) {
+						// Command passed as part of the command line.
+						// This can probably be evaded.
+						ci_pr_info(
+							"DETECTED execution of '%s' with arg '%s' (command passed as arg)",
+							bprm->filename, cur);
+
+						if (ci_hyp_block_interpreter_arg_cmd(
+							    bprm->file,
+							    len > 4 ?
+								    cur + 3 /* -c\0 + space */ :
+								    "???" /* We do not have enough data*/
+							    ,
+							    len > 4 ? len - 4 :
+								      4)) {
+							vfree(arg_buf);
+							return -EPERM;
+						}
+					}
+
+					if (ci_interpreter_with_new_file_execution_detection_is_enabled() &&
+					    is_new_file_by_filename(cur)) {
+						ci_pr_info(
+							"DETECTED execution of '%s' with arg '%s' (new file passed as arg)",
+							bprm->filename, cur);
+
+						if (ci_hyp_block_interpreter_arg_new_file(
+							    bprm->file, cur,
+							    len)) {
+							vfree(arg_buf);
+							return -EPERM;
+						}
+					}
+
+					pos += (cur_len + 1);
+					len -= (cur_len + 1);
+					cur_buf_pos += (cur_len + 1);
+					argc--;
+					i++;
+				}
+			}
+
+			vfree(arg_buf);
+			ci_pr_debug("Done processing arguments!");
+		}
+	}
+
+	return 0;
+}
+
+int bhv_container_integrity_inode_setxattr(const char *name)
+{
+	if (!bhv_container_integrity_is_enabled())
+		return 0;
+
+	if (strncmp(BHV_XATTR_PREFIX, name, strlen(BHV_XATTR_PREFIX)) == 0) {
+		ci_pr_debug("BLOCKED change of xattr: %s", name);
+		return -EPERM;
+	}
+
+	return 0;
+}
\ No newline at end of file
diff --git security/bhv/creds.c security/bhv/creds.c
new file mode 100644
index 00000000000..71edfcb13bc
--- /dev/null
+++ security/bhv/creds.c
@@ -0,0 +1,472 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/gfp.h>
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/event.h>
+#include <bhv/inode.h>
+#include <bhv/interface/common.h>
+
+#include <bhv/interface/hypercall.h>
+#include <bhv/keyring.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#define BHV_CRED_USAGE_SET(arg1, arg2)         \
+	_Generic((arg1), atomic_t *            \
+		 : atomic_set, atomic_long_t * \
+		 : atomic_long_set)(arg1, arg2)
+
+siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	BHV_CRED_USAGE_SET(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+#undef MAX_BUF_SIZE
+}
+
+#define LOG_PREPARE(logarg)                                                    \
+	HypABI__Creds__Log__arg__T *logarg = HypABI__Creds__Log__arg__ALLOC(); \
+	rc = populate_event_context(&logarg->context, true);                   \
+	if (rc) {                                                              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}
+
+#define LOG_SEND(rc, logarg)                                                  \
+	rc = HypABI__Creds__Log__hypercall_noalloc(logarg);                   \
+	if (rc) {                                                             \
+		pr_err("%s: BHV Cannot log event with type=%d", __FUNCTION__, \
+		       type);                                                 \
+	}
+
+#define LOG_FREE(logarg) HypABI__Creds__Log__arg__FREE(logarg)
+
+static int __bhv_cred_assign(struct task_struct *t,
+			     struct task_struct *_current, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Assign__arg__T *assarg = NULL;
+	struct task_struct *parent = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(_current);
+	if (rc)
+		return -EPERM;
+
+	assarg = HypABI__Creds__Assign__arg__ALLOC();
+	if (assarg == NULL)
+		return -ENOMEM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = _current->real_parent;
+	else
+		parent = _current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	assarg->new_task.addr = (uint64_t)t;
+	assarg->new_task.cred = (uint64_t)t->cred;
+	assarg->new_task.hmac = hmac;
+	assarg->parent.addr = (uint64_t)parent;
+	assarg->parent.cred = (uint64_t)parent->cred;
+
+	rc = HypABI__Creds__Assign__hypercall_noalloc(assarg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = assarg->ret;
+
+	HypABI__Creds__Assign__arg__FREE(assarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		/* Note that we currently log only the parent's information. */
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)parent;
+		logarg->task_cred = (uint64_t)parent->cred;
+		logarg->task_pid = parent->pid;
+		strscpy(logarg->task_name, parent->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return __bhv_cred_assign(t, current, clone_flags);
+}
+
+#ifdef VASKM // out of tree
+int __init bhv_cred_assign_init(struct task_struct *t)
+{
+	return __bhv_cred_assign(t, t->real_parent, 0);
+}
+#endif // VASKM
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	HypABI__Creds__AssignPriv__arg__T *aparg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	aparg = HypABI__Creds__AssignPriv__arg__ALLOC_NOCHECK();
+	if (aparg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	aparg->cred = (uint64_t)c;
+	aparg->daemon = (uint64_t)daemon;
+
+	rc = HypABI__Creds__AssignPriv__hypercall_noalloc(aparg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = aparg->ret;
+	HypABI__Creds__AssignPriv__arg__FREE(aparg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Commit__arg__T *commarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	commarg = HypABI__Creds__Commit__arg__ALLOC_NOCHECK();
+	if (commarg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	commarg->currnt.cred = (uint64_t)c;
+	commarg->currnt.addr = (uint64_t)current;
+	commarg->currnt.hmac = hmac;
+
+	rc = HypABI__Creds__Commit__hypercall_noalloc(commarg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = commarg->ret;
+	HypABI__Creds__Commit__arg__FREE(commarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)current;
+		logarg->task_cred = (uint64_t)c;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, current->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+
+		LOG_FREE(logarg);
+	}
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	HypABI__Creds__Verification__arg__T *verarg = NULL;
+	enum HypABI__Creds__EventType type =
+		HypABI__Creds__EventType__EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	verarg = HypABI__Creds__Verification__arg__ALLOC_NOCHECK();
+	if (verarg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	verarg->task.cred = (uint64_t)t->cred;
+	verarg->task.addr = (uint64_t)t;
+	verarg->task.hmac = hmac;
+
+	rc = HypABI__Creds__Verification__hypercall_noalloc(verarg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = verarg->ret;
+	HypABI__Creds__Verification__arg__FREE(verarg);
+
+	if (!rc && type != HypABI__Creds__EventType__EVENT_NONE) {
+		LOG_PREPARE(logarg);
+
+		logarg->event_type = type;
+		logarg->task_addr = (uint64_t)t;
+		logarg->task_cred = (uint64_t)t->cred;
+		logarg->task_pid = current->pid;
+		strscpy(logarg->task_name, t->comm, TASK_COMM_LEN);
+
+		LOG_SEND(rc, logarg);
+
+		/* Check if the policy is configured to be blocking. */
+		if (logarg->block) {
+			rc = -EPERM;
+		}
+
+		LOG_FREE(logarg);
+	}
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	HypABI__Creds__Release__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__Release__arg__ALLOC_GFP_NOCHECK(GFP_ATOMIC);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->cred = (uint64_t)c;
+
+	rc = HypABI__Creds__Release__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	HypABI__Creds__Release__arg__FREE(arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	HypABI__Creds__RegisterInitTask__arg__T *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = HypABI__Creds__RegisterInitTask__arg__ALLOC_NOCHECK();
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->init_task.addr = (uint64_t)t;
+	arg->init_task.cred = (uint64_t)c;
+	arg->init_task.hmac = siphash_cred_context(c, t);
+
+	rc = HypABI__Creds__RegisterInitTask__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	HypABI__Creds__RegisterInitTask__arg__FREE(arg);
+}
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_cred(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_cred_register_init_task(KLN_SYMBOL_P(struct cred *const, init_cred),
+				    &init_task);
+}
+/***********************************************************************/
+
+/***********************************************************************
+ * init
+ ***********************************************************************/
+int __init bhv_init_cred(void)
+{
+	int rc = 0;
+
+	static HypABI__Creds__Configure__arg__T fbarg;
+	HypABI__Creds__Configure__arg__T *arg;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = HypABI__Creds__Configure__arg__ALLOC_STATICFALLBACK(fbarg);
+
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = HypABI__Creds__Configure__hypercall_noalloc(arg);
+
+	if (!rc) {
+		static_assert(HypABI__Creds__Configure__arg__SZ ==
+			      sizeof(siphash_key_t));
+		memcpy(&bhv_siphash_key, arg->key, sizeof(siphash_key_t));
+		memset(arg->key, 0, sizeof(siphash_key_t));
+	}
+
+	HypABI__Creds__Configure__arg__FREE_STATICFALLBACK(arg, fbarg);
+	if (rc)
+		return -EINVAL;
+
+	rc = bhv_inode_init();
+	if (rc)
+		return rc;
+#ifndef VASKM
+	rc = bhv_init_keyring();
+	if (rc)
+		return rc;
+#endif
+
+	return rc;
+}
+/***********************************************************************/
diff --git security/bhv/domain.c security/bhv/domain.c
new file mode 100644
index 00000000000..2ea3b443f57
--- /dev/null
+++ security/bhv/domain.c
@@ -0,0 +1,911 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifdef CONFIG_MEM_NS
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/nsproxy.h>
+#include <linux/mem_namespace.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+#include <linux/hugetlb.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <asm/bhv/domain.h>
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+#include <asm/stacktrace.h>
+#include <asm/unwind.h>
+#endif
+
+DEFINE_PER_CPU(uint64_t, bhv_domain_current_domain);
+EXPORT_PER_CPU_SYMBOL_GPL(bhv_domain_current_domain);
+
+static DEFINE_XARRAY_ALLOC(xa_domids);
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static bhv_domain_batched_arg_t _batch_area;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+bool bhv_domain_initialized __ro_after_init = false;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static bool isolate __ro_after_init = false;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+static bool disallow_forced_mem_access __ro_after_init = true;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_create_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_CREATE_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot create new domain\n", __FUNCTION__);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_create(uint64_t *domid)
+{
+	int rc = 0;
+	unsigned int id;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == NULL)
+		return -EINVAL;
+
+	/*
+	 * Start allocating domain IDs from ID 1. Domain with ID 0 is reserved.
+	 *
+	 * NOTE: This implementation allocates domain IDs inside of the guest.
+	 * The allocations are done sequentially, yet (unless otherwise required
+	 * by BHV), do not necessarily have to be. These can be passed to BHV
+	 * for management purposes or, if needed, allocated directly by BHV
+	 * instead.
+	 *
+	 * XXX: Consider binding struct mem_namespace (or another datastructure)
+	 * to the allocated ID.
+	 */
+	rc = xa_alloc(&xa_domids, &id, NULL, xa_limit_32b, GFP_KERNEL);
+	if (rc) {
+		pr_err("%s: Cannot allocate new domain ID\n", __FUNCTION__);
+		*domid = BHV_INVALID_DOMAIN;
+		return rc;
+	}
+
+	*domid = (uint64_t)id;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate) {
+		rc = bhv_domain_create_isolated_view(*domid);
+		if (rc) {
+			xa_release(&xa_domids, id);
+			*domid = BHV_INVALID_DOMAIN;
+		}
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	return 0;
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_destroy_isolated_view(uint64_t domid)
+{
+	int rc;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_DESTROY_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot destroy domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+void bhv_domain_destroy(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate)
+		rc = bhv_domain_destroy_isolated_view(domid);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	/*
+	 * Release the allocated domain IDs only if BHV did not return an error,
+	 * or if guest isolation was not enabled in the first place.
+	 */
+	if (!rc)
+		xa_release(&xa_domids, domid);
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+static int bhv_domain_switch_isolated_view(uint64_t domid)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	arg.domain.id = domid;
+
+	rc = BHV_DOMAIN_SWITCH_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot switch to domain[%llu]\n", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_switch(uint64_t domid)
+{
+	int rc = 0;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return -EINVAL;
+
+	if (domid == this_cpu_read(bhv_domain_current_domain))
+		return 0;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	if (isolate) {
+		rc = bhv_domain_switch_isolated_view(domid);
+		if (rc)
+			return rc;
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	this_cpu_write(bhv_domain_current_domain, domid);
+
+	return rc;
+}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+int bhv_domain_transfer_mm(struct mm_struct *const mm,
+			   struct nsproxy *const old_ns,
+			   struct nsproxy *const new_ns)
+{
+	int rc = 0;
+	bhv_domain_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	if (mm == NULL)
+		return -EINVAL;
+
+	if (old_ns == NULL || new_ns == NULL)
+		return -EINVAL;
+
+	arg.domain.id = old_ns->mem_ns->domain;
+	arg.domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.id = new_ns->mem_ns->domain;
+
+	rc = BHV_DOMAIN_TRANSFER_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV cannot transfer PGD 0x%llx to domain[%llu]\n",
+		       __FUNCTION__, arg.domain.pgd, new_ns->mem_ns->domain);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+static inline void bhv_domain_batch_from_info(uint64_t info, uint32_t *head,
+					      uint32_t *tail)
+{
+	(*tail) = (info & 0xffffffff);
+	(*head) = (info >> 32);
+}
+
+static inline uint64_t bhv_domain_batch_to_info(uint32_t head, uint32_t tail)
+{
+	return (((uint64_t)head << 32) | tail);
+}
+
+static inline uint32_t bhv_domain_get_nr_entries(uint32_t head, uint32_t tail)
+{
+	return head <= tail ? tail - head :
+				    BHV_DOMAIN_MAX_ENTRIES - head + tail;
+}
+
+static int bhv_domain_batch_send_locked(void)
+{
+	// LOCK MUST BE HELD!
+	int rc = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	rc = BHV_DOMAIN_BATCH_HYP(batch_area);
+	if (rc) {
+		uint32_t head;
+		uint32_t tail;
+		uint64_t info = atomic_long_read(&_batch_area.info);
+		bhv_domain_batch_from_info(info, &head, &tail);
+		pr_err("%s: BHV could not process %u batched hypercalls!\n",
+		       __FUNCTION__, bhv_domain_get_nr_entries(head, tail));
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+
+// DISCLAIMER: Disabled for now, since it may change the order of operations.
+#define BHV_BATCH_STEAL_PROCESSED 0
+
+static inline uint32_t bhv_domain_batch_get_slot(void)
+{
+	static atomic_t sending = { false };
+	uint32_t in_send = false;
+
+	uint64_t info = atomic64_read(&_batch_area.info);
+	uint64_t new_info;
+	uint32_t head;
+	uint32_t tail;
+	uint32_t nr_entries;
+
+#if BHV_BATCH_STEAL_PROCESSED
+	uint32_t i;
+#endif
+
+	while (true) {
+		// Update head and tail
+		bhv_domain_batch_from_info(info, &head, &tail);
+		// Calculate the current number of entries
+		nr_entries = bhv_domain_get_nr_entries(head, tail);
+
+		// Check if we need to send the batch area down because it is full
+		// Note that we check whether we reached BHV_DOMAIN_MAX_ENTRIES - 1
+		// as the tail always points to the next free entry
+		if (nr_entries >= BHV_DOMAIN_MAX_ENTRIES - 1) {
+			// Make sure in send is always false when we try to win
+			// the race.
+			in_send = false;
+
+			if (atomic_try_cmpxchg(&sending, &in_send, true)) {
+				// Hooray! We are a winner! Send it down.
+				bhv_domain_batch_send_locked();
+				// Reset sending afterwards.
+				atomic_set(&sending, false);
+			} else {
+#if BHV_BATCH_STEAL_PROCESSED
+				// Lets try to steal an existing entry.
+				// This works as processed entries always come after an INVALID entry
+				// Thus we can try to reuse them in case little space is left in our
+				// batch area.
+				for (i = head; i < nr_entries; i++) {
+					uint32_t cur =
+						i % BHV_DOMAIN_MAX_ENTRIES;
+					uint32_t expected =
+						BHV_VAS_DOMAIN_BATCH_STATE_PROCESSED;
+
+					if (atomic_try_cmpxchg(
+						    &_batch_area.entries[cur]
+							     .state,
+						    &expected,
+						    BHV_VAS_DOMAIN_BATCH_STATE_INVALID)) {
+						// We manged to steal an entry. Lets use it.
+						return cur;
+					}
+				}
+#endif
+			}
+
+			// Reread the the info and try again
+			info = atomic64_read(&_batch_area.info);
+			continue;
+		}
+
+		// There are still free entries! Lets see if we can get one
+		new_info = bhv_domain_batch_to_info(
+			head, (tail + 1) % BHV_DOMAIN_MAX_ENTRIES);
+		if (atomic64_try_cmpxchg(&_batch_area.info, &info, new_info)) {
+			// We won the race!
+			if (atomic_read(&_batch_area.entries[tail].state) !=
+			    BHV_VAS_DOMAIN_BATCH_STATE_INVALID) {
+				pr_err("Found valid/processed slot! (state: %u, head: %u, tail: %u, info: 0x%llx)",
+				       atomic_read(
+					       &_batch_area.entries[tail].state),
+				       head, tail,
+				       atomic64_read(&_batch_area.info));
+				panic("Batching failed!");
+			}
+			return tail;
+		}
+	}
+}
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+static void bhv_domain_add_stack_trace(bhv_domain_batched_entry_arg_t *target)
+{
+	struct unwind_state state;
+	struct stack_info stack_info = { 0 };
+	unsigned long visit_mask = 0;
+	unsigned long *stack = get_stack_pointer(current, NULL);
+	char *cur = &target->stack_trace[0];
+	size_t stack_buf_len = BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+
+	unwind_start(&state, current, NULL, stack);
+
+	stack = PTR_ALIGN(stack, sizeof(long));
+	if (get_stack_info(stack, current, &stack_info, &visit_mask)) {
+		pr_err("Could not get stack info!");
+		target->stack_trace[0] = '\0';
+		return;
+	}
+
+	for (; stack < stack_info.end && stack_buf_len > 0 &&
+	       stack_buf_len <= BHV_VAS_DOMAIN_STACK_TRACE_BUF_SIZE;
+	     stack++) {
+		unsigned long addr = READ_ONCE_NOCHECK(*stack);
+		unsigned long *ret_addr_p =
+			unwind_get_return_address_ptr(&state);
+		int reliable = 0;
+		int written;
+
+		if (!__kernel_text_address(addr))
+			continue;
+
+		if (stack == ret_addr_p)
+			reliable = 1;
+
+		// Skip unreliable for now.
+		if (!reliable)
+			continue;
+
+		written = snprintf(cur, stack_buf_len, "%c%pB\n",
+				   reliable ? ' ' : '?', (void *)addr);
+		if (written >= stack_buf_len)
+			break;
+		stack_buf_len -= written;
+		cur += written;
+
+		if (!reliable)
+			continue;
+
+		unwind_next_frame(&state);
+	}
+}
+#endif
+
+static int bhv_domain_batch_op(bhv_domain_batched_entry_arg_t *arg)
+{
+	int rc = 0;
+	uint32_t dest;
+	uint32_t invalid_state = BHV_VAS_DOMAIN_BATCH_STATE_INVALID;
+	unsigned long flags = 0;
+	bhv_domain_batched_arg_t *batch_area = &_batch_area;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg->state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	dest = bhv_domain_batch_get_slot();
+
+	// disable interrupts for the completion of the entry.
+	local_irq_save(flags);
+	BUG_ON(atomic_read(&batch_area->entries[dest].state) !=
+	       BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	memcpy(&batch_area->entries[dest], arg,
+	       sizeof(bhv_domain_batched_entry_arg_t));
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+	bhv_domain_add_stack_trace(&batch_area->entries[dest]);
+#endif
+
+	if (!atomic_try_cmpxchg(&batch_area->entries[dest].state,
+				&invalid_state,
+				BHV_VAS_DOMAIN_BATCH_STATE_VALID)) {
+		panic("Could not set valid state!");
+	}
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+
+static int bhv_domain_batch_map(uint32_t op, struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages, bool read, bool write,
+				bool exec, bool kernel)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	if (!isolate)
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = op;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.map.read = read;
+	arg.map.write = write;
+	arg.map.exec = exec;
+	arg.map.kernel = kernel;
+	arg.map.pfn.pfn = pfn;
+	arg.map.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+int bhv_domain_map_kernel(struct mm_struct *mm, uint64_t pfn, uint64_t nr_pages,
+			  bool read, bool write, bool exec)
+{
+	if (!isolate)
+		return 0;
+
+	return bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pfn, nr_pages,
+				    read, write, exec, true);
+}
+
+void bhv_domain_set_pte_at(struct mm_struct *mm, unsigned long addr,
+			   pte_t *ptep, pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(pte))
+		return;
+
+	if (pte_special(pte) && !pte_exec(pte)) {
+		bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), true);
+	} else {
+		uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+		if (pte_present(*ptep))
+			bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+		bhv_domain_batch_map(bhv_domain_op, mm, pte_pfn(pte), 1,
+				     pte_read(pte), pte_write(pte),
+				     pte_exec(pte), false);
+	}
+}
+EXPORT_SYMBOL(bhv_domain_set_pte_at);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+
+void bhv_domain_set_pte_at_kernel(struct mm_struct *mm, unsigned long addr,
+				  pte_t *ptep, pte_t pte)
+{
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte)) {
+		set_pte(ptep, pte);
+		return;
+	}
+#endif
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	bhv_domain_batch_map(BHV_VAS_DOMAIN_OP_MAP, mm, pte_pfn(pte), 1,
+			     pte_read(pte), pte_write(pte), pte_exec(pte),
+			     true);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	set_pte(ptep, pte);
+}
+
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+void bhv_domain_set_pmd_at(struct mm_struct *mm, unsigned long addr,
+			   pmd_t *pmdp, pmd_t pmd)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(pmd))
+		return;
+
+	if (!pmd_large(pmd))
+		return;
+
+	if (pmd_present(*pmdp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pmd_pfn(pmd), 512,
+			     pmd_read(pmd), pmd_write(pmd), pmd_exec(pmd),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pmd_at);
+
+void bhv_domain_set_pud_at(struct mm_struct *mm, unsigned long addr,
+			   pud_t *pudp, pud_t pud)
+{
+	uint32_t bhv_domain_op = BHV_VAS_DOMAIN_OP_MAP;
+
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(pud))
+		return;
+
+	if (!pud_large(pud))
+		return;
+
+	if (pud_present(*pudp))
+		bhv_domain_op = BHV_VAS_DOMAIN_OP_UPDATE;
+
+	bhv_domain_batch_map(bhv_domain_op, mm, pud_pfn(pud), 512 * 512,
+			     pud_read(pud), pud_write(pud), pud_exec(pud),
+			     false);
+}
+EXPORT_SYMBOL(bhv_domain_set_pud_at);
+
+static int bhv_domain_unmap_pte(struct mm_struct *mm, uint64_t pfn,
+				uint64_t nr_pages)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_UNMAP;
+	arg.domain.id = bhv_get_domain(mm->owner);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg.unmap.pfn.pfn = pfn;
+	arg.unmap.pfn.count = nr_pages;
+
+	return bhv_domain_batch_op(&arg);
+}
+
+void bhv_domain_clear_pte(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+			  pte_t pte)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pte_devmap(pte))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pte(*ptep))
+		return;
+
+	bhv_domain_unmap_pte(mm, pte_pfn(*ptep), 1);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pte);
+
+void bhv_domain_clear_pmd(struct mm_struct *mm, unsigned long addr, pmd_t *pmdp,
+			  pmd_t pmd)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pmd_devmap(pmd))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pmd(*pmdp))
+		return;
+
+	if (!pmd_large(*pmdp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pmd_pfn(*pmdp), 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pmd);
+
+void bhv_domain_clear_pud(struct mm_struct *mm, unsigned long addr, pud_t *pudp,
+			  pud_t pud)
+{
+	if (!isolate)
+		return;
+
+#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP
+	if (pud_devmap(pud))
+		return;
+#endif
+
+	if (!bhv_domain_is_user_pud(*pudp))
+		return;
+
+	if (!pud_large(*pudp))
+		return;
+
+	bhv_domain_unmap_pte(mm, pud_pfn(*pudp), 512 * 512);
+}
+EXPORT_SYMBOL(bhv_domain_clear_pud);
+
+#ifdef BHV_VAS_DOMAIN_DEBUG
+void bhv_domain_debug_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	int rc = 0;
+	bhv_domain_debug_arg_t arg = {};
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	arg.msg_type = BHV_VAS_DOMAIN_DEBUG_DESTROY_PGD;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	rc = BHV_DOMAIN_DEBUG_HYP(&arg);
+	if (rc) {
+		pr_err("%s: BHV an error occurred during DEBUG destroy pgd hypercall\n",
+		       __FUNCTION__);
+	}
+}
+#endif
+
+void bhv_domain_destroy_pgd(struct task_struct *tsk, struct mm_struct *mm)
+{
+	bhv_domain_batched_entry_arg_t arg;
+
+	if (!bhv_domain_is_active())
+		return;
+
+	if (!isolate)
+		return;
+
+	atomic_set(&arg.state, BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	arg.op = BHV_VAS_DOMAIN_OP_PGD_DESTROY;
+	arg.domain.id = bhv_get_domain(tsk);
+	arg.domain.pgd = bhv_virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	bhv_domain_batch_op(&arg);
+}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+int bhv_domain_report(const struct task_struct *t,
+		      const struct mm_struct *mm_target,
+		      const struct vm_area_struct *vma, unsigned int gup_flags)
+{
+	int rc = 0;
+	HypABI__Domain__Report__arg__T arg = { 0 };
+
+	if (!bhv_domain_is_active())
+		return 0;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return 0;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	rc = populate_event_context(&arg.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context",
+		       __FUNCTION__);
+	}
+
+	arg.domain_src.id = bhv_get_domain(t);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.domain_src.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(t->active_mm->pgd));
+	arg.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.domain_target.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.gva_start = vma->vm_start;
+	arg.gva_end = vma->vm_end;
+	arg.write = ((gup_flags & FOLL_WRITE) || (gup_flags & FOLL_FORCE));
+
+	rc = HypABI__Domain__Report__hypercall_noalloc(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+	if (arg.block)
+		rc = -EPERM;
+
+#if 0
+	pr_info("[-BHV-] %s: dom[%llu] -> dom[%llu] write=%d block=%d", __FUNCTION__, arg.report.domain_src.id, arg.report.domain_target.id, arg.report.write, arg.report.block);
+#endif
+
+	return rc;
+}
+
+bool bhv_forced_mem_access_permitted(struct vm_area_struct *vma, bool write,
+				     bool foreign)
+{
+	int rc = 0;
+	HypABI__Domain__ReportForcedMemAccess__arg__T arg = { 0 };
+	struct mm_struct *mm_target = vma->vm_mm;
+
+	if (!bhv_domain_is_active())
+		return true;
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/*
+	 * XXX: Consider enabling this hypercall even if the system
+	 * configuration is set to strong_isolation=isolate.
+	 */
+	if (isolate)
+		return true;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (!disallow_forced_mem_access)
+		return true;
+
+	rc = populate_event_context(&arg.context, true);
+	if (rc) {
+		pr_err("[-BHV-] %s: cannot retrieve event context",
+		       __FUNCTION__);
+	}
+
+	arg.domain_src.id = bhv_get_domain(current);
+	/* XXX: THIS IS NOT IDEAL. CONSIDER REMOVING PGDs COMPLETELY FROM THE STRUCT! */
+	arg.domain_src.pgd = bhv_virt_to_phys(
+		bhv_domain_get_user_pgd(current->active_mm->pgd));
+	arg.domain_target.id = bhv_get_domain(mm_target->owner);
+	arg.domain_target.pgd =
+		bhv_virt_to_phys(bhv_domain_get_user_pgd(mm_target->pgd));
+	arg.gva_start = vma->vm_start;
+	arg.gva_end = vma->vm_end;
+	arg.write = write;
+
+	rc = HypABI__Domain__ReportForcedMemAccess__hypercall_noalloc(&arg);
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REPORT hypercall\n",
+		       __FUNCTION__);
+		return false;
+	}
+
+	pr_info("[-BHV-] %s: vma @ [0x%lx:0x%lx] write=%d", __FUNCTION__,
+		vma->vm_start, vma->vm_end, write);
+
+	return !arg.block;
+}
+
+static void __init bhv_domain_init(void)
+{
+	int rc;
+	bool pti_active = false;
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	// We use the CPU region before the initialization!
+	// This is important, otherwise this would overwrite nr_entries!!!
+	bhv_domain_arg_t *arg = (bhv_domain_arg_t *)&_batch_area;
+#else
+	static HypABI__Domain__Configure__arg__T args = { 0 };
+	HypABI__Domain__Configure__arg__T *arg = &args;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (bhv_domain_initialized)
+		return;
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		pti_active = 1;
+#endif
+
+	arg->pti = (uint8_t)pti_active;
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	arg->batched_region = bhv_virt_to_phys(&_batch_area);
+#else
+	arg->batched_region = BHV_INVALID_PHYS_ADDR;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	rc = HypABI__Domain__Configure__hypercall_noalloc(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot configure StrongIsolation\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	isolate = !!arg->isolate;
+#else // !BHV_VAS_DOMAIN_SPACES_BASED
+	BUG_ON(arg->isolate);
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+	disallow_forced_mem_access = !arg->allow_forced_mem_access;
+
+	bhv_domain_initialized = true;
+}
+
+int __init bhv_domain_mm_init(void)
+{
+	int cpu;
+	int rc;
+	
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	uint32_t i;
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	if (!bhv_domain_is_enabled())
+		return -EPERM;
+
+	rc = xa_reserve_bh(&xa_domids, BHV_INIT_DOMAIN, GFP_KERNEL);
+	if (rc) {
+		bhv_fail("%s: BHV: cannot reserve domain ID %lu", __FUNCTION__,
+			 BHV_INIT_DOMAIN);
+		return -EFAULT;
+	}
+
+	bhv_domain_init();
+
+	for_each_possible_cpu(cpu) {
+		per_cpu(bhv_domain_current_domain, cpu) = 0;
+	}
+
+#if BHV_VAS_DOMAIN_SPACES_BASED
+	/* Reserve the ID of the first, static memory namespace. */
+
+	atomic64_set(&_batch_area.info, 0);
+
+	for (i = 0; i < BHV_DOMAIN_MAX_ENTRIES; i++) {
+		atomic_set(&_batch_area.entries[i].state,
+			   BHV_VAS_DOMAIN_BATCH_STATE_INVALID);
+	}
+#endif // BHV_VAS_DOMAIN_SPACES_BASED
+
+	return 0;
+}
+
+#ifdef CONFIG_MEM_NS
+__initcall(bhv_domain_mm_init);
+#endif
+
+#endif /* CONFIG_MEM_NS */
diff --git security/bhv/file_protection.c security/bhv/file_protection.c
new file mode 100644
index 00000000000..eef0ef95fa8
--- /dev/null
+++ security/bhv/file_protection.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/file_protection.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+HypABI__FileProtection__Init__Config__T bhv_file_protection_config
+	__ro_after_init = HypABI__FileProtection__Init__Config__NONE;
+
+/***********************************************************************
+ * mm_init
+ ***********************************************************************/
+void __init bhv_mm_init_file_protection(void)
+{
+	unsigned long r;
+
+	HypABI__FileProtection__Init__arg__T *bhv_arg;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	bhv_arg = HypABI__FileProtection__Init__arg__ALLOC();
+
+	r = HypABI__FileProtection__Init__hypercall_noalloc(bhv_arg);
+	if (r) {
+		pr_err("File protection init failed");
+	} else {
+		bhv_file_protection_config = bhv_arg->feature_bitmap;
+	}
+
+	HypABI__FileProtection__Init__arg__FREE(bhv_arg);
+}
+/***********************************************************************/
+
+#define READ_ONLY_FUNC(T)                                                     \
+	bool bhv_block_read_only_file_write_##T(const char *target)           \
+	{                                                                     \
+		unsigned long r;                                              \
+		bool rv;                                                      \
+		HypABI__FileProtection__##T##__arg__T *volatile violation =   \
+			HypABI__FileProtection__##T##__arg__ALLOC();          \
+                                                                              \
+		rv = populate_event_context(&violation->context, true);       \
+		if (rv) {                                                     \
+			bhv_fail("%s: BHV cannot retrieve event context",     \
+				 __FUNCTION__);                               \
+		}                                                             \
+                                                                              \
+		/* Setup arg */                                               \
+		violation->name_len =                                         \
+			strlen(target) + 1 /* NULL terminator */;             \
+		violation->name = bhv_virt_to_phys((void *)target);           \
+                                                                              \
+		/* Hypercall */                                               \
+		r = HypABI__FileProtection__##T##__hypercall_noalloc(         \
+			violation);                                           \
+		if (r) {                                                      \
+			pr_err("file protection violation hypercall failed"); \
+			rv = true;                                            \
+		} else {                                                      \
+			/* Note: in case of dirtycred, "block" halts */       \
+			/* the guest with a panic */                          \
+			/* Read block and free */                             \
+			rv = (bool)violation->block;                          \
+		}                                                             \
+		HypABI__FileProtection__##T##__arg__FREE(violation);          \
+                                                                              \
+		return rv;                                                    \
+	}
+
+READ_ONLY_FUNC(ViolationWriteReadOnlyFile)
+READ_ONLY_FUNC(ViolationDirtyCredWrite)
+
+#undef READ_ONLY_FUNC
\ No newline at end of file
diff --git security/bhv/fileops_protection.c security/bhv/fileops_protection.c
new file mode 100644
index 00000000000..073f03eaf7d
--- /dev/null
+++ security/bhv/fileops_protection.c
@@ -0,0 +1,360 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <uapi/linux/magic.h>
+#include <linux/sort.h>
+#include <linux/bsearch.h>
+#include <linux/moduleparam.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/file_protection.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/fileops_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/guestlog.h>
+
+#include <asm/sections.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+static bool bhv_strict_fileops __ro_after_init = false;
+
+bool bhv_strict_fileops_enforced(void)
+{
+	return bhv_strict_fileops;
+}
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+u8 bhv_fileops_type(u32 fs_magic)
+{
+	switch (fs_magic) {
+#if defined CONFIG_EXT4_FS || defined VASKM
+	case EXT4_SUPER_MAGIC:
+		return FT(EXT4);
+#endif
+#if defined CONFIG_XFS_FS || defined VASKM
+	case XFS_SUPER_MAGIC:
+		return FT(XFS);
+#endif
+	case TMPFS_MAGIC:
+		return FT(TMPFS);
+	case PROC_SUPER_MAGIC:
+		return FT(PROC);
+	case CGROUP2_SUPER_MAGIC:
+		fallthrough;
+	case SYSFS_MAGIC:
+		return FT(SYSFS);
+	case DEBUGFS_MAGIC:
+		return FT(DEBUGFS);
+	default:
+		return FT(UNSUPPORTED);
+	}
+}
+#undef FT
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod && ((unsigned long)mod->mem[MOD_RODATA].base <= addr) &&
+	       ((unsigned long)mod->mem[MOD_RODATA].base +
+			mod->mem[MOD_RODATA].size >
+		addr);
+}
+#else
+static inline bool _is_module_ro_data(unsigned long addr, struct module *mod)
+{
+	return mod &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.text_size <=
+		addr) &&
+	       ((unsigned long)mod->core_layout.base +
+			mod->core_layout.ro_size >
+		addr);
+}
+#endif
+
+bool bhv_fileops_is_ro(u64 f_op)
+{
+	struct module *mod;
+	if (f_op >= KLN_SYM(__start_rodata) && f_op < KLN_SYM(__end_rodata))
+		return true;
+
+	preempt_disable();
+	mod = __module_address(f_op);
+	preempt_enable();
+	if (mod == NULL)
+		return false;
+
+	if (_is_module_ro_data(f_op, mod))
+		return true;
+
+	return false;
+}
+
+bool bhv_block_fileops(const char *target, u8 fops_type, bool is_dir,
+		       const void *fops_ptr)
+{
+	unsigned long err;
+	bool retval;
+	HypABI__FileProtection__ViolationFileOps__arg__T *volatile violation;
+	size_t path_sz;
+	int rv;
+
+	if (!bhv_fileops_file_protection_is_enabled())
+		return false;
+
+	violation = HypABI__FileProtection__ViolationFileOps__arg__ALLOC();
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ) {
+		path_sz =
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ -
+			1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->fops_type = fops_type;
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+	violation->fops_ptr = (uint64_t)fops_ptr;
+
+	// pr_err("Bad fops %d %s %d %px %pS\n", violation->fops_type, violation->path_name, violation->is_dir, (void*)violation->fops_ptr, (void*)violation->fops_ptr);
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = HypABI__FileProtection__ViolationFileOps__hypercall_noalloc(
+		violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+		return true;
+	}
+
+	// block is set by host, might need to be volatile when set in guest
+	retval = (bool)violation->block;
+	HypABI__FileProtection__ViolationFileOps__arg__FREE(violation);
+
+	return retval;
+}
+
+#ifndef VASKM // inside kernel tree
+
+const fops_t fileops_map[] __section(".rodata") = {
+#define FOPS_MAP(_, idx, file_ops, dir_ops) [idx] = { &file_ops, &dir_ops },
+#define FOPS_MAP_DIRNULL(_, idx, file_ops) [idx] = { &file_ops, NULL },
+#include <bhv/fileops_internal_fopsmap.h>
+};
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[] __ro_after_init = {
+#define FOPS(_)
+#define FOPS_PROC(sym) &sym,
+#include <bhv/fileops_internal_symlist.h>
+};
+
+#define init_fileops_data()
+
+#else // out of tree
+fops_t *fileops_map;
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[0
+#define FOPS(_)
+#define FOPS_PROC(_) +1
+#include <bhv/fileops_internal_symlist.h>
+] __ro_after_init = {};
+
+static void init_fileops_data(void)
+{
+	// Here we use UNSUPPORTED for simplicity. If this assert fails, we
+	// will have to write a more clever check. For now it works fine.
+	static_assert(
+		sizeof(fops_t) *
+			HypABI__FileProtection__ViolationFileOps__FopsType__UNSUPPORTED <=
+		PAGE_SIZE);
+	fileops_map = (fops_t *)vzalloc(PAGE_SIZE);
+	BUG_ON(!fileops_map);
+	BUG_ON(!PAGE_ALIGNED(fileops_map));
+
+#define KFOPS(sym) KLN_SYMBOL_P(const struct file_operations *, sym)
+#define FOPS_MAP(_, idx, file_ops, dir_ops)    \
+	fileops_map[idx][0] = KFOPS(file_ops); \
+	fileops_map[idx][1] = KFOPS(dir_ops);
+#define FOPS_MAP_DIRNULL(_, idx, file_ops)     \
+	fileops_map[idx][0] = KFOPS(file_ops); \
+	fileops_map[idx][1] = NULL;
+#include <bhv/fileops_internal_fopsmap.h>
+
+	int c = 0;
+#define FOPS(_)
+#define FOPS_PROC(sym) proc_fops[c++] = KFOPS(sym);
+#include <bhv/fileops_internal_symlist.h>
+	BUG_ON(c != ARRAY_SIZE(proc_fops));
+}
+#endif // VASKM
+
+int cmp_fileops(const void *a_ptr, const void *b_ptr)
+{
+	u64 a = *(u64 *)a_ptr;
+	u64 b = *(u64 *)b_ptr;
+	return ((a == b) ? 0 : (a > b ? 1 : -1));
+}
+
+/***************************************************************
+ * init
+ ***************************************************************/
+void __init bhv_init_fileops(void)
+{
+	static HypABI__Confserver__StrictFileops__arg__T bhv_arg_fb;
+	int rc;
+
+	HypABI__Confserver__StrictFileops__arg__T *bhv_arg =
+		HypABI__Confserver__StrictFileops__arg__ALLOC_STATICFALLBACK(
+			bhv_arg_fb);
+
+	init_fileops_data();
+
+	rc = HypABI__Confserver__StrictFileops__hypercall_noalloc(bhv_arg);
+
+	if (rc == 0) {
+		bhv_strict_fileops = bhv_arg->strict_fileops;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__StrictFileops__arg__FREE_STATICFALLBACK(bhv_arg,
+								    bhv_arg_fb);
+
+	if (rc == 0 && bhv_file_protection_is_enabled()) {
+		sort(proc_fops, ARRAY_SIZE(proc_fops), sizeof(proc_fops[0]),
+		     cmp_fileops, NULL);
+	}
+}
+/***************************************************************/
+
+bool is_valid_proc_fop(const struct file_operations **fop_ptr)
+{
+	if (bsearch(fop_ptr, proc_fops, ARRAY_SIZE(proc_fops),
+		    sizeof(proc_fops[0]), cmp_fileops))
+		return true;
+
+	return false;
+}
+
+#ifndef VASKM // inside kernel tree
+extern int full_proxy_release(struct inode *inode, struct file *filp);
+extern loff_t full_proxy_llseek(struct file *, loff_t, int);
+extern ssize_t full_proxy_read(struct file *, char __user *, size_t, loff_t *);
+extern ssize_t full_proxy_write(struct file *, const char __user *, size_t,
+				loff_t *);
+extern __poll_t full_proxy_poll(struct file *, struct poll_table_struct *);
+extern long full_proxy_unlocked_ioctl(struct file *, unsigned int,
+				      unsigned long);
+#endif
+
+bool is_valid_debugfs_fop(const struct file_operations *fop_ptr)
+{
+	size_t i;
+
+	if (fop_ptr == NULL)
+		return false;
+
+	// Check whether the fop points to one of three proxy fops
+	// see fs/debugfs/file.c
+	if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+				    debugfs_noop_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_open_proxy_file_operations))
+		return true;
+	else if (fop_ptr == KLN_SYMBOL_P(const struct file_operations *,
+					 debugfs_full_proxy_file_operations))
+		return true;
+
+	// No. This should be a dynamically allocated fops struct.
+	// We will validate the pointer within fops.
+	// See __full_proxy_fops_init in fs/debugfs/file.c
+	if (fop_ptr->llseek != NULL &&
+	    (u64)fop_ptr->llseek != KLN_SYM(full_proxy_llseek))
+		return false;
+
+	if (fop_ptr->read != NULL &&
+	    (u64)fop_ptr->read != KLN_SYM(full_proxy_read))
+		return false;
+
+	if (fop_ptr->write != NULL &&
+	    (u64)fop_ptr->write != KLN_SYM(full_proxy_write))
+		return false;
+
+	if (fop_ptr->read_iter != NULL)
+		return false;
+
+	if (fop_ptr->write_iter != NULL)
+		return false;
+
+	if (fop_ptr->iopoll != NULL)
+		return false;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 5, 0)
+	if (fop_ptr->iterate != NULL)
+		return false;
+#endif
+
+	if (fop_ptr->iterate_shared != NULL)
+		return false;
+
+	if (fop_ptr->poll != NULL &&
+	    (u64)fop_ptr->poll != KLN_SYM(full_proxy_poll))
+		return false;
+
+	if (fop_ptr->unlocked_ioctl != NULL &&
+	    (u64)fop_ptr->unlocked_ioctl != KLN_SYM(full_proxy_unlocked_ioctl))
+		return false;
+
+	if (fop_ptr->compat_ioctl != NULL)
+		return false;
+
+	if (fop_ptr->mmap != NULL)
+		return false;
+
+	if (fop_ptr->open != NULL)
+		return false;
+
+	if (fop_ptr->flush != NULL)
+		return false;
+
+	if ((u64)fop_ptr->release != KLN_SYM(full_proxy_release))
+		return false;
+
+	// Remainder of pointers must be NULL as well
+	for (i = offsetof(struct file_operations, fsync);
+	     i < sizeof(struct file_operations); i++) {
+		if (((uint8_t *)fop_ptr)[i] != '\0') {
+			return false;
+		}
+	}
+
+	return true;
+}
diff --git security/bhv/guestconn.c security/bhv/guestconn.c
new file mode 100644
index 00000000000..e3691a93c40
--- /dev/null
+++ security/bhv/guestconn.c
@@ -0,0 +1,279 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#define bhv_guestconn_configured true
+#else // out of tree
+static bool bhv_guestconn_configured = false;
+#endif // VASKM
+
+typedef struct {
+	struct list_head list;
+	size_t to_send;
+	GuestConnABI__Header__T *msg;
+} bhv_guestconn_send_item_t;
+
+uint32_t bhv_guestconn_cid __ro_after_init = 0;
+uint32_t bhv_guestconn_port __ro_after_init = 0;
+
+static struct socket *vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+static LIST_HEAD(bhv_guestconn_msg_list);
+static DEFINE_RAW_SPINLOCK(bhv_guestconn_msg_lock);
+
+static struct workqueue_struct *bhv_guestconn_workqueue = NULL;
+static struct work_struct bhv_guestconn_work_struct;
+static struct delayed_work bhv_guestconn_delayed_work_struct;
+static struct kmem_cache *bhv_guestconn_send_item_cache;
+
+GuestConnABI__Header__T *bhv_guestconn_alloc_msg(void)
+{
+	_Static_assert(GuestConnABI__MAX_MSG_SZ <= PAGE_SIZE,
+		       "GuestConnABI__MAX_MSG_SZ > PAGE_SIZE");
+	GuestConnABI__Header__T *rv =
+		(GuestConnABI__Header__T *)__get_free_page(GFP_KERNEL);
+	memset(rv, 0, PAGE_SIZE);
+	return rv;
+}
+
+void bhv_guestconn_free_msg(GuestConnABI__Header__T *msg)
+{
+	free_page((unsigned long)msg);
+}
+
+static inline size_t bhv_send(void *data, size_t size, size_t to_send)
+{
+	int r;
+	struct kvec vec;
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+		r = kernel_sendmsg(vsock, &msghdr, &vec, 1, vec.iov_len);
+		if (r == -EAGAIN) {
+			return to_send;
+		} else if (r < 0) {
+			pr_err("BHV GuestLog: Send Failed (%d)", r);
+			return 0;
+		}
+		to_send -= r;
+	}
+	return 0;
+}
+
+static void bhv_guestconn_sendmsg(struct work_struct *ws)
+{
+	bhv_guestconn_send_item_t *item;
+	unsigned long flags;
+
+	while (true) {
+		raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+		if (list_empty(&bhv_guestconn_msg_list)) {
+			raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock,
+						   flags);
+			return;
+		}
+		item = list_first_entry(&bhv_guestconn_msg_list,
+					bhv_guestconn_send_item_t, list);
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+		item->to_send =
+			bhv_send(item->msg, item->msg->sz, item->to_send);
+
+		if (item->to_send == 0 ||
+		    unlikely(atomic_read(&reboot_in_progress))) {
+			raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+			list_del(&(item->list));
+			raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock,
+						   flags);
+			bhv_guestconn_free_msg(item->msg);
+			kmem_cache_free(bhv_guestconn_send_item_cache, item);
+		} else {
+			queue_delayed_work(bhv_guestconn_workqueue,
+					   &bhv_guestconn_delayed_work_struct,
+					   msecs_to_jiffies(1000));
+			return;
+		}
+	}
+}
+
+int bhv_guestconn_send(uint16_t backend, GuestConnABI__Header__T *msg,
+		       size_t size)
+{
+	bhv_guestconn_send_item_t *cur;
+	unsigned long flags;
+
+	if (!bhv_guestconn_configured)
+		return 0;
+
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+
+	pr_debug("BHV GuestConn: Queuing msg for backend %u with size %lu",
+		 backend, size);
+
+	BUG_ON(!bhv_guestconn_send_item_cache);
+
+	cur = kmem_cache_alloc(bhv_guestconn_send_item_cache, GFP_ATOMIC);
+	if (cur == NULL) {
+		bhv_fail("BHV: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	cur->msg = msg;
+
+	cur->msg->backend = backend;
+	cur->msg->sz = GuestConnABI__Header__SZ + size;
+
+	cur->to_send = cur->msg->sz;
+
+	raw_spin_lock_irqsave(&bhv_guestconn_msg_lock, flags);
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+		kmem_cache_free(bhv_guestconn_send_item_cache, cur);
+		bhv_guestconn_free_msg(msg);
+		return 0;
+	}
+
+	list_add_tail(&(cur->list), &bhv_guestconn_msg_list);
+	raw_spin_unlock_irqrestore(&bhv_guestconn_msg_lock, flags);
+
+	if (atomic_read(&workqueue_ready))
+		queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+
+	return 0;
+}
+
+static int bhv_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	int expected = 0;
+	if (atomic_read(&workqueue_ready) && atomic_try_cmpxchg(&reboot_in_progress, &expected, 1)) {
+		// Cancel pending work.
+		cancel_delayed_work_sync(&bhv_guestconn_delayed_work_struct);
+
+		// Drain the workqueue
+		drain_workqueue(bhv_guestconn_workqueue);
+
+		// We assume all messages are gone now and we shut down the socket
+		sock_release(vsock);
+		vsock = NULL;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bhv_guestconn_reboot_notifier = {
+	.notifier_call = bhv_guestconn_reboot,
+	.priority = 0,
+};
+
+/**********************************************************
+ * start
+ **********************************************************/
+void bhv_start_guestconn(void)
+{
+	int err;
+	struct sockaddr_vm addr;
+
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	vsock_addr_init(&addr, bhv_guestconn_cid, bhv_guestconn_port);
+	pr_info("bhv guestconn started with cid %u, port %u", bhv_guestconn_cid,
+		bhv_guestconn_port);
+#ifdef VASKM // out of tree
+	bhv_guestconn_configured = true;
+#endif // VASKM
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not create kernel socket (%d)", err);
+		return;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not connect to host (%d)", err);
+		return;
+	}
+
+	// Initialize work queue
+	INIT_WORK(&bhv_guestconn_work_struct, bhv_guestconn_sendmsg);
+	INIT_DELAYED_WORK(&bhv_guestconn_delayed_work_struct,
+			  bhv_guestconn_sendmsg);
+	bhv_guestconn_workqueue =
+		alloc_workqueue("bhv_guestconn_workqueue", WQ_UNBOUND, 1);
+	// queue = create_singlethread_workqueue("bhv_guestlog_work_queue");
+	if (bhv_guestconn_workqueue == NULL) {
+		bhv_fail("BHV: Could not allocate work queue!");
+		kmem_cache_destroy(bhv_guestconn_send_item_cache);
+		return;
+	}
+	atomic_inc(&workqueue_ready);
+
+	register_reboot_notifier(&bhv_guestconn_reboot_notifier);
+
+	queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+}
+/**********************************************************/
+
+/**********************************************************
+ * mm_init
+ **********************************************************/
+void __init bhv_mm_init_guestconn(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	// Create cache
+	bhv_guestconn_send_item_cache = kmem_cache_create(
+		"bhv_guestconn_send_item_cache",
+		sizeof(bhv_guestconn_send_item_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_guestconn_send_item_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+}
+/**********************************************************/
+
+/**********************************************************
+ * init
+ **********************************************************/
+int __init bhv_init_guestconn(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	bhv_guestconn_cid = cid;
+	bhv_guestconn_port = port;
+	return 0;
+}
+/**********************************************************/
diff --git security/bhv/guestlog.c security/bhv/guestlog.c
new file mode 100644
index 00000000000..9d7928c60e9
--- /dev/null
+++ security/bhv/guestlog.c
@@ -0,0 +1,468 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/types.h>
+
+#include <linux/cgroup.h>
+#include <linux/ipc_namespace.h>
+#include <linux/mnt_namespace.h>
+#include <linux/pid_namespace.h>
+#include <linux/time_namespace.h>
+#include <linux/user_namespace.h>
+#include <linux/utsname.h>
+#include <net/net_namespace.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+HypABI__Guestlog__Init__arg__T bhv_guestlog_config __ro_after_init = { 0,
+								       false };
+
+#define BHV_GUESTLOG_MAX_PAYLOAD_SZ \
+	BHV_GUESTCONN_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__Message__SZ
+
+#define BHV_GUESTLOG_MAX_BUF_SZ(EVT_T) \
+	BHV_GUESTLOG_MAX_PAYLOAD_SZ - GuestConnABI__GuestLog__##EVT_T##__SZ
+
+static inline uint16_t bhv_guestlog_calc_msg_sz(uint16_t event_id,
+						size_t buf_sz)
+{
+	size_t size = GuestConnABI__GuestLog__Message__SZ;
+	switch (event_id) {
+	case GuestConnABI__GuestLog__StringMsg__EVT_ID:
+		size += GuestConnABI__GuestLog__StringMsg__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessFork__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessFork__SZ;
+		break;
+	case GuestConnABI__GuestLog__ProcessExec__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__ProcessExit__EVT_ID:
+		size += GuestConnABI__GuestLog__ProcessExit__SZ;
+		break;
+	case GuestConnABI__GuestLog__DriverLoad__EVT_ID:
+		size += GuestConnABI__GuestLog__DriverLoad__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelAccess__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelAccess__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__FopsUnknown__EVT_ID:
+		size += GuestConnABI__GuestLog__FopsUnknown__SZ;
+		break;
+	case GuestConnABI__GuestLog__KernelExec__EVT_ID:
+		size += GuestConnABI__GuestLog__KernelExec__SZ;
+		BUG_ON(buf_sz != 0);
+		break;
+	case GuestConnABI__GuestLog__CgroupCreate__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupCreate__SZ;
+		break;
+	case GuestConnABI__GuestLog__CgroupDestroy__EVT_ID:
+		size += GuestConnABI__GuestLog__CgroupDestroy__SZ;
+		break;
+	case GuestConnABI__GuestLog__NamespaceChange__EVT_ID:
+		size += GuestConnABI__GuestLog__NamespaceChange__SZ;
+		break;
+	default:
+		BUG();
+	}
+	size += buf_sz;
+	BUG_ON(size > BHV_GUESTCONN_MAX_PAYLOAD_SZ);
+	return size;
+}
+
+#define ALLOC_MSG(EVT, EVT_T)                                            \
+	GuestConnABI__Header__T *__gc_hdr;                               \
+	GuestConnABI__GuestLog__Message__T *__msg;                       \
+	GuestConnABI__GuestLog__##EVT_T##__T *EVT;                       \
+                                                                         \
+	__gc_hdr = bhv_guestconn_alloc_msg();                            \
+	if (!__gc_hdr)                                                   \
+		return -ENOMEM;                                          \
+                                                                         \
+	__msg = (GuestConnABI__GuestLog__Message__T *)__gc_hdr->payload; \
+	EVT = (GuestConnABI__GuestLog__##EVT_T##__T *)__msg->payload
+
+#define SEND_MSG(EVT, EVT_T, BUF_SIZE, FULL_PATH)                              \
+	__msg->header.evt = GuestConnABI__GuestLog__##EVT_T##__EVT_ID;         \
+	__msg->header.sz = bhv_guestlog_calc_msg_sz(                           \
+		GuestConnABI__GuestLog__##EVT_T##__EVT_ID, BUF_SIZE);          \
+                                                                               \
+	if (populate_event_context(&__msg->context, FULL_PATH)) {              \
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__); \
+	}                                                                      \
+                                                                               \
+	return bhv_guestconn_send(GuestConnABI__GuestLog__BACKEND_ID,          \
+				  __gc_hdr, __msg->header.sz)
+
+#define SEND_MSG_STRLEN(EVT, EVT_T, FULL_PATH)                          \
+	SEND_MSG(EVT, EVT_T,                                            \
+		 strnlen(EVT->buf, BHV_GUESTLOG_MAX_BUF_SZ(EVT_T)) + 1, \
+		 FULL_PATH);
+
+int bhv_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+
+	ALLOC_MSG(evt, StringMsg);
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(evt->buf, BHV_GUESTLOG_MAX_BUF_SZ(StringMsg), fmt,
+			 args);
+	va_end(args);
+
+	SEND_MSG(evt, StringMsg, len, true);
+}
+
+#define NS_DEREF(NS) (NS) ? (NS)->ns.inum : 0
+static void _bhv_get_incoming_ns_inums(struct task_struct *tsk,
+				       struct nsset *nsset,
+				       HypABI__Context__Inums__T *inums)
+{
+	if (nsset->nsproxy) {
+		inums->cgroup_ns_inum = NS_DEREF(nsset->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(nsset->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(nsset->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(nsset->nsproxy->net_ns);
+		inums->time_ns_inum = NS_DEREF(nsset->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->time_ns_for_children);
+		inums->uts_ns_inum = NS_DEREF(nsset->nsproxy->uts_ns);
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(nsset->nsproxy->pid_ns_for_children);
+	} else {
+		inums->cgroup_ns_inum = NS_DEREF(tsk->nsproxy->cgroup_ns);
+		inums->ipc_ns_inum = NS_DEREF(tsk->nsproxy->ipc_ns);
+		inums->mnt_ns_inum = from_mnt_ns(tsk->nsproxy->mnt_ns)->inum;
+		inums->net_ns_inum = NS_DEREF(tsk->nsproxy->net_ns);
+		inums->time_ns_inum = NS_DEREF(tsk->nsproxy->time_ns);
+		inums->time_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->time_ns_for_children);
+		inums->uts_ns_inum = NS_DEREF(tsk->nsproxy->uts_ns);
+		inums->pid_for_children_ns_inum =
+			NS_DEREF(tsk->nsproxy->pid_ns_for_children);
+	}
+
+	inums->pid_ns_inum = NS_DEREF(task_active_pid_ns(tsk));
+	if (nsset->flags & CLONE_NEWUSER) {
+		inums->user_ns_inum = NS_DEREF(nsset->cred->user_ns);
+	} else {
+		rcu_read_lock();
+		inums->user_ns_inum = NS_DEREF(__task_cred(tsk)->user_ns);
+		rcu_read_unlock();
+	}
+}
+#undef NS_DEREF
+
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm)
+{
+	ALLOC_MSG(evt, ProcessFork);
+
+	evt->child_pid = child_pid;
+	evt->parent_pid = parent_pid;
+	evt->child_comm_offset = 0;
+	strscpy(evt->buf, child_comm, TASK_COMM_LEN);
+	evt->parent_comm_offset = strnlen(evt->buf, TASK_COMM_LEN) + 1;
+	strscpy(&(evt->buf[evt->parent_comm_offset]), parent_comm,
+		TASK_COMM_LEN);
+
+	SEND_MSG(evt, ProcessFork,
+		 evt->parent_comm_offset +
+			 strnlen(&evt->buf[evt->parent_comm_offset],
+				 TASK_COMM_LEN) +
+			 1,
+		 true);
+}
+
+static inline struct page *bhv_get_page(struct linux_binprm *bprm,
+					unsigned long addr)
+{
+	struct page *page;
+#ifdef CONFIG_MMU
+	/*
+		 * This is called at execve() time in order to dig around
+		 * in the argv/environment of the new proceess
+		 * (represented by bprm).  'current' is the process doing
+		 * the execve().
+		 */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0)
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL) <=
+	    0)
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+	if (get_user_pages_remote(bprm->mm, addr, 1, FOLL_FORCE, &page, NULL,
+				  NULL) <= 0)
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0) */
+		return NULL;
+#else /* CONFIG_MMU */
+	page = bprm->page[addr / PAGE_SIZE];
+#endif /* CONFIG_MMU */
+	return page;
+}
+
+static uint32_t bhv_get_args_env(struct linux_binprm *bprm,
+				 uint32_t start_offset, uint32_t count,
+				 char *dst, size_t dst_sz)
+{
+	uint32_t rv = 0;
+	char *kaddr;
+	struct page *page;
+	int cur = 0;
+	unsigned long i, j;
+	unsigned long pos = bprm->p + start_offset;
+	unsigned int offset = pos % PAGE_SIZE;
+
+	if (dst_sz <= 0)
+		return 0;
+
+	if (count <= 0) {
+		dst[0] = '\0';
+		return 1;
+	}
+
+	page = bhv_get_page(bprm, pos);
+	if (page == NULL) {
+		pr_err("BHV: unable to find user page\n");
+		return rv;
+	}
+	kaddr = kmap_atomic(page);
+
+	for (i = 0, j = 0; i < dst_sz; i++) {
+		dst[i] = *(char *)(kaddr + (offset + j));
+
+		if (dst[i] == '\0') {
+			cur++;
+			if (cur == count) {
+				rv = i + 1;
+				break;
+			}
+			dst[i] = ' ';
+		}
+
+		if ((offset + j + 1) >= PAGE_SIZE) {
+			kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+			put_page(page);
+#endif
+
+			page = bhv_get_page(bprm, pos + i);
+			if (page == NULL) {
+				pr_err("BHV: unable to find user page\n");
+				dst[dst_sz - 1] = '\0';
+				return i + 1;
+			}
+			kaddr = kmap_atomic(page);
+
+			offset = 0;
+			j = 0;
+		} else {
+			j++;
+		}
+	}
+	dst[dst_sz - 1] = '\0';
+
+	kunmap_atomic(kaddr);
+#ifdef CONFIG_MMU
+	put_page(page);
+#endif
+	return rv;
+}
+
+int bhv_guestlog_log_process_exec(struct linux_binprm *bprm, uint32_t pid,
+				  uint32_t parent_pid, const char *comm)
+{
+	uint32_t env_offset;
+
+	ALLOC_MSG(evt, ProcessExec);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	strscpy(evt->name, comm, TASK_COMM_LEN);
+	env_offset =
+		bhv_get_args_env(bprm, 0, bprm->argc, evt->args,
+				 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+	bhv_get_args_env(bprm, env_offset, bprm->envc, evt->env,
+			 GuestConnABI__GuestLog__PROC_EXEC_ARGS_ENV_SZ);
+
+	SEND_MSG(evt, ProcessExec, 0, true);
+}
+
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	ALLOC_MSG(evt, ProcessExit);
+
+	evt->pid = pid;
+	evt->parent_pid = parent_pid;
+	strscpy(evt->buf, comm, TASK_COMM_LEN);
+
+	SEND_MSG_STRLEN(evt, ProcessExit, false);
+}
+
+int bhv_guestlog_log_cgroup_create(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupCreate);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupCreate, 0, true);
+}
+
+int bhv_guestlog_log_cgroup_destroy(struct cgroup *cgrp)
+{
+	ALLOC_MSG(evt, CgroupDestroy);
+
+	evt->cgroup_id = cgroup_id(cgrp);
+	cgroup_name(cgrp, evt->cgroup_name,
+		    HypABI__Context__CGroupInfo__PROC_CGROUP_NAME_SZ);
+
+	SEND_MSG(evt, CgroupDestroy, 0, true);
+}
+
+int bhv_guestlog_log_namespace_change(struct task_struct *tsk,
+				      struct nsset *nsset)
+{
+	ALLOC_MSG(evt, NamespaceChange);
+
+	evt->target_task_pid = tsk->pid;
+	strncpy(evt->target_task_name, tsk->comm,
+		GuestConnABI__GuestLog__PROC_COMM_SZ);
+
+	_bhv_get_incoming_ns_inums(tsk, nsset, &evt->incoming_inums);
+
+	SEND_MSG(evt, NamespaceChange, 0, true);
+}
+
+int bhv_guestlog_log_driver_load(const char *name)
+{
+	ALLOC_MSG(evt, DriverLoad);
+
+	strscpy(evt->buf, name, BHV_GUESTLOG_MAX_BUF_SZ(DriverLoad));
+
+	SEND_MSG_STRLEN(evt, DriverLoad, true);
+}
+
+int bhv_guestlog_log_kaccess(uint64_t addr, uint8_t type)
+{
+	ALLOC_MSG(evt, KernelAccess);
+
+	evt->address = addr;
+	evt->type = type;
+
+	SEND_MSG(evt, KernelAccess, 0, true);
+}
+
+int bhv_guestlog_log_fops_unknown(uint32_t magic, const char *pathname,
+				  uint8_t type, uint32_t major, uint64_t minor,
+				  uint64_t fops_ptr)
+{
+	ALLOC_MSG(evt, FopsUnknown);
+
+	evt->magic = (u64)magic;
+	evt->struct_type = type;
+	evt->special_major = major;
+	evt->special_minor = minor;
+	evt->address = (u64)fops_ptr;
+	strscpy(evt->buf, pathname, BHV_GUESTLOG_MAX_BUF_SZ(FopsUnknown));
+
+	SEND_MSG_STRLEN(evt, FopsUnknown, true);
+}
+
+static void bhv_guestlog_char_array_into_buf(char *dst, size_t dst_size,
+					     char **arr)
+{
+	char *cur;
+	unsigned int i;
+	ssize_t cur_len = 0;
+	ssize_t total_len = 0;
+
+	BUG_ON(dst == NULL);
+
+	if (arr == NULL) {
+		if (dst_size > 0)
+			dst[0] = '\0';
+
+		return;
+	}
+
+	for (i = 0, cur = arr[0]; cur != NULL; i++, cur = arr[i]) {
+		if (i != 0) {
+			// Replace NULL of previous round with a space
+			dst[total_len] = ' ';
+			// Increase len since strscpy returns len without NULL
+			total_len++;
+		}
+
+		cur_len = strscpy(&dst[total_len], cur, dst_size);
+
+		// No more room. We are done
+		if (cur_len == -E2BIG)
+			return;
+
+		// Update size
+		dst_size -= cur_len;
+		total_len += cur_len;
+
+		// No more room. We are done. The last character is '\0'
+		if (dst_size <= 1)
+			return;
+	}
+}
+
+int bhv_guestlog_log_kernel_exec(const char *path, char **argv, char **envp)
+{
+	ALLOC_MSG(evt, KernelExec);
+
+	strscpy(evt->path, path, sizeof(evt->path));
+	bhv_guestlog_char_array_into_buf(evt->args, sizeof(evt->args), argv);
+	bhv_guestlog_char_array_into_buf(evt->env, sizeof(evt->env), envp);
+
+	SEND_MSG(evt, KernelExec, 0, true);
+}
+
+/*****************************************************************
+ * init
+ *****************************************************************/
+int __init bhv_init_guestlog()
+{
+	static HypABI__Guestlog__Init__arg__T glc_fb;
+	HypABI__Guestlog__Init__arg__T *glc;
+	int rc;
+
+	if (!bhv_guestlog_enabled())
+		return 0;
+
+	glc = HypABI__Guestlog__Init__arg__ALLOC_STATICFALLBACK(glc_fb);
+
+	rc = HypABI__Guestlog__Init__hypercall_noalloc(glc);
+	if (rc == 0) {
+		bhv_guestlog_config = *glc;
+	} else {
+		bhv_fail("BHV: guestlog init failed");
+	}
+
+	HypABI__Guestlog__Init__arg__FREE_STATICFALLBACK(glc, glc_fb);
+
+	return rc;
+}
+/***************************************************************/
diff --git security/bhv/init/init.c security/bhv/init/init.c
new file mode 100644
index 00000000000..e59c9857ac8
--- /dev/null
+++ security/bhv/init/init.c
@@ -0,0 +1,554 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/syscall.h>
+#include <linux/jump_label.h>
+#include <linux/kmod.h>
+#include <linux/mm.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv_print.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/creds.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/init/init.h>
+#include <bhv/integrity.h>
+#include <bhv/creds.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_comm_start[];
+extern char __bhv_vault_comm_end[];
+#endif
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+extern char __bhv_vault_text_jump_label_start[];
+extern char __bhv_vault_text_jump_label_end[];
+
+extern char __start_static_call_sites[];
+extern char __stop_static_call_sites[];
+extern char __start_static_call_tramp_key[];
+extern char __stop_static_call_tramp_key[];
+
+extern char __alt_instructions[];
+extern char __alt_instructions_end[];
+extern char __retpoline_sites[];
+extern char __retpoline_sites_end[];
+extern char __return_sites[];
+extern char __return_sites_end[];
+
+extern char __start_bhv_tp_vault[];
+extern char __end_bhv_tp_vault[];
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+
+extern uint8_t __bhv_text_start[];
+extern uint8_t __bhv_text_end[];
+#endif // VASKM
+
+bool __bhv_init_done __ro_after_init = false;
+
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+
+#ifdef CONFIG_RETPOLINE
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)			\
+{									\
+        extern char __indirect_thunk_start[];                           \
+        extern char __indirect_thunk_end[];                             \
+									\
+	vault->thunks.gpa = bhv_virt_to_phys(__indirect_thunk_start); 	\
+	vault->thunks.size =                                         	\
+        __indirect_thunk_end - __indirect_thunk_start;                  \
+}
+#else /* !CONFIG_RETPOLINE */
+#define BHV_VAULT_REGISTER_INDIRECT_THUNKS(vault)			\
+{									\
+	vault->thunks.gpa = 0;						\
+	vault->thunks.size = 0;						\
+}
+#endif /* CONFIG_RETPOLINE */
+
+#define BHV_VAULT_REGISTER(vault, buf, buf_size)                        \
+{                                                                       \
+        extern char __bhv_vault_text_##vault##_start[];                 \
+        extern char __bhv_vault_text_##vault##_end[];                   \
+        extern char __bhv_vault_ref_text_##vault##_start[];             \
+        extern char __bhv_vault_ref_text_##vault##_end[];               \
+        extern char __bhv_vault_shared_text_##vault##_start[];          \
+        extern char __bhv_vault_shared_text_##vault##_end[];            \
+        extern char __bhv_vault_data_##vault##_start[];                 \
+        extern char __bhv_vault_data_##vault##_end[];                   \
+        extern char __bhv_vault_ro_data_##vault##_start[];              \
+        extern char __bhv_vault_ro_data_##vault##_end[];                \
+        extern char __altinstr_aux_start[];                             \
+        extern char __altinstr_aux_end[];                               \
+        extern char __noinstr_text_start[];                             \
+        extern char __noinstr_text_end[];                               \
+        uint32_t ep_ctr = 0;                                            \
+        uint32_t rp_ctr = 0;                                            \
+        uint32_t __cur_ctr = 0;                                         \
+        uint32_t __i = 0;                                               \
+        int __r = 0;                                                    \
+                                                                        \
+	HypABI__Wagner__Create__arg__T *_vault =                        \
+        (HypABI__Wagner__Create__arg__T *)buf;                          \
+                                                                        \
+	bhv_vault_return_point_helper_t *ret;                           \
+	bhv_vault_entry_point_helper_t *entry;                          \
+	BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                    \
+	{                                                               \
+        	ep_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +                \
+        			sizeof(uint64_t) * ep_ctr) > buf_size); \
+                                                                        \
+	_vault->nr_entry_points = ep_ctr;                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_ENTRY_POINT(vault, entry)                    \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys(entry->ep);           \
+        	__cur_ctr++;                                            \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                     \
+	{                                                               \
+        	rp_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                    \
+	{                                                               \
+        	rp_ctr++;                                               \
+	}                                                               \
+                                                                        \
+	BUG_ON((sizeof(HypABI__Wagner__Create__arg__T) +                \
+               (sizeof(uint64_t) * ep_ctr) +                            \
+               (sizeof(uint64_t) * rp_ctr)) > buf_size);                \
+                                                                        \
+	__cur_ctr = _vault->nr_entry_points;                            \
+	_vault->nr_return_points = rp_ctr;                              \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETURN_POINT(vault, ret)                     \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys(ret->rp);             \
+        	__cur_ctr++;                                            \
+	}                                                               \
+                                                                        \
+	BHV_VAULT_FOR_EACH_RETHUNK_POINT(vault, ret)                    \
+	{                                                               \
+        	uint64_t *tps = (uint64_t *)&_vault->transit_points +   \
+                sizeof(uint64_t);                                       \
+        	tps[__cur_ctr] = bhv_virt_to_phys(ret->rp);             \
+        	__cur_ctr++;                                            \
+	}                                                               \
+	_vault->transit_points =                                        \
+        bhv_virt_to_phys((uint64_t *)&_vault->transit_points +          \
+                        sizeof(uint64_t));                              \
+                                                                        \
+	_vault->code.gpa =                                              \
+        bhv_virt_to_phys(__bhv_vault_text_##vault##_start);             \
+	_vault->code.size = __bhv_vault_text_##vault##_end -            \
+        __bhv_vault_text_##vault##_start;                               \
+	_vault->ref_code.gpa = bhv_virt_to_phys(                        \
+        		__bhv_vault_ref_text_##vault##_start);          \
+	_vault->ref_code.size = __bhv_vault_ref_text_##vault##_end -    \
+        __bhv_vault_ref_text_##vault##_start;                           \
+	_vault->shared_code.gpa = bhv_virt_to_phys(                     \
+        		__bhv_vault_shared_text_##vault##_start);       \
+	_vault->shared_code.size =                                      \
+        __bhv_vault_shared_text_##vault##_end -                         \
+        __bhv_vault_shared_text_##vault##_start;                        \
+	_vault->altinstr_aux.gpa =                                      \
+        bhv_virt_to_phys(__altinstr_aux_start);                         \
+	_vault->altinstr_aux.size =                                     \
+        __altinstr_aux_end - __altinstr_aux_start;                      \
+	_vault->noinstr_text.gpa =                                      \
+        bhv_virt_to_phys(__noinstr_text_start);                         \
+	_vault->noinstr_text.size =                                     \
+        __noinstr_text_end - __noinstr_text_start;                      \
+	_vault->data.gpa =                                              \
+        bhv_virt_to_phys(__bhv_vault_data_##vault##_start);             \
+	_vault->data.size = __bhv_vault_data_##vault##_end -            \
+        __bhv_vault_data_##vault##_start;                               \
+	_vault->ro_data.gpa =                                           \
+        bhv_virt_to_phys(__bhv_vault_ro_data_##vault##_start);          \
+	_vault->ro_data.size = __bhv_vault_ro_data_##vault##_end -      \
+        __bhv_vault_ro_data_##vault##_start;                            \
+	_vault->entry_text.start = (uint64_t)__entry_text_start;        \
+	_vault->entry_text.end = (uint64_t)__entry_text_end;            \
+									\
+	BHV_VAULT_REGISTER_INDIRECT_THUNKS(_vault)			\
+                                                                        \
+	pr_err("Registering vault '%s':\n", #vault);                    \
+	for (__i = 1; __i < ep_ctr + 1; __i++) {                        \
+        	pr_err("\t Entry point @ 0x%llx\n",                     \
+               		(uint64_t)(&_vault->transit_points)[__i]);      \
+	}                                                               \
+                                                                        \
+	pr_err("\t CODE Region 0x%llx->0x%llx | 0x%px:0x%px\n",         \
+       	       _vault->code.gpa, _vault->code.gpa + _vault->code.size,  \
+       	       __bhv_vault_text_##vault##_start,                        \
+       	       __bhv_vault_text_##vault##_end);                         \
+	pr_err("\t REF_CODE Region 0x%llx->0x%llx | 0x%px:0x%px\n",     \
+       	       _vault->ref_code.gpa,                                    \
+       	       _vault->ref_code.gpa + _vault->ref_code.size,            \
+       	       __bhv_vault_ref_text_##vault##_start,                    \
+       	       __bhv_vault_ref_text_##vault##_end);                     \
+	pr_err("\t SHARED_CODE Region 0x%llx->0x%llx | 0x%px:0x%px\n",  \
+       	       _vault->shared_code.gpa,                                 \
+       	       _vault->shared_code.gpa + _vault->shared_code.size,      \
+       	       __bhv_vault_shared_text_##vault##_start,                 \
+       	       __bhv_vault_shared_text_##vault##_end);                  \
+	pr_err("\t ALTINSTR_AUX Region 0x%llx->0x%llx | 0x%px:0x%px\n", \
+       	       _vault->altinstr_aux.gpa,                                \
+       	       _vault->altinstr_aux.gpa + _vault->altinstr_aux.size,    \
+       	       __altinstr_aux_start, __altinstr_aux_end);               \
+	pr_err("\t DATA Region 0x%llx->0x%llx | 0x%px:0x%px\n",         \
+       	       _vault->data.gpa, _vault->data.gpa + _vault->data.size,  \
+       	       __bhv_vault_data_##vault##_start,                        \
+       	       __bhv_vault_data_##vault##_end);                         \
+	pr_err("\t RODATA Region 0x%llx->0x%llx | 0x%px:0x%px\n",       \
+       	       _vault->ro_data.gpa,                                     \
+       	       _vault->ro_data.gpa + _vault->ro_data.size,              \
+       	       __bhv_vault_ro_data_##vault##_start,                     \
+       	       __bhv_vault_ro_data_##vault##_end);                      \
+                                                                        \
+	__r = HypABI__Wagner__Create__hypercall_noalloc(_vault);        \
+}
+
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+
+
+static inline void
+bhv_section_run_ctor(HypABI__Init__Init__BHVSectionRun__T *curr_item,
+		     HypABI__Init__Init__BHVSectionRun__T *prev_item,
+		     uint64_t gpa_start, uint64_t size, uint8_t type)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (HypABI__Init__Init__BHVSectionRun__T){
+		.gpa_start = gpa_start,
+		.size = size,
+		.type = type,
+		.next = BHV_INVALID_PHYS_ADDR,
+	};
+
+	if (prev_item)
+		prev_item->next = bhv_virt_to_phys(curr_item);
+}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+static int __init bhv_vault_extend(void)
+{
+	int r = 0;
+	HypABI__Wagner__Extend__arg__T vault;
+
+	if (!bhv_vault_is_enabled())
+		return 0;
+
+	/*
+	 * Note: jump lables ((__start|__stop)___jump_table) and static calls
+	 * ((__start|__stop)_static_call_sites) are part of the __ro_after_init
+	 * section. As such, they will be protected by BRASS integrity and do
+	 * not need to be explicitly propagated to the spaces-based BRASS vault.
+	 */
+
+	/* Alternative instructions */
+	vault.mem.gpa = bhv_virt_to_phys(__alt_instructions);
+	vault.mem.size = (unsigned long)__alt_instructions_end - (unsigned long)__alt_instructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+#ifdef CONFIG_PARAVIRT
+	/* Paravirt instructions */
+	vault.mem.gpa = bhv_virt_to_phys(__parainstructions);
+	vault.mem.size = (unsigned long)__parainstructions_end - (unsigned long)__parainstructions;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+#ifdef CONFIG_RETPOLINE
+	/* Retpolines instructions */
+	vault.mem.gpa = bhv_virt_to_phys(__retpoline_sites);
+	vault.mem.size = (unsigned long)__retpoline_sites_end - (unsigned long)__retpoline_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+
+	vault.mem.gpa = bhv_virt_to_phys(__return_sites);
+	vault.mem.size = (unsigned long)__return_sites_end - (unsigned long)__return_sites;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+#ifdef CONFIG_TRACEPOINTS
+	/* Tracepoints */
+	vault.mem.gpa = bhv_virt_to_phys(__start_bhv_tp_vault);
+	vault.mem.size = (unsigned long)__end_bhv_tp_vault - (unsigned long)__start_bhv_tp_vault;
+	r = HypABI__Wagner__Extend__hypercall_noalloc(&vault);
+	if (r)
+		return r;
+#endif
+
+	return 0;
+}
+
+#endif
+
+static int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size)
+{
+	unsigned long r;
+	unsigned int region_counter = 0;
+
+	struct {
+		HypABI__Init__Init__arg__T init_arg;
+		bhv_mem_region_t mem_regions[BHV_INIT_MAX_REGIONS];
+		HypABI__Init__Init__BHVSectionRun__T bhv_section_runs[];
+	} *arg = bhv_data;
+	BUG_ON((void *)&arg->bhv_section_runs[2] - bhv_data > +bhv_data_size);
+
+#ifndef VASKM // inside kernel tree
+#define BI_ALIGN_START(start) (unsigned long)(start)
+#else // out of tree
+#define BI_ALIGN_START(start) round_down((unsigned long)(start), PAGE_SIZE)
+#endif // VASKM
+
+#define BI_ALIGN_START_SIZE(start, end)                                        \
+	(bhv_virt_to_phys((void *)BI_ALIGN_START(start))),                     \
+		(round_up((unsigned long)(end), PAGE_SIZE) -                   \
+		 BI_ALIGN_START(start))
+#define BI_ALIGN_START_SIZE_KLN(start, end)                                    \
+	BI_ALIGN_START_SIZE(KLN_SYM(start), KLN_SYM(end))
+#define BI_LL_FIRST(ll) &(ll)[region_counter], NULL
+#define BI_LL_NEXT(ll) &(ll)[region_counter], &(ll)[region_counter - 1]
+
+	bhv_mem_region_create_ctor(BI_LL_FIRST(arg->mem_regions),
+				   BI_ALIGN_START_SIZE_KLN(_stext, _etext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "KERNEL TEXT SECTION");
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sinittext, _einittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL INIT TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sexittext, _eexittext),
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   HypABI__Integrity__MemFlags__TRANSIENT,
+				   "KERNEL EXIT TEXT SECTION");
+	region_counter++;
+#endif // VASKM
+
+	bhv_init_hyp_arch(arg->mem_regions, &region_counter);
+
+	region_counter = 0;
+	BUG_ON((unsigned long)bhv_data & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_FIRST(arg->bhv_section_runs), bhv_virt_to_phys(bhv_data),
+		bhv_data_size,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__DATA);
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		BI_ALIGN_START_SIZE(__bhv_text_start, __bhv_text_end),
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+	region_counter++;
+#endif /* CONFIG_BHV_VAULT_SPACES */
+
+#else // out of tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(
+		BI_LL_NEXT(arg->bhv_section_runs),
+		vmalloc_to_phys(__bhv_text_start), PAGE_SIZE,
+		HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+
+	for (uint8_t *p = __bhv_text_start + PAGE_SIZE; p < __bhv_text_end;
+	     p += PAGE_SIZE) {
+		phys_addr_t phy = vmalloc_to_phys(p);
+		if (phy == arg->bhv_section_runs[region_counter].gpa_start +
+				   arg->bhv_section_runs[region_counter].size) {
+			arg->bhv_section_runs[region_counter].size += PAGE_SIZE;
+		} else {
+			region_counter++;
+			BUG_ON((uint64_t)&arg->bhv_section_runs[region_counter +
+								1] -
+				       (uint64_t)arg >
+			       bhv_data_size);
+
+			bhv_section_run_ctor(
+				BI_LL_NEXT(arg->bhv_section_runs), phy,
+				PAGE_SIZE,
+				HypABI__Init__Init__BHVSectionRun__BHVSectionRunType__RICHARD_VAULT);
+		}
+	}
+	region_counter++;
+#endif // VASKM
+
+#ifdef BHV_CONST_MODPROBE_PATH
+	arg->init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	arg->init_arg.modprobe_path = bhv_virt_to_phys((void *)&modprobe_path);
+#else
+	arg->init_arg.modprobe_path_sz = 0;
+	arg->init_arg.modprobe_path = BHV_INVALID_PHYS_ADDR;
+#endif /* BHV_CONST_MODPROBE_PATH */
+
+	arg->init_arg.owner = 0;
+	arg->init_arg.region_head = bhv_virt_to_phys(&arg->mem_regions);
+	arg->init_arg.bhv_region_head =
+		bhv_virt_to_phys(&arg->bhv_section_runs);
+
+	r = HypABI__Init__Init__hypercall_noalloc(&arg->init_arg);
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+
+void __init bhv_init_platform(void)
+{
+	int rv;
+	uint32_t cid, port;
+	void *bhv_data_ptr = NULL;
+	HypABI__Init__Init__BHVData__T *data_ptr = NULL;
+	static_assert(sizeof(uint64_t) ==
+		      sizeof(unsigned long)); //for pointer cast below
+
+	bhv_init_arch();
+
+#ifndef VASKM // inside kernel tree
+	rv = bhv_init_hyp(__bhv_data_start, __bhv_data_end - __bhv_data_start);
+	bhv_data_ptr = __bhv_data_start;
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+#else // out of tree
+	bhv_data_ptr = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	// no kfree on purpose
+	if (!bhv_data_ptr) {
+		pr_err("BHV: cannot allocate bhv_data\n");
+		return;
+	}
+
+	rv = bhv_init_hyp(bhv_data_ptr, PAGE_SIZE);
+#endif // VASKM
+
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return;
+	}
+
+	bhv_initialized = true;
+	data_ptr = bhv_data_ptr;
+	bhv_configuration_bitmap =
+		(unsigned long *)&data_ptr
+			->config_bitmap; // see static_cast above
+	cid = data_ptr->vsocket_cid;
+	port = data_ptr->vsocket_port;
+
+#if !defined(VASKM) && defined(CONFIG_BHV_VAULT_SPACES)
+	if (bhv_vault_is_enabled()) {
+		rv = (BHV_VAULT_REGISTER(jump_label, __bhv_vault_comm_start,
+  				         __bhv_vault_comm_end - __bhv_vault_comm_start));
+		if (rv) {
+			bhv_fail("BHV: Cannot create spaces-based Vault");
+			return;
+		}
+
+		rv = bhv_vault_extend();
+		if (rv) {
+			bhv_fail("BHV: Cannot extend the spaces-based Vault");
+			return;
+		}
+	}
+#endif
+
+	rv = bhv_init_guestconn(cid, port);
+	if (rv) {
+		bhv_fail(
+			"BHV: Cannot configure the BHV guest connection subsystem");
+		return;
+	}
+
+	rv = bhv_init_guestlog();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV logging subsystem");
+		return;
+	}
+
+	rv = bhv_init_cred();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV creds subsystem");
+		return;
+	}
+
+	bhv_init_fileops();
+
+#ifndef VASKM // inside kernel tree
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+#endif // VASKM
+
+	__bhv_init_done = true;
+
+#if !defined(VASKM) && defined(CONFIG_BHV_VAULT_SPACES)
+	/*
+	 * Initialize static keys.
+	 *
+	 * XXX: Consider moving this to another place.
+	 */
+	bhv_init_jump_label();
+	bhv_init_alternatives();
+	bhv_init_static_call();
+#endif
+}
diff --git security/bhv/init/late_start.c security/bhv/init/late_start.c
new file mode 100644
index 00000000000..06859978899
--- /dev/null
+++ security/bhv/init/late_start.c
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+#include <bhv/init/late_start.h>
+
+void bhv_late_start(void)
+{
+	int r = bhv_late_start_init_ptpg();
+	if (r) {
+		bhv_fail("ptpg init failed");
+	}
+}
diff --git security/bhv/init/mm_init.c security/bhv/init/mm_init.c
new file mode 100644
index 00000000000..ed5293c8172
--- /dev/null
+++ security/bhv/init/mm_init.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/file_protection.h>
+#include <bhv/container_integrity.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/acl.h>
+#endif // VASKM
+
+#include <bhv/init/mm_init.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+void __init bhv_mm_init(void)
+{
+	HypABI__init_slabs();
+
+	bhv_mm_init_integrity();
+#ifndef VASKM // inside kernel tree
+	bhv_mm_init_acl();
+#endif // VASKM
+	bhv_mm_init_guestconn();
+	bhv_mm_init_cred();
+	bhv_mm_init_file_protection();
+	bhv_mm_init_container_integrity();
+}
diff --git security/bhv/init/start.c security/bhv/init/start.c
new file mode 100644
index 00000000000..8882cfaad9f
--- /dev/null
+++ security/bhv/init/start.c
@@ -0,0 +1,178 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <linux/types.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_ml_autogen.h>
+
+#include <bhv/bhv.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/reg_protect.h>
+#include <bhv/sysfs.h>
+#include <bhv/guestconn.h>
+
+#include <bhv/init/start.h>
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_SECURITY_SELINUX
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#endif // VASKM
+
+
+static inline void do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	HypABI__Init__Start__arg__T *config =
+		(HypABI__Init__Start__arg__T *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = HypABI__Init__Start__hypercall_noalloc(config);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (HypABI__Init__Start__arg__T *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = HypABI__Init__Start__hypercall_noalloc(config);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#if !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		if ((sizeof(HypABI__Init__Start__arg__T) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#else // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		bhv_fail("guest policy available without target LSM");
+#endif // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+bool __init_km bhv_start(void)
+{
+	int rc;
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_node_t *n[2];
+#endif // VASKM
+
+	if (!is_bhv_initialized())
+		return false;
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_ptpg();
+
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return false;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region.remove));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+#endif // VASKM
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+#ifndef VASKM // inside kernel tree
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_start_delete_alternatives();
+	}
+#endif
+
+	bhv_start_guestconn();
+#endif // VASKM
+
+	do_start();
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_start_sysfs();
+	}
+
+	if (bhv_reg_protect_is_enabled()) {
+		bhv_start_reg_protect();
+	}
+
+	return true;
+}
diff --git security/bhv/inode.c security/bhv/inode.c
new file mode 100644
index 00000000000..b026e3f698d
--- /dev/null
+++ security/bhv/inode.c
@@ -0,0 +1,384 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fs.h>
+#include <linux/siphash.h>
+#include <linux/uidgid.h>
+
+#include <bhv/creds.h>
+#include <bhv/event.h>
+#include <bhv/inode.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_INODE_DEBUG 0
+
+static bool bhv_inode_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_inode_is_active(void)
+{
+	if (!bhv_inode_initialized)
+		return false;
+
+	return bhv_inode_is_enabled();
+}
+
+static size_t collect_inode_invariants(char *buf, const struct inode *inode,
+				       size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t i_addr = (uint64_t)inode;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(umode_t) +
+				       sizeof(kuid_t) + sizeof(kgid_t) +
+				       sizeof(unsigned long);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	/*
+	 * Consider tracking the integrity of the remaining fields as well (not
+	 * just the below). Determine, which of them are subject to change
+	 * during the inode's life time. Otherwise, tracking the integrity of
+	 * those would require an inode tag update every time the fields change.
+	 *
+	 * NOTE: The difficulty in tracking these fields is that we need to make
+	 * sure that we track every time any of the below fields change at
+	 * run-time.
+	 */
+
+	_buf = memcpy(_buf, &i_addr, sizeof(i_addr));
+	_buf += sizeof(i_addr);
+
+	_buf = memcpy(_buf, &inode->i_mode, sizeof(umode_t));
+	_buf += sizeof(umode_t);
+
+	_buf = memcpy(_buf, &inode->i_uid, sizeof(kuid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_gid, sizeof(kgid_t));
+	_buf += sizeof(kuid_t);
+
+	_buf = memcpy(_buf, &inode->i_ino, sizeof(unsigned long));
+	_buf += sizeof(unsigned long);
+
+	return buf_size;
+}
+
+static uint64_t siphash_inode_state(const struct inode *inode)
+{
+#define MAX_BUF_SIZE                                                           \
+	sizeof(uint64_t) + sizeof(umode_t) + sizeof(kuid_t) + sizeof(kgid_t) + \
+		sizeof(unsigned long)
+
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_inode_invariants(buf, inode, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static inline bool __is_setuid(struct cred *new, const struct cred *old)
+{
+	return !uid_eq(new->euid, old->uid);
+}
+
+static inline bool __is_setgid(struct cred *new, const struct cred *old)
+{
+	return !gid_eq(new->egid, old->gid);
+}
+
+static inline void bhv_inode_register(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Register__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Register inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_register.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_update(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Update__HYPERCALL(
+			.inode = { .addr = (uint64_t)inode,
+				   .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the UPDATE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Update inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_update.inode.hmac);
+#endif
+}
+
+static inline void bhv_inode_release(struct inode *inode)
+{
+	int rc = 0;
+
+	if (!inode)
+		return;
+
+	rc = HypABI__Inode__Release__HYPERCALL(.inode = { .addr = (uint64_t)
+								  inode });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the RELEASE hypercall\n",
+		       __FUNCTION__);
+		return;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu\n", __FUNCTION__, inode->i_ino);
+#endif
+}
+
+static inline int bhv_inode_verify(struct inode *inode, struct file *file)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Inode__EventType__EVENT_NONE;
+	int block;
+
+	if (!inode)
+		return 0;
+
+	rc = HypABI__Inode__Verify__HYPERCALL(
+		&type, .inode = { .addr = (uint64_t)inode,
+				  .hmac = siphash_inode_state(inode) });
+	if (rc) {
+		pr_err("[-BHV-] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Verify inode=0x%lu | HMAC=0x%llx\n", __FUNCTION__,
+		inode->i_ino, arg.inode_verify.inode.hmac);
+#endif
+
+	if (!rc && type != HypABI__Inode__EventType__EVENT_NONE) {
+		HypABI__Inode__Log__arg__T *log_arg =
+			HypABI__Inode__Log__arg__ALLOC();
+		populate_event_context(&log_arg->context, true);
+		rc = get_file_path(&file->f_path, log_arg->file_path,
+				   HypABI__Context__MAX_PATH_SZ);
+		if (rc) {
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		log_arg->event_type = type;
+		log_arg->inode_addr = (uint64_t)(inode);
+		log_arg->inode_uid = (uint32_t)(__kuid_val(inode->i_uid));
+		log_arg->inode_gid = (uint32_t)(__kgid_val(inode->i_gid));
+		log_arg->inode_mode = (uint16_t)(inode->i_mode);
+
+		rc = HypABI__Inode__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[-BHV-] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Inode__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Inode__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+int bhv_inode_bprm_creds_from_file(struct linux_binprm *bprm, struct file *file)
+{
+	int rc = 0;
+	bool is_setxid = false;
+	const struct cred *old = current_cred();
+	struct cred *new = bprm->cred;
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return 0;
+
+	is_setxid = __is_setuid(new, old) || __is_setgid(new, old);
+	if (!is_setxid)
+		return 0;
+
+	inode = d_backing_inode(file->f_path.dentry);
+
+	/*
+	 * We consider the following cases if is_setxid == true:
+	 * - The new executable has the SXID bits set: verify its inode.
+	 * - The new executable does not have the SXID bits set: verify
+	 *   credentials of the current process.
+	 */
+
+	inode_lock(inode);
+
+	if (is_sxid(inode->i_mode)) {
+		rc = bhv_inode_verify(inode, file);
+		inode_unlock(inode);
+	} else {
+		inode_unlock(inode);
+		rc = bhv_cred_verify(current);
+	}
+
+	return rc;
+}
+
+int bhv_inode_task_fix_setuid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+int bhv_inode_task_fix_setgid(struct cred *new, const struct cred *old, int _)
+{
+	if (!bhv_inode_is_active())
+		return 0;
+
+	return bhv_cred_verify(current);
+}
+
+/* Caller holds semaphore inode->i_rwsem. */
+void bhv_inode_post_setattr(struct dentry *dentry, int ia_valid,
+			    umode_t old_mode)
+{
+	struct inode *inode = NULL;
+
+	if (!bhv_inode_is_active())
+		return;
+
+	/*
+	 * Unfortunately, we cannot verify the current task's credentials at
+	 * this point. This function is sometimes called after having
+	 * temporarily overwritten the current tasks' credentials via
+	 * override_creds; this is done e.g., by the overlay fs to temporarily
+	 * elevate permissions to the filesystem's owner so allow ops on the
+	 * filesystem. As long as we do not track the temporarily overwritten
+	 * credentials, we cannot verify the current task's credentials.
+	 */
+
+	/* We are interested only in mode, UID, and GID changes. */
+	if (!(ia_valid & (ATTR_MODE | ATTR_UID | ATTR_GID)))
+		return;
+
+	/* Note that the caller of this function locked the inode. */
+	inode = d_backing_inode(dentry);
+
+	/* Mode changes should be updated only if the SXID bits are set. */
+	if ((old_mode & (S_ISUID | S_ISGID)) ==
+	    (inode->i_mode & ((S_ISUID | S_ISGID)))) {
+		if (!is_sxid(inode->i_mode))
+			return;
+	}
+
+	/* Register inode if the previous mode did not have SXID bits set. */
+	if (!is_sxid(old_mode) && is_sxid(inode->i_mode)) {
+		bhv_inode_register(inode);
+		return;
+	}
+
+	/* Release tracked inode if setattr removed SXID bits. */
+	if (is_sxid(old_mode) && !is_sxid(inode->i_mode)) {
+		bhv_inode_release(inode);
+		return;
+	}
+
+	/* In any other case, the inode is tracked: update the changes */
+	bhv_inode_update(inode);
+}
+
+void bhv_inode_d_instantiate(struct dentry *dentry, struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+	if (!is_sxid(inode->i_mode))
+		return;
+
+	/*
+	 * Note: we could check whether the inode has any existing dentry
+	 * aliases, via d_find_any_alias, and if so return. Yet, we chose to do
+	 * this check from within BHV to ensure that an attacker cannot modify
+	 * this value to skip inode registrations.
+	 */
+
+	spin_lock(&inode->i_lock);
+	bhv_inode_register(inode);
+	spin_unlock(&inode->i_lock);
+}
+
+/* Caller holds the spinlock inode->i_lock. */
+void bhv_inode_iput_final(struct inode *inode)
+{
+	if (!bhv_inode_is_active())
+		return;
+
+	if (!inode)
+		return;
+
+#if BHV_INODE_DEBUG
+	pr_info("%s [BHV]: Release inode=0x%lu i_count=%d i_nlink=%d\n",
+		__FUNCTION__, inode->i_ino, atomic_read(&inode->i_count),
+		inode->i_nlink);
+#endif
+
+	/*
+	 * Note: when releasing inodes, we unconditionally call into BHV (i.e.,
+	 * disregarding whether the inode is privileged or not) to avoid
+	 * potential reuse attacks.  Otherwise, the attacker could maliciously
+	 * modify a privileged (and by BHV registered) inode, cause the kernel
+	 * to drop the inode in  the kernel, and then re-allocate an
+	 * unprivileged file, the meta data of which can be adjusted to match
+	 * the previous meta information that were incorporated into the HMAC of
+	 * the previously privileged inode.
+	 *
+	 * XXX: Consider conditionally calling into BHV at this point if the
+	 * above renders to be not critical (every new inode allocation receives
+	 * a new inode->i_ino). This would reduce the number of hypercalls.
+	 */
+
+	bhv_inode_release(inode);
+}
+
+int __init bhv_inode_init(void)
+{
+	if (!bhv_inode_is_enabled())
+		return -EPERM;
+
+	if (bhv_inode_initialized)
+		return -EPERM;
+
+	bhv_inode_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/integrity.c security/bhv/integrity.c
new file mode 100644
index 00000000000..1a5dca2fef9
--- /dev/null
+++ security/bhv/integrity.c
@@ -0,0 +1,215 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/filter.h>
+#include <linux/module.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+
+/************************************************************
+ * mm_init
+ ************************************************************/
+void __init bhv_mm_init_integrity(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+/************************************************************/
+
+int bhv_integrity_freeze_events(uint64_t flags)
+{
+	if (HypABI__Integrity__Freeze__HYPERCALL(.flags = flags))
+		return -EINVAL;
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+extern struct mutex module_mutex;
+extern struct list_head modules;
+#else // out of tree
+#include <kln.h>
+#endif //VASKM
+
+static void lock_all_modules(void)
+{
+	static bool all_modules_locked = false;
+	struct module *mptr;
+
+	if (all_modules_locked)
+		return;
+
+	mutex_lock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+
+	list_for_each_entry (mptr, KLN_SYMBOL_P(struct list_head *, modules),
+			     list) {
+		if (mptr != THIS_MODULE) {
+			if (!try_module_get(mptr)) {
+				printk(KERN_WARNING
+				       "%s: Cannot lock module %s\n",
+				       __FUNCTION__, mptr->name);
+			}
+		}
+	}
+
+	mutex_unlock(KLN_SYMBOL_P(struct mutex *, module_mutex));
+	all_modules_locked = true;
+	return;
+}
+
+bool bhv_allow_kmod_loads = true;
+bool bhv_allow_patch = true;
+bool bhv_integrity_freeze_create_currently_frozen = false;
+bool bhv_integrity_freeze_update_currently_frozen = false;
+bool bhv_integrity_freeze_remove_currently_frozen = false;
+bool bhv_integrity_freeze_patch_currently_frozen = false;
+
+int bhv_enable_integrity_freeze_flag(uint64_t flags, bool skip_locks)
+{
+	int ret;
+	if (!skip_locks) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE ||
+		    flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_kmod_loads = false;
+			lock_all_modules();
+		}
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH) {
+			bhv_allow_patch = false;
+		}
+	} else {
+		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+		       __FUNCTION__);
+	}
+
+	ret = bhv_integrity_freeze_events(flags);
+
+	if (!ret) {
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__CREATE)
+			bhv_integrity_freeze_create_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__UPDATE)
+			bhv_integrity_freeze_update_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__REMOVE)
+			bhv_integrity_freeze_remove_currently_frozen = true;
+		if (flags & HypABI__Integrity__Freeze__FreezeFlags__PATCH)
+			bhv_integrity_freeze_patch_currently_frozen = true;
+	}
+
+	return ret;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(
+	HypABI__MemoryRegionOwner owner,
+	HypABI__Integrity__Create__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Create__HYPERCALL(.owner = owner,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_update_kern_phys_mem_region_hyp(
+	HypABI__Integrity__Update__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Update__HYPERCALL(.region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(
+	HypABI__Integrity__Remove__Mem_Region__T *region_head)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) false,
+						    .region_head = virt_to_phys(
+							    region_head));
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(HypABI__MemoryRegionOwner owner)
+{
+	return HypABI__Integrity__Remove__HYPERCALL(.rm_by_owner =
+							    (uint8_t) true,
+						    .owner = owner);
+}
+
+static uint64_t _ptpg_pgd_offset __ro_after_init;
+static uint64_t _ptpg_pgd_value __ro_after_init;
+static atomic_t _ptpg_ready __ro_after_init = ATOMIC_INIT(0);
+
+/**************************************************************
+ * start
+ **************************************************************/
+void __init_km bhv_start_ptpg(void)
+{
+	_ptpg_pgd_offset = 0;
+	_ptpg_pgd_value = 0;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+
+	bhv_start_get_pt_protect_pgd_data(&_ptpg_pgd_offset, &_ptpg_pgd_value);
+
+	atomic_inc(&_ptpg_ready);
+}
+/**************************************************************/
+
+/**************************************************************
+ * late_start
+ **************************************************************/
+int bhv_late_start_init_ptpg(void)
+{
+	unsigned long r;
+	HypABI__Integrity__PtpgInit__arg__T *arg;
+
+	if (!bhv_integrity_pt_prot_is_enabled() || !atomic_read(&_ptpg_ready)) {
+		return 0;
+	}
+
+	BUG_ON(sizeof(HypABI__Integrity__PtpgInit__arg__T) > PAGE_SIZE);
+
+	arg = HypABI__Integrity__PtpgInit__arg__ALLOC();
+
+	bhv_late_start_get_pt_protect_data(arg);
+
+	r = HypABI__Integrity__PtpgInit__hypercall_noalloc(arg);
+
+	HypABI__Integrity__PtpgInit__arg__FREE(arg);
+
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+/**************************************************************/
+
+void bhv_pt_protect_check_pgd(struct mm_struct *mm)
+{
+	unsigned long r;
+	bool success;
+
+	if (!bhv_integrity_pt_prot_is_enabled()) {
+		return;
+	}
+	success = bhv_pt_protect_check_pgd_arch(mm, _ptpg_pgd_offset,
+						_ptpg_pgd_value);
+	if (!success) {
+		r = HypABI__Integrity__PtpgReport__hypercall_noalloc();
+		if (r) {
+			pr_err("BHV: error reporting pt violation to host!");
+		}
+	}
+}
diff --git security/bhv/keyring.c security/bhv/keyring.c
new file mode 100644
index 00000000000..24c0e101a59
--- /dev/null
+++ security/bhv/keyring.c
@@ -0,0 +1,198 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/key-type.h>
+#include <linux/siphash.h>
+
+#include <bhv/event.h>
+#include <bhv/keyring.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/abi_hl_autogen.h>
+
+#define BHV_KEYRING_DEBUG 0
+
+static bool bhv_keyring_initialized __ro_after_init = false;
+
+extern siphash_key_t bhv_siphash_key __ro_after_init;
+
+static inline bool bhv_keyring_is_active(void)
+{
+	if (!bhv_keyring_initialized)
+		return false;
+
+	return bhv_keyring_is_enabled();
+}
+
+static size_t collect_keyring_invariants(char *buf, const struct key *keyring,
+					 uint64_t anchor, size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct key keyring_copy;
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct key);
+
+	BUG_ON(!buf || max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&keyring_copy, keyring, sizeof(struct key));
+
+	/* Exclude mutable fields from the keyring to be hashed. */
+
+	refcount_set(&keyring_copy.usage, 0);
+	memset(&keyring_copy.graveyard_link, 0, sizeof(struct list_head));
+	memset(&keyring_copy.serial_node, 0, sizeof(struct rb_node));
+#ifdef CONFIG_KEY_NOTIFICATIONS
+	keyring_copy.watchers = NULL;
+#endif
+	memset(&keyring_copy.sem, 0, sizeof(struct rw_semaphore));
+	keyring_copy.last_used_at = 0;
+	keyring_copy.datalen = 0;
+	memset(&keyring_copy.payload, 0, sizeof(union key_payload));
+
+	bound_context = (uint64_t)keyring ^ anchor;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &keyring_copy, sizeof(struct key));
+
+	return buf_size;
+}
+
+static uint64_t siphash_keyring_state(const struct key *keyring,
+				      uint64_t anchor)
+{
+#define MAX_BUF_SIZE sizeof(uint64_t) + sizeof(struct key)
+	char buf[MAX_BUF_SIZE];
+	size_t size =
+		collect_keyring_invariants(buf, keyring, anchor, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+int bhv_keyring_verify_locked(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t type = HypABI__Keyring__EventType__EVENT_NONE;
+	int block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	rc = HypABI__Keyring__Verify__HYPERCALL(
+		&type, .keyring = { .addr = (uint64_t)anchor,
+				    .hmac = siphash_keyring_state(
+					    keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the VERIFY hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	if (!rc && type != HypABI__Keyring__EventType__EVENT_NONE) {
+		HypABI__Keyring__Log__arg__T *log_arg =
+			HypABI__Keyring__Log__arg__ALLOC();
+		populate_event_context(&log_arg->context, true);
+
+		log_arg->event_type = type;
+		log_arg->keyring_addr = (uint64_t)(keyring);
+		log_arg->keyring_uid = (uint32_t)(__kuid_val(keyring->uid));
+		log_arg->keyring_gid = (uint32_t)(__kgid_val(keyring->gid));
+		log_arg->keyring_perm = (uint32_t)(keyring->perm);
+		log_arg->keyring_serial = (uint32_t)(keyring->serial);
+
+		strncpy(log_arg->keyring_desc, keyring->description,
+			HypABI__Context__MAX_PATH_SZ);
+
+		rc = HypABI__Keyring__Log__hypercall_noalloc(log_arg);
+		if (rc) {
+			pr_err("[BHV] %s: an error occurred during the LOG hypercall\n",
+			       __FUNCTION__);
+			HypABI__Keyring__Log__arg__FREE(log_arg);
+			return rc;
+		}
+
+		block = (log_arg->block) ? -EPERM : 0;
+		HypABI__Keyring__Log__arg__FREE(log_arg);
+		return block;
+	}
+
+	return rc;
+}
+
+int bhv_keyring_verify(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	down_read(&keyring->sem);
+	rc = bhv_keyring_verify_locked(keyring, anchor);
+	up_read(&keyring->sem);
+
+	return rc;
+}
+
+static int bhv_keyring_register(struct key *keyring, void *anchor)
+{
+	int rc = 0;
+	uint8_t block;
+
+	if (!bhv_keyring_is_active())
+		return 0;
+
+	if (!keyring || !anchor)
+		return -ENOKEY;
+
+	/* Note: we index system-trusted keyrings by its global anchor. */
+
+	rc = HypABI__Keyring__Register__HYPERCALL(
+		&block, .keyring = { .addr = (uint64_t)anchor,
+				     .hmac = siphash_keyring_state(
+					     keyring, (uint64_t)anchor) });
+	if (rc) {
+		pr_err("[BHV] %s: an error occurred during the REGISTER hypercall\n",
+		       __FUNCTION__);
+		return rc;
+	}
+
+#if BHV_KEYRING_DEBUG
+	pr_info("[BHV] %s: keyring @ %px anchor @ %px | \"%s\" serial=%d",
+		__FUNCTION__, keyring, anchor, keyring->index_key.description,
+		keyring->serial);
+#endif
+
+	return block ? -EPERM : 0;
+}
+
+int bhv_keyring_register_system_trusted(struct key **k)
+{
+	void *anchor = k;
+	return bhv_keyring_register(*k, anchor);
+}
+
+int __init bhv_init_keyring(void)
+{
+	if (!bhv_keyring_is_enabled())
+		return -EPERM;
+
+	if (bhv_keyring_initialized)
+		return -EPERM;
+
+	bhv_keyring_initialized = true;
+
+	return 0;
+}
diff --git security/bhv/lsm.c security/bhv/lsm.c
new file mode 100644
index 00000000000..4b24bb91cdd
--- /dev/null
+++ security/bhv/lsm.c
@@ -0,0 +1,740 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/dcache.h>
+#include <linux/fdtable.h>
+#include <linux/highmem.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <uapi/linux/magic.h>
+
+#include <bhv/acl.h>
+#include <bhv/bhv.h>
+#include <bhv/container_integrity.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/guestlog.h>
+#include <bhv/inode.h>
+#include <bhv/integrity.h>
+#include <bhv/sysfs_fops.h>
+
+#ifndef VASKM // inside kernel tree
+#include <linux/lsm_hooks.h>
+#include <bhv/kernel-kln.h>
+#define static_vk static
+
+#else // out of tree
+#include <module.h>
+#include <kln.h>
+#include <fsreg.h>
+#define static_vk
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+static int bhv_read_file(struct file *file, enum kernel_read_file_id id,
+			 bool whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+
+static int bhv_load_data(enum kernel_load_data_id id, bool contents)
+{
+	const char *origin = kernel_read_file_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			bhv_guestlog_log_driver_load("[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+#endif // VASKM
+
+static_vk int bhv_task_alloc(struct task_struct *target,
+			     long unsigned int clone_flags)
+{
+	pr_debug("[bhv] TASK CREATE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_fork(target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+static_vk void bhv_task_free(struct task_struct *target)
+{
+	pr_debug("[bhv] TASK FREE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exit(target->pid, target->parent->pid,
+					      target->comm);
+	}
+}
+
+static_vk int bhv_bprm_check_security(struct linux_binprm *bprm)
+{
+	int rv = 0;
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+	if (bhv_acl_is_proc_acl_enabled()) {
+		const char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&bprm->file->f_path, filename_buf, PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+
+		// check executed filename (name on cli)
+		if (bhv_block_process(bprm->filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		// check underlying file (e.g., busybox or interpreter)
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_container_integrity_is_enabled() &&
+	    (rv = bhv_container_integrity_bprm_check_security(bprm)) != 0)
+		return rv;
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exec(bprm, current->pid,
+					      current->parent->pid,
+					      bprm->filename);
+	}
+
+	return rv;
+}
+
+static const inline char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+static bool try_update_bhv_fops_map(u8 bhv_fops, bool is_dir,
+				    const struct file_operations *fops_ptr)
+{
+	BUG_ON(fileops_map[bhv_fops][is_dir == true ? 1 : 0]);
+
+	if (!bhv_allow_update_fileops_map)
+		return false;
+
+	if (is_module_ro_data((unsigned long)fops_ptr)) {
+		fileops_map[bhv_fops][is_dir == true ? 1 : 0] = fops_ptr;
+		pr_info("%s: Added fops %d %d\n", __FUNCTION__, bhv_fops,
+			is_dir);
+		return true;
+	}
+	return false;
+}
+#endif // defined VASKM && defined VASKM_AUTO_TRUST_FOPS
+
+#define FT(T) HypABI__FileProtection__ViolationFileOps__FopsType__##T
+static bool bhv_perform_check_fileops(struct file *file, u8 bhv_fops,
+				      bool is_dir)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+	bool block = false;
+
+	if (bhv_fops != FT(UNSUPPORTED)) {
+		if (file->f_op ==
+		    fileops_map[bhv_fops][is_dir == true ? 1 : 0]) {
+			// fops matches
+			return false;
+		}
+	}
+
+	switch (bhv_fops) {
+#if defined VASKM && defined VASKM_AUTO_TRUST_FOPS // out of tree
+	case FT(EXT4):
+	case FT(XFS):
+		if (!fileops_map[bhv_fops][is_dir == true ? 1 : 0])
+			if (try_update_bhv_fops_map(bhv_fops, is_dir,
+						    file->f_op))
+				return false;
+		break;
+#endif // VASKM
+	case FT(UNSUPPORTED): {
+		if (bhv_strict_fileops_enforced()) {
+			/*
+			 * strict mode configured or forced by kernel boot option
+			 * continue to report and optionally block
+			 */
+			break;
+		}
+
+		/*
+		 * fallback in case of unknown fops:
+		 * check if pointer points to ro section
+		 */
+		if (bhv_fileops_is_ro((u64)file->f_op)) {
+			uint64_t struct_type = 0;
+			uint32_t minor = 0;
+			uint64_t major = 0;
+
+			if (!bhv_guestlog_log_unknown_fileops()) {
+				return false;
+			}
+
+			// set the type for logging
+			if (d_is_reg(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__FILE;
+			} else if (d_can_lookup(file->f_path.dentry)) {
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__DIRECTORY;
+			} else if (d_is_special(file->f_path.dentry)) {
+				// save i_rdev which contains major and minor
+				struct_type =
+					GuestConnABI__GuestLog__FopsUnknown__FileStructType__SPECIAL;
+				major = MAJOR(file->f_inode->i_rdev);
+				minor = MINOR(file->f_inode->i_rdev);
+			}
+
+			// log info about unsupported
+			pathname_buf = kzalloc(
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+				GFP_KERNEL);
+			pathname = get_pathname(
+				file, pathname_buf,
+				HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+
+			bhv_guestlog_log_fops_unknown(
+				file->f_inode->i_sb->s_magic, pathname,
+				struct_type, major, minor,
+				(uint64_t)file->f_op);
+
+			pathname = NULL;
+			kfree(pathname_buf);
+			// OK
+			return false;
+		}
+
+		break;
+	}
+	// additional check for empty dir ops
+	case FT(SYSFS):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       empty_dir_operations)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	// additional check for dummy fops
+	case FT(DEV_TTY):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       hung_up_tty_fops)) {
+			// we're all set
+			return false;
+		}
+		fallthrough;
+	case FT(DEV_NULL):
+	case FT(DEV_URANDOM):
+	case FT(DEV_RANDOM):
+	case FT(DEV_CONSOLE):
+	case FT(DEV_KMSG):
+	case FT(DEV_MEM):
+	case FT(DEV_ZERO):
+		if (file->f_op == KLN_SYMBOL_P(const struct file_operations *,
+					       def_chr_fops)) {
+			// we're all set
+			return false;
+		}
+		break;
+	case FT(PROC):
+		// additional check for proc fops:
+		if (is_valid_proc_fop(&(file->f_op)))
+			return false;
+		break;
+	case FT(DEBUGFS):
+		if (is_valid_debugfs_fop(file->f_op))
+			return false;
+		break;
+	default:
+		break;
+	}
+
+	// by now we have decided that the fops ptr is bad
+
+	pathname_buf =
+		kzalloc(HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+	pathname = get_pathname(
+		file, pathname_buf,
+		HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+	if (bhv_block_fileops(pathname, bhv_fops, is_dir, file->f_op)) {
+		// block file operation
+		block = true;
+	}
+
+	pathname = NULL;
+	kfree(pathname_buf);
+
+	return block;
+}
+
+static bool bhv_check_fileops(struct file *file)
+{
+	// set up fops check data
+	u8 bhv_fops = FT(UNSUPPORTED);
+	bool is_dir = false;
+
+	unsigned dev_major = imajor(file->f_inode);
+	unsigned dev_minor = iminor(file->f_inode);
+
+	if (d_can_lookup(file->f_path.dentry)) {
+		/* directory S_ISDIR(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+		is_dir = true;
+	} else if (d_is_reg(file->f_path.dentry)) {
+		/* regular file  S_ISREG(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+	} else if (d_is_special(file->f_path.dentry)) {
+		// DCACHE_SPECIAL_TYPE
+		if (S_ISSOCK(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == SOCKFS_MAGIC
+			bhv_fops = FT(SOCKFS);
+		} else if (S_ISFIFO(file->f_inode->i_mode)) {
+			// file->f_inode->i_sb->s_magic == PIPEFS_MAGIC
+			bhv_fops = FT(PIPEFS);
+		} else if (S_ISCHR(file->f_inode->i_mode)) {
+			// character device
+			if (dev_major == 1) {
+				// mem
+				switch (dev_minor) {
+				case 1: // DEVMEM_MINOR
+					// /dev/mem
+					bhv_fops = FT(DEV_MEM);
+					break;
+				case 3:
+					// /dev/null
+					bhv_fops = FT(DEV_NULL);
+					break;
+				case 4:
+					// /dev/port
+					bhv_fops = FT(DEV_PORT);
+					break;
+				case 5:
+					// /dev/zero
+					bhv_fops = FT(DEV_ZERO);
+					break;
+				case 7:
+					// /dev/full
+					bhv_fops = FT(DEV_FULL);
+					break;
+				case 8:
+					// /dev/random
+					bhv_fops = FT(DEV_RANDOM);
+					break;
+				case 9:
+					// /dev/urandom
+					bhv_fops = FT(DEV_URANDOM);
+					break;
+				case 11:
+					// /dev/kmsg
+					bhv_fops = FT(DEV_KMSG);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 5) {
+				switch (dev_minor) {
+				case 0:
+					// /dev/tty
+					bhv_fops = FT(DEV_TTY);
+					break;
+				case 1:
+					// /dev/console
+					bhv_fops = FT(DEV_CONSOLE);
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 4 && dev_minor == 64) {
+				// /dev/ttyS0
+				bhv_fops = FT(DEV_TTY);
+			} else if (dev_major == 229 && dev_minor == 0) {
+				// /dev/hvc0
+				bhv_fops = FT(DEV_TTY);
+			}
+		}
+	}
+
+#ifdef DEBUG
+	if (bhv_fops == FT(UNSUPPORTED)) {
+		char *pathname_buf = kzalloc(
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ,
+			GFP_KERNEL);
+		const char *pathname = get_pathname(
+			file, pathname_buf,
+			HypABI__FileProtection__ViolationFileOps__PATH_MAX_SZ);
+		pr_debug(
+			"name: %s magic: 0x%lx dentry_type: 0x%x stat: 0o%o path: %s fops: 0x%px",
+			file->f_inode->i_sb->s_type->name,
+			file->f_inode->i_sb->s_magic,
+			file->f_path.dentry->d_flags & DCACHE_ENTRY_TYPE,
+			(file->f_inode->i_mode & S_IFMT), pathname, file->f_op);
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+#endif
+
+	// perform fops check
+	return bhv_perform_check_fileops(file, bhv_fops, is_dir);
+}
+#undef FT
+
+#define BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ 256
+
+static_vk int bhv_check_files_dirty_pipe(const void *address_space,
+					 struct file *file, unsigned int number)
+{
+	char *filename_buf = NULL;
+	const char *filename = NULL;
+	int rv;
+
+	struct inode *ipipe =
+		address_space != NULL ?
+			((struct address_space *)address_space)->host :
+			      NULL;
+	struct inode *ifile = d_real_inode(file->f_path.dentry);
+
+	if (ipipe == NULL || ifile == NULL || !virt_addr_valid(ipipe))
+		return 0;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (ifile->i_ino == ipipe->i_ino && ifile->i_sb == ipipe->i_sb &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Allocate memory
+		filename_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+
+		if (filename_buf == NULL) {
+			pr_err("Could not allocate file buffer");
+			filename = "UNKNOWN";
+		} else {
+			// Get Path of the file we are trying to write
+			filename = d_path(&file->f_path, filename_buf,
+					  BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+			if (IS_ERR(filename)) {
+				pr_err("Could not retrieve file name (%ld)",
+				       PTR_ERR(filename));
+				filename = "UNKNOWN";
+			}
+		}
+
+		// Ask the HOST whether we should block this attempt.
+		rv = bhv_block_read_only_file_write_ViolationWriteReadOnlyFile(
+			filename);
+
+		if (filename_buf != NULL) {
+			kfree(filename_buf);
+		}
+
+		return rv;
+	}
+
+	return 0;
+}
+
+/*
+ * Dirty cred detection
+ * When a write happens this checks whether the struct file object is opened for
+ * writing. If this is not the case notify and optionally stop the write
+ * attempt immediately.
+ */
+void bhv_check_file_dirty_cred(struct file *file, int mask)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+
+	if (!bhv_dirtycred_file_protection_is_enabled())
+		return;
+
+	// No need to continue if no write attempt is ongoing.
+	if (!(mask & MAY_WRITE))
+		return;
+
+	if (!(file->f_mode & FMODE_WRITE) ||
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		pathname_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+		pathname = get_pathname(file, pathname_buf,
+					BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+		pr_err("Write to read-only file \"%s\" detected!", pathname);
+		// At this point a struct file object was already placed illegitimately
+		// at the location of another. When we're be called from a LSM file
+		// permission hook, we could still bail out gracefully. However, as
+		// illegitimately placing struct file objects often happens after
+		// permission and LSM hooks, we need to check deep inside write attempts
+		// (e.g. in mm/filemap.c:generic_perform_write()). There, we do not have
+		// much choice as the attempts might have already corrupted the
+		// read-only file. Hence, on a block, we panic after having informed the
+		// HOST.
+		if (bhv_block_read_only_file_write_ViolationDirtyCredWrite(
+			    pathname)) {
+			panic("possible dirtycred compromise detected");
+		}
+
+		kfree(pathname_buf);
+	}
+}
+
+static_vk int bhv_file_open(struct file *file)
+{
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	return 0;
+}
+
+static_vk int bhv_file_permission(struct file *file, int mask)
+{
+	struct pipe_inode_info *info;
+	struct pipe_buffer *buf;
+	int rv;
+
+	if (bhv_fileops_file_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	/* check for dirty cred inside LSM hook (e.g. for aio_write()) */
+	bhv_check_file_dirty_cred(file, mask);
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return 0;
+
+	/*
+	 * Dirty pipe detection. Whenever we write to a file and this file is
+	 * a pipe, we are going to check whether this pipe points to a read-only
+	 * file. This check happens in `bhv_check_files_dirty_pipe` above.
+	 */
+	if ((mask & MAY_WRITE) == MAY_WRITE &&
+	    (info = get_pipe_info(file, false))) {
+		// Check current buffer in the pipe for dirty pipe
+		buf = &info->bufs[(info->head - 1) & (info->ring_size - 1)];
+		// Iterate over all open files and see whether the pipe points to the same file.
+		if (buf && buf->page && current->files &&
+		    virt_addr_valid(buf->page->mapping)) {
+			if (iterate_fd(current->files, 0,
+				       bhv_check_files_dirty_pipe,
+				       buf->page->mapping)) {
+				return -EACCES;
+			}
+		}
+	}
+
+	if (bhv_container_integrity_is_enabled() &&
+	    (rv = bhv_container_integrity_file_permission(file, mask)) != 0)
+		return rv;
+
+	return 0;
+}
+
+static_vk int bhv_cgroup_mkdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_create(cgrp);
+	return 0;
+}
+
+static_vk void bhv_cgroup_rmdir(struct cgroup *cgrp)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_cgroup_destroy(cgrp);
+}
+
+static_vk int bhv_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+static_vk int bhv_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	if (bhv_guestlog_log_container_events())
+		bhv_guestlog_log_namespace_change(tsk, nsset);
+	return 0;
+}
+
+#if defined(VASKM_IS_UBUNTU) && (LINUX_VERSION_CODE >= KERNEL_VERSION(6, 5, 0))
+static_vk int bhv_inode_setxattr(struct mnt_idmap *mnt_idmap,
+				 struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(5, 12, 0)
+static_vk int bhv_inode_setxattr(struct user_namespace *mnt_userns,
+				 struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#else
+static_vk int bhv_inode_setxattr(struct dentry *dentry, const char *name,
+				 const void *value, size_t size, int flags)
+{
+#endif
+	int rv;
+
+	if (!bhv_container_integrity_is_enabled())
+		return 0;
+
+	if ((rv = bhv_container_integrity_inode_setxattr(name)) != 0)
+		return rv;
+
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+static void bhv_module_loaded(struct module *mod)
+{
+	if (bhv_guestlog_log_driver_events()) {
+		bhv_guestlog_log_driver_load(mod->name);
+	}
+}
+
+static struct security_hook_list bhv_hooks[] __lsm_ro_after_init = {
+	LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+	LSM_HOOK_INIT(kernel_load_data, bhv_load_data), // init_module
+	LSM_HOOK_INIT(task_alloc, bhv_task_alloc), // fork
+	LSM_HOOK_INIT(task_free, bhv_task_free), // exit
+	LSM_HOOK_INIT(bprm_check_security, bhv_bprm_check_security), // execve
+	LSM_HOOK_INIT(file_permission, bhv_file_permission), // file read/write
+	LSM_HOOK_INIT(file_open, bhv_file_open), // file open
+	LSM_HOOK_INIT(module_loaded,
+		      bhv_module_loaded), // module has successfully loaded
+
+	/* Inode protection */
+	LSM_HOOK_INIT(bprm_creds_from_file, bhv_inode_bprm_creds_from_file),
+	LSM_HOOK_INIT(task_fix_setuid, bhv_inode_task_fix_setuid),
+	LSM_HOOK_INIT(task_fix_setgid, bhv_inode_task_fix_setgid),
+	LSM_HOOK_INIT(d_instantiate, bhv_inode_d_instantiate),
+
+	/* Container visibility */
+	LSM_HOOK_INIT(cgroup_mkdir, bhv_cgroup_mkdir),
+	LSM_HOOK_INIT(cgroup_rmdir, bhv_cgroup_rmdir),
+	LSM_HOOK_INIT(unshare, bhv_unshare),
+	LSM_HOOK_INIT(setns, bhv_setns),
+
+	/* Xattr */
+	LSM_HOOK_INIT(inode_setxattr, bhv_inode_setxattr),
+};
+
+static int __init bhv_lsm_init(void)
+{
+	pr_info("[bhv] LSM active");
+	security_add_hooks(bhv_hooks, ARRAY_SIZE(bhv_hooks), "bhv");
+	return 0;
+}
+
+DEFINE_LSM(bhv) = {
+	.name = "bhv",
+	.init = bhv_lsm_init,
+};
+#endif // VASKM
diff --git security/bhv/memory_freeze.c security/bhv/memory_freeze.c
new file mode 100644
index 00000000000..3a9a7bc6e65
--- /dev/null
+++ security/bhv/memory_freeze.c
@@ -0,0 +1,105 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/printk.h>
+#include <linux/umh.h>
+
+#include <bhv/integrity.h>
+#include <bhv/guestlog.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+static int __maybe_unused bhv_memory_freeze_main(void *_)
+{
+	static const __section(".rodata") char *const argv[] = {
+		"/usr/bin/systemctl", "is-system-running", NULL
+	};
+	static const __section(".rodata") char *const envp[] = {
+		"HOME=/", "PATH=/sbin:/bin:/usr/sbin:/usr/bin", NULL
+	};
+	int ret;
+
+	while (true) {
+		msleep(500);
+
+		ret = call_usermodehelper(
+			argv[0], (char **)argv, (char **)envp,
+			UMH_WAIT_PROC /* wait for the process to complete */);
+
+		if (ret == -ENOENT) {
+			pr_info("%s: Non-systemd filesystem\n", __FUNCTION__);
+			msleep(120000);
+			pr_info("%s: Assuming system is up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+
+		if (ret == 0) {
+			pr_info("%s: System up\n", __FUNCTION__);
+			return bhv_enable_integrity_freeze_flag(
+				HypABI__Integrity__Freeze__FreezeFlags__CREATE |
+					HypABI__Integrity__Freeze__FreezeFlags__UPDATE |
+					HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+				false);
+		}
+	}
+}
+
+void bhv_memory_freeze_init(void)
+{
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__T *arg;
+	int rc;
+	bool fmab = false;
+
+	if (!is_bhv_initialized())
+		return;
+
+	arg = HypABI__Confserver__FreezeMemoryAfterBoot__arg__ALLOC();
+
+	rc = HypABI__Confserver__FreezeMemoryAfterBoot__hypercall_noalloc(arg);
+
+	if (rc == 0) {
+		fmab = arg->freeze_memory_after_boot;
+	} else {
+		bhv_fail("%s: Hypercall failed!", __FUNCTION__);
+	}
+
+	HypABI__Confserver__FreezeMemoryAfterBoot__arg__FREE(arg);
+
+	if (fmab) {
+#ifdef CONFIG_STATIC_USERMODEHELPER
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_STATIC_USERMODEHELPER.");
+#endif
+#ifdef CONFIG_BPF_JIT
+#define NO_FREEZE
+		pr_err("%s: Freezing memory is incompatible with CONFIG_BPF_JIT.\n",
+		       __FUNCTION__);
+		if (bhv_guestlog_enabled())
+			bhv_guestlog_log_str(
+				"Error: freezing memory is incompatible with CONFIG_BPF_JIT.");
+#endif
+
+#ifndef NO_FREEZE
+		if (ERR_PTR(-ENOMEM) == kthread_run(bhv_memory_freeze_main,
+						    NULL, "memory_freeze")) {
+			panic("%s: Could not create memory_freeze thread",
+			      __FUNCTION__);
+		}
+		pr_err("%s: started thread", __FUNCTION__);
+#endif
+	}
+}
\ No newline at end of file
diff --git security/bhv/module.c security/bhv/module.c
new file mode 100644
index 00000000000..13da50a3e3c
--- /dev/null
+++ security/bhv/module.c
@@ -0,0 +1,624 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/bhv_print.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t, const char *);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags, label);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t type,
+				    uint64_t flags, const char *unused2)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3, const char *unused4)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags,
+				  label);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags, label);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags,
+			       const char *label)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, label,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == HypABI__Integrity__MemType__UNKNOWN)
+		return;
+
+	type &= ~HypABI__Integrity__MemFlags__MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, "INVALID",
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size,
+				HypABI__Integrity__MemType__UNKNOWN,
+				HypABI__Integrity__MemFlags__NONE, "INVALID",
+				_bhv_link_node_op_remove);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static void bhv_prepare_mod_init(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_INIT_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_INIT_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static void bhv_prepare_mod_core(struct list_head *head,
+				 const struct module *mod,
+				 unsigned long base_flags)
+{
+	if (mod->mem[MOD_TEXT].size) {
+		/* Prepare the module region's .text section. */
+		bhv_create_section(head, mod->mem[MOD_TEXT].base,
+				   mod->mem[MOD_TEXT].size,
+				   HypABI__Integrity__MemType__CODE_PATCHABLE,
+				   base_flags, "MODULE TEXT SECTION");
+	}
+
+	if (mod->mem[MOD_RODATA].size) {
+		/* Prepare the module region's .rodata section. */
+		bhv_create_section(head, mod->mem[MOD_RODATA].base,
+				   mod->mem[MOD_RODATA].size,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (mod->mem[MOD_RO_AFTER_INIT].size) {
+		bhv_create_section(head, mod->mem[MOD_RO_AFTER_INIT].base,
+				   mod->mem[MOD_RO_AFTER_INIT].size,
+				   HypABI__Integrity__MemType__DATA,
+				   base_flags |
+					   HypABI__Integrity__MemFlags__MUTABLE,
+				   "MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_DATA].size) {
+		bhv_create_section(head, mod->mem[MOD_DATA].base,
+				   mod->mem[MOD_DATA].size,
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout,
+				   unsigned long base_flags)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   HypABI__Integrity__MemType__CODE_PATCHABLE,
+			   base_flags, "MODULE TEXT SECTION");
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   base_flags, "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			HypABI__Integrity__MemType__DATA,
+			base_flags | HypABI__Integrity__MemFlags__MUTABLE,
+			"MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   HypABI__Integrity__MemType__DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static inline void bhv_prepare_mod_init(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout, base_flags);
+}
+
+static inline void bhv_prepare_mod_core(struct list_head *head,
+					const struct module *mod,
+					unsigned long base_flags)
+{
+	bhv_prepare_mod_layout(head, &mod->core_layout, base_flags);
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_init(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+
+#ifndef VASKM // inside kernel tree
+	bhv_prepare_mod_core(head, mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_prepare_mod_core(head, mod,
+				     HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	/* Prepare the module region's .text section. */
+	if (mod->mem[MOD_INIT_TEXT].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_TEXT].base,
+				   mod->mem[MOD_INIT_TEXT].size);
+
+	/* Prepare the module region's .rodata section. */
+	if (mod->mem[MOD_INIT_RODATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_RODATA].base,
+				   mod->mem[MOD_INIT_RODATA].size);
+
+	/* Prepare the module region's .data section. */
+	if (mod->mem[MOD_INIT_DATA].size)
+		bhv_remove_section(bhv_region_list_head,
+				   mod->mem[MOD_INIT_DATA].base,
+				   mod->mem[MOD_INIT_DATA].size);
+}
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+static inline void
+_bhv_complete_free_init(const struct module *mod,
+			struct list_head *bhv_region_list_head)
+{
+	const struct module_layout *layout = &mod->init_layout;
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+}
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+static void bhv_complete_free_init(const struct module *mod)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	_bhv_complete_free_init(mod, &bhv_region_list_head);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region.remove);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod,
+				     unsigned long base_flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0)
+	void *base = mod->mem[MOD_RO_AFTER_INIT].base;
+	unsigned int size = mod->mem[MOD_RO_AFTER_INIT].size;
+#else /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+#endif /* LINUX_VERSION_CODE >= KERNEL_VERSION(6, 4, 0) */
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   HypABI__Integrity__MemType__DATA_READ_ONLY,
+			   base_flags);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region.update);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#ifndef VASKM // inside kernel tree
+	bhv_update_ro_after_init(mod, HypABI__Integrity__MemFlags__TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_update_ro_after_init(mod,
+					 HypABI__Integrity__MemFlags__NONE);
+	} else {
+		bhv_update_ro_after_init(
+			mod, HypABI__Integrity__MemFlags__TRANSIENT);
+	}
+#endif // VASKM
+	bhv_complete_free_init(mod);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags,
+			   "BPF SECTION");
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__DATA_READ_ONLY,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, HypABI__Integrity__MemType__CODE_PATCHABLE,
+			HypABI__Integrity__MemFlags__TRANSIENT);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_add_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT,
+			       ((size + PAGE_SIZE - 1) >> PAGE_SHIFT));
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) ||                           \
+	defined(VASKM_HAVE_BPF_PACK)
+	bhv_rm_bpf_code_range(((uint64_t)base) >> PAGE_SHIFT);
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
+	bhv_release_memory_by_owner((uint64_t)base);
+}
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags,
+				char *description)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+	BUG_ON(owner == 0);
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	bhv_create_section(&bhv_region_list_head, base, size, type, flags,
+			   description);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region.create);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+#endif //VASKM
diff --git security/bhv/patch_alternative.c security/bhv/patch_alternative.c
new file mode 100644
index 00000000000..aed5d9b742e
--- /dev/null
+++ security/bhv/patch_alternative.c
@@ -0,0 +1,235 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/sync_core.h>
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/patch.h>
+#include <bhv/kversion.h>
+
+#include <asm/bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+/**************************************************
+ * start
+ **************************************************/
+void bhv_start_delete_alternatives(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+/**************************************************/
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe(i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		pr_err("BHV: %s: Unknown module!\n", __FUNCTION__);
+		rv = -EACCES;
+		goto out;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+out:
+	// Close vault.
+	if (HypABI__Richard__Close__hypercall())
+		pr_err("%s: could not close vault", __FUNCTION__);
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(&search, arch,
+					  bhv_alternatives_find_by_alt);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+// CONFIG_STACK_VALIDATION is used in 5.15 and CONFIG_OBJTOOL in 6.1
+#if defined(CONFIG_RETPOLINE) && \
+	(defined(CONFIG_STACK_VALIDATION) || defined(CONFIG_OBJTOOL))
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) */
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+/* XXX: CONSIDER MOVING THIS PART INTO TEXT_POKE_EARLY!! */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+void bhv_apply_alternatives(void *addr, const void *opcode, size_t len)
+{
+	int rc;
+	unsigned long flags;
+
+	/* XXX: Do we still need this? */
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(addr, opcode, len);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+}
+
+#endif
diff --git security/bhv/patch_bpf.c security/bhv/patch_bpf.c
new file mode 100644
index 00000000000..13aaa72cbce
--- /dev/null
+++ security/bhv/patch_bpf.c
@@ -0,0 +1,219 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com> 
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+void init_xmem(void);
+
+#include <linux/version.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/vault.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(6, 1, 0) || \
+	defined(VASKM_HAVE_BPF_PACK)
+struct bhv_bpf_code_range {
+	uint64_t start_pfn;
+	size_t num_pages;
+	struct list_head next;
+};
+
+static DEFINE_MUTEX(bhv_bpf_mutex);
+static LIST_HEAD(bhv_bpf_code_ranges_head);
+
+static void __always_inline bhv_bpf_lock(void)
+{
+	mutex_lock(&bhv_bpf_mutex);
+}
+
+static void __always_inline bhv_bpf_unlock(void)
+{
+	mutex_unlock(&bhv_bpf_mutex);
+}
+
+void bhv_add_bpf_code_range(uint64_t pfn, size_t num_pages)
+{
+	struct bhv_bpf_code_range *n =
+		kzalloc(sizeof(struct bhv_bpf_code_range), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->start_pfn = pfn;
+	n->num_pages = num_pages;
+
+	bhv_bpf_lock();
+	list_add(&(n->next), &bhv_bpf_code_ranges_head);
+	bhv_bpf_unlock();
+}
+
+void bhv_rm_bpf_code_range(uint64_t pfn)
+{
+	struct bhv_bpf_code_range *i, *tmp;
+
+	bhv_bpf_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_bpf_code_ranges_head, next) {
+		if (i->start_pfn == pfn) {
+			list_del(&(i->next));
+			kfree(i);
+			break;
+		}
+	}
+	bhv_bpf_unlock();
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool __bhv_text bhv_bpf_check_write(void *dst, size_t sz)
+{
+	struct bhv_bpf_code_range *i;
+	uint64_t start_pfn = ((uint64_t)dst) >> PAGE_SHIFT;
+	size_t num_pages = ((sz + PAGE_SIZE - 1) >> PAGE_SHIFT);
+	uint64_t end_pfn = (start_pfn + num_pages) - 1;
+
+	list_for_each_entry(i, &bhv_bpf_code_ranges_head, next) {
+		if (start_pfn >= i->start_pfn &&
+		    start_pfn < (i->start_pfn + i->num_pages)) {
+			if (end_pfn >= i->start_pfn &&
+			    end_pfn < (i->start_pfn + i->num_pages))
+				return true;
+		}
+	}
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_write_vault(void *dst, void *src, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change.
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall(dst, src, sz);
+#else
+	rv = bhv_patch_hypercall(dst, src, sz, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_write_vault);
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int __bhv_text bhv_bpf_invalidate_vault(void *dst, uint8_t b, size_t sz)
+{
+	int rv;
+
+	if (sz == 0)
+		return 0;
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+#endif
+
+	if (!bhv_bpf_check_write(dst, sz)) {
+		pr_warn("BHV: Attempt to overwrite non-BPF region!\n");
+
+		if (bhv_patch_violation_hypercall(
+			    dst, "Attempt to overwrite non-BPF region")) {
+			// Block attempt.
+			rv = -EINVAL;
+			goto out;
+		}
+
+		// Allow change
+	}
+
+#ifdef CONFIG_BHV_VAULT_SPACES
+	rv = bhv_patch_hypercall_memset(dst, sz, b);
+#else
+	rv = bhv_patch_hypercall_memset(dst, sz, b, false);
+#endif
+
+	if (rv) {
+		pr_err("BHV: patch write fail.\n");
+		goto out;
+	}
+
+out:
+#ifndef CONFIG_BHV_VAULT_SPACES
+	if (HypABI__Richard__Close__hypercall())
+        	pr_err("%s: could not close vault", __FUNCTION__);
+#endif
+	return rv;
+}
+BHV_VAULT_ADD_ENTRY_POINT(jump_label, bhv_bpf_invalidate_vault);
+
+int bhv_bpf_write(void *dst, void *src, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_write_vault(dst, src, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+
+int bhv_bpf_invalidate(void *dst, uint8_t b, size_t sz)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	bhv_bpf_lock();
+	local_irq_save(flags);
+	rv = bhv_bpf_invalidate_vault(dst, b, sz);
+	local_irq_restore(flags);
+	bhv_bpf_unlock();
+
+	BUG_ON(rv);
+
+	return rv;
+}
+#endif // LINUX_VERSION_CODE >= 6.1 || VASKM_HAVE_BPF_PACK
diff --git security/bhv/patch_jump_label.c security/bhv/patch_jump_label.c
new file mode 100644
index 00000000000..cc80ad107af
--- /dev/null
+++ security/bhv/patch_jump_label.c
@@ -0,0 +1,573 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BlueRock Security Inc.
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#ifndef CONFIG_BHV_VAULT_SPACES
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static void __always_inline bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static void __always_inline bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe(i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+enum jump_label_type __bhv_text bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+typedef enum {
+	FAIL = 0,
+	SKIP,
+	SUCCESS,
+} jump_label_validation_t;
+
+jump_label_validation_t __bhv_text validate_jmp_labels(struct jump_entry *entry,
+						       const void *opcode,
+						       size_t len,
+						       char *error_msg)
+{
+	struct bhv_static_key_mod *i;
+	unsigned long addr = (unsigned long)jump_entry_code(entry);
+	struct jump_entry *iter;
+	unsigned long tmp_addr;
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		// We should only modify the kernel text section
+		if (!kernel_text_address(addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				addr);
+			return FAIL;
+		}
+
+		if (!bhv_jump_label_validate_opcode(
+			    entry, bhv_jump_label_type(entry), opcode, len)) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Jump label opcode validation failed");
+			return FAIL;
+		}
+
+		// Search for overlapping jump labels.
+		for (iter = KLN_SYMBOL(struct jump_entry *,
+				       __start___jump_table);
+		     iter <
+		     KLN_SYMBOL(struct jump_entry *, __stop___jump_table);
+		     iter++) {
+			if (iter == entry)
+				continue;
+
+			tmp_addr = (unsigned long)jump_entry_code(iter);
+
+			if (addr <= tmp_addr && tmp_addr < addr + len) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label (0x%lx) overlaps with jump label (0x%lx)",
+					(unsigned long)(entry),
+					(unsigned long)(iter));
+				return FAIL;
+			}
+		}
+
+		return SUCCESS;
+	}
+
+	list_for_each_entry(i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			struct module *tmp;
+
+			if (i->mod->state != MODULE_STATE_COMING &&
+			    i->mod->state != MODULE_STATE_LIVE) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Invalid module state for jump label (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			if ((jump_entry_is_init(entry) &&
+			     i->mod->state != MODULE_STATE_COMING)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Trying to apply jump label in incorrect module state (%u)",
+					i->mod->state);
+				return FAIL;
+			}
+
+			// Module entries should only point to the text section
+			// of the module
+			// Note: We use the kernel API to check this. We could perform
+			// the check manually but using the kernel API seems more stable.
+			tmp = __module_text_address(addr);
+			if (tmp == NULL) {
+				// No module found
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Jump label trying to write address outside of module (0x%lx)",
+					addr);
+				return FAIL;
+			}
+
+			if (tmp != i->mod) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label (0x%lx<->0x%lx)",
+					(unsigned long)tmp,
+					(unsigned long)i->mod);
+				return FAIL;
+			}
+
+			if (!bhv_jump_label_validate_opcode(
+				    entry, bhv_jump_label_type(entry), opcode,
+				    len)) {
+				snprintf(
+					error_msg,
+					HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+					"Found invalid module for jump label");
+				return FAIL;
+			}
+
+			// Search for overlapping jump labels.
+			for (iter = i->entries_start; iter < i->entries_stop;
+			     iter++) {
+				if (iter == entry)
+					continue;
+
+				tmp_addr = (unsigned long)jump_entry_code(iter);
+
+				if (addr <= tmp_addr && tmp_addr < addr + len) {
+					snprintf(
+						error_msg,
+						HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+						"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+						(unsigned long)entry,
+						(unsigned long)iter);
+					return FAIL;
+				}
+			}
+
+			return SUCCESS;
+		}
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+	return FAIL;
+}
+
+int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+					  const void *opcode, size_t len)
+{
+	int rv = 0;
+	unsigned long r;
+	jump_label_validation_t validation_ok;
+	void *dest_virt_addr = (void *)jump_entry_code(entry);
+	char message[HypABI__Patch__PatchViolation__MAX_MSG_SZ];
+
+	rv = HypABI__Richard__Open__hypercall();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > HypABI__Patch__MAX_PATCH_SZ) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return -E2BIG;
+	}
+
+	validation_ok = validate_jmp_labels(entry, opcode, len, message);
+	if (validation_ok == SKIP) {
+		if (HypABI__Richard__Close__hypercall())
+			pr_err("%s: could not close vault", __FUNCTION__);
+		return 0;
+	}
+
+	if (validation_ok == FAIL) {
+		if (bhv_patch_violation_hypercall(dest_virt_addr, message)) {
+			// Block this patch
+			if (HypABI__Richard__Close__hypercall())
+				pr_err("%s: could not close vault",
+				       __FUNCTION__);
+			return -EACCES;
+		}
+
+		// The violation should only be logged. Thus we continue.
+	}
+
+	r = bhv_patch_hypercall(dest_virt_addr, opcode, (uint64_t)len, true);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+	return rv;
+}
+
+#else /* CONFIG_BHV_VAULT_SPACES */
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool has_overlapping_entries(struct jump_entry *entry,
+				    struct jump_entry *start,
+				    struct jump_entry *stop, struct module *mod,
+				    size_t op_size,
+				    char *error_msg)
+{
+	unsigned long addr = jump_entry_code(entry);
+	struct jump_entry *iter = NULL;
+	struct module *tmp_mod;
+
+	/*
+        * Module entries should only point to the text section of the module.
+        */
+	preempt_disable();
+	tmp_mod = __module_text_address(addr);
+	preempt_enable();
+
+	if (tmp_mod == NULL) {
+		snprintf(
+			error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			"Jump label trying to write address outside of module (0x%lx | entry @ 0x%px)",
+			addr, entry);
+		return false;
+	}
+
+	if (tmp_mod != mod) {
+		snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+			 "Found invalid module for jump label (0x%lx<->0x%lx)",
+			 (unsigned long)tmp_mod, (unsigned long)mod);
+		return false;
+	}
+
+	/* Disallow overlapping entries. */
+	for (iter = start; iter < stop; iter++) {
+		unsigned long tmp_addr;
+		int tmp_size;
+
+		if (iter == entry)
+			continue;
+
+		tmp_addr = (unsigned long)jump_entry_code(iter);
+#ifdef JUMP_LABEL_NOP_SIZE
+		tmp_size = JUMP_LABEL_NOP_SIZE;
+#else
+		arch_jump_entry_size(entry);
+#endif
+
+		if ((addr <= tmp_addr && tmp_addr < addr + op_size) ||
+		    (addr >= tmp_addr && addr < tmp_addr + tmp_size)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label (0x%lx) overlaps with other jump label (0x%lx)",
+				(unsigned long)entry, (unsigned long)iter);
+			return false;
+		}
+	}
+
+	return true;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_mod(struct jump_entry *entry,
+					size_t op_size,
+					char *error_msg)
+{
+	struct static_key_mod *m;
+	struct static_key_mod *jlm;
+
+	struct jump_entry *iter_start;
+	struct jump_entry *iter_stop;
+
+	struct static_key *key = jump_entry_key(entry);
+
+	if (!static_key_linked(key)) {
+		struct module *mod;
+
+		preempt_disable();
+		mod = __module_address((unsigned long)key);
+		if (mod) {
+			iter_start = mod->jump_entries;
+			iter_stop = mod->jump_entries + mod->num_jump_entries;
+		}
+		preempt_enable();
+
+		if (!mod) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Unknown module jump label (%lx)",
+				 (unsigned long)entry);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	jlm = static_key_mod(jump_entry_key(entry));
+
+	//pr_info("%s:%d\n", __FUNCTION__, __LINE__);
+
+	for (m = jlm; m; m = m->next) {
+		struct module *mod = NULL;
+
+		if (!m->entries)
+			continue;
+
+		mod = m->mod;
+
+		if (!mod)
+			continue;
+
+		//pr_info("%s:%d | %s addr @ 0x%lx\n", __FUNCTION__, __LINE__, mod->name, addr);
+
+		iter_start = mod->jump_entries;
+		iter_stop = iter_start + mod->num_jump_entries;
+
+		/* Continue if we did not find the correct jump label table. */
+		if (entry < iter_start || entry >= iter_stop)
+			continue;
+
+		if (mod->state != MODULE_STATE_COMING &&
+		    mod->state != MODULE_STATE_LIVE) {
+			snprintf(error_msg,
+				 HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				 "Invalid module state for jump label (%u)",
+				 mod->state);
+			return false;
+		}
+
+		if (jump_entry_is_init(entry) &&
+		    mod->state != MODULE_STATE_COMING) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Trying to apply jump label in incorrect module state (%u)",
+				mod->state);
+			return false;
+		}
+
+		return has_overlapping_entries(entry, iter_start, iter_stop,
+					       mod, op_size, error_msg);
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Unknown jump label (0x%lx)", (unsigned long)entry);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static bool bhv_validate_jump_label_kern(struct jump_entry *entry,
+					 char *error_msg)
+{
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		/*
+                 * Ensure that this jump label only modifies the kernel .text
+                 * segment.
+                 */
+		if (!__kernel_text_address((unsigned long)dest_addr)) {
+			snprintf(
+				error_msg,
+				HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+				"Jump label destination (0x%lx) outside of kernel text region",
+				(unsigned long)dest_addr);
+			return false;
+		}
+
+		/*
+                 * Note: we do not search for overlapping target addresses of
+                 * jump labels that are part of the kernel image; the section
+                 * holding the jump labels cannot be overwritten from the
+                 * outside of the vault.
+                 */
+
+		//pr_info("%s:%d | kern addr @ 0x%px\n", __FUNCTION__, __LINE__, dest_addr);
+
+		return true;
+	}
+
+	snprintf(error_msg, HypABI__Patch__PatchViolation__MAX_MSG_SZ,
+		 "Jump label destination (0x%lx) outside of kernel text region",
+		 (unsigned long)dest_addr);
+
+	return false;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+static int bhv_validate_jump_label_entry(struct jump_entry *entry,
+					 const void *opcode, size_t op_size)
+{
+	bool is_valid = false;
+	HypABI__Patch__PatchViolation__arg__T violation_arg;
+	char *error_msg_ptr = violation_arg.message;
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		is_valid = bhv_validate_jump_label_kern(entry, error_msg_ptr);
+	} else {
+		is_valid = bhv_validate_jump_label_mod(entry, op_size, error_msg_ptr);
+	}
+
+	if (!is_valid) {
+		if (bhv_patch_violation_hypercall(
+			    (void *)jump_entry_code(entry), error_msg_ptr)) {
+			return -EACCES;
+		}
+
+		dump_stack();
+		BUG();
+
+		/* XXX: Shall we continue without reporting an issue? */
+		return -1;
+	}
+
+	return 0;
+}
+
+BHV_VAULT_ADD_TO_CODE_REGION(jump_label)
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t op_size)
+{
+	int rc = 0;
+	unsigned long flags;
+	void *dest_addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+	if (op_size > HypABI__Patch__PatchViolation__MAX_MSG_SZ) {
+		return -E2BIG;
+	}
+
+#ifdef VASKM // out of tree
+	bhv_arg_ptr =
+		(bhv_patch_arg_t *)kmalloc(sizeof(bhv_patch_arg_t), GFP_KERNEL);
+
+	if (!bhv_arg_ptr) {
+		pr_err("BHV: cannot allocate bhv_arg\n");
+		return -ENOMEM;
+	}
+#endif // VASKM
+
+	rc = bhv_validate_jump_label_entry(entry, opcode, op_size);
+	if (rc) {
+		dump_stack();
+		BUG();
+
+		return rc;
+	}
+
+	//pr_info("[BHV] Patching addr @ 0x%px with %llx (size=%d)\n", dest_addr, *((uint64_t *)opcode), op_size);
+
+	local_irq_save(flags);
+
+	rc = bhv_patch_hypercall(dest_addr, opcode, (uint64_t)op_size);
+	if (rc)
+		panic("BHV patch hypercall failure! hypercall returned %u", rc);
+
+	local_irq_restore(flags);
+
+	return rc;
+}
+#endif /* CONFIG_BHV_VAULT_SPACES */
diff --git security/bhv/reg_protect.c security/bhv/reg_protect.c
new file mode 100644
index 00000000000..5cae2a0da5b
--- /dev/null
+++ security/bhv/reg_protect.c
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2024 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+
+#include <bhv/reg_protect.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/interface/abi_hl_autogen.h>
+
+int bhv_reg_protect_freeze(
+	enum HypABI__RegisterProtection__Freeze__RegisterSelector reg_selector,
+	uint64_t freeze_bitfield)
+{
+	return HypABI__RegisterProtection__Freeze__HYPERCALL(
+			.register_selector = reg_selector,
+			.freeze_bitfield = freeze_bitfield);
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void bhv_start_reg_protect(void)
+{
+	bhv_start_reg_protect_arch();
+}
+/***************************************************/
diff --git security/bhv/sysfs.c security/bhv/sysfs.c
new file mode 100644
index 00000000000..4708438d34b
--- /dev/null
+++ security/bhv/sysfs.c
@@ -0,0 +1,67 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef CONFIG_SYSFS
+#error CONFIG_SYSFS required!
+#endif
+
+#include <linux/kobject.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_fops.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/sysfs_reg_protect.h>
+#include <bhv/sysfs_version.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+
+/**********************************************************
+ * start
+ **********************************************************/
+void __init_km bhv_start_sysfs(void)
+{
+	struct kobject *bhv_kobj;
+	struct kobject *version_kobj;
+	struct kobject *integrity_kobj;
+	struct kobject *integrity_freeze_kobj;
+	struct kobject *register_kobj;
+	struct kobject *register_freeze_kobj;
+	struct kobject *fops_kobj;
+	struct kobject *fopsstatus_kobj;
+
+#define CREATE_KOBJ(kobj, name, parent)                                        \
+	kobj = kobject_create_and_add(name, parent);                           \
+	BUG_ON(!kobj);
+
+#ifndef CONFIG_SYS_HYPERVISOR
+	struct kobject *hypervisor_kobj;
+	CREATE_KOBJ(hypervisor_kobj, "hypervisor", NULL);
+#endif // CONFIG_SYS_HYPERVISOR
+
+	CREATE_KOBJ(bhv_kobj, "bhv", hypervisor_kobj);
+	
+	CREATE_KOBJ(version_kobj, "version", bhv_kobj);
+	bhv_start_sysfs_version(version_kobj);
+
+	CREATE_KOBJ(integrity_kobj, "integrity", bhv_kobj);
+	CREATE_KOBJ(integrity_freeze_kobj, "freeze", integrity_kobj);
+	bhv_start_sysfs_integrity_freeze(integrity_freeze_kobj);
+
+	if (bhv_reg_protect_is_enabled()) {
+		CREATE_KOBJ(register_kobj, "register", bhv_kobj);
+		CREATE_KOBJ(register_freeze_kobj, "freeze", register_kobj);
+
+		bhv_start_sysfs_reg_protect(register_freeze_kobj);
+	}
+
+	if (bhv_fileops_file_protection_is_enabled()) {
+		CREATE_KOBJ(fops_kobj, "fileops_protection", bhv_kobj);
+		CREATE_KOBJ(fopsstatus_kobj, "status", fops_kobj);
+
+		bhv_start_sysfs_fileops_protection(fops_kobj, fopsstatus_kobj);
+	}
+}
+/**********************************************************/
diff --git security/bhv/sysfs_fops.c security/bhv/sysfs_fops.c
new file mode 100644
index 00000000000..281abdd089f
--- /dev/null
+++ security/bhv/sysfs_fops.c
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kobject.h>
+#include <linux/printk.h>
+#include <asm-generic/set_memory.h>
+
+#include <bhv/bhv.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/module.h>
+#include <bhv/interface/abi_base_autogen.h>
+
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf);
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf);
+
+struct bhv_fopsstatus_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint8_t param;
+} static const __section(".rodata") _bhv_fopsstatus_sysfs_data[] = {
+#define FOPS_MAP(name, idx, _, __)                                             \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected,     \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FOPS_MAP_DIRNULL(name, idx, _)                                         \
+	{                                                                      \
+		.attr = __ATTR(name, 0400, _bhv_fops_sysfs_show_protected_dn,  \
+			       NULL),                                          \
+		.param = idx,                                                  \
+	},
+#define FILEOPS_INTERNAL_FOPSMAP_ALL
+#include <bhv/fileops_internal_fopsmap.h>
+};
+#define attr_to_bsd(ptr)                                                       \
+	container_of((ptr), struct bhv_fopsstatus_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_fops_sysfs_show_protected(struct kobject *kobj,
+					      struct kobj_attribute *attr,
+					      char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c %c\n",
+			 fileops_map[data->param][0] ? '1' : '0',
+			 fileops_map[data->param][1] ? '1' : '0');
+}
+
+static ssize_t _bhv_fops_sysfs_show_protected_dn(struct kobject *kobj,
+						 struct kobj_attribute *attr,
+						 char *buf)
+{
+	struct bhv_fopsstatus_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n",
+			 fileops_map[data->param][0] ? '1' : '0');
+}
+#undef attr_to_bsd
+
+#ifdef VASKM // out of tree
+
+bool bhv_allow_update_fileops_map = true;
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf);
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count);
+
+static char _bhv_fopsfreeze_flag;
+struct bhv_fopsfreeze_sysfs_data {
+	const struct kobj_attribute attr;
+} static _bhv_fopsfreeze_sysfs_data[] = {
+	{
+		.attr = __ATTR(freeze, 0644, _bhv_intfr_sysfs_show_freeze,
+			       _bhv_fops_sysfs_store_freeze),
+	},
+};
+
+int bhv_freeze_fops_map(void)
+{
+	int rc;
+	bhv_allow_update_fileops_map = false;
+	rc = set_memory_ro((unsigned long)fileops_map, 1);
+	if (rc)
+		return rc;
+	bhv_protect_generic_memory((unsigned long)THIS_MODULE, fileops_map,
+				   PAGE_SIZE,
+				   HypABI__Integrity__MemType__DATA_READ_ONLY,
+				   HypABI__Integrity__MemFlags__NONE,
+				   "FILEOPS_MAP");
+	_bhv_fopsfreeze_flag = '1';
+	return 0;
+}
+
+static ssize_t _bhv_intfr_sysfs_show_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%c\n", _bhv_fopsfreeze_flag);
+}
+
+static ssize_t _bhv_fops_sysfs_store_freeze(struct kobject *kobj,
+					    struct kobj_attribute *attr,
+					    const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1')) {
+		if (_bhv_fopsfreeze_flag == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((_bhv_fopsfreeze_flag == '0') && (buf[0] == '1')) {
+			int ret = bhv_freeze_fops_map();
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_fileops_protection(struct kobject *fops,
+						  struct kobject *status)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsstatus_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsstatus_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(status, attr_array));
+
+#ifdef VASKM // out of tree
+	static_assert(ARRAY_SIZE(_bhv_fopsstatus_sysfs_data) >=
+		      ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data));
+	for (i = 0; i < ARRAY_SIZE(_bhv_fopsfreeze_sysfs_data); ++i)
+		attr_array[i] = &_bhv_fopsfreeze_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(fops, attr_array));
+#endif // VASKM
+}
+/***************************************************/
diff --git security/bhv/sysfs_integrity_freeze.c security/bhv/sysfs_integrity_freeze.c
new file mode 100644
index 00000000000..ca7dd70ac55
--- /dev/null
+++ security/bhv/sysfs_integrity_freeze.c
@@ -0,0 +1,102 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count);
+
+struct bhv_intfr_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t param;
+	const bool *const flagp;
+} static const __section(".rodata") _bhv_intfr_sysfs_data[] = {
+	{ .attr = __ATTR(create, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__CREATE,
+	  .flagp = &bhv_integrity_freeze_create_currently_frozen },
+	{ .attr = __ATTR(update, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__UPDATE,
+	  .flagp = &bhv_integrity_freeze_update_currently_frozen },
+	{ .attr = __ATTR(remove, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__REMOVE,
+	  .flagp = &bhv_integrity_freeze_remove_currently_frozen },
+	{ .attr = __ATTR(patch, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .param = HypABI__Integrity__Freeze__FreezeFlags__PATCH,
+	  .flagp = &bhv_integrity_freeze_patch_currently_frozen },
+};
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_intfr_sysfs_data, attr)
+#define cur_flag_val(bisdp) (*((bisdp)->flagp) ? '1' : '0')
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+	return scnprintf(buf, PAGE_SIZE, "%c\n", cur_flag_val(data));
+}
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1' || buf[0] == '2')) {
+		struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+		if (cur_flag_val(data) == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((cur_flag_val(data) == '0') &&
+			   (buf[0] == '1' || buf[0] == '2')) {
+			int ret = bhv_enable_integrity_freeze_flag(
+				data->param, buf[0] == '2');
+			if (ret) {
+				return ret;
+			} else {
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_integrity_freeze(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_intfr_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_intfr_sysfs_data); ++i)
+		attr_array[i] = &_bhv_intfr_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_reg_protect.c security/bhv/sysfs_reg_protect.c
new file mode 100644
index 00000000000..812cc266ba7
--- /dev/null
+++ security/bhv/sysfs_reg_protect.c
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/abi_base_autogen.h>
+#include <bhv/reg_protect.h>
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count);
+
+struct bhv_rp_sysfs_data {
+	const struct kobj_attribute attr;
+	const enum HypABI__RegisterProtection__Freeze__RegisterSelector reg;
+} static const __section(".rodata") _bhv_rp_sysfs_data[] = {
+#define OP(regn)                                                               \
+	{ .attr = __ATTR(regn, 0640, _bhv_rp_sysfs_show, _bhv_rp_sysfs_store), \
+	  .reg = HypABI__RegisterProtection__Freeze__RegisterSelector__##regn },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+struct bhv_rp_sysfs_data_var {
+	uint64_t curr_status;
+} static _bhv_rp_sysfs_data_var[] = {
+#define OP(regn) { .curr_status = 0UL },
+	HypABI__RegisterProtection__Freeze__RegisterSelector__LABELS
+#undef OP
+};
+
+static_assert(ARRAY_SIZE(_bhv_rp_sysfs_data) ==
+	      ARRAY_SIZE(_bhv_rp_sysfs_data_var));
+
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_rp_sysfs_data, attr)
+#define attr_to_bsd_var(ptr) \
+	_bhv_rp_sysfs_data_var + (attr_to_bsd(ptr) - _bhv_rp_sysfs_data)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static DEFINE_MUTEX(_bhv_rp_sysfs_lock);
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int b;
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	b = scnprintf(buf, PAGE_SIZE, "%016llx\n", data_var->curr_status);
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+	return b;
+}
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	struct bhv_rp_sysfs_data_var *data_var = attr_to_bsd_var(attr);
+	uint64_t nval;
+	int rv = 0;
+	int i = sscanf(buf, "%llx", &nval);
+	if (i != 1) {
+		return -EINVAL;
+	}
+
+	mutex_lock(&_bhv_rp_sysfs_lock);
+	nval |= data_var->curr_status;
+
+	rv = bhv_reg_protect_freeze(data->reg, nval);
+
+	if (!rv)
+		data_var->curr_status = nval;
+	mutex_unlock(&_bhv_rp_sysfs_lock);
+
+	if (rv < 0) {
+		return rv;
+	} else {
+		return count;
+	}
+}
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_reg_protect(struct kobject *kobj)
+{
+	int i;
+	const struct attribute *attr_array
+		[1 +
+		 HypABI__RegisterProtection__Freeze__RegisterSelector__COUNT];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_rp_sysfs_data); ++i)
+		attr_array[i] = &_bhv_rp_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/sysfs_version.c security/bhv/sysfs_version.c
new file mode 100644
index 00000000000..bf81c6c86bf
--- /dev/null
+++ security/bhv/sysfs_version.c
@@ -0,0 +1,110 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BlueRock Security Inc.
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_version.h>
+
+#define __BHV_VERSION(a, b, c) a, b, c
+#define __BHV_VAS_ABI_VERSION(a, b, c, d) a, b, c, d
+#include <bhv/version.h>
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf);
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf);
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf);
+#endif // VASKM
+
+
+struct bhv_version_sysfs_data {
+	const struct kobj_attribute attr;
+} static const __section(".rodata") _bhv_version_sysfs_data[] = {
+
+	{
+		.attr = __ATTR(bhv_version, 0600,
+			       _bhv_version_sysfs_show_bhv_version, NULL),
+	},
+	{
+		.attr = __ATTR(bhv_abi_version, 0600,
+			       _bhv_version_sysfs_show_bhv_abi_version, NULL),
+	},
+#ifndef VASKM // inside kernel tree
+	{
+		.attr = __ATTR(linux_commit, 0600,
+			       _bhv_version_sysfs_show_linux_commit, NULL),
+	},
+#else // out of tree
+	{
+		.attr = __ATTR(vaskm_commit, 0600,
+			       _bhv_version_sysfs_show_vaskm_commit, NULL),
+	},
+#endif // VASKM
+};
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_version_sysfs_show_bhv_version(struct kobject *kobj,
+						   struct kobj_attribute *attr,
+						   char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02d.%02d.%d\n", BHV_VERSION);
+}
+
+static ssize_t
+_bhv_version_sysfs_show_bhv_abi_version(struct kobject *kobj,
+					struct kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%02u.%02u.%02u:%05u\n", HypABI__ABI_VERSION);
+}
+
+#ifndef VASKM // inside kernel tree
+static ssize_t _bhv_version_sysfs_show_linux_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", KERNEL_COMMIT_HASH);
+}
+
+#else // out of tree
+static ssize_t _bhv_version_sysfs_show_vaskm_commit(struct kobject *kobj,
+						    struct kobj_attribute *attr,
+						    char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%s\n", VASKM_COMMIT_HASH);
+}
+#endif // VASKM
+
+/***************************************************
+ * start
+ ***************************************************/
+void __init_km bhv_start_sysfs_version(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_version_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_version_sysfs_data); ++i)
+		attr_array[i] = &_bhv_version_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
+/***************************************************/
diff --git security/bhv/vmalloc_to_page.c security/bhv/vmalloc_to_page.c
new file mode 100644
index 00000000000..8abf8b3a052
--- /dev/null
+++ security/bhv/vmalloc_to_page.c
@@ -0,0 +1,86 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copied from kernel v6.1, mm/vmalloc.c
+/*
+ *  Copyright (C) 1993  Linus Torvalds
+ *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
+ *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
+ *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002
+ *  Numa awareness, Christoph Lameter, SGI, June 2005
+ *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <bhv/vault.h>
+
+#ifdef VASKM // out of tree
+#include <kln.h>
+#undef pgd_offset_k
+#define pgd_offset_k(address)                                                  \
+	pgd_offset(KLN_SYMBOL(struct mm_struct *, init_mm), (address))
+#endif // VASKM
+
+
+
+/*
+ * Walk a vmap address to the struct page it maps. Huge vmap mappings will
+ * return the tail page that corresponds to the base page address, which
+ * matches small vmap mappings.
+ */
+BHV_VAULT_ADD_TO_SHARED_CODE_REGION(jump_label)
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr)
+{
+	unsigned long addr = (unsigned long) vmalloc_addr;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	/*
+	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
+	 * architectures that do not vmalloc module space
+	 */
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (pgd_none(*pgd))
+		return NULL;
+	if (WARN_ON_ONCE(pgd_leaf(*pgd)))
+		return NULL; /* XXX: no allowance for huge pgd */
+	if (WARN_ON_ONCE(pgd_bad(*pgd)))
+		return NULL;
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return NULL;
+	if (p4d_leaf(*p4d))
+		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(p4d_bad(*p4d)))
+		return NULL;
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return NULL;
+	if (pud_leaf(*pud))
+		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return NULL;
+	if (pmd_leaf(*pmd))
+		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pmd_bad(*pmd)))
+		return NULL;
+
+	ptep = pte_offset_map(pmd, addr);
+	pte = *ptep;
+	if (pte_present(pte))
+		page = pte_page(pte);
+	pte_unmap(ptep);
+
+	return page;
+}
diff --git security/integrity/digsig.c security/integrity/digsig.c
index f2193c531f4..8da00cc5394 100644
--- security/integrity/digsig.c
+++ security/integrity/digsig.c
@@ -17,6 +17,8 @@
 #include <crypto/public_key.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
+
 #include "integrity.h"
 
 static struct key *keyring[INTEGRITY_KEYRING_MAX];
@@ -45,6 +47,7 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 		return ERR_PTR(-EINVAL);
 
 	if (!keyring[id]) {
+		int rc = 0;
 		keyring[id] =
 			request_key(&key_type_keyring, keyring_name[id], NULL);
 		if (IS_ERR(keyring[id])) {
@@ -53,6 +56,19 @@ static struct key *integrity_keyring_from_id(const unsigned int id)
 			keyring[id] = NULL;
 			return ERR_PTR(err);
 		}
+
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] gets registered
+		 * directly in set_platform_trusted_keys.
+		 */
+		rc = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
+
+	} else {
+		int rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+		if (rc)
+			return ERR_PTR(rc);
 	}
 
 	return keyring[id];
@@ -111,6 +127,21 @@ static int __init __integrity_init_keyring(const unsigned int id,
 			keyring_name[id], err);
 		keyring[id] = NULL;
 	} else {
+		/*
+		 * The keyring[INTEGRITY_KEYRING_PLATFORM] and
+		 * keyring[INTEGRITY_KEYRING_MACHINE] get also registered in
+		 * set_platform_trusted_keys and set_machine_trusted_keys,
+		 * respectively, to allow passing the given keyring directly and
+		 * indirectly (through INTEGRITY_KEYRING_PLATFORM and
+		 * INTEGRITY_KEYRING_MACHINE) to verify_pkcs7_message_sig.
+		 */
+		err = bhv_keyring_register_system_trusted(&keyring[id]);
+		if (err) {
+			key_put(keyring[id]);
+			keyring[id] = NULL;
+			return err;
+		}
+
 		if (id == INTEGRITY_KEYRING_PLATFORM)
 			set_platform_trusted_keys(keyring[id]);
 		if (id == INTEGRITY_KEYRING_MACHINE && trust_moklist())
@@ -170,6 +201,10 @@ static int __init integrity_add_key(const unsigned int id, const void *data,
 	if (!keyring[id])
 		return -EINVAL;
 
+	rc = bhv_keyring_verify(keyring[id], &keyring[id]);
+	if (rc)
+		return rc;
+
 	key = key_create_or_update(make_key_ref(keyring[id], 1), "asymmetric",
 				   NULL, data, size, perm,
 				   KEY_ALLOC_NOT_IN_QUOTA);
diff --git security/integrity/ima/ima_mok.c security/integrity/ima/ima_mok.c
index 95cc31525c5..87c7d970336 100644
--- security/integrity/ima/ima_mok.c
+++ security/integrity/ima/ima_mok.c
@@ -15,6 +15,7 @@
 #include <linux/slab.h>
 #include <keys/system_keyring.h>
 
+#include <bhv/keyring.h>
 
 struct key *ima_blacklist_keyring;
 
@@ -44,6 +45,10 @@ static __init int ima_mok_init(void)
 
 	if (IS_ERR(ima_blacklist_keyring))
 		panic("Can't allocate IMA blacklist keyring.");
+	else
+		if (bhv_keyring_register_system_keyring(&ima_blacklist_keyring))
+			panic("Can't register IMA blacklist keyring.");
+
 	return 0;
 }
 device_initcall(ima_mok_init);
diff --git security/security.c security/security.c
index 1b504c29655..27e0a6e10d7 100644
--- security/security.c
+++ security/security.c
@@ -2717,3 +2717,28 @@ int security_uring_cmd(struct io_uring_cmd *ioucmd)
 	return call_int_hook(uring_cmd, 0, ioucmd);
 }
 #endif /* CONFIG_IO_URING */
+
+void security_module_loaded(struct module *mod)
+{
+	call_void_hook(module_loaded, mod);
+}
+
+int security_unshare(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(unshare, 0, tsk, nsset);
+}
+
+int security_setns(struct task_struct *tsk, struct nsset *nsset)
+{
+	return call_int_hook(setns, 0, tsk, nsset);
+}
+
+int security_cgroup_mkdir(struct cgroup *cgrp)
+{
+	return call_int_hook(cgroup_mkdir, 0, cgrp);
+}
+
+void security_cgroup_rmdir(struct cgroup *cgrp)
+{
+	call_void_hook(cgroup_rmdir, cgrp);
+}
diff --git security/selinux/hooks.c security/selinux/hooks.c
index 78f3da39b03..157d753f86c 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -125,7 +125,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index a00d1913943..2fdfa351731 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -612,9 +616,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
 	struct selinux_load_state load_state;
@@ -669,6 +673,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2250,6 +2280,40 @@ static int __init init_sel_fs(void)
 
 __initcall(init_sel_fs);
 
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinuxfs_mount->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&fsi->state->policy_mutex);
+
+	rv = security_load_policy(fsi->state, data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(fsi->state, &load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(fsi->state, &load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&fsi->state->policy_mutex);
+	return rv;
+}
+#endif
+
 #ifdef CONFIG_SECURITY_SELINUX_DISABLE
 void exit_sel_fs(void)
 {
diff --git tools/objtool/Build tools/objtool/Build
index 33f2ee5a46d..9dbfc54cd02 100644
--- tools/objtool/Build
+++ tools/objtool/Build
@@ -4,6 +4,7 @@ objtool-y += weak.o
 
 objtool-y += check.o
 objtool-y += special.o
+objtool-y += vault.o
 objtool-y += builtin-check.o
 objtool-y += elf.o
 objtool-y += objtool.o
diff --git tools/objtool/builtin-check.c tools/objtool/builtin-check.c
index 24fbe803a0d..ee8384ed703 100644
--- tools/objtool/builtin-check.c
+++ tools/objtool/builtin-check.c
@@ -74,6 +74,7 @@ const struct option check_options[] = {
 	OPT_BOOLEAN('s', "stackval", &opts.stackval, "validate frame pointer rules"),
 	OPT_BOOLEAN('t', "static-call", &opts.static_call, "annotate static calls"),
 	OPT_BOOLEAN('u', "uaccess", &opts.uaccess, "validate uaccess rules for SMAP"),
+	OPT_BOOLEAN('v', "vault", &opts.vault, "generate vault metadata"),
 	OPT_CALLBACK_OPTARG(0, "dump", NULL, NULL, "orc", "dump metadata", parse_dump),
 
 	OPT_GROUP("Options:"),
@@ -129,7 +130,8 @@ static bool opts_valid(void)
 	    opts.sls			||
 	    opts.stackval		||
 	    opts.static_call		||
-	    opts.uaccess) {
+	    opts.uaccess                ||
+	    opts.vault) {
 		if (opts.dump_orc) {
 			ERROR("--dump can't be combined with other options");
 			return false;
diff --git tools/objtool/check.c tools/objtool/check.c
index cb363b507a3..77297c1eee1 100644
--- tools/objtool/check.c
+++ tools/objtool/check.c
@@ -14,6 +14,7 @@
 #include <objtool/arch.h>
 #include <objtool/check.h>
 #include <objtool/special.h>
+#include <objtool/vault.h>
 #include <objtool/warn.h>
 #include <objtool/endianness.h>
 
@@ -58,8 +59,8 @@ static struct instruction *next_insn_same_sec(struct objtool_file *file,
 	return next;
 }
 
-static struct instruction *next_insn_same_func(struct objtool_file *file,
-					       struct instruction *insn)
+struct instruction *next_insn_same_func(struct objtool_file *file,
+					struct instruction *insn)
 {
 	struct instruction *next = list_next_entry(insn, list);
 	struct symbol *func = insn->func;
@@ -895,6 +896,154 @@ static int create_mcount_loc_sections(struct objtool_file *file)
 	return 0;
 }
 
+#define SEC_VAULT_TEXT ".bhv.vault.text.jump_label"
+#define SEC_VAULT_TEXT_SHARED ".bhv.vault.shared.text.jump_label"
+#define SEC_VAULT_TEXT_REF ".ref.text.bhv.vault.text.jump_label"
+
+static bool is_sec_in_vault(struct section *sec)
+{
+	if (!sec)
+		return false;
+
+	if (strncmp(sec->name, VAULT_TEXT_SECTION,
+		    strlen(VAULT_TEXT_SECTION)) &&
+	    strncmp(sec->name, VAULT_REF_TEXT_SECTION,
+		    strlen(VAULT_REF_TEXT_SECTION)))
+		return false;
+
+	return true;
+}
+
+static struct reloc *insn_reloc(struct objtool_file *file,
+				struct instruction *insn);
+
+static int add_vault_return_site(struct objtool_file *file,
+				 struct instruction *insn,
+				 struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_return_list,
+			    vault_return_node) {
+		if (insn_iter == next_insn)
+			return 0;
+	}
+
+	list_add_tail(&next_insn->vault_return_node, &file->vault_return_list);
+
+	return 0;
+}
+
+static int add_vault_rethunk_site(struct objtool_file *file,
+				  struct instruction *insn,
+				  struct symbol *dest_func)
+{
+	struct instruction *insn_iter = NULL;
+	struct instruction *next_insn = NULL;
+
+	/* Nothing to do if the destination function is a dead end. */
+	if (dead_end_function(file, dest_func) || insn->dead_end)
+		return 0;
+
+	next_insn = next_insn_same_func(file, insn);
+	if (next_insn == NULL) {
+		printf("Cannot find next instruction of %s:0x%lx\n",
+		       insn->sec->name, insn->offset);
+		return -1;
+	}
+
+	/* Avoid duplicates in the list */
+	list_for_each_entry(insn_iter, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+		if (insn_iter == insn)
+			return 0;
+	}
+
+	list_add_tail(&insn->vault_rethunk_node, &file->vault_rethunk_list);
+
+	return 0;
+}
+
+static int collect_vault_return_sites(struct objtool_file *file,
+				      struct section *sec)
+{
+	struct symbol *func;
+	struct instruction *insn;
+
+	list_for_each_entry(func, &sec->symbol_list, list) {
+		func_for_each_insn(file, func, insn) {
+			struct section *dest_sec = NULL;
+			struct symbol *sym = NULL;
+
+			/* We are interested only in jumps and calls */
+			if (!is_jump(insn) && !is_call(insn))
+				continue;
+
+			if (insn->jump_dest) {
+				sym = insn->jump_dest->func;
+				dest_sec = insn->jump_dest->sec;
+			} else if (insn->call_dest) {
+				sym = insn->call_dest;
+				dest_sec = sym->sec;
+			}
+
+			if (!dest_sec) {
+				struct reloc *reloc = insn_reloc(file, insn);
+				if (!reloc)
+					continue;
+
+				sym = reloc->sym;
+				dest_sec = sym->sec;
+			}
+
+			if (!is_sec_in_vault(dest_sec)) {
+				if (!sym)
+					continue;
+
+				if (sym->retpoline_thunk)
+					add_vault_rethunk_site(file, insn, sym);
+				else
+					add_vault_return_site(file, insn, sym);
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int inspect_vault(struct objtool_file *file)
+{
+	struct section *sec;
+
+	sec = find_section_by_name(file->elf, ".bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	sec = find_section_by_name(file->elf,
+				   ".ref.text.bhv.vault.text.jump_label");
+	if (sec) {
+		printf("Found section %s\n", sec->name);
+		collect_vault_return_sites(file, sec);
+	}
+
+	return 0;
+}
+
 /*
  * Warnings shouldn't be reported for ignored functions.
  */
@@ -4353,6 +4502,23 @@ int check(struct objtool_file *file)
 		warnings += ret;
 	}
 
+	if (opts.vault) {
+		ret = inspect_vault(file);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		//ret = create_vault_entries_section(file);
+		ret = create_vault_section(file, VAULT_SECTION_RETURN_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+
+		ret = create_vault_section(file, VAULT_SECTION_RETHUNK_SITES);
+		if (ret < 0)
+			goto out;
+		warnings += ret;
+	}
 
 	if (opts.stats) {
 		printf("nr_insns_visited: %ld\n", nr_insns_visited);
diff --git tools/objtool/include/objtool/builtin.h tools/objtool/include/objtool/builtin.h
index 42a52f1a0ad..ddebce017b8 100644
--- tools/objtool/include/objtool/builtin.h
+++ tools/objtool/include/objtool/builtin.h
@@ -25,6 +25,7 @@ struct opts {
 	bool stackval;
 	bool static_call;
 	bool uaccess;
+	bool vault;
 
 	/* options: */
 	bool backtrace;
diff --git tools/objtool/include/objtool/check.h tools/objtool/include/objtool/check.h
index 036129cebee..d88b335171a 100644
--- tools/objtool/include/objtool/check.h
+++ tools/objtool/include/objtool/check.h
@@ -40,6 +40,9 @@ struct instruction {
 	struct list_head list;
 	struct hlist_node hash;
 	struct list_head call_node;
+	struct list_head vault_entry_node;
+	struct list_head vault_return_node;
+	struct list_head vault_rethunk_node;
 	struct section *sec;
 	unsigned long offset;
 	unsigned int len;
@@ -94,9 +97,27 @@ static inline bool is_jump(struct instruction *insn)
 	return is_static_jump(insn) || is_dynamic_jump(insn);
 }
 
+static inline bool is_static_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL;
+}
+
+static inline bool is_dynamic_call(struct instruction *insn)
+{
+        return insn->type == INSN_CALL_DYNAMIC;
+}
+
+static inline bool is_call(struct instruction *insn)
+{
+        return is_static_call(insn) || is_dynamic_call(insn);
+}
+
 struct instruction *find_insn(struct objtool_file *file,
 			      struct section *sec, unsigned long offset);
 
+struct instruction *next_insn_same_func(struct objtool_file *file,
+				        struct instruction *insn);
+
 #define for_each_insn(file, insn)					\
 	list_for_each_entry(insn, &file->insn_list, list)
 
diff --git tools/objtool/include/objtool/objtool.h tools/objtool/include/objtool/objtool.h
index 7f2d1b09533..26dfe12907a 100644
--- tools/objtool/include/objtool/objtool.h
+++ tools/objtool/include/objtool/objtool.h
@@ -28,6 +28,9 @@ struct objtool_file {
 	struct list_head static_call_list;
 	struct list_head mcount_loc_list;
 	struct list_head endbr_list;
+	struct list_head vault_entry_list;
+	struct list_head vault_return_list;
+	struct list_head vault_rethunk_list;
 	bool ignore_unreachables, hints, rodata;
 
 	unsigned int nr_endbr;
diff --git tools/objtool/include/objtool/vault.h tools/objtool/include/objtool/vault.h
new file mode 100644
index 00000000000..3c6e83cd4a3
--- /dev/null
+++ tools/objtool/include/objtool/vault.h
@@ -0,0 +1,20 @@
+#ifndef _VAULT_H
+#define _VAULT_H
+
+#include <stdbool.h>
+#include <objtool/check.h>
+#include <objtool/elf.h>
+
+/* XXX: Change names! */
+#define VAULT_TEXT_SECTION 		".bhv.vault.text.jump_label"
+#define VAULT_REF_TEXT_SECTION 		".ref.text.bhv.vault.text.jump_label"
+
+enum vault_section_type {
+	VAULT_SECTION_ENTRY_POINTS,
+	VAULT_SECTION_RETURN_SITES,
+	VAULT_SECTION_RETHUNK_SITES,
+};
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t);
+
+#endif /* _VAULT_H */
diff --git tools/objtool/objtool.c tools/objtool/objtool.c
index cda649644e3..3ae43bb0478 100644
--- tools/objtool/objtool.c
+++ tools/objtool/objtool.c
@@ -106,6 +106,9 @@ struct objtool_file *objtool_open_read(const char *_objname)
 	INIT_LIST_HEAD(&file.static_call_list);
 	INIT_LIST_HEAD(&file.mcount_loc_list);
 	INIT_LIST_HEAD(&file.endbr_list);
+	INIT_LIST_HEAD(&file.vault_entry_list);
+	INIT_LIST_HEAD(&file.vault_return_list);
+	INIT_LIST_HEAD(&file.vault_rethunk_list);
 	file.ignore_unreachables = opts.no_unreachable;
 	file.hints = false;
 
diff --git tools/objtool/vault.c tools/objtool/vault.c
new file mode 100644
index 00000000000..410adf9b2f6
--- /dev/null
+++ tools/objtool/vault.c
@@ -0,0 +1,174 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * Copyright (C) 2024 Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <assert.h>
+#include <objtool/arch.h>
+#include <objtool/builtin.h>
+#include <objtool/vault.h>
+#include <objtool/warn.h>
+#include <objtool/check.h>
+
+static const char *vault_section_names[] = { ".vault_entries", ".vault_sites",
+					     ".vault_rethunks" };
+
+static int create_entry_points(struct objtool_file *file)
+{
+	/* XXX: We do not identify vault entry points, yet. */
+	return 0;
+}
+
+static int create_return_sites(struct objtool_file *file)
+{
+	int idx = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETURN_SITES];
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_return_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_return_list))
+		return 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node)
+		idx++;
+
+	printf("Found %d returns to the vault\n", idx);
+
+	sec = elf_create_section(file->elf, sec_name, 0, sizeof(unsigned long),
+				 idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_return_list, vault_return_node) {
+		unsigned long *loc = (unsigned long *)sec->data->d_buf + idx;
+		memset(loc, 0, sizeof(unsigned long));
+
+		if (elf_add_reloc_to_insn(
+			    file->elf, sec, idx * sizeof(unsigned long),
+			    R_X86_64_64, insn->sec, insn->offset)) {
+			WARN("Cannot add entry[%d] to section %s\n", idx,
+			     sec->name);
+			return -1;
+		}
+
+		printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			idx, insn->offset, insn->func->name,
+			sec->name);
+
+		idx++;
+	}
+
+	return 0;
+}
+
+static int create_rethunk_sites(struct objtool_file *file)
+{
+	int idx = 0;
+	struct section *sec;
+	struct instruction *insn;
+	const char *sec_name = vault_section_names[VAULT_SECTION_RETHUNK_SITES];
+
+	static const int NUM_POSSIBLE_RETURNS = 2;
+
+	sec = find_section_by_name(file->elf, sec_name);
+	if (sec) {
+		INIT_LIST_HEAD(&file->vault_rethunk_list);
+		WARN("file already has %s section, skipping", sec_name);
+		return 0;
+	}
+
+	if (list_empty(&file->vault_rethunk_list))
+		return 0;
+
+	/* Account for all possible return points to the patched instruction. */
+	list_for_each_entry(insn, &file->vault_rethunk_list, vault_rethunk_node) {
+		/*
+		 * Note: depending on the system's configuration, retpolines can
+		 * patch a jump/call instruction differently. Yet, there can be
+		 * only two possible return locations, to which the retpoline
+		 * can return to:
+		 *
+		 *   (i)  instruction, right after a short jump (2 bytes)
+		 *   (ii) next instruction after the original jmp/call
+		 */
+		idx += NUM_POSSIBLE_RETURNS;
+	}
+
+	printf("Found %d possible thunk returns to the vault\n", idx);
+
+	sec = elf_create_section(file->elf, sec_name, 0, sizeof(unsigned long),
+				 idx);
+	if (!sec) {
+		WARN("Cannot create section %s\n", sec_name);
+		return -1;
+	}
+
+	idx = 0;
+
+	list_for_each_entry(insn, &file->vault_rethunk_list,
+			    vault_rethunk_node) {
+
+		unsigned long insn_offsets[NUM_POSSIBLE_RETURNS];
+		struct instruction *next_insn = next_insn_same_func(file, insn);
+		if (next_insn == NULL) {
+        		printf("Cannot find next instruction of %s:0x%lx\n",
+               			insn->sec->name, insn->offset);
+        		return -1;
+		}
+
+		insn_offsets[0] = insn->offset + 2;
+		insn_offsets[1] = next_insn->offset;
+
+		for (int i = 0; i < NUM_POSSIBLE_RETURNS; ++i) {
+			unsigned long *loc =
+				(unsigned long *)sec->data->d_buf + idx;
+			memset(loc, 0, sizeof(unsigned long));
+
+			if (elf_add_reloc_to_insn(file->elf, sec,
+						  (idx) * sizeof(unsigned long),
+						  R_X86_64_64, insn->sec,
+						  insn_offsets[i])) {
+				WARN("Cannot add entry[%d] to section %s\n",
+				     idx, sec->name);
+				return -1;
+			}
+
+			printf("Section entry[%d] insn @ 0x%lx (%s) to section %s\n",
+			       idx, insn_offsets[i], insn->func->name,
+			       sec->name);
+
+			idx++;
+		}
+	}
+
+	return 0;
+}
+
+int create_vault_section(struct objtool_file *file, enum vault_section_type t)
+{
+	switch (t) {
+	case VAULT_SECTION_ENTRY_POINTS:
+		return create_entry_points(file);
+	case VAULT_SECTION_RETURN_SITES:
+		return create_return_sites(file);
+	case VAULT_SECTION_RETHUNK_SITES:
+		return create_rethunk_sites(file);
+	default:
+		WARN("Unknown vault section type");
+		return -1;
+	}
+
+	return 0;
+}
+
