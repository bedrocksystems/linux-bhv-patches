diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index ea7ab4ca8..81f6d39e0 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,4 +3,5 @@ obj-y			+= kernel/ mm/ net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
 obj-$(subst m,y,$(CONFIG_HYPERV))	+= hyperv/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
diff --git arch/arm64/Kconfig arch/arm64/Kconfig
index 24cce3b9f..f5603abd1 100644
--- arch/arm64/Kconfig
+++ arch/arm64/Kconfig
@@ -1180,6 +1180,16 @@ config XEN
 	help
 	  Say Y if you want to run Linux in a Virtual Machine on Xen on ARM64.
 
+config BHV_VAS
+	def_bool y
+	bool "BHV guest support on ARM64"
+	depends on ARM64 && OF
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on BHV on ARM64
+	  and benefit from Virtualization-assisted Security.
+
+source "kernel/bhv/Kconfig"
+
 config FORCE_MAX_ZONEORDER
 	int
 	default "14" if ARM64_64K_PAGES
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 000000000..e66e83fdb
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
diff --git arch/arm64/bhv/init.c arch/arm64/bhv/init.c
new file mode 100644
index 000000000..70a504119
--- /dev/null
+++ arch/arm64/bhv/init.c
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <bhv/interface/integrity.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 000000000..454a56894
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,68 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+extern char vdso_start[], vdso_end[];
+
+int bhv_start_integrity_arch(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 2
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON((uint64_t)vdso_start < (uint64_t)__start_rodata ||
+	       (uint64_t)vdso_start >= (uint64_t)__end_rodata);
+	BUG_ON((uint64_t)vdso_end < (uint64_t)__start_rodata ||
+	       (uint64_t)vdso_end >= (uint64_t)__end_rodata);
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 * NOTE: We split this out such that the VDSO section is not included.
+	 */
+	n[0]->region.bhv_mem_region_create.start_addr =
+		virt_to_phys((void *)__start_rodata);
+	n[0]->region.bhv_mem_region_create.size = vdso_start - __start_rodata;
+	n[0]->region.bhv_mem_region_create.type = BHV_MEM_TYPE_DATA_READ_ONLY;
+	n[0]->region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+	n[0]->region.bhv_mem_region_create.next = virt_to_phys(&(n[1]->region));
+
+	n[1]->region.bhv_mem_region_create.start_addr =
+		virt_to_phys((void *)vdso_end);
+	n[1]->region.bhv_mem_region_create.size = __end_rodata - vdso_end;
+	n[1]->region.bhv_mem_region_create.type = BHV_MEM_TYPE_DATA_READ_ONLY;
+	n[1]->region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+	n[1]->region.bhv_mem_region_create.next = BHV_INVALID_PHYS_ADDR;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 000000000..2af48e512
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,375 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+#include <bhv/vault.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst,
+					      bhv_patch_arg_t *bhv_arg)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)&updptr[i]);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(ctr_el0,
+							   CTR_DMINLINE_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+
+		if (!test_bit(alt->cpufeature, feature_mask))
+			continue;
+
+		/* Use ARM64_CB_PATCH as an unconditional patch */
+		if (alt->cpufeature < ARM64_CB_PATCH &&
+		    !cpus_have_cap(alt->cpufeature))
+			continue;
+
+		if (alt->cpufeature == ARM64_CB_PATCH) {
+			if (alt->alt_len != 0) {
+				return -EACCES;
+			}
+		} else {
+			if (alt->alt_len != alt->orig_len) {
+				return -EACCES;
+			}
+		}
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (alt->cpufeature < ARM64_CB_PATCH) {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst,
+					       bhv_arg);
+		} else {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst, bhv_arg);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	return 0;
+}
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 000000000..610215168
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+u32 __always_inline bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/start.c arch/arm64/bhv/start.c
new file mode 100644
index 000000000..605d0ae18
--- /dev/null
+++ arch/arm64/bhv/start.c
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+
+int bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index a38b92e11..8bdbcf5d1 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -10,6 +10,8 @@
 #include <linux/types.h>
 #include <linux/stddef.h>
 
+#include <bhv/interface/patch.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
@@ -18,8 +20,14 @@ struct alt_instr {
 	u8  alt_len;		/* size of new instruction(s), <= orig_len */
 };
 
+#ifdef CONFIG_BHV_VAS
+typedef void (*alternative_cb_t)(struct alt_instr *alt, __le32 *origptr,
+				 __le32 *updptr, int nr_inst,
+				 bhv_patch_arg_t *bhv_arg);
+#else /* !CONFIG_BHV_VAS */
 typedef void (*alternative_cb_t)(struct alt_instr *alt,
 				 __le32 *origptr, __le32 *updptr, int nr_inst);
+#endif /* CONFIG_BHV_VAS */
 
 void __init apply_boot_alternatives(void);
 void __init apply_alternatives_all(void);
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..16f767181
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint32_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 000000000..0a0a0dedd
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#else /* !CONFIG_BHV_VAS */
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 02d378887..4ae56cc26 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -116,8 +116,17 @@ alternative_cb_end
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
+#ifdef CONFIG_BHV_VAS
+void kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr, __le32 *updptr,
+			int nr_inst, bhv_patch_arg_t *bhv_arg);
+#else /* CONFIG_BHV_VAS */
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
+#endif /* CONFIG_BHV_VAS */
 void kvm_compute_layout(void);
 void kvm_apply_hyp_relocations(void);
 
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 7bbf5104b..bfdb6ceae 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -17,6 +17,9 @@
 #include <asm/sections.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
 #define __ALT_PTR(a, f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -95,8 +98,13 @@ static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
 	return insn;
 }
 
+#ifdef CONFIG_BHV_VAS
+static noinstr void patch_alternative(struct alt_instr *alt, __le32 *origptr,
+			      __le32 *updptr, int nr_inst, bhv_patch_arg_t *arg)
+#else
 static noinstr void patch_alternative(struct alt_instr *alt,
 			      __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif
 {
 	__le32 *replptr;
 	int i;
@@ -140,6 +148,14 @@ static void __nocfi __apply_alternatives(struct alt_region *region, bool is_modu
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 
@@ -167,7 +183,11 @@ static void __nocfi __apply_alternatives(struct alt_region *region, bool is_modu
 		else
 			alt_cb  = ALT_REPL_PTR(alt);
 
+#ifdef CONFIG_BHV_VAS
+		alt_cb(alt, origptr, updptr, nr_inst, NULL);
+#else
 		alt_cb(alt, origptr, updptr, nr_inst);
+#endif
 
 		if (!is_module) {
 			clean_dcache_range_nopatch((u64)origptr,
@@ -254,10 +274,15 @@ void apply_alternatives_module(void *start, size_t length)
 		.begin	= start,
 		.end	= start + length,
 	};
+
 	DECLARE_BITMAP(all_capabilities, ARM64_NPATCHABLE);
 
 	bitmap_fill(all_capabilities, ARM64_NPATCHABLE);
 
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
 #endif
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index fc98037e1..a6fceaa70 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -10,10 +10,14 @@
 #include <asm/insn.h>
 #include <asm/patching.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -24,6 +28,13 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 }
 
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index 40be3a7c2..e7956f96f 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -32,6 +32,13 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -577,9 +584,16 @@ static enum mitigation_state spectre_v4_enable_hw_mitigation(void)
  * Patch a branch over the Spectre-v4 mitigation code with a NOP so that
  * we fallthrough and check whether firmware needs to be called on this CPU.
  */
+#ifdef CONFIG_BHV_VAS
+void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
+						  __le32 *origptr,
+						  __le32 *updptr, int nr_inst,
+						  bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 						  __le32 *origptr,
 						  __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1); /* Branch -> NOP */
 
@@ -589,17 +603,41 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_final_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
  * Patch a NOP in the Spectre-v4 mitigation code with an SMC/HVC instruction
  * to call into firmware to adjust the mitigation state.
  */
+#ifdef CONFIG_BHV_VAS
+void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst,
+						   bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u32 insn;
 
@@ -616,7 +654,22 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1062,30 +1115,88 @@ void spectre_bhb_enable_mitigation(const struct arm64_cpu_capabilities *entry)
 }
 
 /* Patched to NOP when enabled */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
+						      __le32 *origptr,
+						      __le32 *updptr,
+						      int nr_inst,
+						      bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
 						     __le32 *origptr,
 						      __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_LOOP, &system_bhb_mitigations))
+	if (test_bit(BHB_LOOP, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to NOP when enabled */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
+						     __le32 *origptr,
+						     __le32 *updptr,
+						     int nr_inst,
+						     bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
 						   __le32 *origptr,
 						   __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_FW, &system_bhb_mitigations))
+	if (test_bit(BHB_FW, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to correct the immediate */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt, __le32 *origptr,
+					 __le32 *updptr, int nr_inst,
+					 bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u8 rd;
 	u32 insn;
@@ -1101,12 +1212,33 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, loop_count, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to mov WA3 when supported */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt, __le32 *origptr,
+				   __le32 *updptr, int nr_inst,
+				   bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u8 rd;
 	u32 insn;
@@ -1127,20 +1259,68 @@ void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 	if (WARN_ON_ONCE(insn == AARCH64_BREAK_FAULT))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to NOP when not supported */
+#ifdef CONFIG_BHV_VAS
+void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt, __le32 *origptr,
+				       __le32 *updptr, int nr_inst,
+				       bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 2);
 
 	if (test_bit(BHB_INSN, &system_bhb_mitigations))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+
+		insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index be5f85b0a..315221a68 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -52,6 +52,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/start.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -330,6 +332,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT) && ((u64)_text % MIN_KIMG_ALIGN) != 0)
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index 184abd7c4..11ffc6b0b 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -161,6 +161,7 @@ SECTIONS
 			IDMAP_TEXT
 			HIBERNATE_TEXT
 			TRAMP_TEXT
+			BHV_TEXT
 			*(.fixup)
 			*(.gnu.warning)
 		. = ALIGN(16);
@@ -204,13 +205,27 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
 		__alt_instructions = .;
 		*(.altinstructions)
@@ -280,6 +295,17 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index acdb7b3cc..4f67c1abc 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -13,6 +13,11 @@
 #include <asm/kvm_mmu.h>
 #include <asm/memory.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -151,8 +156,26 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn, bhv_patch_arg_t *bhv_arg)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)dest);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &le32_insn,
+	       sizeof(le32_insn));
+	bhv_arg->bhv_patch_patch_arg.size = sizeof(le32_insn);
+	bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+}
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst,
+			       bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	int i;
 
@@ -170,7 +193,17 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (has_vhe() || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop(),
+						  bhv_arg);
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -181,12 +214,26 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn, bhv_arg);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
+#ifdef CONFIG_BHV_VAS
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst,
+			     bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
 void kvm_patch_vector_branch(struct alt_instr *alt,
 			     __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u64 addr;
 	u32 insn;
@@ -216,15 +263,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -232,12 +293,28 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst)
@@ -256,7 +333,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 16) & 0xffff), lsl #16 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -264,7 +349,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 16,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -272,7 +365,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 48) & 0xffff), lsl #48 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -280,7 +381,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 48,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 void kvm_get_kimage_voffset(struct alt_instr *alt,
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 3b269c756..18998fbb8 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -46,6 +46,9 @@
 #include <asm/alternative.h>
 #include <asm/xen/swiotlb-xen.h>
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -439,6 +442,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+	bhv_start();
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
 			   POISON_FREE_INITMEM, "unused kernel");
diff --git arch/x86/Kbuild arch/x86/Kbuild
index 30dec0197..3235bbc86 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -13,6 +13,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/Kconfig arch/x86/Kconfig
index 57f5e8817..40425bf3c 100644
--- arch/x86/Kconfig
+++ arch/x86/Kconfig
@@ -862,6 +862,16 @@ config ACRN_GUEST
 	  IOT with small footprint and real-time features. More details can be
 	  found in https://projectacrn.org/.
 
+config BHV_VAS
+	def_bool y
+	bool "BHV guest support on x86_64"
+	depends on X86_64
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on BHV on x86_64
+	  and benefit from Virtualization-assisted Security.
+
+source "kernel/bhv/Kconfig"
+
 endif #HYPERVISOR_GUEST
 
 source "arch/x86/Kconfig.cpu"
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 000000000..f7f95bc2a
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
diff --git arch/x86/bhv/init.c arch/x86/bhv/init.c
new file mode 100644
index 000000000..1833c1b75
--- /dev/null
+++ arch/x86/bhv/init.c
@@ -0,0 +1,102 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/integrity.h>
+#include <bhv/init.h>
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	if ((*region_counter) > 0)
+		init_phys_mem_regions[(*region_counter) - 1]
+			.bhv_mem_region_create.next = virt_to_phys(
+			&(init_phys_mem_regions[(*region_counter)]));
+
+	init_phys_mem_regions[(*region_counter)]
+		.bhv_mem_region_create.start_addr =
+		virt_to_phys(vdso_image_64.data);
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.size =
+		vdso_image_64.size;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_TRANSIENT;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.next =
+		BHV_INVALID_PHYS_ADDR;
+	(*region_counter)++;
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	if ((*region_counter) > 0)
+		init_phys_mem_regions[(*region_counter) - 1]
+			.bhv_mem_region_create.next = virt_to_phys(
+			&(init_phys_mem_regions[(*region_counter)]));
+
+	init_phys_mem_regions[(*region_counter)]
+		.bhv_mem_region_create.start_addr =
+		virt_to_phys(vdso_image_x32.data);
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.size =
+		vdso_image_x32.size;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_TRANSIENT;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.next =
+		BHV_INVALID_PHYS_ADDR;
+	(*region_counter)++;
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	if ((*region_counter) > 0)
+		init_phys_mem_regions[(*region_counter) - 1]
+			.bhv_mem_region_create.next = virt_to_phys(
+			&(init_phys_mem_regions[(*region_counter)]));
+
+	init_phys_mem_regions[(*region_counter)]
+		.bhv_mem_region_create.start_addr =
+		virt_to_phys(vdso_image_32.data);
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.size =
+		vdso_image_32.size;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_TRANSIENT;
+	init_phys_mem_regions[(*region_counter)].bhv_mem_region_create.next =
+		BHV_INVALID_PHYS_ADDR;
+	(*region_counter)++;
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+}
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 000000000..9b8a4ea7f
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,342 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/efi.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+
+struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} typedef table_data_t;
+
+static table_data_t table_data __ro_after_init;
+
+static int bhv_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = virt_to_phys((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(head, addr, size,
+				       BHV_MEM_TYPE_DATA_READ_ONLY,
+				       BHV_MEM_FLAGS_NONE);
+}
+
+static int bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+static inline int rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(head, virt_to_phys(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(head, virt_to_phys(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(head, virt_to_phys(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+static void bhv_start_integrity_add_vdso_bump(unsigned int i, uint64_t addr,
+					      uint64_t *range, size_t range_sz)
+{
+	unsigned int j;
+	uint64_t tmp1, tmp2;
+	BUG_ON(i >= range_sz);
+	tmp1 = addr;
+	for (j = i; j < range_sz; j++) {
+		if (tmp1 == 0)
+			return;
+		BUG_ON(range[j] != 0 && tmp1 > range[j]);
+		tmp2 = range[j];
+		range[j] = tmp1;
+		tmp1 = tmp2;
+	}
+	BUG_ON(tmp1 != 0);
+}
+
+static void bhv_start_integrity_add_vdso_common(uint64_t start, uint64_t end,
+						uint64_t *range,
+						size_t range_sz)
+{
+	unsigned int i;
+	BUG_ON(start == end);
+	BUG_ON(range_sz == 0);
+	BUG_ON(start < range[0]);
+
+	for (i = 1; i < range_sz - 1; i++) {
+		BUG_ON(range[i] == 0);
+		if (start < range[i]) {
+			bhv_start_integrity_add_vdso_bump(i, start, range,
+							  range_sz);
+			bhv_start_integrity_add_vdso_bump(i + 1, end, range,
+							  range_sz);
+			return;
+		}
+	}
+	BUG();
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_64_to_range(uint64_t *range, size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start = virt_to_phys(vdso_image_64.data);
+	uint64_t end = start + vdso_image_64.size;
+	bhv_start_integrity_add_vdso_common(start, end, range, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_x32_to_range(uint64_t *range,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start = virt_to_phys(vdso_image_x32.data);
+	uint64_t end = start + vdso_image_x32.size;
+	bhv_start_integrity_add_vdso_common(start, end, range, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_start_integrity_add_vdso_image_32_to_range(uint64_t *range, size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start = virt_to_phys(vdso_image_32.data);
+	uint64_t end = start + vdso_image_32.size;
+	bhv_start_integrity_add_vdso_common(start, end, range, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void bhv_start_integrity_get_ro_ranges(uint64_t *range, size_t range_sz)
+{
+	BUG_ON(range_sz < 2);
+	range[0] = virt_to_phys((void *)__start_rodata);
+	range[1] = virt_to_phys((void *)__end_rodata);
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, range_sz);
+}
+
+static int bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+#define BHV_NUM_MEM_REGION_NODES (BHV_MAX_RO_RANGES / 2)
+	unsigned int i, j;
+	uint64_t start, size;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	uint64_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_NUM_MEM_REGION_NODES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   BHV_NUM_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < BHV_NUM_MEM_REGION_NODES; i++) {
+		j = i * 2;
+		start = ro_ranges[j];
+		size = ro_ranges[j + 1] - start;
+
+		if (start == 0)
+			break;
+
+		if (size == 0)
+			continue;
+
+		n[i]->region.bhv_mem_region_create.start_addr = start;
+		n[i]->region.bhv_mem_region_create.size = size;
+		n[i]->region.bhv_mem_region_create.type =
+			BHV_MEM_TYPE_DATA_READ_ONLY;
+		n[i]->region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+		n[i]->region.bhv_mem_region_create.next = BHV_INVALID_PHYS_ADDR;
+
+		if (prev != NULL) {
+			prev->region.bhv_mem_region_create.next =
+				virt_to_phys(&(n[i]->region));
+		}
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, BHV_NUM_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rc;
+}
+
+static int bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		n->region.bhv_mem_region_create.start_addr = md->phys_addr;
+		n->region.bhv_mem_region_create.size = (md->num_pages)
+						       << PAGE_SHIFT;
+		n->region.bhv_mem_region_create.type = BHV_MEM_TYPE_CODE;
+		n->region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+		n->region.bhv_mem_region_create.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1, &(n->region));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+
+int bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+	return bhv_start_integrity_add_ro();
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
+
+void __init bhv_integrity_arch_init(void)
+{
+	memset(&table_data, 0, sizeof(table_data));
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 000000000..371da7859
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,737 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/vault.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end,
+				      const s32 *locks_begin,
+				      const s32 *locks_end, u8 *text_begin,
+				      u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len,
+				    bhv_patch_arg_t *bhv_arg, bool patch)
+{
+	size_t total_length = 0;
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) >
+			    sizeof(bhv_arg->bhv_patch_patch_arg.src_value))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value +
+				       total_length,
+			       x86_nops[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, x86_nops[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys(insns);
+		bhv_arg->bhv_patch_patch_arg.size = total_length;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+}
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off,
+						   bhv_patch_arg_t *bhv_arg,
+						   bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, bhv_arg, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bhv_patch_arg_t *bhv_arg,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, bhv_arg,
+						     patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, NULL, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(
+	u8 *target, bool lock, bhv_patch_arg_t *bhv_arg)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode)
+			return -EACCES;
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode)
+			return -EACCES;
+
+		opcode = unlock_opcode;
+	}
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)target);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &opcode, 1);
+	bhv_arg->bhv_patch_patch_arg.size = 1;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			      BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	if (r) {
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock, bhv_patch_arg_t *bhv_arg)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+		    ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock,
+							     bhv_arg);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+		bytes[i++] = BYTES_NOP1;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s, bhv_patch_arg_t *bhv_arg)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		goto out;
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+	if (len == insn.length) {
+		if (len > BHV_MAX_PATCH_SZ)
+			goto out;
+
+		bhv_optimize_nops(bytes, len, bhv_arg, true);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)addr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &bytes, len);
+		bhv_arg->bhv_patch_patch_arg.size = len;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+
+out:
+	bhv_vault_close_hyp();
+}
+
+#ifdef CONFIG_RETHUNK
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s, bhv_patch_arg_t *bhv_arg)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+	    WARN_ONCE(dest != &__x86_return_thunk,
+		      "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+		      addr))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	if (len == insn.length) {
+		if (len > BHV_MAX_PATCH_SZ)
+			goto out;
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)addr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &bytes, len);
+		bhv_arg->bhv_patch_patch_arg.size = len;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+
+out:
+	bhv_vault_close_hyp();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+
+#ifdef CONFIG_PARAVIRT
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p,
+					 bhv_patch_arg_t *bhv_arg)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used, bhv_patched, bhv_patch_sz;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, bhv_arg, false);
+
+	bhv_patched = 0;
+	while (bhv_patched < p->len) {
+		bhv_patch_sz =
+			((p->len - bhv_patched) <
+			 sizeof(bhv_arg->bhv_patch_patch_arg.src_value)) ?
+				(p->len - bhv_patched) :
+				sizeof(bhv_arg->bhv_patch_patch_arg.src_value);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys(((void *)(p->instr)) + bhv_patched);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value + bhv_patched,
+		       (&insn_buff) + bhv_patched, bhv_patch_sz);
+		bhv_arg->bhv_patch_patch_arg.size = bhv_patch_sz;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+		bhv_patched += bhv_patch_sz;
+	}
+
+	bhv_vault_close_hyp();
+}
+#endif /* CONFIG_PARAVIRT */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a,
+						   bhv_patch_arg_t *bhv_arg)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+	u16 feature = a->cpuid & ~ALTINSTR_FLAG_INV;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		return -EACCES;
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		return -EACCES;
+	}
+
+	if (!boot_cpu_has(feature) == !(a->cpuid & ALTINSTR_FLAG_INV)) {
+		bhv_optimize_nops(instr, a->instrlen, bhv_arg, true);
+		return 0;
+	}
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+
+	if (insn_buff_sz >= sizeof(bhv_arg->bhv_patch_patch_arg.src_value))
+		panic("Instruction buffer size too small!");
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)instr);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn_buff,
+	       insn_buff_sz);
+	bhv_arg->bhv_patch_patch_arg.size = insn_buff_sz;
+
+	rv = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			       BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+	bhv_optimize_nops(instr, a->instrlen, bhv_arg, true);
+
+	return rv;
+}
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp, bhv_arg);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a, bhv_arg);
+		else
+			bhv_alternatives_patch_vault(a, bhv_arg);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+	    cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) &&                   \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) &&                  \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) &&                  \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) &&                  \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 000000000..801ba630c
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,85 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len == 5) {
+		DEF_CHECK_NOP(STATIC_KEY_INIT_NOP);
+		CHECK_NOP(ideal_nops)[NOP_ATOMIC5];
+	}
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len == 2) {
+		CHECK_NOP(x86_nops[2]);
+	} else if (len == 5) {
+		CHECK_NOP(x86_nops[5]);
+	}
+#endif
+
+	return false;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len != 5)
+		return false;
+
+	code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+
+
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+#endif
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 000000000..047e4cb9c
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,104 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/vault.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static void __always_inline bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static void __always_inline bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func, bool modinit,
+						       bhv_patch_arg_t *bhv_arg)
+{
+	int ret;
+	const void *emulate = NULL;
+	int size = CALL_INSN_SIZE;
+	const void *code;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+
+		break;
+
+	case NOP:
+		code = x86_nops[5];
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+		break;
+	}
+
+	if (memcmp(insn, code, size) == 0)
+		goto out;
+
+	if (size > BHV_MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)insn);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, code, size);
+	bhv_arg->bhv_patch_patch_arg.size = size;
+
+	bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+out:
+	bhv_vault_close_hyp();
+}
+
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func,
+			       bool modinit)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func, modinit, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
diff --git arch/x86/bhv/start.c arch/x86/bhv/start.c
new file mode 100644
index 000000000..d038b5a2d
--- /dev/null
+++ arch/x86/bhv/start.c
@@ -0,0 +1,44 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/integrity.h>
+#include <bhv/start.h>
+
+static uint32_t __init bhv_detect(void)
+{
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0;	/* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return hypervisor_cpuid_base("BHV.VMM.VAS.", 0);
+
+	return 0;
+}
+
+static void __init bhv_init_platform_x86(void)
+{
+	bhv_integrity_arch_init();
+	bhv_init_platform();
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform_x86
+};
+
+int bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..5140906c8
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint32_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 000000000..f1213d0a8
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,32 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_integrity_arch_init(void);
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+#else /* CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 000000000..ab762fd71
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,82 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+void __bhv_text bhv_apply_retpolines_vault(s32 *s, bhv_patch_arg_t *bhv_arg);
+#ifdef CONFIG_RETHUNK
+void __bhv_text bhv_apply_returns_vault(s32 *s, bhv_patch_arg_t *bhv_arg);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+
+#ifdef CONFIG_PARAVIRT
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p,
+					 bhv_patch_arg_t *bhv_arg);
+#endif /* CONFIG_PARAVIRT */
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+};
+
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func,
+			       bool modinit);
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
\ No newline at end of file
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec..591e48b05 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 43dd7f281..96505e444 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -31,6 +31,9 @@
 #include <asm/paravirt.h>
 #include <asm/asm-prototypes.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
@@ -268,6 +271,13 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 	u8 *instr, *replacement;
 	u8 insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	DPRINTK("alt table %px, -> %px", start, end);
 	/*
 	 * The scan order should be from start to end. A later scanned
@@ -465,6 +475,14 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_retpolines(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -538,6 +556,14 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_returns(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *dest = NULL, *addr = (void *)s + *s;
 		struct insn insn;
@@ -614,7 +640,6 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 			text_poke(ptr, ((unsigned char []){0x3E}), 1);
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -640,6 +665,11 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+
 	mutex_lock(&text_mutex);
 	if (!uniproc_patched)
 		goto unlock;
@@ -665,7 +695,25 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end, locks,
+							locks_end, text, text_end);
+		}
+		// Apply
+		smp_lock = false;
+		p.locks_begin = locks;
+		p.locks_end = locks_end;
+		bhv_alternatives_apply_custom_filter(
+			&p, &smp_lock, bhv_alternatives_find_by_lock);
+	} else {
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
+	}
+#else /* !CONFIG_BHV_VAS */
 	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#endif /* CONFIG_BHV_VAS */
 unlock:
 	mutex_unlock(&text_mutex);
 }
@@ -699,9 +747,26 @@ void alternatives_enable_smp(void)
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else {
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+			}
+#else /* !CONFIG_BHV_VAS */
 			alternatives_smp_lock(mod->locks, mod->locks_end,
 					      mod->text, mod->text_end);
+#endif /* CONFIG_BHV_VAS */
+		}
 		uniproc_patched = false;
 	}
 	mutex_unlock(&text_mutex);
@@ -742,6 +807,14 @@ void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
 	struct paravirt_patch_site *p;
 	char insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (p = start; p < end; p++)
+			bhv_apply_paravirt(p);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (p = start; p < end; p++) {
 		unsigned int used;
 
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3..20bfa373b 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index df0fa695b..5ca9afe7d 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -10,6 +10,7 @@
 #include <asm/proto.h>
 #include <asm/desc.h>
 #include <asm/hw_irq.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -294,6 +295,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index 68f091ba8..c20797ea8 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -16,6 +16,9 @@
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
 #include <asm/insn.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
 
 int arch_jump_entry_size(struct jump_entry *entry)
 {
@@ -79,10 +82,30 @@ __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 	return (struct jump_label_patch){.code = code, .size = size};
 }
 
-static __always_inline void
-__jump_label_transform(struct jump_entry *entry,
-		       enum jump_label_type type,
-		       int init)
+#ifdef CONFIG_BHV_VAS
+static void __orig_jump_label_transform(struct jump_entry *entry,
+					enum jump_label_type type, int init);
+
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type,
+					  int init)
+{
+	if (bhv_integrity_is_enabled()) {
+		const struct jump_label_patch jlp =
+			__jump_label_patch(entry, type);
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+	} else {
+		__orig_jump_label_transform(entry, type, init);
+	}
+}
+
+static __always_inline void __orig_jump_label_transform(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       int init)
+#else /* CONFIG_BHV_VAS */
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type, int init)
+#endif /* CONFIG_BHV_VAS */
 {
 	const struct jump_label_patch jlp = __jump_label_patch(entry, type);
 
@@ -135,17 +158,36 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 
 	mutex_lock(&text_mutex);
 	jlp = __jump_label_patch(entry, type);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled())
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+	else {
+		text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
+	}
+#else
 	text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
+#endif
 	mutex_unlock(&text_mutex);
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+void arch_jump_label_transform_apply(void)
+{
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+		text_poke_finish();
+		mutex_unlock(&text_mutex);
+	}
+}
+#else /* CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
 
 static enum {
 	JL_STATE_START,
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index 06b53ea94..8d0de0b6e 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -256,6 +259,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*retpolines = NULL, *returns = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#ifdef CONFIG_BHV_VAS
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".text", secstrings + s->sh_name))
 			text = s;
@@ -291,9 +300,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#ifdef CONFIG_BHV_VAS
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks && text) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = (void *)text->sh_addr;
+		arch.text_end = (void *)text->sh_addr + text->sh_size;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif /* CONFIG_BHV_VAS */
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (locks && text) {
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index 2fc4f9670..b3335abca 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,12 +4,17 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
+#ifndef CONFIG_BHV_VAS
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
 	JMP = 2,  /* tramp / site tail-call */
 	RET = 3,  /* tramp / site cond-tail-call */
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
@@ -32,6 +37,13 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	int size = CALL_INSN_SIZE;
 	const void *code;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func, modinit);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	switch (type) {
 	case CALL:
 		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index c1efcd194..00f604fd9 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -137,6 +137,7 @@ SECTIONS
 		ALIGN_ENTRY_TEXT_END
 		SOFTIRQENTRY_TEXT
 		STATIC_CALL_TEXT
+		BHV_TEXT
 		*(.fixup)
 		*(.gnu.warning)
 
@@ -333,15 +334,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -375,6 +390,17 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index 0e3667e52..fe936f129 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -36,6 +36,9 @@
 
 #include "mm_internal.h"
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -947,6 +950,8 @@ void __ref free_initmem(void)
 
 	mem_encrypt_free_decrypted_mem();
 
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index 72f1e2a8c..5201954a9 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifndef dereference_function_descriptor
 #define dereference_function_descriptor(p) ((void *)(p))
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 9eac202fb..35d9b4585 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -641,6 +641,17 @@
 		*(.static_call.text)					\
 		__static_call_text_end = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_start = .;				\
+		*(.bhv.text)						\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_end = .;
+#else
+#define BHV_TEXT
+#endif
+
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  KEEP(*(.head.text))
 
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 000000000..9f507a87a
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,51 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+void __init bhv_acl_mm_init(void);
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_PROC_ACL, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_DRIVER_ACL, bhv_configuration_bitmap);
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 000000000..c5f401918
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,74 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#define __bhv_data __section(".bhv.data") noinline
+#define __bhv_text __section(".bhv.text") noinline
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+extern bool bhv_initialized __ro_after_init;
+extern unsigned long *bhv_configuration_bitmap __ro_after_init;
+
+static inline bool is_bhv_initialized(void)
+{
+	BUG_ON(bhv_initialized && bhv_configuration_bitmap == NULL);
+	return bhv_initialized;
+}
+
+void __init bhv_mm_init(void);
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys(void *address)
+{
+	if (is_vmalloc_or_module_addr(address)) {
+		struct page *p = vmalloc_to_page(address);
+		uint64_t offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		// Check for HugePage
+		BUG_ON(PageHuge(p) || PageCompound(p));
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	} else {
+		return virt_to_phys(address);
+	}
+}
+#else /* CONFIG_BHV_VAS */
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+static inline void bhv_mm_init(void)
+{
+}
+
+static inline phys_addr_t bhv_virt_to_phys(volatile void *address)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 000000000..35d267d47
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BHV_VAS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 000000000..81a73366f
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/init.h>
+
+#ifdef CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_CREDS, bhv_configuration_bitmap);
+}
+
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+int __init bhv_cred_init(void);
+void __init bhv_cred_mm_init(void);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+
+static inline void bhv_cred_mm_init(void)
+{
+}
+
+
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 000000000..967b39f66
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/file_protection.h>
+
+extern bhv_file_protection_config_t bhv_file_protection_config __ro_after_init;
+
+void __init bhv_file_protection_init(void);
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILE_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	{
+		if (!bhv_file_protection_is_enabled())
+			return false;
+
+		if (!test_bit(
+			    BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION,
+			    (unsigned long *)&bhv_file_protection_config
+				    .feature_bitmap))
+			return false;
+
+		return true;
+	}
+}
+
+bool bhv_block_read_only_file_write(const char *target);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 000000000..0f574b198
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+#include <bhv/interface/fileops_protection.h>
+
+void __init bhv_fileops_protection_mm_init(void);
+
+static inline bool bhv_fileops_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILEOPS_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+bool bhv_block_fileops(const char *, bool, uint8_t);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_fileops_protection_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 000000000..7c990fa74
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_H__
+#define __BHV_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/guestconn.h>
+
+int __init bhv_guestconn_init(uint32_t cid, uint32_t port);
+void __init bhv_guestconn_mm_init(void);
+void bhv_guestconn_start(void);
+
+int bhv_guestconn_send(uint16_t type, void *data, size_t size);
+
+#endif /* __BHV_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 000000000..88b1ac4aa
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,85 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/guestlog.h>
+
+extern bhv_guestlog_config_t bhv_guestlog_config __ro_after_init;
+
+int __init bhv_guestlog_init(void);
+
+static inline bool bhv_guestlog_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_LOGGING, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_guestlog_log_process_events(void)
+{
+	if (!bhv_guestlog_enabled())
+		return false;
+
+	if (!bhv_guestlog_config.valid ||
+	    !test_bit(BHV_GUESTLOG_CONFIG_LOG_PROCESS_EVENTS,
+		      (unsigned long *)&bhv_guestlog_config.log_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_guestlog_log_driver_events(void)
+{
+	if (!bhv_guestlog_enabled())
+		return false;
+
+	if (!bhv_guestlog_config.valid ||
+	    !test_bit(BHV_GUESTLOG_CONFIG_LOG_DRIVER_EVENTS,
+		      (unsigned long *)&bhv_guestlog_config.log_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline uint16_t bhv_guestlog_calc_msg_sz(uint16_t type, size_t buf_sz)
+{
+	switch (type) {
+	case BHV_GUESTLOG_MSG_TYPE_STR:
+		return sizeof(guestlog_msg_header_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_fork_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_exec_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_exit_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_driver_load_t) + buf_sz;
+	default:
+		return 0;
+	}
+}
+
+int bhv_guestlog_log_str(char *fmt, ...);
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm);
+int bhv_guestlog_log_process_exec(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_driver_load(const char *name);
+
+#endif /* __BHV_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 000000000..8f4ebeb4f
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_GUEST_POLICY,
+			      bhv_configuration_bitmap);
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init.h include/bhv/init.h
new file mode 100644
index 000000000..e3fbe5c93
--- /dev/null
+++ include/bhv/init.h
@@ -0,0 +1,23 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_H__
+#define __BHV_INIT_H__
+
+#include <bhv/interface/integrity.h>
+#include <bhv/interface/init.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+static const unsigned int BHV_INIT_MAX_REGIONS = 6;
+
+int __init bhv_init_hyp(void *init_start, uint64_t init_size, void *text_start,
+			uint64_t text_size, void *exit_start,
+			uint64_t exit_size);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+int bhv_start_hyp(bhv_init_start_config_t *config);
+
+#endif /* __BHV_INIT_H__ */
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 000000000..71a1881ea
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,194 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/integrity.h>
+
+#ifdef CONFIG_BHV_VAS
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_INTEGRITY, bhv_configuration_bitmap);
+}
+
+void __init bhv_integrity_mm_init(void);
+
+int bhv_start_integrity_arch(void);
+
+int bhv_integrity_disable_events(void);
+int bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+					bhv_mem_region_t *region_head);
+int bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.bhv_mem_region_create.start_addr = addr;
+	n->region.bhv_mem_region_create.size = size;
+	n->region.bhv_mem_region_create.type = type;
+	n->region.bhv_mem_region_create.flags = flags;
+	n->region.bhv_mem_region_create.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.bhv_mem_region_create.next =
+			virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.bhv_mem_region_update.start_addr = addr;
+	n->region.bhv_mem_region_update.type = type;
+	n->region.bhv_mem_region_update.flags = flags;
+	n->region.bhv_mem_region_update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.bhv_mem_region_update.next =
+			virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.bhv_mem_region_remove.start_addr = addr;
+	n->region.bhv_mem_region_remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.bhv_mem_region_remove.next =
+			virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+
+static inline void bhv_integrity_mm_init(void)
+{
+}
+
+static inline int bhv_integrity_disable_events(void)
+{
+	return 0;
+}
+
+static inline int
+bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+				    bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner)
+{
+	return 0;
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/interface/acl.h include/bhv/interface/acl.h
new file mode 100644
index 000000000..c6822e302
--- /dev/null
+++ include/bhv/interface/acl.h
@@ -0,0 +1,33 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_ACL_H__
+#define __BHV_INTERFACE_ACL_H__
+
+#include <linux/types.h>
+
+/* BHV VAS ACL BACKEND OPS */
+#define BHV_VAS_ACL_OP_INIT_PROC_ACL 0U
+#define BHV_VAS_ACL_OP_INIT_DRIVER_ACL 1U
+#define BHV_VAS_ACL_OP_VIOLATION_PROC_ACL 2U
+#define BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL 3U
+
+typedef struct {
+	uint8_t valid;
+	uint8_t is_allow;
+	uint16_t num_pages;
+	uint16_t list_len;
+	uint16_t padding;
+	uint64_t list[];
+} __attribute__((__packed__)) bhv_acl_config_t;
+
+typedef struct {
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_acl_violation_t;
+#endif /* __BHV_INTERFACE_ACL_H__ */
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 000000000..fcdfd1606
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define BHV_VAS_ABI_VERSION 0xbed00001u
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+
+#define BHV_VAS_BACKEND_INIT 1
+#define BHV_VAS_BACKEND_INTEGRITY 2
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_FILEOPS_PROTECTION 9
+
+/* BHV CONFIGURATION BITS */
+#define BHV_CONFIG_INTEGRITY 0
+#define BHV_CONFIG_PROC_ACL 1
+#define BHV_CONFIG_DRIVER_ACL 2
+#define BHV_CONFIG_LOGGING 3
+#define BHV_CONFIG_CREDS 4
+#define BHV_CONFIG_FILE_PROTECTION 5
+#define BHV_CONFIG_GUEST_POLICY 6
+#define BHV_CONFIG_FILEOPS_PROTECTION 7
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/creds.h include/bhv/interface/creds.h
new file mode 100644
index 000000000..3bdee7376
--- /dev/null
+++ include/bhv/interface/creds.h
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_CREDS_H__
+#define __BHV_INTERFACE_CREDS_H__
+
+#include <linux/types.h>
+
+/* BHV VAS CREDS BACKEND OPS */
+#define BHV_VAS_CREDS_OP_CONFIGURE 0U
+#define BHV_VAS_CREDS_OP_REGISTER_INIT_TASK 1U
+#define BHV_VAS_CREDS_OP_ASSIGN 2U
+#define BHV_VAS_CREDS_OP_ASSIGN_PRIV 3U
+#define BHV_VAS_CREDS_OP_COMMIT 4U
+#define BHV_VAS_CREDS_OP_RELEASE 5U
+#define BHV_VAS_CREDS_OP_VERIFICATION 6U
+#define BHV_VAS_CREDS_OP_LOG 7U
+
+enum event_type {
+	EVENT_NONE = 0,
+	CORRUPTION,
+	INVALID_ASSIGNMENT,
+	DOUBLE_ASSIGNMENT,
+	INVALID_COMMIT,
+	DOUBLE_COMMIT,
+	MAX_EVENTS
+};
+
+typedef struct {
+	uint64_t addr;
+	uint64_t cred;
+	uint64_t hmac;
+} __attribute__((__packed__)) bhv_task_cred_t;
+
+typedef struct {
+	bhv_task_cred_t init_task;
+} __attribute__((__packed__)) bhv_creds_init_task_arg_t;
+
+typedef struct {
+	bhv_task_cred_t new_task;
+	bhv_task_cred_t parent;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_arg_t;
+
+typedef struct {
+	uint64_t cred;
+	uint64_t daemon;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_priv_arg_t;
+
+typedef struct {
+	bhv_task_cred_t cur;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_commit_arg_t;
+
+typedef struct {
+	uint64_t cred;
+} __attribute__((__packed__)) bhv_creds_release_arg_t;
+
+typedef struct {
+	bhv_task_cred_t task;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_verification_arg_t;
+
+typedef struct {
+	uint8_t event_type;
+	uint8_t block;
+	uint16_t pad1;
+	uint32_t task_pid;
+	uint64_t task_addr;
+	uint64_t task_cred;
+	char task_name[TASK_COMM_LEN];
+} __attribute__((__packed__)) bhv_creds_log_arg_t;
+
+typedef struct {
+	union {
+		bhv_creds_init_task_arg_t creds_register;
+		bhv_creds_assign_arg_t creds_assign;
+		bhv_creds_assign_priv_arg_t creds_assign_priv;
+		bhv_creds_commit_arg_t creds_commit;
+		bhv_creds_release_arg_t creds_release;
+		bhv_creds_verification_arg_t creds_verify;
+		bhv_creds_log_arg_t creds_log;
+	};
+} __attribute__((__packed__)) bhv_creds_arg_t;
+
+#endif /* __BHV_INTERFACE_CREDS_H__ */
diff --git include/bhv/interface/file_protection.h include/bhv/interface/file_protection.h
new file mode 100644
index 000000000..095a9dbc3
--- /dev/null
+++ include/bhv/interface/file_protection.h
@@ -0,0 +1,29 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_FILE_PROTECTION_H__
+#define __BHV_INTERFACE_FILE_PROTECTION_H__
+
+#include <linux/types.h>
+
+/* BHV VAS FILE PROTECTION BACKEND OPS */
+#define BHV_VAS_FILE_PROTECTION_OP_INIT 0U
+#define BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION 1U
+
+/* BHV VAS FILE PROTECTION FEATURES */
+#define BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION 0U
+
+typedef struct {
+	uint64_t feature_bitmap;
+} __attribute__((__packed__)) bhv_file_protection_config_t;
+
+typedef struct {
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_file_protection_violation_t;
+#endif /* __BHV_INTERFACE_FILE_PROTECTION_H__ */
diff --git include/bhv/interface/fileops_protection.h include/bhv/interface/fileops_protection.h
new file mode 100644
index 000000000..fa0740de6
--- /dev/null
+++ include/bhv/interface/fileops_protection.h
@@ -0,0 +1,29 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_FILEOPS_PROTECTION_H__
+#define __BHV_INTERFACE_FILEOPS_PROTECTION_H__
+
+#include <linux/types.h>
+
+/* BHV VAS FILE OPERATIONS PROTECTION BACKEND OPS */
+#define BHV_VAS_FILEOPS_PROTECTION_OP_VIOLATION 0U
+
+/* supported file system types */
+#define BHV_VAS_FILEOPS_PROTECTION_EXT4 0U
+#define BHV_VAS_FILEOPS_PROTECTION_TMPFS 1U
+
+#define BHV_VAS_FILEOPS_PATH_MAX_SZ 1024
+
+typedef struct {
+	uint8_t is_dir;
+	uint8_t fs_type;
+	uint8_t block;
+	char padding[5];
+	char path_name[BHV_VAS_FILEOPS_PATH_MAX_SZ];
+} __attribute__((__packed__)) bhv_fileops_protection_violation_t;
+
+#endif /* __BHV_INTERFACE_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/interface/guestconn.h include/bhv/interface/guestconn.h
new file mode 100644
index 000000000..ca02dccd2
--- /dev/null
+++ include/bhv/interface/guestconn.h
@@ -0,0 +1,28 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_GUESTCONN_H__
+#define __BHV_INTERFACE_GUESTCONN_H__
+
+#include <linux/types.h>
+
+/* BHV GUESTCONN MESSAGE TYPES */
+#define BHV_GUESTCONN_MSG_TYPE_LOG 0U
+
+#define BHV_GUESTCONN_MAX_BODY_SZ 1024
+
+typedef struct {
+	uint16_t type;
+	uint16_t sz;
+	uint8_t body[];
+} __attribute__((__packed__)) guestconn_msg_header_t;
+
+#define BHV_GUESTCONN_MAX_MSG_SZ                                               \
+	(sizeof(guestconn_msg_header_t) + BHV_GUESTCONN_MAX_BODY_SZ)
+
+#endif /* __BHV_INTERFACE_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/interface/guestlog.h include/bhv/interface/guestlog.h
new file mode 100644
index 000000000..409367b0a
--- /dev/null
+++ include/bhv/interface/guestlog.h
@@ -0,0 +1,95 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_GUESTLOG_H__
+#define __BHV_INTERFACE_GUESTLOG_H__
+
+#include <bhv/interface/guestconn.h>
+
+#include <linux/types.h>
+
+/* BHV VAS ACL BACKEND OPS */
+#define BHV_VAS_GUESTLOG_OP_INIT_GUESTLOG 0U
+
+/* BHV GUESTLOG MESSAGE TYPES */
+#define BHV_GUESTLOG_MSG_TYPE_STR 0U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK 1U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC 2U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT 3U
+#define BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD 4U
+
+typedef struct {
+	uint16_t type;
+	uint16_t sz;
+} __attribute__((__packed__)) guestlog_msg_header_t;
+
+#define BHV_GUESTLOG_MAX_MSG_SZ (BHV_GUESTCONN_MAX_BODY_SZ)
+#define BHV_GUESTLOG_MAX_MSG_DATA_SZ                                           \
+	(BHV_GUESTLOG_MAX_MSG_SZ - sizeof(guestlog_msg_header_t))
+
+typedef struct {
+	char buf[BHV_GUESTLOG_MAX_MSG_DATA_SZ];
+} __attribute__((__packed__)) guestlog_msg_str_t;
+static_assert(sizeof(guestlog_msg_str_t) <= BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	uint32_t child_pid;
+	uint32_t parent_pid;
+	uint32_t child_comm_offset;
+	uint32_t parent_comm_offset;
+	char buf[];
+} __attribute__((__packed__)) guestlog_msg_process_fork_t;
+static_assert(sizeof(guestlog_msg_process_fork_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	uint32_t pid;
+	uint32_t parent_pid;
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_process_exec_t;
+static_assert(sizeof(guestlog_msg_process_exec_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	uint32_t pid;
+	uint32_t parent_pid;
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_process_exit_t;
+static_assert(sizeof(guestlog_msg_process_exit_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	uint8_t __[0];
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_driver_load_t;
+static_assert(sizeof(guestlog_msg_driver_load_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	guestlog_msg_header_t header;
+	union {
+		guestlog_msg_str_t str;
+		guestlog_msg_process_fork_t process_fork;
+		guestlog_msg_process_exec_t process_exec;
+		guestlog_msg_process_exit_t process_exit;
+		guestlog_msg_driver_load_t driver_load;
+		uint8_t __raw_data[BHV_GUESTLOG_MAX_MSG_DATA_SZ];
+	};
+} __attribute__((__packed__)) guestlog_msg_t;
+static_assert(sizeof(guestlog_msg_t) <= BHV_GUESTLOG_MAX_MSG_SZ);
+
+#define BHV_GUESTLOG_CONFIG_LOG_PROCESS_EVENTS 0
+#define BHV_GUESTLOG_CONFIG_LOG_DRIVER_EVENTS 1
+
+typedef struct {
+	uint64_t log_bitmap;
+	uint8_t valid;
+} __attribute__((__packed__)) bhv_guestlog_config_t;
+
+#endif /* __BHV_INTERFACE_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 000000000..d9c92066f
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,39 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/interface/common.h>
+
+static __always_inline unsigned long bhv_hypercall_vas(uint32_t backend,
+						       uint32_t op, void *arg)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	if (arg != NULL)
+		phys_addr = virt_to_phys(arg);
+
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, BHV_VAS_ABI_VERSION,
+			   phys_addr);
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+	if (rv) {
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %x %p %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, BHV_VAS_ABI_VERSION, arg,
+		      phys_addr);
+	}
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+
+	return rv;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/init.h include/bhv/interface/init.h
new file mode 100644
index 000000000..93bb5deae
--- /dev/null
+++ include/bhv/interface/init.h
@@ -0,0 +1,41 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_INIT_H__
+#define __BHV_INTERFACE_INIT_H__
+
+/* BHV VAS INIT BACKEND OPS */
+#define BHV_VAS_INIT_OP_INIT 0
+#define BHV_VAS_INIT_OP_START 1
+
+typedef struct {
+	uint64_t bhv_data_start;
+	uint64_t bhv_data_size;
+	uint64_t bhv_text_start;
+	uint64_t bhv_text_size;
+	uint64_t modprobe_path_sz;
+	uint64_t modprobe_path;
+	uint64_t owner;
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_init_init_arg_t;
+
+typedef struct {
+	uint8_t valid;
+	uint8_t padding;
+	uint16_t num_pages;
+	uint32_t data_sz;
+	uint8_t data[];
+} __attribute__((__packed__)) bhv_init_start_config_t;
+
+typedef struct {
+	union {
+		bhv_init_init_arg_t bhv_init_init_arg;
+		bhv_init_start_config_t bhv_init_start_config;
+	};
+} __attribute__((__packed__)) bhv_init_arg_t;
+
+#endif /* __BHV_INTERFACE_INIT_H__ */
diff --git include/bhv/interface/integrity.h include/bhv/interface/integrity.h
new file mode 100644
index 000000000..d7ddf79d1
--- /dev/null
+++ include/bhv/interface/integrity.h
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_INTEGRITY_H__
+#define __BHV_INTERFACE_INTEGRITY_H__
+
+#include <linux/slab.h>
+
+#define BHV_MEM_TYPE_UNKNOWN			0U
+#define BHV_MEM_TYPE_CODE			1U
+#define BHV_MEM_TYPE_CODE_WRITABLE		2U
+#define BHV_MEM_TYPE_CODE_PATCHABLE		3U
+#define BHV_MEM_TYPE_DATA			4U
+#define BHV_MEM_TYPE_DATA_READ_ONLY		5U
+
+#define BHV_MEM_FLAGS_NONE			0UL
+#define BHV_MEM_FLAGS_TRANSIENT		(1UL << 0)
+#define BHV_MEM_FLAGS_MUTABLE		(1UL << 1)
+
+/* BHV VAS INTEGRITY BACKEND OPS */
+#define BHV_VAS_INTEGRITY_OP_CREATE_PHYS	0U
+#define BHV_VAS_INTEGRITY_OP_UPDATE_PHYS	1U
+#define BHV_VAS_INTEGRITY_OP_REMOVE_PHYS	2U
+#define BHV_VAS_INTEGRITY_OP_DISABLE		3U
+
+/*********************************************
+ * BHV memory region definitions
+ ********************************************/
+
+typedef struct {
+	uint64_t start_addr;
+	uint64_t size;
+	uint32_t type;
+	uint32_t pad;
+	uint64_t flags;
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_create_t;
+
+typedef struct {
+	uint64_t start_addr;
+	uint32_t type;
+	uint32_t pad;
+	uint64_t flags;
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_update_t;
+
+typedef struct {
+	uint64_t start_addr;
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_remove_t;
+
+typedef struct {
+	union {
+		bhv_mem_region_create_t bhv_mem_region_create;
+		bhv_mem_region_update_t bhv_mem_region_update;
+		bhv_mem_region_remove_t bhv_mem_region_remove;
+	};
+} __attribute__((__packed__)) bhv_mem_region_t;
+
+/*********************************************
+ * BHV arg definitions
+ ********************************************/
+
+typedef struct {
+	uint64_t owner;
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_integrity_create_arg_t;
+
+typedef struct {
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_integrity_update_arg_t;
+
+typedef struct {
+	uint64_t rm_by_owner;
+	union {
+		uint64_t owner;
+		uint64_t region_head;
+	};
+} __attribute__((__packed__)) bhv_integrity_remove_arg_t;
+
+typedef struct {
+	union {
+		bhv_integrity_create_arg_t bhv_integrity_create_arg;
+		bhv_integrity_update_arg_t bhv_integrity_update_arg;
+		bhv_integrity_remove_arg_t bhv_integrity_remove_arg;
+	};
+} __attribute__((__packed__)) bhv_integrity_arg_t;
+
+#endif /* __BHV_INTERFACE_INTEGRITY_H__ */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 000000000..b862189bc
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_PATCH_H__
+#define __BHV_INTERFACE_PATCH_H__
+
+#include <linux/types.h>
+
+/* BHV VAS PATCH BACKEND OPS */
+#define BHV_VAS_PATCH_OP_PATCH 0
+#define BHV_VAS_PATCH_OP_PATCH_NO_CLOSE 1
+
+#define BHV_MAX_PATCH_SZ 32
+
+typedef struct {
+	uint64_t dest_phys_addr;
+	uint8_t src_value[BHV_MAX_PATCH_SZ];
+	uint64_t size;
+} __attribute__((__packed__)) bhv_patch_patch_arg_t;
+
+typedef struct {
+	union {
+		bhv_patch_patch_arg_t bhv_patch_patch_arg;
+	};
+} __attribute__((__packed__)) bhv_patch_arg_t;
+
+#endif /* __BHV_INTERFACE_PATCH_H__ */
diff --git include/bhv/interface/vault.h include/bhv/interface/vault.h
new file mode 100644
index 000000000..075752c19
--- /dev/null
+++ include/bhv/interface/vault.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_VAULT_H__
+#define __BHV_INTERFACE_VAULT_H__
+
+/* BHV VAS VAULT BACKEND OPS */
+#define BHV_VAS_VAULT_OP_OPEN	0
+#define BHV_VAS_VAULT_OP_CLOSE	1
+
+#endif /* __BHV_INTERFACE_VAULT_H__ */
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 000000000..6a099f891
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 000000000..e3c46f30d
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_JUMP_LABEL */
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+void bhv_alternatives_delete_after_init(void);
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+void __init_or_module bhv_apply_retpolines(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+
+#ifdef CONFIG_PARAVIRT
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+
+#else /* CONFIG_BHV_VAS */
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/start.h include/bhv/start.h
new file mode 100644
index 000000000..67c90a54c
--- /dev/null
+++ include/bhv/start.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_START_H__
+#define __BHV_START_H__
+
+#ifdef CONFIG_BHV_VAS
+void __init bhv_init_platform(void);
+void bhv_start(void);
+int bhv_start_arch(void);
+#else /* CONFIG_BHV_VAS */
+static inline void bhv_init_platform(void)
+{
+}
+static inline void bhv_start(void)
+{
+}
+static inline int bhv_start_arch(void)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_START_H__ */
\ No newline at end of file
diff --git include/bhv/vault.h include/bhv/vault.h
new file mode 100644
index 000000000..902647976
--- /dev/null
+++ include/bhv/vault.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VAULT_H__
+#define __BHV_VAULT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/vault.h>
+#include <bhv/bhv.h>
+
+static __always_inline int bhv_vault_open_hyp(void)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_VAULT,
+					    BHV_VAS_VAULT_OP_OPEN, NULL);
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+
+static __always_inline void bhv_vault_close_hyp(void)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_VAULT,
+					    BHV_VAS_VAULT_OP_CLOSE, NULL);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+}
+#else /* CONFIG_BHV_VAS */
+static inline int bhv_vault_open_hyp(void)
+{
+}
+
+static inline void bhv_vault_close_hyp(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_VAULT_H__ */
diff --git include/linux/filter.h include/linux/filter.h
index a9956b681..e5c9dc057 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -28,6 +28,8 @@
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
+#include <bhv/module.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -878,6 +880,7 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 	if (!fp->jited) {
 		set_vm_flush_reset_perms(fp);
 		set_memory_ro((unsigned long)fp, fp->pages);
+		bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
 	}
 #endif
 }
@@ -887,6 +890,7 @@ static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->pages);
 	set_memory_x((unsigned long)hdr, hdr->pages);
+	bhv_bpf_protect_x(hdr, hdr->pages << PAGE_SHIFT);
 }
 
 static inline struct bpf_binary_header *
@@ -923,6 +927,9 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited)
+		bhv_bpf_unprotect(fp);
+
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/kmod.h include/linux/kmod.h
index 68f69362d..36934d810 100644
--- include/linux/kmod.h
+++ include/linux/kmod.h
@@ -17,7 +17,11 @@
 #define KMOD_PATH_LEN 256
 
 #ifdef CONFIG_MODULES
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+extern const char modprobe_path[]; /* for sysctl */
+#else
 extern char modprobe_path[]; /* for sysctl */
+#endif
 /* modprobe exit status on success, -ve on error.  Return value
  * usually useless though. */
 extern __printf(2, 3)
diff --git init/main.c init/main.c
index 649d9e420..c398cc259 100644
--- init/main.c
+++ init/main.c
@@ -114,6 +114,9 @@
 
 #include <kunit/test.h>
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -859,6 +862,8 @@ static void __init mm_init(void)
 	init_espfix_bsp();
 	/* Should be run after espfix64 is set up. */
 	pti_init();
+
+	bhv_mm_init();
 }
 
 #ifdef CONFIG_HAVE_ARCH_RANDOMIZE_KSTACK_OFFSET
diff --git kernel/Makefile kernel/Makefile
index 0e119c52a..f14bd7e49 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -53,6 +53,7 @@ obj-y += rcu/
 obj-y += livepatch/
 obj-y += dma/
 obj-y += entry/
+obj-y += bhv/
 
 obj-$(CONFIG_KCMP) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
diff --git kernel/bhv/Kconfig kernel/bhv/Kconfig
new file mode 100644
index 000000000..ec27d2dd6
--- /dev/null
+++ kernel/bhv/Kconfig
@@ -0,0 +1,30 @@
+config BHV_PANIC_ON_FAIL
+	def_bool y
+	bool "BHV guest panics on Hypercall failure"
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	def_bool n
+	bool "Build BHV guest support with DEBUG information"
+	depends on BHV_VAS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	def_bool n
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_MODPROBE_PATH
+	def_bool y
+	bool "Make modprobe_path global constant such that it cannot be updated"
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to make the modprobe_path global constant such that it cannot be updated.
diff --git kernel/bhv/Makefile kernel/bhv/Makefile
new file mode 100644
index 000000000..539aea2a1
--- /dev/null
+++ kernel/bhv/Makefile
@@ -0,0 +1,22 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn.o
+obj-$(CONFIG_BHV_VAS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
diff --git kernel/bhv/acl.c kernel/bhv/acl.c
new file mode 100644
index 000000000..b319213d5
--- /dev/null
+++ kernel/bhv/acl.c
@@ -0,0 +1,298 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/acl.h>
+
+#include <bhv/interface/integrity.h>
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#define PATH_DELIMITER '/'
+
+bhv_acl_config_t *process_acl_config __ro_after_init = NULL;
+bhv_acl_config_t *driver_acl_config __ro_after_init = NULL;
+
+struct kmem_cache *bhv_acl_violation_cache;
+
+static void __init bhv_acl_init_acl(uint32_t op, bhv_acl_config_t **global_acl_config)
+{
+	unsigned long r;
+	bhv_acl_config_t *acl_config;
+	static bhv_mem_region_t config_region;
+
+	bhv_acl_violation_cache = kmem_cache_create(
+		"bhv_acl_violation_cache", sizeof(bhv_acl_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	acl_config = (bhv_acl_config_t *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (acl_config == NULL) {
+		bhv_fail("Unable to allocate process acl config");
+		return;
+	}
+
+	acl_config->num_pages = 1;
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+	if (r) {
+		pr_err("proc acl init fail");
+		return;
+	}
+
+	if (!acl_config->valid) {
+		free_pages((unsigned long)acl_config, 0);
+
+		acl_config =
+		(bhv_acl_config_t *)__get_free_pages(GFP_KERNEL, order_base_2(acl_config->num_pages));
+
+		if (acl_config == NULL) {
+			bhv_fail("Unable to allocate process acl config");
+			return;
+		}
+
+		r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+		if (r) {
+			pr_err("proc acl init fail");
+			return;
+		}
+
+		if (!acl_config->valid) {
+			bhv_fail("host returned invalid configuration");
+			return;
+		}
+	}
+
+	// Protect memory
+	if (bhv_integrity_is_enabled()) {
+		config_region.bhv_mem_region_create.start_addr =
+			virt_to_phys(acl_config);
+		config_region.bhv_mem_region_create.size =
+			acl_config->num_pages * PAGE_SIZE;
+		config_region.bhv_mem_region_create.type =
+			BHV_MEM_TYPE_DATA_READ_ONLY;
+		config_region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+		config_region.bhv_mem_region_create.next =
+			BHV_INVALID_PHYS_ADDR;
+
+		r = bhv_create_kern_phys_mem_region_hyp(0, &config_region);
+		if (r) {
+			pr_err("Unable to protect acl config");
+			return;
+		}
+	}
+
+	*global_acl_config = acl_config;
+}
+
+void __init bhv_acl_mm_init(void)
+{
+	if (bhv_acl_is_proc_acl_enabled())
+		bhv_acl_init_acl(BHV_VAS_ACL_OP_INIT_PROC_ACL,
+				 &process_acl_config);
+	if (bhv_acl_is_driver_acl_enabled())
+		bhv_acl_init_acl(BHV_VAS_ACL_OP_INIT_DRIVER_ACL,
+				 &driver_acl_config);
+}
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+static bool _matches(const char *target, bool strip_ext,
+		     bhv_acl_config_t *acl_config)
+{
+	size_t target_len = 0;
+	size_t target_ext_len = 0;
+	uint16_t i;
+
+	BUG_ON(target[0] != PATH_DELIMITER);
+
+	// Get target len
+	target_len = strnlen(target, PATH_MAX);
+	if (strip_ext) {
+		target_ext_len = _get_ext_len(target);
+		target_len -= target_ext_len;
+	}
+
+	if (target_len == 0 || target_len >= PATH_MAX)
+		return false;
+
+	for (i = 0; i < acl_config->list_len; i++) {
+		const char *cur = ((char *)acl_config) + acl_config->list[i];
+		size_t cur_len = 0;
+
+		if (cur[0] != PATH_DELIMITER) {
+			if (_match_names(cur, target, target_ext_len,
+					 strip_ext))
+				return true;
+			else
+				continue;
+		}
+
+		cur_len = strnlen(cur, PATH_MAX);
+		if (strip_ext) {
+			cur_len -= _get_ext_len(cur);
+		}
+
+		if (cur_len == 0 || cur_len >= PATH_MAX)
+			continue;
+
+		if (cur[cur_len - 1] == '*') {
+			cur_len--;
+
+			if (target_len < cur_len)
+				continue;
+		} else if (target_len != cur_len)
+			continue;
+
+		if (strncmp(cur, target, cur_len) == 0)
+			return true;
+	}
+
+	return false;
+}
+
+static bool _block_entity(const char *target, bool strip_ext,
+			  bhv_acl_config_t *acl_config, uint32_t op)
+{
+	bool rv;
+	unsigned long r;
+	// Tell the compiler this points to volatile data as the hypercall
+	// will update it.
+	bhv_acl_violation_t *volatile acl_violation = NULL;
+	size_t target_len = strlen(target);
+	bool m;
+
+	if (acl_config == NULL || !acl_config->valid) {
+		bhv_fail("unable to resolve entity due to init error");
+		return false;
+	}
+
+	m = _matches(target, strip_ext, acl_config);
+
+	// Is this entity part of the allow list?
+	if (m && acl_config->is_allow)
+		return false;
+	// Is this entity _NOT_ in the deny list?
+	if (!m && !acl_config->is_allow)
+		return false;
+
+	BUG_ON(target_len >= PAGE_SIZE);
+
+	// Prepare hypercall
+	acl_violation = kmem_cache_alloc(bhv_acl_violation_cache, GFP_KERNEL);
+	if (acl_violation == NULL) {
+		bhv_fail("Unable to allocate acl violation");
+		return true;
+	}
+
+	// Setup arg
+	acl_violation->name_len = target_len;
+	acl_violation->name = virt_to_phys((volatile void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_violation);
+	if (r) {
+		pr_err("entity hypercall failed");
+		kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+		return true;
+	}
+
+	// Read block and free
+	rv = (bool)acl_violation->block;
+	kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+	return rv;
+}
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity(target, true, driver_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity(target, false, process_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_PROC_ACL);
+}
\ No newline at end of file
diff --git kernel/bhv/bhv.c kernel/bhv/bhv.c
new file mode 100644
index 000000000..16840bfba
--- /dev/null
+++ kernel/bhv/bhv.c
@@ -0,0 +1,28 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/acl.h>
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/patch.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+
+#include <bhv/bhv.h>
+
+bool bhv_initialized __ro_after_init = false;
+unsigned long *bhv_configuration_bitmap __ro_after_init = NULL;
+
+void __init bhv_mm_init(void)
+{
+	bhv_integrity_mm_init();
+	bhv_acl_mm_init();
+	bhv_guestconn_mm_init();
+	bhv_cred_mm_init();
+	bhv_file_protection_init();
+	bhv_fileops_protection_mm_init();
+}
diff --git kernel/bhv/creds.c kernel/bhv/creds.c
new file mode 100644
index 000000000..237793492
--- /dev/null
+++ kernel/bhv/creds.c
@@ -0,0 +1,416 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com> 
+ */
+
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/creds.h>
+#include <bhv/interface/hypercall.h>
+
+#define BHV_CREDS_HYP(op, arg) bhv_hypercall_vas(BHV_VAS_BACKEND_CREDS, op, arg)
+
+#define BHV_CREDS_CONFIGURE_HYP(arg)                                           \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_CONFIGURE, arg)
+#define BHV_CREDS_REGISTER_INIT_TASK_HYP(arg)                                  \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_REGISTER_INIT_TASK, arg)
+#define BHV_CREDS_ASSIGN_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN, arg)
+#define BHV_CREDS_ASSIGN_PRIV_HYP(arg)                                         \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN_PRIV, arg)
+#define BHV_CREDS_COMMIT_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_COMMIT, arg)
+#define BHV_CREDS_RELEASE_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_RELEASE, arg)
+#define BHV_CREDS_VERIFY_HYP(arg)                                              \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_VERIFICATION, arg)
+#define BHV_CREDS_LOG_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_LOG, arg)
+
+struct kmem_cache *bhv_creds_arg_cache = NULL;
+
+static siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	atomic_set(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	struct task_struct *parent = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(current);
+	if (rc)
+		return -EPERM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = current->real_parent;
+	else
+		parent = current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_assign.new_task.addr = (uint64_t)t;
+	arg->creds_assign.new_task.cred = (uint64_t)t->cred;
+	arg->creds_assign.new_task.hmac = hmac;
+	arg->creds_assign.parent.addr = (uint64_t)parent;
+	arg->creds_assign.parent.cred = (uint64_t)parent->cred;
+
+	rc = BHV_CREDS_ASSIGN_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		/* Note that we currently log only the parent's information. */
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)parent;
+		arg->creds_log.task_cred = (uint64_t)parent->cred;
+		arg->creds_log.task_pid = parent->pid;
+		strscpy(arg->creds_log.task_name, parent->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	arg->creds_assign_priv.cred = (uint64_t)c;
+	arg->creds_assign_priv.daemon = (uint64_t)daemon;
+
+	rc = BHV_CREDS_ASSIGN_PRIV_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign_priv.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	arg->creds_commit.cur.cred = (uint64_t)c;
+	arg->creds_commit.cur.addr = (uint64_t)current;
+	arg->creds_commit.cur.hmac = hmac;
+
+	rc = BHV_CREDS_COMMIT_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = arg->creds_commit.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_verify.task.cred = (uint64_t)t->cred;
+	arg->creds_verify.task.addr = (uint64_t)t;
+	arg->creds_verify.task.hmac = hmac;
+
+	rc = BHV_CREDS_VERIFY_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_verify.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)t;
+		arg->creds_log.task_cred = (uint64_t)t->cred;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, t->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->creds_release.cred = (uint64_t)c;
+
+	rc = BHV_CREDS_RELEASE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->creds_register.init_task.addr = (uint64_t)t;
+	arg->creds_register.init_task.cred = (uint64_t)c;
+	arg->creds_register.init_task.hmac = siphash_cred_context(c, t);
+
+	rc = BHV_CREDS_REGISTER_INIT_TASK_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+void __init bhv_cred_mm_init(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_creds_arg_cache = kmem_cache_create(
+		"bhv_creds_arg_cache", sizeof(bhv_creds_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	bhv_cred_register_init_task(&init_cred, &init_task);
+}
+
+int __init bhv_cred_init(void)
+{
+	int rc = 0;
+
+	if (!bhv_cred_is_enabled())
+		return -EINVAL;
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = BHV_CREDS_CONFIGURE_HYP(&bhv_siphash_key);
+	if (rc) {
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
diff --git kernel/bhv/file_protection.c kernel/bhv/file_protection.c
new file mode 100644
index 000000000..0b0943222
--- /dev/null
+++ kernel/bhv/file_protection.c
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/file_protection.h>
+
+#include <bhv/file_protection.h>
+
+bhv_file_protection_config_t bhv_file_protection_config __ro_after_init = { 0 };
+
+struct kmem_cache *bhv_file_protection_violation_cache;
+
+void __init bhv_file_protection_init(void)
+{
+	unsigned long r;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	// Create slab cache
+	bhv_file_protection_violation_cache = kmem_cache_create(
+		"bhv_file_protection_violation_cache",
+		sizeof(bhv_file_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_file_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION,
+			      BHV_VAS_FILE_PROTECTION_OP_INIT,
+			      &bhv_file_protection_config);
+	if (r) {
+		pr_err("File protection init failed");
+		return;
+	}
+}
+
+bool bhv_block_read_only_file_write(const char *target)
+{
+	unsigned long r;
+	bool rv;
+	bhv_file_protection_violation_t *volatile violation = NULL;
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return false;
+
+	// Prepare hypercall
+	violation = kmem_cache_alloc(bhv_file_protection_violation_cache,
+				     GFP_KERNEL);
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate file protection violation");
+		return true;
+	}
+
+	// Setup arg
+	violation->name_len = strlen(target);
+	violation->name = virt_to_phys((volatile void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(
+		BHV_VAS_BACKEND_FILE_PROTECTION,
+		BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION,
+		violation);
+	if (r) {
+		pr_err("file protection violation hypercall failed");
+		kmem_cache_free(bhv_file_protection_violation_cache, violation);
+		return true;
+	}
+
+	// Read block and free
+	rv = (bool)violation->block;
+	kmem_cache_free(bhv_file_protection_violation_cache, violation);
+	return rv;
+}
\ No newline at end of file
diff --git kernel/bhv/fileops_protection.c kernel/bhv/fileops_protection.c
new file mode 100644
index 000000000..6615a5a84
--- /dev/null
+++ kernel/bhv/fileops_protection.c
@@ -0,0 +1,78 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/fileops_protection.h>
+
+#include <bhv/fileops_protection.h>
+
+struct kmem_cache *bhv_fileops_protection_violation_cache;
+
+void __init bhv_fileops_protection_mm_init(void)
+{
+	if (!bhv_fileops_protection_is_enabled())
+		return;
+
+	// Create slab cache for violation data being set by host
+	bhv_fileops_protection_violation_cache = kmem_cache_create(
+		"bhv_fileops_protection_violation_cache",
+		sizeof(bhv_fileops_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_fileops_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache.");
+		return;
+	}
+}
+
+bool bhv_block_fileops(const char *target, bool is_dir, uint8_t fs_type)
+{
+	unsigned long err;
+	bool retval;
+	bhv_fileops_protection_violation_t *violation;
+	size_t path_sz;
+
+	if (!bhv_fileops_protection_is_enabled())
+		return false;
+
+	violation = kmem_cache_alloc(bhv_fileops_protection_violation_cache,
+				     GFP_KERNEL);
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate fileops protection violation");
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= BHV_VAS_FILEOPS_PATH_MAX_SZ) {
+		path_sz = BHV_VAS_FILEOPS_PATH_MAX_SZ - 1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+	violation->fs_type = fs_type;
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = bhv_hypercall_vas(BHV_VAS_BACKEND_FILEOPS_PROTECTION,
+				BHV_VAS_FILEOPS_PROTECTION_OP_VIOLATION,
+				violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		kmem_cache_free(bhv_fileops_protection_violation_cache,
+				violation);
+		return true;
+	}
+
+	retval = (bool)violation->block;
+	kmem_cache_free(bhv_fileops_protection_violation_cache, violation);
+
+	return retval;
+}
diff --git kernel/bhv/guestconn.c kernel/bhv/guestconn.c
new file mode 100644
index 000000000..d6b24d927
--- /dev/null
+++ kernel/bhv/guestconn.c
@@ -0,0 +1,234 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/guestconn.h>
+
+typedef struct {
+	struct list_head list;
+	size_t to_send;
+	guestconn_msg_header_t header;
+	uint8_t body[BHV_GUESTCONN_MAX_MSG_SZ];
+} bhv_guestconn_send_item_t;
+
+uint32_t bhv_guestconn_cid __ro_after_init = 0;
+uint32_t bhv_guestconn_port __ro_after_init = 0;
+
+static struct socket *vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+static LIST_HEAD(bhv_guestconn_msg_list);
+static DEFINE_SPINLOCK(bhv_guestconn_msg_lock);
+
+static struct workqueue_struct *bhv_guestconn_workqueue = NULL;
+static struct work_struct bhv_guestconn_work_struct;
+static struct delayed_work bhv_guestconn_delayed_work_struct;
+static struct kmem_cache *bhv_guestconn_send_item_cache;
+
+static inline size_t bhv_send(void *data, size_t size, size_t to_send)
+{
+	int r;
+	struct kvec vec;
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+		r = kernel_sendmsg(vsock, &msghdr, &vec, 1, vec.iov_len);
+		if (r == -EAGAIN) {
+			return to_send;
+		} else if (r < 0) {
+			pr_err("BHV GuestLog: Send Failed (%d)", r);
+			return 0;
+		}
+		to_send -= r;
+	}
+	return 0;
+}
+
+static void bhv_guestconn_sendmsg(struct work_struct *ws)
+{
+	bhv_guestconn_send_item_t *item;
+
+	while (true) {
+		spin_lock(&bhv_guestconn_msg_lock);
+		if (list_empty(&bhv_guestconn_msg_list)) {
+			spin_unlock(&bhv_guestconn_msg_lock);
+			return;
+		}
+		item = list_first_entry(&bhv_guestconn_msg_list,
+					bhv_guestconn_send_item_t, list);
+		spin_unlock(&bhv_guestconn_msg_lock);
+
+		item->to_send =
+			bhv_send(&item->header, item->header.sz, item->to_send);
+
+		if (item->to_send == 0 ||
+		    unlikely(atomic_read(&reboot_in_progress))) {
+			spin_lock(&bhv_guestconn_msg_lock);
+			list_del(&(item->list));
+			spin_unlock(&bhv_guestconn_msg_lock);
+			kmem_cache_free(bhv_guestconn_send_item_cache, item);
+		} else {
+			queue_delayed_work(bhv_guestconn_workqueue,
+					   &bhv_guestconn_delayed_work_struct,
+					   msecs_to_jiffies(1000));
+			return;
+		}
+	}
+}
+
+int bhv_guestconn_send(uint16_t type, void *data, size_t size)
+{
+	bhv_guestconn_send_item_t *cur;
+
+	BUG_ON(size > BHV_GUESTCONN_MAX_BODY_SZ);
+
+	pr_debug("BHV GuestConn: Queuing msg of type %u with size %lu", type,
+		 size);
+
+	cur = kmem_cache_alloc(bhv_guestconn_send_item_cache, GFP_KERNEL);
+	if (cur == NULL) {
+		bhv_fail("BHV: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	cur->header.type = type;
+	cur->header.sz = sizeof(guestconn_msg_header_t) + size;
+
+	memcpy(cur->body, data, size);
+
+	cur->to_send = cur->header.sz;
+
+	spin_lock(&bhv_guestconn_msg_lock);
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		spin_unlock(&bhv_guestconn_msg_lock);
+		kmem_cache_free(bhv_guestconn_send_item_cache, cur);
+		return 0;
+	}
+
+	list_add_tail(&(cur->list), &bhv_guestconn_msg_list);
+	spin_unlock(&bhv_guestconn_msg_lock);
+
+	if (atomic_read(&workqueue_ready))
+		queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+
+	return 0;
+}
+
+static int bhv_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	if (atomic_read(&workqueue_ready)) {
+		atomic_inc(&reboot_in_progress);
+
+		// Cancel pending work.
+		cancel_delayed_work_sync(&bhv_guestconn_delayed_work_struct);
+
+		// Drain the workqueue
+		drain_workqueue(bhv_guestconn_workqueue);
+
+		// We assume all messages are gone now and we shut down the socket
+		sock_release(vsock);
+		vsock = NULL;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bhv_guestconn_reboot_notifier = {
+	.notifier_call = bhv_guestconn_reboot,
+	.priority = 0,
+};
+
+void bhv_guestconn_start(void)
+{
+	int err;
+	struct sockaddr_vm addr;
+
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	vsock_addr_init(&addr, bhv_guestconn_cid, bhv_guestconn_port);
+	pr_info("bhv_guestconn_started with cid %u, port %u", bhv_guestconn_cid,
+		bhv_guestconn_port);
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not create kernel socket (%d)", err);
+		return;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not connect to host (%d)", err);
+		return;
+	}
+
+	// Initialize work queue
+	INIT_WORK(&bhv_guestconn_work_struct, bhv_guestconn_sendmsg);
+	INIT_DELAYED_WORK(&bhv_guestconn_delayed_work_struct,
+			  bhv_guestconn_sendmsg);
+	bhv_guestconn_workqueue =
+		alloc_workqueue("bhv_guestconn_workqueue", WQ_UNBOUND, 1);
+	// queue = create_singlethread_workqueue("bhv_guestlog_work_queue");
+	if (bhv_guestconn_workqueue == NULL) {
+		bhv_fail("BHV: Could not allocate work queue!");
+		kmem_cache_destroy(bhv_guestconn_send_item_cache);
+		return;
+	}
+	atomic_inc(&workqueue_ready);
+
+	register_reboot_notifier(&bhv_guestconn_reboot_notifier);
+
+	queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+}
+
+void __init bhv_guestconn_mm_init(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	// Create cache
+	bhv_guestconn_send_item_cache = kmem_cache_create(
+		"bhv_guestconn_send_item_cache",
+		sizeof(bhv_guestconn_send_item_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_guestconn_send_item_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+}
+
+int __init bhv_guestconn_init(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	bhv_guestconn_cid = cid;
+	bhv_guestconn_port = port;
+	return 0;
+}
\ No newline at end of file
diff --git kernel/bhv/guestlog.c kernel/bhv/guestlog.c
new file mode 100644
index 000000000..3d661a465
--- /dev/null
+++ kernel/bhv/guestlog.c
@@ -0,0 +1,150 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/types.h>
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/guestlog.h>
+
+bhv_guestlog_config_t bhv_guestlog_config __ro_after_init = { 0, false };
+
+int bhv_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(msg.str.buf, BHV_GUESTLOG_MAX_MSG_DATA_SZ, fmt, args);
+	va_end(args);
+
+	sz = bhv_guestlog_calc_msg_sz(BHV_GUESTLOG_MSG_TYPE_STR, len);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_STR;
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK;
+	msg.process_fork.child_pid = child_pid;
+	msg.process_fork.parent_pid = parent_pid;
+	msg.process_fork.child_comm_offset = 0;
+	strscpy(msg.process_fork.buf, child_comm, TASK_COMM_LEN);
+	msg.process_fork.parent_comm_offset =
+		strnlen(msg.process_fork.buf, TASK_COMM_LEN) + 1;
+	strscpy(&msg.process_fork.buf[msg.process_fork.parent_comm_offset],
+		parent_comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		msg.process_fork.parent_comm_offset +
+			strnlen(&msg.process_fork.buf
+					 [msg.process_fork.parent_comm_offset],
+				TASK_COMM_LEN) +
+			1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_exec(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC;
+	msg.process_exec.pid = pid;
+	msg.process_exec.parent_pid = parent_pid;
+	strscpy(msg.process_exec.name, comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		strnlen(msg.process_exec.name, TASK_COMM_LEN) + 1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT;
+	msg.process_exit.pid = pid;
+	msg.process_exit.parent_pid = parent_pid;
+	strscpy(msg.process_exit.name, comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		strnlen(msg.process_exit.name, TASK_COMM_LEN) + 1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_driver_load(const char *name)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD;
+	strscpy(msg.driver_load.name, name, BHV_GUESTLOG_MAX_MSG_DATA_SZ - 1);
+	sz = bhv_guestlog_calc_msg_sz(msg.header.type,
+				      strnlen(msg.driver_load.name,
+					      BHV_GUESTLOG_MAX_MSG_DATA_SZ));
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int __init bhv_guestlog_init()
+{
+	unsigned long r;
+
+	if (!bhv_guestlog_enabled())
+		return 0;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_GUESTLOG,
+			      BHV_VAS_GUESTLOG_OP_INIT_GUESTLOG,
+			      &bhv_guestlog_config);
+	if (r) {
+		bhv_fail("BHV: guestlog init failed");
+		return -EFAULT;
+	}
+
+	return 0;
+}
diff --git kernel/bhv/init.c kernel/bhv/init.c
new file mode 100644
index 000000000..542c1f664
--- /dev/null
+++ kernel/bhv/init.c
@@ -0,0 +1,111 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <linux/kmod.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/init.h>
+#include <bhv/interface/integrity.h>
+#include <bhv/init.h>
+#include <bhv/creds.h>
+
+int __init bhv_init_hyp(void *init_start, uint64_t init_size, void *text_start,
+			uint64_t text_size, void *exit_start, uint64_t exit_size)
+{
+	unsigned long r;
+	bhv_mem_region_t *init_phys_mem_regions;
+	bhv_init_arg_t *init_arg;
+
+	void *bhv_data = __bhv_data_start;
+	uint64_t bhv_data_size = __bhv_data_end - __bhv_data_start;
+	void *bhv_text = __bhv_text_start;
+	uint64_t bhv_text_size = __bhv_text_end - __bhv_text_start;
+
+	unsigned int region_counter = 0;
+
+	BUG_ON(((BHV_INIT_MAX_REGIONS * sizeof(bhv_mem_region_t)) +
+		sizeof(bhv_init_arg_t)) > bhv_data_size);
+
+	init_phys_mem_regions = bhv_data;
+
+	init_size = round_up(init_size, PAGE_SIZE);
+
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.start_addr =
+		virt_to_phys(init_start);
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.size =
+		init_size;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_TRANSIENT;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.next =
+		virt_to_phys(&(init_phys_mem_regions[region_counter + 1]));
+	region_counter++;
+
+	text_size = round_up(text_size, PAGE_SIZE);
+
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.start_addr =
+		virt_to_phys(text_start);
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.size =
+		text_size;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_NONE;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.next =
+		virt_to_phys(&(init_phys_mem_regions[region_counter + 1]));
+	region_counter++;
+
+	exit_size = round_up(exit_size, PAGE_SIZE);
+
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.start_addr =
+		virt_to_phys(exit_start);
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.size =
+		exit_size;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.type =
+		BHV_MEM_TYPE_CODE_PATCHABLE;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.flags =
+		BHV_MEM_FLAGS_TRANSIENT;
+	init_phys_mem_regions[region_counter].bhv_mem_region_create.next =
+		BHV_INVALID_PHYS_ADDR;
+	region_counter++;
+
+	bhv_init_hyp_arch(init_phys_mem_regions, &region_counter);
+
+	// Set size of init args
+	init_arg = bhv_data + (BHV_INIT_MAX_REGIONS * sizeof(bhv_mem_region_t));
+
+	init_arg->bhv_init_init_arg.bhv_data_start = virt_to_phys(bhv_data);
+	init_arg->bhv_init_init_arg.bhv_data_size = bhv_data_size;
+	init_arg->bhv_init_init_arg.bhv_text_start = virt_to_phys(bhv_text);
+	init_arg->bhv_init_init_arg.bhv_text_size = bhv_text_size;
+	init_arg->bhv_init_init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	init_arg->bhv_init_init_arg.modprobe_path =
+		virt_to_phys((void *)&modprobe_path);
+	init_arg->bhv_init_init_arg.owner = 0;
+	init_arg->bhv_init_init_arg.region_head =
+		virt_to_phys(init_phys_mem_regions);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INIT, BHV_VAS_INIT_OP_INIT,
+			      init_arg);
+	if (r)
+		return -EINVAL;
+	return 0;
+}
+
+int bhv_start_hyp(bhv_init_start_config_t *config)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_INIT,
+					    BHV_VAS_INIT_OP_START, config);
+	if (r)
+		return -EINVAL;
+	return 0;
+}
diff --git kernel/bhv/integrity.c kernel/bhv/integrity.c
new file mode 100644
index 000000000..25ac14dfd
--- /dev/null
+++ kernel/bhv/integrity.c
@@ -0,0 +1,131 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+struct kmem_cache *bhv_integrity_arg_cache;
+
+void __init bhv_integrity_mm_init(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	bhv_integrity_arg_cache = kmem_cache_create(
+		"bhv_integrity_arg_cache", sizeof(bhv_integrity_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+
+int bhv_integrity_disable_events(void)
+{
+	unsigned long ret = bhv_hypercall_vas(
+		BHV_VAS_BACKEND_INTEGRITY, BHV_VAS_INTEGRITY_OP_DISABLE, NULL);
+
+	return (ret) ? -EINVAL : 0;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+					bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_create_arg.owner = owner;
+	bhv_arg->bhv_integrity_create_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_CREATE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_update_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_UPDATE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_remove_arg.rm_by_owner = 0;
+	bhv_arg->bhv_integrity_remove_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_REMOVE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_remove_arg.rm_by_owner = 1;
+	bhv_arg->bhv_integrity_remove_arg.owner = owner;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_REMOVE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
diff --git kernel/bhv/module.c kernel/bhv/module.c
new file mode 100644
index 000000000..0288d3fb4
--- /dev/null
+++ kernel/bhv/module.c
@@ -0,0 +1,416 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused, uint32_t type,
+				    uint64_t flags)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == BHV_MEM_TYPE_UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == BHV_MEM_TYPE_UNKNOWN)
+		return;
+
+	type &= ~BHV_MEM_FLAGS_MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags,
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size, BHV_MEM_TYPE_UNKNOWN,
+				BHV_MEM_FLAGS_NONE, _bhv_link_node_op_remove);
+}
+
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   BHV_MEM_TYPE_CODE_PATCHABLE,
+			   BHV_MEM_FLAGS_TRANSIENT);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   BHV_MEM_TYPE_DATA_READ_ONLY,
+				   BHV_MEM_FLAGS_TRANSIENT);
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			BHV_MEM_TYPE_DATA,
+			BHV_MEM_FLAGS_TRANSIENT | BHV_MEM_FLAGS_MUTABLE);
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   BHV_MEM_TYPE_DATA, BHV_MEM_FLAGS_TRANSIENT);
+	}
+}
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout);
+	bhv_prepare_mod_layout(head, &mod->core_layout);
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+static void bhv_complete_free_init(const struct module_layout *layout)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(&bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(&bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			&bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(&bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   BHV_MEM_TYPE_DATA_READ_ONLY,
+			   BHV_MEM_FLAGS_TRANSIENT);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_update_ro_after_init(mod);
+	bhv_complete_free_init(&mod->init_layout);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags);
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, BHV_MEM_TYPE_DATA_READ_ONLY,
+			BHV_MEM_FLAGS_TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, BHV_MEM_TYPE_CODE, BHV_MEM_FLAGS_TRANSIENT);
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)base);
+}
diff --git kernel/bhv/patch_alternative.c kernel/bhv/patch_alternative.c
new file mode 100644
index 000000000..3942c365b
--- /dev/null
+++ kernel/bhv/patch_alternative.c
@@ -0,0 +1,225 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/vault.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#include <asm/bhv/patch.h>
+
+static DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+static void __always_inline bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static void __always_inline bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+void bhv_alternatives_delete_after_init(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe (i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter,
+	bhv_patch_arg_t *arg)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = bhv_vault_open_hyp();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe (i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		bhv_vault_close_hyp();
+		return -EACCES;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch, arg);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+	// Close vault.
+	bhv_vault_close_hyp();
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(
+		&search, arch, bhv_alternatives_find_by_alt, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETHUNK */
+
+#ifdef CONFIG_PARAVIRT
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_PARAVIRT */
+
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
diff --git kernel/bhv/patch_jump_label.c kernel/bhv/patch_jump_label.c
new file mode 100644
index 000000000..d057a00b3
--- /dev/null
+++ kernel/bhv/patch_jump_label.c
@@ -0,0 +1,164 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static void __always_inline bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static void __always_inline bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe (i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+enum jump_label_type __bhv_text bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+bool __bhv_text validate_jmp_labels(struct jump_entry *entry,
+				    const void *opcode, size_t len,
+				    uint64_t *dest_phys_addr)
+{
+	struct bhv_static_key_mod *i;
+	*dest_phys_addr = bhv_virt_to_phys((void *)jump_entry_code(entry));
+
+	if (entry >= __start___jump_table && entry < __stop___jump_table) {
+		return bhv_jump_label_validate_opcode(
+			entry, bhv_jump_label_type(entry), opcode, len);
+	}
+
+	list_for_each_entry (i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			return bhv_jump_label_validate_opcode(
+				entry, bhv_jump_label_type(entry), opcode, len);
+		}
+	}
+
+	return false;
+}
+
+int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+					  const void *opcode, size_t len,
+					  bhv_patch_arg_t *bhv_arg)
+{
+	int rv = 0;
+	unsigned long r;
+	bool validation_ok;
+	uint64_t dest_phys_addr;
+
+	rv = bhv_vault_open_hyp();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > BHV_MAX_PATCH_SZ) {
+		bhv_vault_close_hyp();
+		return -E2BIG;
+	}
+
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, opcode, len);
+	bhv_arg->bhv_patch_patch_arg.size = (uint64_t)len;
+
+	validation_ok =
+		validate_jmp_labels(entry,
+				    bhv_arg->bhv_patch_patch_arg.src_value, len,
+				    &dest_phys_addr);
+	if (!validation_ok) {
+		bhv_vault_close_hyp();
+		return -EACCES;
+	}
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr = dest_phys_addr;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH, BHV_VAS_PATCH_OP_PATCH,
+			      bhv_arg);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+
+	/*
+	 * This allocation assumes synchronization around
+	 * bhv_vault_patch_jump_label(...).  This is the current solution
+	 * here as jump label patching happens before kmem caches are
+	 * allocated.
+	 */
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+	return rv;
+}
diff --git kernel/bhv/start.c kernel/bhv/start.c
new file mode 100644
index 000000000..4332461c1
--- /dev/null
+++ kernel/bhv/start.c
@@ -0,0 +1,202 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <asm/sections.h>
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm/syscall.h>
+
+#include <linux/init_task.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/patch.h>
+#include <bhv/start.h>
+
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+
+void __init bhv_init_platform(void)
+{
+	uint32_t cid, port;
+
+	int rv = bhv_init_hyp(_sinittext, _einittext - _sinittext, _text,
+			      _etext - _text, _sexittext,
+			      _eexittext - _sexittext);
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return;
+	}
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+	bhv_initialized = true;
+	bhv_configuration_bitmap = (unsigned long *)__bhv_data_start;
+	cid = *((uint32_t *)(__bhv_data_start + sizeof(unsigned long)));
+	port = *((uint32_t *)(__bhv_data_start + sizeof(unsigned long) +
+			      sizeof(uint32_t)));
+
+	rv = bhv_guestconn_init(cid, port);
+	if (rv) {
+		pr_err("BHV: Cannot configure the BHV guest connection subsystem");
+		return;
+	}
+
+	rv = bhv_guestlog_init();
+	if (rv) {
+		pr_err("BHV: Cannot configure the BHV logging subsystem");
+		return;
+	}
+
+	rv = bhv_cred_init();
+	if (rv) {
+		pr_err("BHV: Cannot configure the BHV creds subsystem");
+		return;
+	}
+
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+}
+
+static void do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	bhv_init_start_config_t *config =
+		(bhv_init_start_config_t *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = bhv_start_hyp(config);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (bhv_init_start_config_t *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = bhv_start_hyp(config);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#ifdef CONFIG_SECURITY_SELINUX
+		if ((sizeof(bhv_init_start_config_t) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#else /* CONFIG_SECURITY_SELINUX */
+		bhv_fail("guest policy available without target LSM");
+#endif /* CONFIG_SECURITY_SELINUX */
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+void bhv_start(void)
+{
+	int rc;
+	bhv_mem_region_node_t *n[2];
+
+	if (!is_bhv_initialized())
+		return;
+
+	if (bhv_integrity_is_enabled()) {
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.bhv_mem_region_remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.bhv_mem_region_remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.bhv_mem_region_remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.bhv_mem_region_remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_alternatives_delete_after_init();
+	}
+
+	bhv_guestconn_start();
+
+	do_start();
+}
diff --git kernel/bpf/bpf_struct_ops.c kernel/bpf/bpf_struct_ops.c
index 9abcc33f0..07196b9c0 100644
--- kernel/bpf/bpf_struct_ops.c
+++ kernel/bpf/bpf_struct_ops.c
@@ -453,6 +453,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 
 	set_memory_ro((long)st_map->image, 1);
 	set_memory_x((long)st_map->image, 1);
+	bhv_bpf_protect_x(st_map->image, PAGE_SIZE);
 	err = st_ops->reg(kdata);
 	if (likely(!err)) {
 		/* Pair with smp_load_acquire() during lookup_elem().
@@ -471,6 +472,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 	 */
 	set_memory_nx((long)st_map->image, 1);
 	set_memory_rw((long)st_map->image, 1);
+	bhv_bpf_unprotect(st_map->image);
 	bpf_map_put(map);
 
 reset_unlock:
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 4ce500eac..de8d0cf3f 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -906,6 +906,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 pages = hdr->pages;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(pages);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index 4fa75791b..fbaddcc83 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -39,6 +39,11 @@ void *bpf_jit_alloc_exec_page(void)
 	 * everytime new program is attached or detached.
 	 */
 	set_memory_x((long)image, 1);
+	/*
+	 * XXX: Make sure that we foresee all cases that would allow new
+	 * programs to attach/detach and handle the permissions appropriately.
+	 */
+	bhv_bpf_protect_x(image, PAGE_SIZE);
 	return image;
 }
 
@@ -203,6 +208,7 @@ static void __bpf_tramp_image_put_deferred(struct work_struct *work)
 
 	im = container_of(work, struct bpf_tramp_image, work);
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 	bpf_jit_uncharge_modmem(1);
 	percpu_ref_exit(&im->pcref);
@@ -321,6 +327,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key, u32 idx)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 out_uncharge:
 	bpf_jit_uncharge_modmem(1);
diff --git kernel/cred.c kernel/cred.c
index 933155c96..d3747cfb4 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -17,6 +17,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -113,6 +115,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 #endif
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -461,6 +464,8 @@ int commit_creds(struct cred *new)
 #endif
 	BUG_ON(atomic_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -720,6 +725,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	if (daemon)
 		old = get_task_cred(daemon);
 	else
diff --git kernel/fork.c kernel/fork.c
index 908ba3c93..677d347e2 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -98,6 +98,8 @@
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
 
+#include <bhv/creds.h>
+
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
 #include <asm/mmu_context.h>
@@ -2177,9 +2179,12 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2464,6 +2469,7 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
diff --git kernel/jump_label.c kernel/jump_label.c
index b156e152d..84c2d165c 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,6 +19,8 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
@@ -680,7 +682,7 @@ static int jump_label_add_module(struct module *mod)
 			__jump_label_update(key, iter, iter_stop, true);
 	}
 
-	return 0;
+	return bhv_jump_label_add_module(mod);
 }
 
 static void jump_label_del_module(struct module *mod)
@@ -691,6 +693,8 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+	bhv_jump_label_del_module(mod);
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
diff --git kernel/kmod.c kernel/kmod.c
index b717134eb..ad1559fb6 100644
--- kernel/kmod.c
+++ kernel/kmod.c
@@ -58,7 +58,11 @@ static DECLARE_WAIT_QUEUE_HEAD(kmod_wq);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+const char modprobe_path[KMOD_PATH_LEN] = CONFIG_MODPROBE_PATH;
+#else
 char modprobe_path[KMOD_PATH_LEN] = CONFIG_MODPROBE_PATH;
+#endif
 
 static void free_modprobe_argv(struct subprocess_info *info)
 {
@@ -84,7 +88,7 @@ static int call_modprobe(char *module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)modprobe_path;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
diff --git kernel/module.c kernel/module.c
index ef79f4dbd..3755954ff 100644
--- kernel/module.c
+++ kernel/module.c
@@ -60,6 +60,8 @@
 #include <uapi/linux/module.h>
 #include "module-internal.h"
 
+#include <bhv/module.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -268,6 +270,14 @@ static void module_assert_mutex_or_preempt(void)
 }
 
 #ifdef CONFIG_MODULE_SIG
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+static const bool sig_enforce = true;
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
@@ -275,6 +285,7 @@ void set_module_sig_enforced(void)
 {
 	sig_enforce = true;
 }
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 #else
 #define sig_enforce false
 #endif
@@ -2198,6 +2209,8 @@ static void free_module(struct module *mod)
 	/* Clean up CFI for the module. */
 	cfi_cleanup(mod);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
@@ -3626,6 +3639,7 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
 	module_memfree(mod->core_layout.base);
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -3769,6 +3783,7 @@ static noinline int do_init_module(struct module *mod)
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	mod->init_layout.base = NULL;
 	mod->init_layout.size = 0;
 	mod->init_layout.ro_size = 0;
@@ -4082,6 +4097,8 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
diff --git kernel/sysctl.c kernel/sysctl.c
index 23c08bf3d..ade7a7860 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -2108,9 +2108,15 @@ static struct ctl_table kern_table[] = {
 #ifdef CONFIG_MODULES
 	{
 		.procname	= "modprobe",
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+		.data		= (char *)&modprobe_path,
+		.maxlen		= KMOD_PATH_LEN,
+		.mode		= 0444,
+#else
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring,
 	},
 	{
diff --git security/Kconfig security/Kconfig
index 5d412b3dd..6ef0ed89a 100644
--- security/Kconfig
+++ security/Kconfig
@@ -227,6 +227,7 @@ source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
 source "security/apparmor/Kconfig"
 source "security/loadpin/Kconfig"
+source "security/bhv/Kconfig"
 source "security/yama/Kconfig"
 source "security/safesetid/Kconfig"
 source "security/lockdown/Kconfig"
@@ -270,11 +271,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf" if DEFAULT_SECURITY_SMACK
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list will be ignored. This can be
diff --git security/Makefile security/Makefile
index 18121f8f8..99425138e 100644
--- security/Makefile
+++ security/Makefile
@@ -19,6 +19,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_SECURITY_BHV)		+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 000000000..309c33608
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config SECURITY_BHV
+	bool "Enable the BHV LSM."
+	depends on SECURITY
+	depends on BHV_VAS
+	default y
+	help
+	  Enables the BHV LSM. The BHV LSM provides hooking capabilities for
+	  additional protections such as the process and the driver ACL.
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 000000000..f921bdf38
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,2 @@
+# SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_SECURITY_BHV) += bhv.o
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 000000000..ef742ea11
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,341 @@
+#include <linux/lsm_hooks.h>
+#include <linux/binfmts.h>
+#include <uapi/linux/magic.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/fdtable.h>
+
+#include <bhv/acl.h>
+#include <bhv/guestlog.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+extern const struct file_operations ext4_dir_operations;
+extern const struct file_operations ext4_file_operations;
+#endif
+
+static int bhv_read_file(struct file *file, enum kernel_read_file_id id,
+			 bool whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			if (filename != NULL)
+				bhv_guestlog_log_driver_load(filename);
+			else
+				bhv_guestlog_log_driver_load(
+					"[ UNKNOWN DRIVER ]");
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+
+static int bhv_load_data(enum kernel_load_data_id id, bool contents)
+{
+	const char *origin = kernel_read_file_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			bhv_guestlog_log_driver_load("[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+
+static int bhv_task_alloc(struct task_struct *target,
+			  long unsigned int clone_flags)
+{
+	pr_debug("[bhv] TASK CREATE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_fork(target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+static void bhv_task_free(struct task_struct *target)
+{
+	pr_debug("[bhv] TASK FREE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exit(target->pid, target->parent->pid,
+					      target->comm);
+	}
+}
+
+static int bhv_bprm_check_security(struct linux_binprm *bprm)
+{
+	int rv = 0;
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+	if (bhv_guestlog_log_process_events() ||
+	    bhv_acl_is_proc_acl_enabled()) {
+		const char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&bprm->file->f_path, filename_buf, PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+
+		if (bhv_acl_is_proc_acl_enabled()) {
+			// check executed filename (name on cli)
+			if (bhv_block_process(bprm->filename)) {
+				free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+
+			// check underlying file (e.g., busybox or interpreter)
+			if (bhv_block_process(filename)) {
+				free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (bhv_guestlog_log_process_events()) {
+			bhv_guestlog_log_process_exec(current->pid,
+						      current->parent->pid,
+						      bprm->filename);
+		}
+
+		free_page((unsigned long)filename_buf);
+	}
+
+	return rv;
+}
+
+static const inline char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+
+static bool bhv_check_fileops(struct file *file)
+{
+	bool block = false;
+// currently only EXT4 is supported
+#ifdef CONFIG_EXT4_FS
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+
+	// ext4
+	if (file->f_inode->i_sb->s_magic == EXT4_SUPER_MAGIC) {
+		/* On a EXT4 file system check whether the file operations struct is
+		 * legitimate for directories and files */
+		if (d_can_lookup(file->f_path.dentry)) {
+			// ext4 directory
+			if (file->f_op != &ext4_dir_operations) {
+				pathname_buf =
+					kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ,
+						GFP_KERNEL);
+				pathname = get_pathname(
+					file, pathname_buf,
+					BHV_VAS_FILEOPS_PATH_MAX_SZ);
+				// report violation
+				if (bhv_block_fileops(
+					    pathname, 1,
+					    BHV_VAS_FILEOPS_PROTECTION_EXT4)) {
+					// block file operation
+					block = true;
+				}
+
+				pathname = NULL;
+				kfree(pathname_buf);
+			}
+		} else {
+			if (file->f_op != &ext4_file_operations) {
+				pathname_buf =
+					kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ,
+						GFP_KERNEL);
+
+				pathname = get_pathname(
+					file, pathname_buf,
+					BHV_VAS_FILEOPS_PATH_MAX_SZ);
+
+				// report (and block) violation
+				if (bhv_block_fileops(
+					    pathname, 0,
+					    BHV_VAS_FILEOPS_PROTECTION_EXT4)) {
+					// block file operation
+					block = true;
+				}
+
+				pathname = NULL;
+				kfree(pathname_buf);
+			}
+		}
+	}
+#endif
+
+	return block;
+}
+
+static int bhv_check_files_dirty_pipe(const void *address_space,
+				      struct file *file, unsigned int number)
+{
+	static char filename_buf[PAGE_SIZE];
+	const char *filename = NULL;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (file->f_mapping == address_space &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Get Path of the file we are trying to write
+		filename = d_path(&file->f_path, filename_buf,
+				  sizeof(filename_buf));
+		if (IS_ERR(filename))
+			filename = "UNKNOWN";
+
+		// Ask the HOST whether we should block this attempt.
+		return bhv_block_read_only_file_write(filename);
+	}
+
+	return 0;
+}
+
+static int bhv_file_open(struct file *file)
+{
+	if (bhv_fileops_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int bhv_file_permission(struct file *file, int mask)
+{
+	struct pipe_inode_info *info;
+	struct pipe_buffer *buf;
+
+	if (bhv_fileops_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return 0;
+
+	/*
+	 * Dirty pipe detection. Whenever we write to a file and this file is
+	 * a pipe, we are going to check whether this pipe points to a read-only
+	 * file. This check happens in `bhv_check_files_dirty_pipe` above.
+	 */
+	if ((mask & MAY_WRITE) == MAY_WRITE && file->f_inode &&
+	    file->f_inode->i_sb->s_magic == PIPEFS_MAGIC) {
+		// This seems to be a pipe
+		info = file->private_data;
+		if (!info)
+			return 0;
+
+		// Check current buffer in the pipe for dirty pipe
+		buf = &info->bufs[(info->head - 1) & (info->ring_size - 1)];
+		// Iterate over all open files and see whether the pipe points to the same file.
+		if (buf && buf->page && current->files) {
+			if (iterate_fd(current->files, 0,
+				       bhv_check_files_dirty_pipe,
+				       buf->page->mapping)) {
+				return 1;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static struct security_hook_list bhv_hooks[] __lsm_ro_after_init = {
+	LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+	LSM_HOOK_INIT(kernel_load_data, bhv_load_data), // init_module
+	LSM_HOOK_INIT(task_alloc, bhv_task_alloc), // fork
+	LSM_HOOK_INIT(task_free, bhv_task_free), // exit
+	LSM_HOOK_INIT(bprm_check_security, bhv_bprm_check_security), // execve
+	LSM_HOOK_INIT(file_permission, bhv_file_permission), // file read/write
+	LSM_HOOK_INIT(file_open, bhv_file_open), // file open
+};
+
+static int __init bhv_init(void)
+{
+	pr_info("[bhv] LSM active");
+	security_add_hooks(bhv_hooks, ARRAY_SIZE(bhv_hooks), "bhv");
+	return 0;
+}
+
+DEFINE_LSM(bhv) = {
+	.name = "bhv",
+	.init = bhv_init,
+};
diff --git security/selinux/hooks.c security/selinux/hooks.c
index 9ce029b2f..1fe9c9a27 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -125,7 +125,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index f2f6203e0..3bef6c2d9 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -610,9 +614,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
 	struct selinux_load_state load_state;
@@ -667,6 +671,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2249,6 +2279,40 @@ static int __init init_sel_fs(void)
 
 __initcall(init_sel_fs);
 
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinuxfs_mount->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&fsi->state->policy_mutex);
+
+	rv = security_load_policy(fsi->state, data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(fsi->state, &load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(fsi->state, &load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&fsi->state->policy_mutex);
+	return rv;
+}
+#endif
+
 #ifdef CONFIG_SECURITY_SELINUX_DISABLE
 void exit_sel_fs(void)
 {
