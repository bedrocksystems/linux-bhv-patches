diff --git arch/arm64/Kbuild arch/arm64/Kbuild
index ea7ab4ca8..81f6d39e0 100644
--- arch/arm64/Kbuild
+++ arch/arm64/Kbuild
@@ -3,4 +3,5 @@ obj-y			+= kernel/ mm/ net/
 obj-$(CONFIG_KVM)	+= kvm/
 obj-$(CONFIG_XEN)	+= xen/
 obj-$(subst m,y,$(CONFIG_HYPERV))	+= hyperv/
+obj-$(CONFIG_BHV_VAS)	+= bhv/
 obj-$(CONFIG_CRYPTO)	+= crypto/
diff --git arch/arm64/Kconfig arch/arm64/Kconfig
index 24cce3b9f..f5603abd1 100644
--- arch/arm64/Kconfig
+++ arch/arm64/Kconfig
@@ -1180,6 +1180,16 @@ config XEN
 	help
 	  Say Y if you want to run Linux in a Virtual Machine on Xen on ARM64.
 
+config BHV_VAS
+	def_bool y
+	bool "BHV guest support on ARM64"
+	depends on ARM64 && OF
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on BHV on ARM64
+	  and benefit from Virtualization-assisted Security.
+
+source "kernel/bhv/Kconfig"
+
 config FORCE_MAX_ZONEORDER
 	int
 	default "14" if ARM64_64K_PAGES
diff --git arch/arm64/bhv/Makefile arch/arm64/bhv/Makefile
new file mode 100644
index 000000000..e66e83fdb
--- /dev/null
+++ arch/arm64/bhv/Makefile
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
diff --git arch/arm64/bhv/init.c arch/arm64/bhv/init.c
new file mode 100644
index 000000000..70a504119
--- /dev/null
+++ arch/arm64/bhv/init.c
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Author: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <asm/sections.h>
+
+#include <bhv/interface/integrity.h>
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+}
diff --git arch/arm64/bhv/integrity.c arch/arm64/bhv/integrity.c
new file mode 100644
index 000000000..3613b1ce1
--- /dev/null
+++ arch/arm64/bhv/integrity.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm-generic/sections.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+extern char vdso_start[], vdso_end[];
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif
+
+int __init_km bhv_start_integrity_arch(void)
+{
+#define NUM_BHV_MEM_REGION_NODES 3
+	int rv = 0;
+	int rc;
+	bhv_mem_region_node_t *n[NUM_BHV_MEM_REGION_NODES];
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   NUM_BHV_MEM_REGION_NODES, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	BUG_ON(KLN_SYM(vdso_start) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_start) >= KLN_SYM(__end_rodata));
+	BUG_ON(KLN_SYM(vdso_end) < KLN_SYM(__start_rodata) ||
+	       KLN_SYM(vdso_end) >= KLN_SYM(__end_rodata));
+
+	/* Add ro_data section
+	 * NOTE: ro_after_init is contained in this section as well
+	 */
+	bhv_mem_region_create_ctor(
+		&n[0]->region, NULL,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, __start_rodata)),
+		KLN_SYM(vdso_start) - KLN_SYM(__start_rodata),
+		BHV_MEM_TYPE_DATA_READ_ONLY, BHV_MEM_FLAGS_NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[1]->region, &n[0]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_end)),
+		KLN_SYM(__end_rodata) - KLN_SYM(vdso_end),
+		BHV_MEM_TYPE_DATA_READ_ONLY, BHV_MEM_FLAGS_NONE,
+		"KERNEL READ-ONLY DATA SECTION");
+
+	bhv_mem_region_create_ctor(
+		&n[2]->region, &n[1]->region,
+		bhv_virt_to_phys(KLN_SYMBOL(void *, vdso_start)),
+		KLN_SYM(vdso_end) - KLN_SYM(vdso_start), BHV_MEM_TYPE_VDSO,
+		BHV_MEM_FLAGS_NONE, "KERNEL VDSO SECTION");
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+		rv = rc;
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, NUM_BHV_MEM_REGION_NODES,
+			     (void **)&n);
+
+	return rv;
+}
diff --git arch/arm64/bhv/patch_alternative.c arch/arm64/bhv/patch_alternative.c
new file mode 100644
index 000000000..2af48e512
--- /dev/null
+++ arch/arm64/bhv/patch_alternative.c
@@ -0,0 +1,375 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+#include <bhv/vault.h>
+#include <bhv/bhv.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <linux/mm.h>
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, bool is_module)
+{
+	struct bhv_alternatives_mod_arch arch = { .is_module = is_module };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static int __bhv_text bhv_aarch64_get_imm_shift_mask(
+	enum aarch64_insn_imm_type type, u32 *maskp, int *shiftp)
+{
+	u32 mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_26:
+		mask = BIT(26) - 1;
+		shift = 0;
+		break;
+	case AARCH64_INSN_IMM_19:
+		mask = BIT(19) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_16:
+		mask = BIT(16) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_14:
+		mask = BIT(14) - 1;
+		shift = 5;
+		break;
+	case AARCH64_INSN_IMM_12:
+		mask = BIT(12) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_9:
+		mask = BIT(9) - 1;
+		shift = 12;
+		break;
+	case AARCH64_INSN_IMM_7:
+		mask = BIT(7) - 1;
+		shift = 15;
+		break;
+	case AARCH64_INSN_IMM_6:
+	case AARCH64_INSN_IMM_S:
+		mask = BIT(6) - 1;
+		shift = 10;
+		break;
+	case AARCH64_INSN_IMM_R:
+		mask = BIT(6) - 1;
+		shift = 16;
+		break;
+	case AARCH64_INSN_IMM_N:
+		mask = 1;
+		shift = 22;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	*maskp = mask;
+	*shiftp = shift;
+
+	return 0;
+}
+
+#define ADR_IMM_HILOSPLIT 2
+#define ADR_IMM_SIZE SZ_2M
+#define ADR_IMM_LOMASK ((1 << ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_HIMASK ((ADR_IMM_SIZE >> ADR_IMM_HILOSPLIT) - 1)
+#define ADR_IMM_LOSHIFT 29
+#define ADR_IMM_HISHIFT 5
+
+static u64 __bhv_text
+bhv_aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (insn >> ADR_IMM_LOSHIFT) & ADR_IMM_LOMASK;
+		immhi = (insn >> ADR_IMM_HISHIFT) & ADR_IMM_HIMASK;
+		insn = (immhi << ADR_IMM_HILOSPLIT) | immlo;
+		mask = ADR_IMM_SIZE - 1;
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return 0;
+		}
+	}
+
+	return (insn >> shift) & mask;
+}
+
+static s32 __bhv_text bhv_aarch64_get_branch_offset(u32 insn)
+{
+	s32 imm;
+
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_26,
+							insn);
+		return (imm << 6) >> 4;
+	}
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_19,
+							insn);
+		return (imm << 13) >> 11;
+	}
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn)) {
+		imm = bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_14,
+							insn);
+		return (imm << 18) >> 16;
+	}
+
+	return 0;
+}
+
+static bool __bhv_text bhv_aarch64_insn_is_branch_imm(u32 insn)
+{
+	return (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn) ||
+		aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn) ||
+		aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+		aarch64_insn_is_bcond(insn));
+}
+
+static u32 __bhv_text bhv_aarch64_insn_encode_immediate(
+	enum aarch64_insn_imm_type type, u32 insn, u64 imm)
+{
+	u32 immlo, immhi, mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	switch (type) {
+	case AARCH64_INSN_IMM_ADR:
+		shift = 0;
+		immlo = (imm & ADR_IMM_LOMASK) << ADR_IMM_LOSHIFT;
+		imm >>= ADR_IMM_HILOSPLIT;
+		immhi = (imm & ADR_IMM_HIMASK) << ADR_IMM_HISHIFT;
+		imm = immlo | immhi;
+		mask = ((ADR_IMM_LOMASK << ADR_IMM_LOSHIFT) |
+			(ADR_IMM_HIMASK << ADR_IMM_HISHIFT));
+		break;
+	default:
+		if (bhv_aarch64_get_imm_shift_mask(type, &mask, &shift) < 0) {
+			return AARCH64_BREAK_FAULT;
+		}
+	}
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+static u32 __bhv_text bhv_aarch64_set_branch_offset(u32 insn, s32 offset)
+{
+	if (aarch64_insn_is_b(insn) || aarch64_insn_is_bl(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_26, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_cbz(insn) || aarch64_insn_is_cbnz(insn) ||
+	    aarch64_insn_is_bcond(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_19, insn,
+						     offset >> 2);
+
+	if (aarch64_insn_is_tbz(insn) || aarch64_insn_is_tbnz(insn))
+		return aarch64_insn_encode_immediate(AARCH64_INSN_IMM_14, insn,
+						     offset >> 2);
+
+	return 0;
+}
+
+static s32 __bhv_text bhv_aarch64_insn_adrp_get_offset(u32 insn)
+{
+	return bhv_aarch64_insn_decode_immediate(AARCH64_INSN_IMM_ADR, insn)
+	       << 12;
+}
+
+static u32 __bhv_text bhv_aarch64_insn_adrp_set_offset(u32 insn, s32 offset)
+{
+	return bhv_aarch64_insn_encode_immediate(AARCH64_INSN_IMM_ADR, insn,
+						 offset >> 12);
+}
+
+#define __ALT_PTR(a, f) ((void *)&(a)->f + (a)->f)
+#define ALT_ORIG_PTR(a) __ALT_PTR(a, orig_offset)
+#define ALT_REPL_PTR(a) __ALT_PTR(a, alt_offset)
+
+static bool __bhv_text branch_insn_requires_update(struct alt_instr *alt,
+						   unsigned long pc)
+{
+	unsigned long replptr = (unsigned long)ALT_REPL_PTR(alt);
+	return !(pc >= replptr && pc <= (replptr + alt->alt_len));
+}
+
+#define align_down(x, a) ((unsigned long)(x) & ~(((unsigned long)(a)) - 1))
+
+static u32 __bhv_text bhv_get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
+				       __le32 *altinsnptr)
+{
+	u32 insn;
+
+	insn = le32_to_cpu(*altinsnptr);
+
+	if (bhv_aarch64_insn_is_branch_imm(insn)) {
+		s32 offset = bhv_aarch64_get_branch_offset(insn);
+		unsigned long target;
+
+		target = (unsigned long)altinsnptr + offset;
+
+		/*
+		 * If we're branching inside the alternate sequence,
+		 * do not rewrite the instruction, as it is already
+		 * correct. Otherwise, generate the new instruction.
+		 */
+		if (branch_insn_requires_update(alt, target)) {
+			offset = target - (unsigned long)insnptr;
+			insn = bhv_aarch64_set_branch_offset(insn, offset);
+		}
+	} else if (aarch64_insn_is_adrp(insn)) {
+		s32 orig_offset, new_offset;
+		unsigned long target;
+
+		/*
+		 * If we're replacing an adrp instruction, which uses PC-relative
+		 * immediate addressing, adjust the offset to reflect the new
+		 * PC. adrp operates on 4K aligned addresses.
+		 */
+		orig_offset = bhv_aarch64_insn_adrp_get_offset(insn);
+		target = align_down(altinsnptr, SZ_4K) + orig_offset;
+		new_offset = target - align_down(insnptr, SZ_4K);
+		insn = bhv_aarch64_insn_adrp_set_offset(insn, new_offset);
+	}
+
+	return insn;
+}
+
+static void __bhv_text bhv_alternatives_patch(struct alt_instr *alt,
+					      __le32 *origptr, __le32 *updptr,
+					      int nr_inst,
+					      bhv_patch_arg_t *bhv_arg)
+{
+	__le32 *replptr = 0;
+	int i;
+
+	replptr = ALT_REPL_PTR(alt);
+	for (i = 0; i < nr_inst; i++) {
+		u32 insn;
+
+		insn = bhv_get_alt_insn(alt, origptr + i, replptr + i);
+		insn = cpu_to_le32(insn);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)&updptr[i]);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+}
+
+/*
+ * We provide our own, private D-cache cleaning function so that we don't
+ * accidentally call into the cache.S code, which is patched by us at
+ * runtime.
+ */
+static void __bhv_text clean_dcache_range_nopatch(u64 start, u64 end)
+{
+	u64 cur, d_size, ctr_el0;
+
+	ctr_el0 = read_sanitised_ftr_reg(SYS_CTR_EL0);
+	d_size = 4 << cpuid_feature_extract_unsigned_field(ctr_el0,
+							   CTR_DMINLINE_SHIFT);
+	cur = start & ~(d_size - 1);
+	do {
+		/*
+		 * We must clean+invalidate to the PoC in order to avoid
+		 * Cortex-A53 errata 826319, 827319, 824069 and 819472
+		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)
+		 */
+		asm volatile("dc civac, %0" : : "r"(cur) : "memory");
+	} while (cur += d_size, cur < end);
+}
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg)
+{
+	unsigned long *feature_mask = (unsigned long *)arch;
+
+	struct alt_instr *alt;
+	__le32 *origptr, *updptr;
+	alternative_cb_t alt_cb;
+
+	for (alt = mod->begin; alt < mod->end; alt++) {
+		int nr_inst;
+
+		if (!test_bit(alt->cpufeature, feature_mask))
+			continue;
+
+		/* Use ARM64_CB_PATCH as an unconditional patch */
+		if (alt->cpufeature < ARM64_CB_PATCH &&
+		    !cpus_have_cap(alt->cpufeature))
+			continue;
+
+		if (alt->cpufeature == ARM64_CB_PATCH) {
+			if (alt->alt_len != 0) {
+				return -EACCES;
+			}
+		} else {
+			if (alt->alt_len != alt->orig_len) {
+				return -EACCES;
+			}
+		}
+
+		origptr = ALT_ORIG_PTR(alt);
+		updptr = mod->arch.is_module ? origptr : lm_alias(origptr);
+		nr_inst = alt->orig_len / AARCH64_INSN_SIZE;
+
+		if (alt->cpufeature < ARM64_CB_PATCH) {
+			bhv_alternatives_patch(alt, origptr, updptr, nr_inst,
+					       bhv_arg);
+		} else {
+			alt_cb = ALT_REPL_PTR(alt);
+			alt_cb(alt, origptr, updptr, nr_inst, bhv_arg);
+		}
+
+		if (!mod->arch.is_module) {
+			clean_dcache_range_nopatch((u64)origptr,
+						   (u64)(origptr + nr_inst));
+		}
+	}
+
+	return 0;
+}
+
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+	static struct bhv_alternatives_mod kernel = {
+		.begin = (struct alt_instr *)__alt_instructions,
+		.end = (struct alt_instr *)__alt_instructions_end,
+		.delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+		.allocated = false,
+		.arch = { .is_module = false },
+		.next = { .next = NULL, .prev = NULL }
+	};
+
+	*nr_mods = 1;
+	return &kernel;
+}
diff --git arch/arm64/bhv/patch_jump_label.c arch/arm64/bhv/patch_jump_label.c
new file mode 100644
index 000000000..610215168
--- /dev/null
+++ arch/arm64/bhv/patch_jump_label.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/string.h>
+#include <asm/insn.h>
+#include <asm/debug-monitors.h>
+#include <asm/bhv/patch.h>
+
+static __always_inline bool bhv_branch_imm_common(unsigned long pc,
+						  unsigned long addr,
+						  long range, long *offset)
+{
+	if ((pc & 0x3) || (addr & 0x3)) {
+		return false;
+	}
+
+	*offset = ((long)addr - (long)pc);
+
+	if (*offset < -range || *offset >= range) {
+		return false;
+	}
+
+	return true;
+}
+
+u32 __always_inline bhv_aarch64_insn_encode_immediate(u32 insn, u64 imm)
+{
+	u32 mask;
+	int shift;
+
+	if (insn == AARCH64_BREAK_FAULT)
+		return AARCH64_BREAK_FAULT;
+
+	mask = BIT(26) - 1;
+	shift = 0;
+
+	/* Update the immediate field. */
+	insn &= ~(mask << shift);
+	insn |= (imm & mask) << shift;
+
+	return insn;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t size)
+{
+	u32 jmp_insn, nop_insn;
+	long offset;
+	void *addr = (void *)jump_entry_code(entry);
+
+	if (!bhv_branch_imm_common((long)addr, jump_entry_target(entry),
+				   SZ_128M, &offset))
+		return false;
+	jmp_insn = aarch64_insn_get_b_value();
+	jmp_insn = bhv_aarch64_insn_encode_immediate(jmp_insn, offset >> 2);
+
+	nop_insn = aarch64_insn_get_hint_value() | AARCH64_INSN_HINT_NOP;
+
+	if (type == JUMP_LABEL_JMP) {
+		if (memcmp(addr, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+	} else {
+		if (memcmp(addr, &jmp_insn, AARCH64_INSN_SIZE))
+			return false;
+		if (memcmp(expected_opcode, &nop_insn, AARCH64_INSN_SIZE))
+			return false;
+	}
+
+	return true;
+}
diff --git arch/arm64/bhv/start.c arch/arm64/bhv/start.c
new file mode 100644
index 000000000..1b140ffe2
--- /dev/null
+++ arch/arm64/bhv/start.c
@@ -0,0 +1,18 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
diff --git arch/arm64/include/asm/alternative.h arch/arm64/include/asm/alternative.h
index a38b92e11..8bdbcf5d1 100644
--- arch/arm64/include/asm/alternative.h
+++ arch/arm64/include/asm/alternative.h
@@ -10,6 +10,8 @@
 #include <linux/types.h>
 #include <linux/stddef.h>
 
+#include <bhv/interface/patch.h>
+
 struct alt_instr {
 	s32 orig_offset;	/* offset to original instruction */
 	s32 alt_offset;		/* offset to replacement instruction */
@@ -18,8 +20,14 @@ struct alt_instr {
 	u8  alt_len;		/* size of new instruction(s), <= orig_len */
 };
 
+#ifdef CONFIG_BHV_VAS
+typedef void (*alternative_cb_t)(struct alt_instr *alt, __le32 *origptr,
+				 __le32 *updptr, int nr_inst,
+				 bhv_patch_arg_t *bhv_arg);
+#else /* !CONFIG_BHV_VAS */
 typedef void (*alternative_cb_t)(struct alt_instr *alt,
 				 __le32 *origptr, __le32 *updptr, int nr_inst);
+#endif /* CONFIG_BHV_VAS */
 
 void __init apply_boot_alternatives(void);
 void __init apply_alternatives_all(void);
diff --git arch/arm64/include/asm/bhv/hypercall.h arch/arm64/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..dfd047fd7
--- /dev/null
+++ arch/arm64/include/asm/bhv/hypercall.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+#define BHV_IMM 0x539
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long x0 __asm__("x0") = target;
+	register unsigned long x1 __asm__("x1") = backend;
+	register unsigned long x2 __asm__("x2") = op;
+	register unsigned long x3 __asm__("x3") = ver;
+	register unsigned long x4 __asm__("x4") = arg;
+	__asm__ __volatile__("hvc " __stringify(BHV_IMM) "\n\t"
+			     : "+r"(x0)
+			     : "r"(x1), "r"(x2), "r"(x3), "r"(x4)
+			     :);
+	return x0;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/arm64/include/asm/bhv/patch.h arch/arm64/include/asm/bhv/patch.h
new file mode 100644
index 000000000..16babbf37
--- /dev/null
+++ arch/arm64/include/asm/bhv/patch.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	bool is_module;
+};
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+#ifndef VASKM // inside kernel tree
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg);
+void __bhv_text bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						 struct alt_instr *end,
+						 bool is_module);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+#endif // VASKM
+
+#else /* !CONFIG_BHV_VAS */
+#ifndef VASKM // inside kernel tree
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *,
+						    struct alt_instr *, bool)
+{
+}
+#endif // VASKM
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
diff --git arch/arm64/include/asm/kvm_mmu.h arch/arm64/include/asm/kvm_mmu.h
index 02d378887..4ae56cc26 100644
--- arch/arm64/include/asm/kvm_mmu.h
+++ arch/arm64/include/asm/kvm_mmu.h
@@ -116,8 +116,17 @@ alternative_cb_end
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/patch.h>
+#endif
+
+#ifdef CONFIG_BHV_VAS
+void kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr, __le32 *updptr,
+			int nr_inst, bhv_patch_arg_t *bhv_arg);
+#else /* CONFIG_BHV_VAS */
 void kvm_update_va_mask(struct alt_instr *alt,
 			__le32 *origptr, __le32 *updptr, int nr_inst);
+#endif /* CONFIG_BHV_VAS */
 void kvm_compute_layout(void);
 void kvm_apply_hyp_relocations(void);
 
diff --git arch/arm64/kernel/alternative.c arch/arm64/kernel/alternative.c
index 7bbf5104b..bfdb6ceae 100644
--- arch/arm64/kernel/alternative.c
+++ arch/arm64/kernel/alternative.c
@@ -17,6 +17,9 @@
 #include <asm/sections.h>
 #include <linux/stop_machine.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
 #define __ALT_PTR(a, f)		((void *)&(a)->f + (a)->f)
 #define ALT_ORIG_PTR(a)		__ALT_PTR(a, orig_offset)
 #define ALT_REPL_PTR(a)		__ALT_PTR(a, alt_offset)
@@ -95,8 +98,13 @@ static __always_inline u32 get_alt_insn(struct alt_instr *alt, __le32 *insnptr,
 	return insn;
 }
 
+#ifdef CONFIG_BHV_VAS
+static noinstr void patch_alternative(struct alt_instr *alt, __le32 *origptr,
+			      __le32 *updptr, int nr_inst, bhv_patch_arg_t *arg)
+#else
 static noinstr void patch_alternative(struct alt_instr *alt,
 			      __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif
 {
 	__le32 *replptr;
 	int i;
@@ -140,6 +148,14 @@ static void __nocfi __apply_alternatives(struct alt_region *region, bool is_modu
 	__le32 *origptr, *updptr;
 	alternative_cb_t alt_cb;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(region->begin, region->end,
+				       feature_mask);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (alt = region->begin; alt < region->end; alt++) {
 		int nr_inst;
 
@@ -167,7 +183,11 @@ static void __nocfi __apply_alternatives(struct alt_region *region, bool is_modu
 		else
 			alt_cb  = ALT_REPL_PTR(alt);
 
+#ifdef CONFIG_BHV_VAS
+		alt_cb(alt, origptr, updptr, nr_inst, NULL);
+#else
 		alt_cb(alt, origptr, updptr, nr_inst);
+#endif
 
 		if (!is_module) {
 			clean_dcache_range_nopatch((u64)origptr,
@@ -254,10 +274,15 @@ void apply_alternatives_module(void *start, size_t length)
 		.begin	= start,
 		.end	= start + length,
 	};
+
 	DECLARE_BITMAP(all_capabilities, ARM64_NPATCHABLE);
 
 	bitmap_fill(all_capabilities, ARM64_NPATCHABLE);
 
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_add_module_arch(region.begin, region.end,
+						 true);
+	}
 	__apply_alternatives(&region, true, &all_capabilities[0]);
 }
 #endif
diff --git arch/arm64/kernel/jump_label.c arch/arm64/kernel/jump_label.c
index fc98037e1..a6fceaa70 100644
--- arch/arm64/kernel/jump_label.c
+++ arch/arm64/kernel/jump_label.c
@@ -10,10 +10,14 @@
 #include <asm/insn.h>
 #include <asm/patching.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)jump_entry_code(entry);
+	void __maybe_unused *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
@@ -24,6 +28,13 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_patch_jump_label(entry, &insn, AARCH64_INSN_SIZE);
+		return;
+	}
+#endif
+
 	aarch64_insn_patch_text_nosync(addr, insn);
 }
 
diff --git arch/arm64/kernel/proton-pack.c arch/arm64/kernel/proton-pack.c
index 40be3a7c2..e7956f96f 100644
--- arch/arm64/kernel/proton-pack.c
+++ arch/arm64/kernel/proton-pack.c
@@ -32,6 +32,13 @@
 #include <asm/vectors.h>
 #include <asm/virt.h>
 
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/patch.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
 /*
  * We try to ensure that the mitigation state can never change as the result of
  * onlining a late CPU.
@@ -577,9 +584,16 @@ static enum mitigation_state spectre_v4_enable_hw_mitigation(void)
  * Patch a branch over the Spectre-v4 mitigation code with a NOP so that
  * we fallthrough and check whether firmware needs to be called on this CPU.
  */
+#ifdef CONFIG_BHV_VAS
+void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
+						  __le32 *origptr,
+						  __le32 *updptr, int nr_inst,
+						  bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 						  __le32 *origptr,
 						  __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1); /* Branch -> NOP */
 
@@ -589,17 +603,41 @@ void __init spectre_v4_patch_fw_mitigation_enable(struct alt_instr *alt,
 	if (cpus_have_final_cap(ARM64_SSBS))
 		return;
 
-	if (spectre_v4_mitigations_dynamic())
+	if (spectre_v4_mitigations_dynamic()) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+		} else {
+			*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /*
  * Patch a NOP in the Spectre-v4 mitigation code with an SMC/HVC instruction
  * to call into firmware to adjust the mitigation state.
  */
+#ifdef CONFIG_BHV_VAS
+void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst,
+						   bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
-					       __le32 *origptr,
-					       __le32 *updptr, int nr_inst)
+						   __le32 *origptr,
+						   __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u32 insn;
 
@@ -616,7 +654,22 @@ void __init smccc_patch_fw_mitigation_conduit(struct alt_instr *alt,
 		return;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		insn = cpu_to_le32(insn);
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	} else {
+		*updptr = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static enum mitigation_state spectre_v4_enable_fw_mitigation(void)
@@ -1062,30 +1115,88 @@ void spectre_bhb_enable_mitigation(const struct arm64_cpu_capabilities *entry)
 }
 
 /* Patched to NOP when enabled */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
+						      __le32 *origptr,
+						      __le32 *updptr,
+						      int nr_inst,
+						      bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_loop_mitigation_enable(struct alt_instr *alt,
 						     __le32 *origptr,
 						      __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_LOOP, &system_bhb_mitigations))
+	if (test_bit(BHB_LOOP, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to NOP when enabled */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
+						     __le32 *origptr,
+						     __le32 *updptr,
+						     int nr_inst,
+						     bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_fw_mitigation_enabled(struct alt_instr *alt,
 						   __le32 *origptr,
 						   __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 1);
 
-	if (test_bit(BHB_FW, &system_bhb_mitigations))
+	if (test_bit(BHB_FW, &system_bhb_mitigations)) {
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+			bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+				bhv_virt_to_phys((void *)updptr);
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+			       sizeof(insn));
+			bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+			bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+					  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE,
+					  bhv_arg);
+			updptr++;
+		} else {
+			*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		}
+#else /* !CONFIG_BHV_VAS */
 		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
+	}
 }
 
 /* Patched to correct the immediate */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt, __le32 *origptr,
+					 __le32 *updptr, int nr_inst,
+					 bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u8 rd;
 	u32 insn;
@@ -1101,12 +1212,33 @@ void noinstr spectre_bhb_patch_loop_iter(struct alt_instr *alt,
 	insn = aarch64_insn_gen_movewide(rd, loop_count, 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to mov WA3 when supported */
+#ifdef CONFIG_BHV_VAS
+void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt, __le32 *origptr,
+				   __le32 *updptr, int nr_inst,
+				   bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u8 rd;
 	u32 insn;
@@ -1127,20 +1259,68 @@ void noinstr spectre_bhb_patch_wa3(struct alt_instr *alt,
 	if (WARN_ON_ONCE(insn == AARCH64_BREAK_FAULT))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 /* Patched to NOP when not supported */
+#ifdef CONFIG_BHV_VAS
+void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt, __le32 *origptr,
+				       __le32 *updptr, int nr_inst,
+				       bhv_patch_arg_t *bhv_arg)
+#else /* !CONFIG_BHV_VAS */
 void __init spectre_bhb_patch_clearbhb(struct alt_instr *alt,
 				   __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	BUG_ON(nr_inst != 2);
 
 	if (test_bit(BHB_INSN, &system_bhb_mitigations))
 		return;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		u32 insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+
+		insn = cpu_to_le32(aarch64_insn_gen_nop());
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)updptr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn,
+		       sizeof(insn));
+		bhv_arg->bhv_patch_patch_arg.size = sizeof(insn);
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+		updptr++;
+	} else {
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+		*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+	}
+#else /* !CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
 	*updptr++ = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 }
 
 #ifdef CONFIG_BPF_SYSCALL
diff --git arch/arm64/kernel/setup.c arch/arm64/kernel/setup.c
index be5f85b0a..315221a68 100644
--- arch/arm64/kernel/setup.c
+++ arch/arm64/kernel/setup.c
@@ -52,6 +52,8 @@
 #include <asm/xen/hypervisor.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/start.h>
+
 static int num_standard_resources;
 static struct resource *standard_resources;
 
@@ -330,6 +332,7 @@ void __init __no_sanitize_address setup_arch(char **cmdline_p)
 	cpu_uninstall_idmap();
 
 	xen_early_init();
+	bhv_init_platform();
 	efi_init();
 
 	if (!efi_enabled(EFI_BOOT) && ((u64)_text % MIN_KIMG_ALIGN) != 0)
diff --git arch/arm64/kernel/vmlinux.lds.S arch/arm64/kernel/vmlinux.lds.S
index 184abd7c4..11ffc6b0b 100644
--- arch/arm64/kernel/vmlinux.lds.S
+++ arch/arm64/kernel/vmlinux.lds.S
@@ -161,6 +161,7 @@ SECTIONS
 			IDMAP_TEXT
 			HIBERNATE_TEXT
 			TRAMP_TEXT
+			BHV_TEXT
 			*(.fixup)
 			*(.gnu.warning)
 		. = ALIGN(16);
@@ -204,13 +205,27 @@ SECTIONS
 
 	INIT_TEXT_SECTION(8)
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	__exittext_begin = .;
 	.exit.text : {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 	__exittext_end = .;
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(4);
+#endif
 	.altinstructions : {
 		__alt_instructions = .;
 		*(.altinstructions)
@@ -280,6 +295,17 @@ SECTIONS
 		__mmuoff_data_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	PECOFF_EDATA_PADDING
 	__pecoff_data_rawsize = ABSOLUTE(. - __initdata_begin);
 	_edata = .;
diff --git arch/arm64/kvm/va_layout.c arch/arm64/kvm/va_layout.c
index acdb7b3cc..4f67c1abc 100644
--- arch/arm64/kvm/va_layout.c
+++ arch/arm64/kvm/va_layout.c
@@ -13,6 +13,11 @@
 #include <asm/kvm_mmu.h>
 #include <asm/memory.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/hypercall.h>
+#include <bhv/integrity.h>
+#endif
+
 /*
  * The LSB of the HYP VA tag
  */
@@ -151,8 +156,26 @@ static u32 compute_instruction(int n, u32 rd, u32 rn)
 	return insn;
 }
 
-void __init kvm_update_va_mask(struct alt_instr *alt,
-			       __le32 *origptr, __le32 *updptr, int nr_inst)
+#ifdef CONFIG_BHV_VAS
+inline void kvm_bhv_alt_patch(__le32 *dest, u32 insn, bhv_patch_arg_t *bhv_arg)
+{
+	__le32 le32_insn = cpu_to_le32(insn);
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)dest);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &le32_insn,
+	       sizeof(le32_insn));
+	bhv_arg->bhv_patch_patch_arg.size = sizeof(le32_insn);
+	bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+}
+
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst,
+			       bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
+void __init kvm_update_va_mask(struct alt_instr *alt, __le32 *origptr,
+			       __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	int i;
 
@@ -170,7 +193,17 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		 * address), NOP everything after masking the kernel VA.
 		 */
 		if (has_vhe() || (!tag_val && i > 0)) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				kvm_bhv_alt_patch(&(updptr[i]),
+						  aarch64_insn_gen_nop(),
+						  bhv_arg);
+			} else {
+				updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+			}
+#else /* CONFIG_BHV_VAS */
 			updptr[i] = cpu_to_le32(aarch64_insn_gen_nop());
+#endif /* CONFIG_BHV_VAS */
 			continue;
 		}
 
@@ -181,12 +214,26 @@ void __init kvm_update_va_mask(struct alt_instr *alt,
 		insn = compute_instruction(i, rd, rn);
 		BUG_ON(insn == AARCH64_BREAK_FAULT);
 
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			kvm_bhv_alt_patch(&(updptr[i]), insn, bhv_arg);
+		} else {
+			updptr[i] = cpu_to_le32(insn);
+		}
+#else /* !CONFIG_BHV_VAS */
 		updptr[i] = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 	}
 }
 
+#ifdef CONFIG_BHV_VAS
+void kvm_patch_vector_branch(struct alt_instr *alt, __le32 *origptr,
+			     __le32 *updptr, int nr_inst,
+			     bhv_patch_arg_t *bhv_arg)
+#else /* CONFIG_BHV_VAS */
 void kvm_patch_vector_branch(struct alt_instr *alt,
 			     __le32 *origptr, __le32 *updptr, int nr_inst)
+#endif /* CONFIG_BHV_VAS */
 {
 	u64 addr;
 	u32 insn;
@@ -216,15 +263,29 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 16) & 0xffff), lsl #16 */
-	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
-					 (u16)(addr >> 16),
-					 16,
-					 AARCH64_INSN_VARIANT_64BIT,
+	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0, (u16)(addr >> 16),
+					 16, AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk x0, #((addr >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(AARCH64_INSN_REG_0,
@@ -232,12 +293,28 @@ void kvm_patch_vector_branch(struct alt_instr *alt,
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* br x0 */
 	insn = aarch64_insn_gen_branch_reg(AARCH64_INSN_REG_0,
 					   AARCH64_INSN_BRANCH_NOLINK);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst)
@@ -256,7 +333,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 0,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_ZERO);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 16) & 0xffff), lsl #16 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -264,7 +349,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 16,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 32) & 0xffff), lsl #32 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -272,7 +365,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 32,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 
 	/* movk rd, #((val >> 48) & 0xffff), lsl #48 */
 	insn = aarch64_insn_gen_movewide(rd,
@@ -280,7 +381,15 @@ static void generate_mov_q(u64 val, __le32 *origptr, __le32 *updptr, int nr_inst
 					 48,
 					 AARCH64_INSN_VARIANT_64BIT,
 					 AARCH64_INSN_MOVEWIDE_KEEP);
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		kvm_bhv_alt_patch(updptr++, insn, bhv_arg);
+	} else {
+		*updptr++ = cpu_to_le32(insn);
+	}
+#else /* CONFIG_BHV_VAS */
 	*updptr++ = cpu_to_le32(insn);
+#endif /* CONFIG_BHV_VAS */
 }
 
 void kvm_get_kimage_voffset(struct alt_instr *alt,
diff --git arch/arm64/mm/init.c arch/arm64/mm/init.c
index 3b269c756..18998fbb8 100644
--- arch/arm64/mm/init.c
+++ arch/arm64/mm/init.c
@@ -46,6 +46,9 @@
 #include <asm/alternative.h>
 #include <asm/xen/swiotlb-xen.h>
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -439,6 +442,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+	bhv_start();
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
 			   POISON_FREE_INITMEM, "unused kernel");
diff --git arch/x86/Kbuild arch/x86/Kbuild
index 30dec0197..3235bbc86 100644
--- arch/x86/Kbuild
+++ arch/x86/Kbuild
@@ -13,6 +13,9 @@ obj-$(CONFIG_PVH) += platform/pvh/
 # Hyper-V paravirtualization support
 obj-$(subst m,y,$(CONFIG_HYPERV)) += hyperv/
 
+# BHV VAS support
+obj-$(CONFIG_BHV_VAS)	+= bhv/
+
 obj-y += realmode/
 obj-y += kernel/
 obj-y += mm/
diff --git arch/x86/Kconfig arch/x86/Kconfig
index 57f5e8817..0dd2b36fe 100644
--- arch/x86/Kconfig
+++ arch/x86/Kconfig
@@ -862,6 +862,19 @@ config ACRN_GUEST
 	  IOT with small footprint and real-time features. More details can be
 	  found in https://projectacrn.org/.
 
+config BHV_VAS
+	def_bool y
+	bool "BHV guest support on x86_64"
+	depends on X86_64
+	select VIRTIO_VSOCKETS
+	select VSOCKETS
+	select VIRTIO_VSOCKETS_COMMON
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on BHV on x86_64
+	  and benefit from Virtualization-assisted Security.
+
+source "kernel/bhv/Kconfig"
+
 endif #HYPERVISOR_GUEST
 
 source "arch/x86/Kconfig.cpu"
diff --git arch/x86/bhv/Makefile arch/x86/bhv/Makefile
new file mode 100644
index 000000000..f7f95bc2a
--- /dev/null
+++ arch/x86/bhv/Makefile
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+#          Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= patch_static_call.o
diff --git arch/x86/bhv/init.c arch/x86/bhv/init.c
new file mode 100644
index 000000000..162ff24e7
--- /dev/null
+++ arch/x86/bhv/init.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/integrity.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/integrity.h>
+
+#include <bhv/init.h>
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <asm/vdso.h>
+
+static __always_inline void
+bhv_init_add_vdso_image_64(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_64
+	BUG_ON((*region_counter) == 0 || (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_64.data),
+				   vdso_image_64.size,
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_TRANSIENT,
+				   "KERNEL VDSO IMAGE 64");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_x32(bhv_mem_region_t *init_phys_mem_regions,
+			    unsigned int *region_counter)
+{
+#ifdef CONFIG_X86_X32_ABI
+	BUG_ON((*region_counter) == 0 || (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_x32.data),
+				   vdso_image_x32.size,
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_TRANSIENT,
+				   "KERNEL VDSO IMAGE X32");
+	(*region_counter)++;
+
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void
+bhv_init_add_vdso_image_32(bhv_mem_region_t *init_phys_mem_regions,
+			   unsigned int *region_counter)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	BUG_ON((*region_counter) == 0 || (*region_counter) >= BHV_INIT_MAX_REGIONS);
+
+	bhv_mem_region_create_ctor(&init_phys_mem_regions[*region_counter],
+				   &init_phys_mem_regions[*region_counter - 1],
+				   bhv_virt_to_phys(vdso_image_32.data),
+				   vdso_image_32.size,
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_TRANSIENT,
+				   "KERNEL VDSO IMAGE 32");
+	(*region_counter)++;
+
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	bhv_init_add_vdso_image_64(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_x32(init_phys_mem_regions, region_counter);
+	bhv_init_add_vdso_image_32(init_phys_mem_regions, region_counter);
+}
+
+#else // out of tree
+
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter)
+{
+	// We handle VDSOs in integrity.c, so no need to do anything here.
+}
+
+#endif // VASKM
diff --git arch/x86/bhv/integrity.c arch/x86/bhv/integrity.c
new file mode 100644
index 000000000..dcc701c3c
--- /dev/null
+++ arch/x86/bhv/integrity.c
@@ -0,0 +1,488 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/syscall.h>
+#include <asm/vdso.h>
+#include <asm/page_types.h>
+#include <asm/sections.h>
+#include <linux/pgtable.h>
+#include <linux/mm.h>
+
+#ifdef CONFIG_EFI
+#include <linux/efi.h>
+#endif /* CONFIG_EFI */
+
+#include <asm/bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/integrity.h>
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif //VASKM
+
+struct {
+	bool valid;
+	uint64_t addr;
+	uint64_t size;
+} typedef table_data_t;
+
+static table_data_t table_data __ro_after_init;
+
+struct {
+	uint64_t start_addr;
+	uint64_t size;
+	const char *label;
+	uint32_t mem_type;
+} typedef ro_region_t;
+
+static int __init_km bhv_alloc_node_idt_region(struct list_head *head)
+{
+	uint64_t addr = bhv_virt_to_phys((void *)table_data.addr);
+	uint64_t size = table_data.size;
+
+	return bhv_link_node_op_create(head, addr, size,
+				       BHV_MEM_TYPE_DATA_READ_ONLY,
+				       BHV_MEM_FLAGS_NONE, "IDT");
+}
+
+static int __init_km bhv_start_integrity_add_idt(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	// NOTE: the x86 system call table does not need explict protection
+	//       it is contained in the ro_data section.
+
+	if (!table_data.valid)
+		return 0;
+
+	rc = bhv_alloc_node_idt_region(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	if (n == NULL) {
+		rc = -ENOENT;
+		goto out;
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &n->region);
+	if (rc) {
+		bhv_fail("BHV: Cannot create phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#ifndef VASKM // inside kernel tree
+
+static inline int __init_km rm_vdso_image_64(struct list_head *head)
+{
+#ifdef CONFIG_X86_64
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_64.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km rm_vdso_image_x32(struct list_head *head)
+{
+#ifdef CONFIG_X86_X32_ABI
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_x32.data));
+#else
+	return 0;
+#endif
+}
+
+static inline int __init_km rm_vdso_image_32(struct list_head *head)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	return bhv_link_node_op_remove(head,
+				       bhv_virt_to_phys(vdso_image_32.data));
+#else
+	return 0;
+#endif
+}
+
+static int __init_km bhv_start_integrity_rm_vdso(void)
+{
+	int rc = 0;
+	bhv_mem_region_node_t *n = NULL;
+
+	LIST_HEAD(head);
+
+	rc = rm_vdso_image_64(&head);
+	if (rc)
+		goto out;
+
+	rc = rm_vdso_image_x32(&head);
+	if (rc)
+		goto out;
+
+	rc = rm_vdso_image_32(&head);
+	if (rc)
+		goto out;
+
+	n = list_first_entry_or_null(&head, bhv_mem_region_node_t, list);
+	BUG_ON(n == NULL);
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region);
+	if (rc) {
+		bhv_fail("BHV: Cannot remove phys mem regions");
+		goto out;
+	}
+
+out:
+	bhv_release_arg_list(&head);
+	return rc;
+}
+
+#endif // VASKM
+
+static void __init_km bhv_start_integrity_add_vdso_common(
+	uint64_t start_addr, uint64_t size, const char *label,
+	ro_region_t *range, unsigned int *cur_entry, size_t range_sz)
+{
+	unsigned int i;
+	uint64_t end = start_addr + size;
+	uint64_t cur_end = 0;
+
+	BUG_ON(size == 0);
+	BUG_ON((*cur_entry) >= range_sz);
+	BUG_ON(start_addr < range[0].start_addr);
+
+	// Check for overlaps
+	for (i = 0; i < (*cur_entry); i++) {
+		cur_end = range[i].start_addr + range[i].size;
+
+		// No overlap. Nothing to do.
+		if (end <= range[i].start_addr || start_addr >= cur_end)
+			continue;
+
+		// No range that we add should be exactly the same as an
+		// existing one.
+		if (start_addr == range[i].start_addr && end == cur_end) {
+			BUG(); // Range already exists
+		}
+
+		// Overlap. Split range.
+		// Case 1 (overlap left): New range starts before current range
+		//                        with/before the current range.
+		//                        Update the new range to end when cur starts.
+		//                        Thus creating range A and B.
+		//  -----------------------
+		// |      A      ##########B##########
+		// |     new     #         |   cur   #
+		// |             ##########|##########
+		// ------------------------
+		if (start_addr < range[i].start_addr && end <= cur_end) {
+			size = range[i].start_addr - start_addr;
+			continue;
+		}
+
+		// Case 2 (overlap right): New range starts within current range
+		//                         and ends with/after the current range.
+		//                         Update cur to end when new starts.
+		//                         Thus creating range A and B.
+		//                         -----------------------
+		//              #####A####|##########  B          |
+		//              #   cur   |         #     new     |
+		//              ##########|##########             |
+		//                        ------------------------
+		if (range[i].start_addr < start_addr && cur_end <= end) {
+			range[i].size = start_addr - range[i].start_addr;
+			continue;
+		}
+
+		// Case 3: New range fully encompasses current range.
+		//         Create three ranges A, B, C.
+		//  --------------------------------------------
+		// |     A     ##########B##########     C      |
+		// |    new    #        cur        #    new     |
+		// |           #####################            |
+		// ---------------------------------------------
+		if (start_addr <= range[i].start_addr && cur_end <= end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				// No. Add new Range A.
+				range[(*cur_entry)].start_addr = start_addr;
+				range[(*cur_entry)].size =
+					range[i].start_addr - start_addr;
+				range[(*cur_entry)].label = label;
+				range[(*cur_entry)].mem_type =
+					BHV_MEM_TYPE_VDSO;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Update new to be C. It will be added after the loop.
+				// B is already in our range list.
+				start_addr = cur_end;
+				size = end - cur_end;
+			} else {
+				// Remaining ranges are the same. Just update B with new label/type.
+				// Since a range can only have one label, it will take the
+				// label of the new range. This generally seems to make sense
+				// since we add the entire read-only section and then split it
+				// with smaller sections.
+				range[i].label = label;
+				range[i].mem_type = BHV_MEM_TYPE_VDSO;
+				return;
+			}
+			continue;
+		}
+
+		// Case 4: Current range fully encompasses new range.
+		//         Create three ranges A, B, C.
+		// ##############################################
+		// #     A      ---------B---------      C      #
+		// #    cur    |        new        |    cur     #
+		// #            -------------------             #
+		// ##############################################
+		if (range[i].start_addr <= start_addr && end <= cur_end) {
+			// Is A a part of B?
+			if (start_addr != range[i].start_addr) {
+				range[(*cur_entry)].start_addr =
+					range[i].start_addr;
+				range[(*cur_entry)].size =
+					start_addr - range[i].start_addr;
+				range[(*cur_entry)].label = range[i].label;
+				range[(*cur_entry)].mem_type =
+					range[i].mem_type;
+				(*cur_entry)++;
+			}
+
+			// Are B and C the same?
+			if (end != cur_end) {
+				// No. Create C. B will be added after the loop.
+				range[i].start_addr = end;
+				range[i].size = cur_end - end;
+			} else {
+				// Remaining ranges are the same.
+				// Shrink the current range to B. We also update its label/type.
+				// This generally seems to make sense since we add the
+				// entire read-only section and then split it with smaller
+				// sections.
+				range[i].start_addr = start_addr;
+				range[i].size = size;
+				range[i].label = label;
+				range[i].mem_type = BHV_MEM_TYPE_VDSO;
+				return;
+			}
+
+			continue;
+		}
+
+		BUG(); // "Unexpected case"
+	}
+
+	if (size != 0) {
+		range[(*cur_entry)].start_addr = start_addr;
+		range[(*cur_entry)].size = size;
+		range[(*cur_entry)].label = label;
+		range[(*cur_entry)].mem_type = BHV_MEM_TYPE_VDSO;
+		(*cur_entry)++;
+	}
+}
+
+#define VDSO_KLN_SYM(sym) KLN_SYMBOL_P(const struct vdso_image *, sym)
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_64_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#ifdef CONFIG_X86_64
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_64)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_64)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 64",
+					    range, cur_entry, range_sz);
+#endif /* CONFIG_X86_64 */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_x32_to_range(ro_region_t *range,
+						unsigned int *cur_entry,
+						size_t range_sz)
+{
+#ifdef CONFIG_X86_X32_ABI
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_x32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_x32)->size;
+	bhv_start_integrity_add_vdso_common(start, size,
+					    "KERNEL VDSO IMAGE X32", range,
+					    cur_entry, range_sz);
+#endif /* CONFIG_X86_X32_ABI */
+}
+
+static __always_inline void __init_km
+bhv_start_integrity_add_vdso_image_32_to_range(ro_region_t *range,
+					       unsigned int *cur_entry,
+					       size_t range_sz)
+{
+#if defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT)
+	uint64_t start = bhv_virt_to_phys(VDSO_KLN_SYM(vdso_image_32)->data);
+	uint64_t size = VDSO_KLN_SYM(vdso_image_32)->size;
+	bhv_start_integrity_add_vdso_common(start, size, "KERNEL VDSO IMAGE 32",
+					    range, cur_entry, range_sz);
+#endif /* defined CONFIG_X86_32 || (defined CONFIG_X86_64 && defined CONFIG_COMPAT) */
+}
+
+static void __init_km bhv_start_integrity_get_ro_ranges(ro_region_t *range,
+							unsigned int *cur_entry,
+							size_t range_sz)
+{
+	BUG_ON(range_sz < 1);
+
+	range[(*cur_entry)].start_addr =
+		bhv_virt_to_phys((void *)(KLN_SYM(__start_rodata) & PAGE_MASK));
+	range[(*cur_entry)].size = PAGE_ALIGN(KLN_SYM(__end_rodata)) -
+				   (KLN_SYM(__start_rodata) & PAGE_MASK);
+	range[(*cur_entry)].label = "KERNEL READ-ONLY DATA SECTION";
+	range[(*cur_entry)].mem_type = BHV_MEM_TYPE_DATA_READ_ONLY;
+	(*cur_entry)++;
+
+	bhv_start_integrity_add_vdso_image_64_to_range(range, cur_entry,
+						       range_sz);
+	bhv_start_integrity_add_vdso_image_x32_to_range(range, cur_entry,
+							range_sz);
+	bhv_start_integrity_add_vdso_image_32_to_range(range, cur_entry,
+						       range_sz);
+}
+
+static int __init_km bhv_start_integrity_add_ro(void)
+{
+#define BHV_MAX_RO_RANGES 8
+	unsigned int i;
+	bhv_mem_region_node_t *prev = NULL;
+	int rc = 0;
+	unsigned int nr_ro_ranges = 0;
+	ro_region_t ro_ranges[BHV_MAX_RO_RANGES] = { 0 };
+	bhv_mem_region_node_t *n[BHV_MAX_RO_RANGES];
+
+	bhv_start_integrity_get_ro_ranges(ro_ranges, &nr_ro_ranges,
+					  BHV_MAX_RO_RANGES);
+
+	rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL,
+				   nr_ro_ranges, (void **)&n);
+	if (!rc) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < nr_ro_ranges; i++) {
+		bhv_mem_region_create_ctor(
+			&n[i]->region, (prev ? &prev->region : NULL),
+			ro_ranges[i].start_addr, ro_ranges[i].size,
+			ro_ranges[i].mem_type, BHV_MEM_FLAGS_NONE,
+			ro_ranges[i].label);
+
+		prev = n[i];
+	}
+
+	rc = bhv_create_kern_phys_mem_region_hyp(0, &(n[0]->region));
+	if (rc) {
+		pr_err("BHV: create phys mem region failed: %d", rc);
+	}
+
+	kmem_cache_free_bulk(bhv_mem_region_cache, nr_ro_ranges, (void **)&n);
+
+	return rc;
+}
+
+#ifdef CONFIG_EFI
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	efi_memory_desc_t *md;
+	bhv_mem_region_node_t *n;
+	int rc = 0;
+
+	n = kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	for_each_efi_memory_desc (md) {
+		if ((md->type != EFI_LOADER_CODE) &&
+		    (md->type != EFI_BOOT_SERVICES_CODE) &&
+		    (md->type != EFI_RUNTIME_SERVICES_CODE) &&
+		    (md->type != EFI_PAL_CODE))
+			continue;
+
+		bhv_mem_region_create_ctor(&n->region, NULL, md->phys_addr,
+					   (md->num_pages) << PAGE_SHIFT,
+					   BHV_MEM_TYPE_CODE,
+					   BHV_MEM_FLAGS_NONE, "EFI REGION");
+
+		rc = bhv_create_kern_phys_mem_region_hyp(1, &(n->region));
+		if (rc) {
+			pr_err("BHV: create phys mem region failed: %d", rc);
+			kmem_cache_free(bhv_mem_region_cache, n);
+			return rc;
+		}
+	}
+
+	kmem_cache_free(bhv_mem_region_cache, n);
+
+	return 0;
+}
+#else /* CONFIG_EFI */
+static int __init_km bhv_start_integrity_add_efi_regions(void)
+{
+	return 0;
+}
+#endif /* CONFIG_EFI */
+
+int __init_km bhv_start_integrity_arch(void)
+{
+	int rc;
+
+	if (!bhv_integrity_is_enabled())
+		return 0;
+
+	rc = bhv_start_integrity_add_idt();
+	if (rc)
+		return rc;
+
+#ifndef VASKM // inside kernel tree
+	rc = bhv_start_integrity_rm_vdso();
+	if (rc)
+		return rc;
+#endif // VASKM
+
+	rc = bhv_start_integrity_add_efi_regions();
+	if (rc)
+		return rc;
+
+	return bhv_start_integrity_add_ro();
+}
+
+void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+	table_data.addr = addr;
+	table_data.size = numpages * PAGE_SIZE;
+	table_data.valid = true;
+}
+
+void __init bhv_integrity_arch_init(void)
+{
+	memset(&table_data, 0, sizeof(table_data));
+}
diff --git arch/x86/bhv/patch_alternative.c arch/x86/bhv/patch_alternative.c
new file mode 100644
index 000000000..514a8b8d1
--- /dev/null
+++ arch/x86/bhv/patch_alternative.c
@@ -0,0 +1,799 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/vault.h>
+#include <bhv/kversion.h>
+
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <asm/bhv/patch.h>
+
+#include <asm/sections.h>
+#include <asm/text-patching.h>
+#include <asm/insn.h>
+
+#include <linux/static_call.h>
+#include <linux/memory.h>
+
+
+#if defined BHV_KVERS_5_15
+#define NOPS x86_nops
+#elif defined BHV_KVERS_5_10
+#define NOPS ideal_nops
+#endif // BHV_KVERS
+
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+					  struct alt_instr *end,
+					  const s32 *locks_begin,
+					  const s32 *locks_end, u8 *text_begin,
+					  u8 *text_end)
+{
+	struct bhv_alternatives_mod_arch arch = { .locks_begin = locks_begin,
+						  .locks_end = locks_end,
+						  .text_begin = text_begin,
+						  .text_end = text_end };
+	bhv_alternatives_add_module(begin, end, &arch);
+}
+
+static void __bhv_text bhv_add_nops(void *insns, unsigned int len,
+					bhv_patch_arg_t *bhv_arg, bool patch)
+{
+	size_t total_length = 0;
+
+	while (len > 0) {
+		unsigned int noplen = len;
+		if (noplen > ASM_NOP_MAX)
+			noplen = ASM_NOP_MAX;
+
+		if (patch) {
+			if ((total_length + noplen) >
+				sizeof(bhv_arg->bhv_patch_patch_arg.src_value))
+				panic("Size for NOP patch exceeded!");
+
+			memcpy(bhv_arg->bhv_patch_patch_arg.src_value +
+					   total_length,
+				   NOPS[noplen], noplen);
+			total_length += noplen;
+		} else {
+			memcpy(insns, NOPS[noplen], noplen);
+			insns += noplen;
+		}
+
+		len -= noplen;
+	}
+
+	if (patch) {
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys(insns);
+		bhv_arg->bhv_patch_patch_arg.size = total_length;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+}
+
+#if defined BHV_KVERS_5_10
+
+static bool __bhv_text bhv_optimize_nops(struct alt_instr *a, u8 *instr,
+					 bhv_patch_arg_t *bhv_arg, bool patch)
+{
+	int i;
+
+	for (i = 0; i < a->padlen; i++) {
+		if (instr[i] != 0x90) {
+			return false;
+		}
+	}
+
+	bhv_add_nops(instr + (a->instrlen - a->padlen), a->padlen, bhv_arg,
+			 patch);
+	return true;
+}
+
+#elif defined BHV_KVERS_5_15
+
+/*
+ * bhv_optimize_nops_range() - Optimize a sequence of single byte NOPs (0x90)
+ *
+ * @instr: instruction byte stream
+ * @instrlen: length of the above
+ * @off: offset within @instr where the first NOP has been detected
+ *
+ * Return: number of NOPs found (and replaced).
+ */
+static __always_inline int bhv_optimize_nops_range(u8 *instr, u8 instrlen,
+						   int off,
+						   bhv_patch_arg_t *bhv_arg,
+						   bool patch)
+{
+	int i = off, nnops;
+
+	while (i < instrlen) {
+		if (instr[i] != 0x90)
+			break;
+
+		i++;
+	}
+
+	nnops = i - off;
+
+	if (nnops <= 1)
+		return nnops;
+
+	bhv_add_nops(instr + off, nnops, bhv_arg, patch);
+
+	return nnops;
+}
+
+/*
+ * "noinline" to cause control flow change and thus invalidate I$ and
+ * cause refetch after modification.
+ */
+static void __bhv_text noinline bhv_optimize_nops(u8 *instr, size_t len,
+						  bhv_patch_arg_t *bhv_arg,
+						  bool patch)
+{
+	struct insn insn;
+	int i = 0;
+
+	/*
+	 * Jump over the non-NOP insns and optimize single-byte NOPs into bigger
+	 * ones.
+	 */
+	for (;;) {
+		if (insn_decode_kernel(&insn, &instr[i]))
+			return;
+
+		/*
+		 * See if this and any potentially following NOPs can be
+		 * optimized.
+		 */
+		if (insn.length == 1 && insn.opcode.bytes[0] == 0x90)
+			i += bhv_optimize_nops_range(instr, len, i, bhv_arg,
+							 patch);
+		else
+			i += insn.length;
+
+		if (i >= len)
+			return;
+	}
+}
+
+#endif // BHV_KVERS
+
+static void __bhv_text bhv_recompute_jump(struct alt_instr *a, u8 *orig_insn,
+					  u8 *repl_insn, u8 *insn_buff)
+{
+	u8 *next_rip, *tgt_rip;
+	s32 n_dspl, o_dspl;
+	int repl_len;
+
+	if (a->replacementlen != 5)
+		return;
+
+	o_dspl = *(s32 *)(insn_buff + 1);
+
+	/* next_rip of the replacement JMP */
+	next_rip = repl_insn + a->replacementlen;
+	/* target rip of the replacement JMP */
+	tgt_rip = next_rip + o_dspl;
+	n_dspl = tgt_rip - orig_insn;
+
+	if (tgt_rip - orig_insn >= 0) {
+		if (n_dspl - 2 <= 127)
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+		/* negative offset */
+	} else {
+		if (((n_dspl - 2) & 0xff) == (n_dspl - 2))
+			goto two_byte_jmp;
+		else
+			goto five_byte_jmp;
+	}
+
+two_byte_jmp:
+	n_dspl -= 2;
+
+	insn_buff[0] = 0xeb;
+	insn_buff[1] = (s8)n_dspl;
+	bhv_add_nops(insn_buff + 2, 3, NULL, false);
+
+	repl_len = 2;
+	goto done;
+
+five_byte_jmp:
+	n_dspl -= 5;
+
+	insn_buff[0] = 0xe9;
+	*(s32 *)&insn_buff[1] = n_dspl;
+
+	repl_len = 5;
+
+done:
+	return;
+}
+
+#ifdef CONFIG_SMP
+static int __bhv_text bhv_alternatives_smp_lock_unlock_apply_vault(
+	u8 *target, bool lock, bhv_patch_arg_t *bhv_arg)
+{
+	static const u8 unlock_opcode = 0x3e;
+	static const u8 lock_opcode = 0xf0;
+
+	unsigned long r = 0;
+	u8 opcode;
+
+	// Check opcode
+	if (lock) {
+		if (*target != unlock_opcode)
+			return -EACCES;
+
+		opcode = lock_opcode;
+	} else {
+		if (*target != lock_opcode)
+			return -EACCES;
+
+		opcode = unlock_opcode;
+	}
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)target);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &opcode, 1);
+	bhv_arg->bhv_patch_patch_arg.size = 1;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	if (r) {
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	}
+	return 0;
+}
+
+static int __bhv_text bhv_alternatives_smp_lock_unlock_vault(
+	struct bhv_alternatives_mod *mod, bool lock, bhv_patch_arg_t *bhv_arg)
+{
+	const s32 *poff;
+
+	for (poff = mod->arch.locks_begin; poff < mod->arch.locks_end; poff++) {
+		u8 *ptr = (u8 *)poff + *poff;
+
+		if (!*poff || ptr < mod->arch.text_begin ||
+			ptr >= mod->arch.text_end)
+			continue;
+
+		bhv_alternatives_smp_lock_unlock_apply_vault(ptr, lock,
+								 bhv_arg);
+	}
+
+	return 0;
+}
+#endif /* CONFIG_SMP */
+
+#if defined BHV_KVERS_5_15
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+
+/*
+ * CALL/JMP *%\reg
+ */
+static int __bhv_text bhv_emit_indirect(int op, int reg, u8 *bytes)
+{
+	int i = 0;
+	u8 modrm;
+
+	switch (op) {
+	case CALL_INSN_OPCODE:
+		modrm = 0x10; /* Reg = 2; CALL r/m */
+		break;
+
+	case JMP32_INSN_OPCODE:
+		modrm = 0x20; /* Reg = 4; JMP r/m */
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		return -1;
+	}
+
+	if (reg >= 8) {
+		bytes[i++] = 0x41; /* REX.B prefix */
+		reg -= 8;
+	}
+
+	modrm |= 0xc0; /* Mod = 3 */
+	modrm += reg;
+
+	bytes[i++] = 0xff; /* opcode */
+	bytes[i++] = modrm;
+
+	return i;
+}
+
+/*
+ * Rewrite the compiler generated retpoline thunk calls.
+ *
+ * For spectre_v2=off (!X86_FEATURE_RETPOLINE), rewrite them into immediate
+ * indirect instructions, avoiding the extra indirection.
+ *
+ * For example, convert:
+ *
+ *   CALL __x86_indirect_thunk_\reg
+ *
+ * into:
+ *
+ *   CALL *%\reg
+ *
+ * It also tries to inline spectre_v2=retpoline,amd when size permits.
+ */
+static int __bhv_text bhv_patch_retpoline(void *addr, struct insn *insn,
+					  u8 *bytes)
+{
+	retpoline_thunk_t *target;
+	int reg, ret, i = 0;
+	u8 op, cc;
+
+	target = addr + insn->length + insn->immediate.value;
+	reg = target - __x86_indirect_thunk_array;
+
+	if (WARN_ON_ONCE(reg & ~0xf))
+		return -1;
+
+	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
+	BUG_ON(reg == 4);
+
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		!cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE))
+		return -1;
+
+	op = insn->opcode.bytes[0];
+
+	/*
+	 * Convert:
+	 *
+	 *   Jcc.d32 __x86_indirect_thunk_\reg
+	 *
+	 * into:
+	 *
+	 *   Jncc.d8 1f
+	 *   [ LFENCE ]
+	 *   JMP *%\reg
+	 *   [ NOP ]
+	 * 1:
+	 */
+	/* Jcc.d32 second opcode byte is in the range: 0x80-0x8f */
+	if (op == 0x0f && (insn->opcode.bytes[1] & 0xf0) == 0x80) {
+		cc = insn->opcode.bytes[1] & 0xf;
+		cc ^= 1; /* invert condition */
+
+		bytes[i++] = 0x70 + cc; /* Jcc.d8 */
+		bytes[i++] = insn->length - 2; /* sizeof(Jcc.d8) == 2 */
+
+		/* Continue as if: JMP.d32 __x86_indirect_thunk_\reg */
+		op = JMP32_INSN_OPCODE;
+	}
+
+	/*
+	 * For RETPOLINE_AMD: prepend the indirect CALL/JMP with an LFENCE.
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
+		bytes[i++] = 0x0f;
+		bytes[i++] = 0xae;
+		bytes[i++] = 0xe8; /* LFENCE */
+	}
+
+	ret = bhv_emit_indirect(op, reg, bytes + i);
+	if (ret < 0)
+		return ret;
+	i += ret;
+
+	for (; i < insn->length;)
+		bytes[i++] = BYTES_NOP1;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_retpolines_vault(s32 *s, bhv_patch_arg_t *bhv_arg)
+{
+	void *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op1, op2;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op1 = insn.opcode.bytes[0];
+	op2 = insn.opcode.bytes[1];
+
+	switch (op1) {
+	case CALL_INSN_OPCODE:
+	case JMP32_INSN_OPCODE:
+		break;
+
+	case 0x0f: /* escape */
+		if (op2 >= 0x80 && op2 <= 0x8f)
+			break;
+		fallthrough;
+	default:
+		WARN_ON_ONCE(1);
+		goto out;
+	}
+
+	len = bhv_patch_retpoline(addr, &insn, bytes);
+	if (len == insn.length) {
+		if (len > BHV_MAX_PATCH_SZ)
+			goto out;
+
+		bhv_optimize_nops(bytes, len, bhv_arg, true);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)addr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &bytes, len);
+		bhv_arg->bhv_patch_patch_arg.size = len;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+
+out:
+	bhv_vault_close_hyp();
+}
+
+#ifdef CONFIG_RETHUNK
+
+static int __bhv_text bhv_patch_return(void *addr, struct insn *insn, u8 *bytes)
+{
+	int i = 0;
+
+	if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+		return -1;
+
+	bytes[i++] = RET_INSN_OPCODE;
+
+	for (; i < insn->length;)
+		bytes[i++] = INT3_INSN_OPCODE;
+
+	return i;
+}
+
+void __bhv_text bhv_apply_returns_vault(s32 *s, bhv_patch_arg_t *bhv_arg)
+{
+	void *dest = NULL, *addr = (void *)s + *s;
+	struct insn insn;
+	int len, ret;
+	u8 bytes[16];
+	u8 op;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	ret = insn_decode_kernel(&insn, addr);
+	if (WARN_ON_ONCE(ret < 0))
+		goto out;
+
+	op = insn.opcode.bytes[0];
+	if (op == JMP32_INSN_OPCODE)
+		dest = addr + insn.length + insn.immediate.value;
+
+	if (__static_call_fixup(addr, op, dest) ||
+		WARN_ONCE(dest != &__x86_return_thunk,
+			  "missing return thunk: %pS-%pS: %*ph", addr, dest, 5,
+			  addr))
+		goto out;
+
+	len = bhv_patch_return(addr, &insn, bytes);
+	if (len == insn.length) {
+		if (len > BHV_MAX_PATCH_SZ)
+			goto out;
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys((void *)addr);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &bytes, len);
+		bhv_arg->bhv_patch_patch_arg.size = len;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+	}
+
+out:
+	bhv_vault_close_hyp();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+
+#endif // BHV_KVERS_5_15
+
+#ifdef CONFIG_PARAVIRT
+
+#define MAX_PATCH_LEN (255 - 1)
+
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p,
+					 bhv_patch_arg_t *bhv_arg)
+{
+	int ret;
+	char insn_buff[MAX_PATCH_LEN];
+	unsigned int used, bhv_patched, bhv_patch_sz;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	BUG_ON(p->len > MAX_PATCH_LEN);
+	/* prep the buffer with the original instructions */
+	memcpy(insn_buff, p->instr, p->len);
+
+#if defined(BHV_KVERS_5_10)
+	used = pv_ops.init.patch(p->type, insn_buff, (unsigned long)p->instr,
+				 p->len);
+#elif defined(BHV_KVERS_5_15)
+	used = paravirt_patch(p->type, insn_buff, (unsigned long)p->instr,
+			      p->len);
+#endif // BHV_KVERS
+
+	BUG_ON(used > p->len);
+
+	/* Pad the rest with nops */
+	bhv_add_nops(insn_buff + used, p->len - used, bhv_arg, false);
+
+	bhv_patched = 0;
+	while (bhv_patched < p->len) {
+		bhv_patch_sz =
+			((p->len - bhv_patched) <
+			 sizeof(bhv_arg->bhv_patch_patch_arg.src_value)) ?
+				(p->len - bhv_patched) :
+				sizeof(bhv_arg->bhv_patch_patch_arg.src_value);
+
+		bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+			bhv_virt_to_phys(((void *)(p->instr)) + bhv_patched);
+		memcpy(bhv_arg->bhv_patch_patch_arg.src_value + bhv_patched,
+		       (&insn_buff) + bhv_patched, bhv_patch_sz);
+		bhv_arg->bhv_patch_patch_arg.size = bhv_patch_sz;
+
+		bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+		bhv_patched += bhv_patch_sz;
+	}
+
+	bhv_vault_close_hyp();
+}
+#endif /* CONFIG_PARAVIRT */
+
+/*
+ * Are we looking at a near JMP with a 1 or 4-byte displacement.
+ */
+static __always_inline bool is_jmp(const u8 opcode)
+{
+	return opcode == 0xeb || opcode == 0xe9;
+}
+
+static int __bhv_text bhv_alternatives_patch_vault(struct alt_instr *a,
+						   bhv_patch_arg_t *bhv_arg)
+{
+	int rv;
+	u8 *instr, *replacement;
+	u8 insn_buff[254];
+	int insn_buff_sz = 0;
+
+	instr = (u8 *)&a->instr_offset + a->instr_offset;
+	replacement = (u8 *)&a->repl_offset + a->repl_offset;
+
+	if (a->instrlen > sizeof(insn_buff)) {
+		return -EACCES;
+	}
+
+	if (a->cpuid >= (NCAPINTS + NBUGINTS) * 32) {
+		return -EACCES;
+	}
+
+#if defined BHV_KVERS_5_10
+	if (!boot_cpu_has(a->cpuid)) {
+		if (a->padlen > 1) {
+			bhv_optimize_nops(a, instr, bhv_arg, true);
+		}
+
+		return 0;
+	}
+#elif defined BHV_KVERS_5_15
+	if (!boot_cpu_has(a->cpuid & ~ALTINSTR_FLAG_INV) ==
+	    !(a->cpuid & ALTINSTR_FLAG_INV)) {
+		bhv_optimize_nops(instr, a->instrlen, bhv_arg, true);
+		return 0;
+	}
+#endif // BHV_KVERS
+
+	memcpy(insn_buff, replacement, a->replacementlen);
+	insn_buff_sz = a->replacementlen;
+
+	/*
+	 * 0xe8 is a relative jump; fix the offset.
+	 *
+	 * Instruction length is checked before the opcode to avoid
+	 * accessing uninitialized bytes for zero-length replacements.
+	 */
+	if (a->replacementlen == 5 && *insn_buff == 0xe8) {
+		*(s32 *)(insn_buff + 1) += replacement - instr;
+	}
+
+	if (a->replacementlen && is_jmp(replacement[0]))
+		bhv_recompute_jump(a, instr, replacement, insn_buff);
+
+#if defined BHV_KVERS_5_10
+	if (a->instrlen > a->replacementlen) {
+		bhv_add_nops(insn_buff + a->replacementlen,
+				 a->instrlen - a->replacementlen, NULL, false);
+		insn_buff_sz += a->instrlen - a->replacementlen;
+	}
+
+#elif defined BHV_KVERS_5_15
+	for (; insn_buff_sz < a->instrlen; insn_buff_sz++)
+		insn_buff[insn_buff_sz] = 0x90;
+#endif // BHV_KVERS
+	
+	if (insn_buff_sz >= sizeof(bhv_arg->bhv_patch_patch_arg.src_value))
+		panic("Instruction buffer size too small!");
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)instr);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, &insn_buff,
+		   insn_buff_sz);
+	bhv_arg->bhv_patch_patch_arg.size = insn_buff_sz;
+
+	rv = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+				   BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+#if defined BHV_KVERS_5_15
+	bhv_optimize_nops(instr, a->instrlen, bhv_arg, true);
+#endif // BHV_KVERS_5_15
+
+	return rv;
+}
+
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg)
+{
+	struct alt_instr *a;
+	int rv = 0;
+
+#ifdef CONFIG_SMP
+	bool *smp = arch;
+	// SMP?
+	if (smp != NULL) {
+		bhv_alternatives_smp_lock_unlock_vault(mod, *smp, bhv_arg);
+	}
+#endif
+	(void)arch;
+
+	for (a = mod->begin; a < mod->end; a++) {
+		if (rv == 0)
+			rv = bhv_alternatives_patch_vault(a, bhv_arg);
+		else
+			bhv_alternatives_patch_vault(a, bhv_arg);
+	}
+
+	return rv;
+}
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+						  struct bhv_alternatives_mod *cur)
+{
+	struct bhv_alternatives_lock_search_param *param = search_param;
+
+	if (cur->arch.locks_begin == param->locks_begin &&
+		cur->arch.locks_end == param->locks_end) {
+		return true;
+	}
+
+	return false;
+}
+
+extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
+extern s32 __smp_locks[], __smp_locks_end[];
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods)
+{
+#if defined(CONFIG_X86_64) && defined(CONFIG_X86_X32_ABI) &&                   \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 4 // kernel + 3 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) &&                  \
+	defined(CONFIG_COMPAT)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_64) && !defined(CONFIG_X86_X32_ABI) &&                  \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_32) && defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 3 // kernel + 2 VDSO
+#endif
+#if defined(CONFIG_X86_32) && !defined(CONFIG_X86_X32_ABI)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+#if defined(CONFIG_X86_X32_ABI) && !defined(CONFIG_X86_32) &&                  \
+	!defined(CONFIG_COMPAT)
+#define MOD_NR 2 // kernel + 1 VDSO
+#endif
+	static struct bhv_alternatives_mod static_mods[MOD_NR];
+	uint32_t counter = 0;
+
+	// Init kernel.
+	static_mods[counter].begin = __alt_instructions;
+	static_mods[counter].end = __alt_instructions_end;
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = __smp_locks;
+	static_mods[counter].arch.locks_end = __smp_locks_end;
+	static_mods[counter].arch.text_begin = _text;
+	static_mods[counter].arch.text_end = _etext;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#if defined(CONFIG_X86_64)
+	// Init 64 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_64.data + vdso_image_64.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_64.data + vdso_image_64.alt +
+			 vdso_image_64.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_32) || defined(CONFIG_COMPAT)
+	// Init 32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_32.data + vdso_image_32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_32.data + vdso_image_32.alt +
+			 vdso_image_32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+#if defined(CONFIG_X86_X32_ABI)
+	// Init x32 VDSO
+	counter++;
+	static_mods[counter].begin =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt);
+	static_mods[counter].end =
+		(void *)(vdso_image_x32.data + vdso_image_x32.alt +
+			 vdso_image_x32.alt_len);
+	static_mods[counter].delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_INIT;
+	static_mods[counter].allocated = false;
+	static_mods[counter].arch.locks_begin = NULL;
+	static_mods[counter].arch.locks_end = NULL;
+	static_mods[counter].arch.text_begin = NULL;
+	static_mods[counter].arch.text_end = NULL;
+	static_mods[counter].next.next = NULL;
+	static_mods[counter].next.prev = NULL;
+#endif
+	*nr_mods = MOD_NR;
+	return &static_mods[0];
+}
diff --git arch/x86/bhv/patch_jump_label.c arch/x86/bhv/patch_jump_label.c
new file mode 100644
index 000000000..18d202148
--- /dev/null
+++ arch/x86/bhv/patch_jump_label.c
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/bhv/integrity.h>
+
+#include <asm-generic/bug.h>
+#include <linux/jump_label.h>
+#include <asm/text-patching.h>
+#include <linux/string.h>
+#include <linux/version.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+#endif // VASKM
+
+static inline bool is_nop(const void *code, size_t len)
+{
+#define CHECK_NOP(nop)                                                         \
+	if (0 == memcmp(code, nop, len))                                       \
+		return true;
+
+#define DEF_CHECK_NOP(...)                                                     \
+	{                                                                      \
+		const uint8_t __nop[] = { __VA_ARGS__ };                       \
+		CHECK_NOP(__nop);                                              \
+	}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len == 5) {
+		DEF_CHECK_NOP(STATIC_KEY_INIT_NOP);
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *,
+				     ideal_nops)[NOP_ATOMIC5]);
+	}
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len == 2) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[2]);
+	} else if (len == 5) {
+		CHECK_NOP(KLN_SYMBOL(const uint8_t *const *, x86_nops)[5]);
+	}
+#endif // LINUX_VERSION_CODE
+
+	return false;
+}
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len)
+{
+	const void *code;
+	const void *addr, *dest;
+
+	addr = (void *)jump_entry_code(entry);
+	dest = (void *)jump_entry_target(entry);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 13, 0)
+	if (len != 5)
+		return false;
+
+	code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+
+
+#else // LINUX_VERSION_CODE >= 5.14
+	if (len != 2 && len != 5)
+		return false;
+
+	if (len == 2) {
+		code = text_gen_insn(JMP8_INSN_OPCODE, addr, dest);
+	} else if (len == 5) {
+		code = text_gen_insn(JMP32_INSN_OPCODE, addr, dest);
+	}
+#endif // LINUX_VERSION_CODE
+
+	if (type != JUMP_LABEL_JMP) {
+		if (memcmp(addr, code, len))
+			return false;
+		if (!is_nop(expected_opcode, len))
+			return false;
+	} else {
+		if (!is_nop(addr, len))
+			return false;
+		if (memcmp(expected_opcode, code, len))
+			return false;
+	}
+	return true;
+}
diff --git arch/x86/bhv/patch_static_call.c arch/x86/bhv/patch_static_call.c
new file mode 100644
index 000000000..0c4900ae0
--- /dev/null
+++ arch/x86/bhv/patch_static_call.c
@@ -0,0 +1,123 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/static_call.h>
+#include <linux/bug.h>
+#include <asm/text-patching.h>
+
+#include <bhv/vault.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+#include <bhv/kversion.h>
+
+/*
+ * cs cs cs xorl %eax, %eax - a single 5 byte instruction that clears %[er]ax
+ */
+static const u8 xor5rax[] = { 0x2e, 0x2e, 0x2e, 0x31, 0xc0 };
+
+static const u8 retinsn[] = { RET_INSN_OPCODE, 0xcc, 0xcc, 0xcc, 0xcc };
+
+static DEFINE_MUTEX(bhv_static_call_mutex);
+
+static void __always_inline bhv_static_call_lock(void)
+{
+	mutex_lock(&bhv_static_call_mutex);
+}
+
+static void __always_inline bhv_static_call_unlock(void)
+{
+	mutex_unlock(&bhv_static_call_mutex);
+}
+
+static void __bhv_text bhv_static_call_transform_vault(void *insn,
+						       enum insn_type type,
+						       void *func,
+						       bhv_patch_arg_t *bhv_arg)
+{
+	int ret;
+#if defined BHV_KVERS_5_15
+	const void *emulate = NULL;
+#endif // BHV_KVERS_5_15
+	int size = CALL_INSN_SIZE;
+	const void *code;
+
+	ret = bhv_vault_open_hyp();
+	if (ret)
+		return;
+
+	switch (type) {
+	case CALL:
+		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
+#if defined BHV_KVERS_5_15
+		if (func == &__static_call_return0) {
+			emulate = code;
+			code = &xor5rax;
+		}
+#endif // BHV_KVERS_5_15
+
+		break;
+
+	case NOP:
+#if defined BHV_KVERS_5_15
+		code = x86_nops[5];
+#elif defined BHV_KVERS_5_10
+		code = ideal_nops[NOP_ATOMIC5];
+#endif // BHV_KVERS
+		break;
+
+	case JMP:
+		code = text_gen_insn(JMP32_INSN_OPCODE, insn, func);
+		break;
+
+	case RET:
+#if defined BHV_KVERS_5_15
+		if (cpu_feature_enabled(X86_FEATURE_RETHUNK))
+			code = text_gen_insn(JMP32_INSN_OPCODE, insn,
+					     &__x86_return_thunk);
+		else
+			code = &retinsn;
+#elif defined BHV_KVERS_5_10
+		code = text_gen_insn(RET_INSN_OPCODE, insn, func);
+		size = RET_INSN_SIZE;
+#endif // BHV_KVERS
+		break;
+	}
+
+	if (memcmp(insn, code, size) == 0)
+		goto out;
+
+	if (size > BHV_MAX_PATCH_SZ)
+		panic("BHV: static call transform patch too large");
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr =
+		bhv_virt_to_phys((void *)insn);
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, code, size);
+	bhv_arg->bhv_patch_patch_arg.size = size;
+
+	bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH,
+			  BHV_VAS_PATCH_OP_PATCH_NO_CLOSE, bhv_arg);
+
+out:
+	bhv_vault_close_hyp();
+}
+
+#if defined BHV_KVERS_5_15
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func,
+			       bool modinit)
+#elif defined BHV_KVERS_5_10
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func)
+#endif // BHV_KVERS
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_static_call_lock();
+	local_irq_save(flags);
+	bhv_static_call_transform_vault(insn, type, func, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_static_call_unlock();
+}
diff --git arch/x86/bhv/start.c arch/x86/bhv/start.c
new file mode 100644
index 000000000..234cc7ce9
--- /dev/null
+++ arch/x86/bhv/start.c
@@ -0,0 +1,60 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/types.h> // This is here so that hypervisor.h knows bool
+#include <asm/hypervisor.h>
+#include <asm/processor.h>
+#include <asm/x86_init.h>
+
+#include <asm/bhv/integrity.h>
+#include <bhv/integrity.h>
+#include <bhv/start.h>
+
+#ifndef VASKM // inside kernel tree
+
+static uint32_t __init bhv_detect(void)
+{
+	if (boot_cpu_data.cpuid_level < 0)
+		return 0;	/* So we don't blow up on old processors */
+
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR))
+		return hypervisor_cpuid_base("BHV.VMM.VAS.", 0);
+
+	return 0;
+}
+
+static void __init bhv_init_platform_x86(void)
+{
+	bhv_integrity_arch_init();
+	bhv_init_platform();
+}
+
+const __initconst struct hypervisor_x86 x86_hyper_bhv = {
+	.name = "BHV BRASS",
+	.detect = bhv_detect,
+	.type = X86_HYPER_BHV,
+	.init.guest_late_init = x86_init_noop,
+	.init.x2apic_available = bool_x86_init_noop,
+	.init.init_platform = bhv_init_platform_x86
+};
+
+int __init_km bhv_start_arch(void)
+{
+	return bhv_start_integrity_arch();
+}
+
+#else // out of tree
+
+#include <common.h>
+
+int __init_km bhv_start_arch(void)
+{
+	bhv_integrity_arch_init();
+	return bhv_start_integrity_arch();
+}
+
+#endif // VASKM
diff --git arch/x86/include/asm/bhv/domain.h arch/x86/include/asm/bhv/domain.h
new file mode 100644
index 000000000..922c0608b
--- /dev/null
+++ arch/x86/include/asm/bhv/domain.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_DOMAIN_H__
+#define __ASM_BHV_DOMAIN_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <asm/pgtable.h>
+#include <bhv/bhv.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#define bhv_domain_arch_get_user_pgd(pgd) kernel_to_user_pgdp(pgd)
+#endif
+
+#endif
+
+#endif /* __ASM_BHV_DOMAIN_H__ */
diff --git arch/x86/include/asm/bhv/hypercall.h arch/x86/include/asm/bhv/hypercall.h
new file mode 100644
index 000000000..2c2890de4
--- /dev/null
+++ arch/x86/include/asm/bhv/hypercall.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_HYPERCALL_H__
+#define __ASM_BHV_HYPERCALL_H__
+
+static __always_inline unsigned long BHV_HYPERCALL(uint16_t target,
+						   uint32_t backend,
+						   uint32_t op, uint64_t ver,
+						   uint64_t arg)
+{
+	unsigned long rv;
+	// https://gcc.gnu.org/onlinedocs/gcc/Local-Register-Variables.html
+	register unsigned long r8 __asm__("r8") = arg;
+	__asm__ __volatile__("vmcall\n\t"
+			     : "=a"(rv)
+			     : "D"(target), "S"(backend), "d"(op), "c"(ver),
+			       "r"(r8)
+			     :);
+	return rv;
+}
+
+#endif /* __ASM_BHV_HYPERCALL_H__ */
diff --git arch/x86/include/asm/bhv/integrity.h arch/x86/include/asm/bhv/integrity.h
new file mode 100644
index 000000000..d1776417d
--- /dev/null
+++ arch/x86/include/asm/bhv/integrity.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_INTEGRITY_H__
+#define __ASM_BHV_INTEGRITY_H__
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+
+void __init bhv_integrity_arch_init(void);
+void __init bhv_register_idt(uint64_t addr,
+							 int numpages);
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+#endif // VASKM
+#else /* CONFIG_BHV_VAS */
+static inline void __init bhv_register_idt(uint64_t addr, int numpages)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_INTEGRITY_H__ */
diff --git arch/x86/include/asm/bhv/patch.h arch/x86/include/asm/bhv/patch.h
new file mode 100644
index 000000000..71028de09
--- /dev/null
+++ arch/x86/include/asm/bhv/patch.h
@@ -0,0 +1,95 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __ASM_BHV_PATCH_H__
+#define __ASM_BHV_PATCH_H__
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+struct bhv_alternatives_mod;
+struct bhv_alternatives_mod_arch {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+	u8 *text_begin;
+	u8 *text_end;
+};
+
+#ifdef CONFIG_BHV_VAS
+
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+
+#ifdef CONFIG_JUMP_LABEL
+#include <linux/jump_label.h>
+
+bool __bhv_text bhv_jump_label_validate_opcode(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       const void *expected_opcode,
+					       size_t len);
+#endif /* CONFIG_JUMP_LABEL */
+
+struct bhv_alternatives_lock_search_param {
+	const s32 *locks_begin;
+	const s32 *locks_end;
+};
+
+bool __bhv_text bhv_alternatives_find_by_lock(void *search_param,
+					      struct bhv_alternatives_mod *cur);
+int __bhv_text bhv_alternatives_apply_vault_arch(
+	struct bhv_alternatives_mod *mod, void *arch, bhv_patch_arg_t *bhv_arg);
+void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+				      struct alt_instr *end, const s32 *locks,
+				      const s32 *locks_end, u8 *text,
+				      u8 *text_end);
+struct bhv_alternatives_mod *__bhv_text
+bhv_alternatives_get_static_mods_vault(uint32_t *nr_mods);
+
+#ifndef VASKM // inside kernel tree
+#if defined BHV_KVERS_5_15
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+void __bhv_text bhv_apply_retpolines_vault(s32 *s, bhv_patch_arg_t *bhv_arg);
+#ifdef CONFIG_RETHUNK
+void __bhv_text bhv_apply_returns_vault(s32 *s, bhv_patch_arg_t *bhv_arg);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+#endif // BHV_KVERS
+
+#ifdef CONFIG_PARAVIRT
+void __bhv_text bhv_apply_paravirt_vault(struct paravirt_patch_site *p,
+					 bhv_patch_arg_t *bhv_arg);
+#endif /* CONFIG_PARAVIRT */
+
+enum insn_type {
+	CALL = 0, /* site call */
+	NOP = 1, /* site cond-call */
+	JMP = 2, /* tramp / site tail-call */
+	RET = 3, /* tramp / site cond-tail-call */
+};
+
+#if defined BHV_KVERS_5_15
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func,
+			       bool modinit);
+
+#elif defined BHV_KVERS_5_10
+void bhv_static_call_transform(void *insn, enum insn_type type, void *func);
+#endif // BHV_KVERS
+#endif // VASKM
+
+#else
+static inline void bhv_alternatives_add_module_arch(struct alt_instr *begin,
+						    struct alt_instr *end,
+						    const s32 *locks,
+						    const s32 *locks_end,
+						    u8 *text, u8 *text_end)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __ASM_BHV_PATCH_H__ */
\ No newline at end of file
diff --git arch/x86/include/asm/hypervisor.h arch/x86/include/asm/hypervisor.h
index e41cbf2ec..591e48b05 100644
--- arch/x86/include/asm/hypervisor.h
+++ arch/x86/include/asm/hypervisor.h
@@ -30,6 +30,7 @@ enum x86_hypervisor_type {
 	X86_HYPER_KVM,
 	X86_HYPER_JAILHOUSE,
 	X86_HYPER_ACRN,
+	X86_HYPER_BHV
 };
 
 #ifdef CONFIG_HYPERVISOR_GUEST
@@ -65,6 +66,7 @@ extern const struct hypervisor_x86 x86_hyper_kvm;
 extern const struct hypervisor_x86 x86_hyper_jailhouse;
 extern const struct hypervisor_x86 x86_hyper_acrn;
 extern struct hypervisor_x86 x86_hyper_xen_hvm;
+extern const struct hypervisor_x86 x86_hyper_bhv;
 
 extern bool nopv;
 extern enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/alternative.c arch/x86/kernel/alternative.c
index 43dd7f281..96505e444 100644
--- arch/x86/kernel/alternative.c
+++ arch/x86/kernel/alternative.c
@@ -31,6 +31,9 @@
 #include <asm/paravirt.h>
 #include <asm/asm-prototypes.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 int __read_mostly alternatives_patched;
 
 EXPORT_SYMBOL_GPL(alternatives_patched);
@@ -268,6 +271,13 @@ void __init_or_module noinline apply_alternatives(struct alt_instr *start,
 	u8 *instr, *replacement;
 	u8 insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_alternatives_apply(start, end, NULL);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	DPRINTK("alt table %px, -> %px", start, end);
 	/*
 	 * The scan order should be from start to end. A later scanned
@@ -465,6 +475,14 @@ void __init_or_module noinline apply_retpolines(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_retpolines(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *addr = (void *)s + *s;
 		struct insn insn;
@@ -538,6 +556,14 @@ void __init_or_module noinline apply_returns(s32 *start, s32 *end)
 {
 	s32 *s;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (s = start; s < end; s++)
+			bhv_apply_returns(s);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (s = start; s < end; s++) {
 		void *dest = NULL, *addr = (void *)s + *s;
 		struct insn insn;
@@ -614,7 +640,6 @@ static void alternatives_smp_unlock(const s32 *start, const s32 *end,
 			text_poke(ptr, ((unsigned char []){0x3E}), 1);
 	}
 }
-
 struct smp_alt_module {
 	/* what is this ??? */
 	struct module	*mod;
@@ -640,6 +665,11 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 {
 	struct smp_alt_module *smp;
 
+#ifdef CONFIG_BHV_VAS
+	struct bhv_alternatives_lock_search_param p;
+	bool smp_lock;
+#endif
+
 	mutex_lock(&text_mutex);
 	if (!uniproc_patched)
 		goto unlock;
@@ -665,7 +695,25 @@ void __init_or_module alternatives_smp_module_add(struct module *mod,
 
 	list_add_tail(&smp->next, &smp_alt_modules);
 smp_unlock:
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		// Add module with locks as this will be used for SMP only
+		if (num_possible_cpus() > 1) {
+			bhv_alternatives_add_module_arch(locks, locks_end, locks,
+							locks_end, text, text_end);
+		}
+		// Apply
+		smp_lock = false;
+		p.locks_begin = locks;
+		p.locks_end = locks_end;
+		bhv_alternatives_apply_custom_filter(
+			&p, &smp_lock, bhv_alternatives_find_by_lock);
+	} else {
+		alternatives_smp_unlock(locks, locks_end, text, text_end);
+	}
+#else /* !CONFIG_BHV_VAS */
 	alternatives_smp_unlock(locks, locks_end, text, text_end);
+#endif /* CONFIG_BHV_VAS */
 unlock:
 	mutex_unlock(&text_mutex);
 }
@@ -699,9 +747,26 @@ void alternatives_enable_smp(void)
 		BUG_ON(num_online_cpus() != 1);
 		clear_cpu_cap(&boot_cpu_data, X86_FEATURE_UP);
 		clear_cpu_cap(&cpu_data(0), X86_FEATURE_UP);
-		list_for_each_entry(mod, &smp_alt_modules, next)
+		list_for_each_entry (mod, &smp_alt_modules, next) {
+#ifdef CONFIG_BHV_VAS
+			if (bhv_integrity_is_enabled()) {
+				struct bhv_alternatives_lock_search_param p;
+				bool smp = true;
+				p.locks_begin = mod->locks;
+				p.locks_end = mod->locks_end;
+				bhv_alternatives_apply_custom_filter(
+					&p, &smp,
+					bhv_alternatives_find_by_lock);
+			} else {
+				alternatives_smp_lock(mod->locks,
+						      mod->locks_end, mod->text,
+						      mod->text_end);
+			}
+#else /* !CONFIG_BHV_VAS */
 			alternatives_smp_lock(mod->locks, mod->locks_end,
 					      mod->text, mod->text_end);
+#endif /* CONFIG_BHV_VAS */
+		}
 		uniproc_patched = false;
 	}
 	mutex_unlock(&text_mutex);
@@ -742,6 +807,14 @@ void __init_or_module apply_paravirt(struct paravirt_patch_site *start,
 	struct paravirt_patch_site *p;
 	char insn_buff[MAX_PATCH_LEN];
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		for (p = start; p < end; p++)
+			bhv_apply_paravirt(p);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	for (p = start; p < end; p++) {
 		unsigned int used;
 
diff --git arch/x86/kernel/cpu/hypervisor.c arch/x86/kernel/cpu/hypervisor.c
index 553bfbfc3..20bfa373b 100644
--- arch/x86/kernel/cpu/hypervisor.c
+++ arch/x86/kernel/cpu/hypervisor.c
@@ -45,6 +45,9 @@ static const __initconst struct hypervisor_x86 * const hypervisors[] =
 #ifdef CONFIG_ACRN_GUEST
 	&x86_hyper_acrn,
 #endif
+#ifdef CONFIG_BHV_VAS
+	&x86_hyper_bhv,
+#endif
 };
 
 enum x86_hypervisor_type x86_hyper_type;
diff --git arch/x86/kernel/idt.c arch/x86/kernel/idt.c
index df0fa695b..5ca9afe7d 100644
--- arch/x86/kernel/idt.c
+++ arch/x86/kernel/idt.c
@@ -10,6 +10,7 @@
 #include <asm/proto.h>
 #include <asm/desc.h>
 #include <asm/hw_irq.h>
+#include <asm/bhv/integrity.h>
 
 #define DPL0		0x0
 #define DPL3		0x3
@@ -294,6 +295,8 @@ void __init idt_setup_apic_and_irq_gates(void)
 	/* Make the IDT table read only */
 	set_memory_ro((unsigned long)&idt_table, 1);
 
+	bhv_register_idt((uint64_t)&idt_table, 1);
+
 	idt_setup_done = true;
 }
 
diff --git arch/x86/kernel/jump_label.c arch/x86/kernel/jump_label.c
index 68f091ba8..ddf7de6e5 100644
--- arch/x86/kernel/jump_label.c
+++ arch/x86/kernel/jump_label.c
@@ -16,6 +16,9 @@
 #include <asm/alternative.h>
 #include <asm/text-patching.h>
 #include <asm/insn.h>
+#include <bhv/bhv.h>
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
 
 int arch_jump_entry_size(struct jump_entry *entry)
 {
@@ -79,10 +82,30 @@ __jump_label_patch(struct jump_entry *entry, enum jump_label_type type)
 	return (struct jump_label_patch){.code = code, .size = size};
 }
 
-static __always_inline void
-__jump_label_transform(struct jump_entry *entry,
-		       enum jump_label_type type,
-		       int init)
+#ifdef CONFIG_BHV_VAS
+static void __orig_jump_label_transform(struct jump_entry *entry,
+					enum jump_label_type type, int init);
+
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type,
+					  int init)
+{
+	if (bhv_integrity_is_enabled()) {
+		const struct jump_label_patch jlp =
+			__jump_label_patch(entry, type);
+		bhv_patch_jump_label(entry, jlp.code, jlp.size);
+	} else {
+		__orig_jump_label_transform(entry, type, init);
+	}
+}
+
+static __always_inline void __orig_jump_label_transform(struct jump_entry *entry,
+					       enum jump_label_type type,
+					       int init)
+#else /* CONFIG_BHV_VAS */
+static __always_inline void __jump_label_transform(struct jump_entry *entry,
+					  enum jump_label_type type, int init)
+#endif /* CONFIG_BHV_VAS */
 {
 	const struct jump_label_patch jlp = __jump_label_patch(entry, type);
 
@@ -133,6 +156,13 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 		return true;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		arch_jump_label_transform(entry, type);
+		return true;
+	}
+#endif
+
 	mutex_lock(&text_mutex);
 	jlp = __jump_label_patch(entry, type);
 	text_poke_queue((void *)jump_entry_code(entry), jlp.code, jlp.size, NULL);
@@ -140,12 +170,23 @@ bool arch_jump_label_transform_queue(struct jump_entry *entry,
 	return true;
 }
 
+#ifdef CONFIG_BHV_VAS
+void arch_jump_label_transform_apply(void)
+{
+	if (!bhv_integrity_is_enabled()) {
+		mutex_lock(&text_mutex);
+		text_poke_finish();
+		mutex_unlock(&text_mutex);
+	}
+}
+#else /* CONFIG_BHV_VAS */
 void arch_jump_label_transform_apply(void)
 {
 	mutex_lock(&text_mutex);
 	text_poke_finish();
 	mutex_unlock(&text_mutex);
 }
+#endif /* CONFIG_BHV_VAS */
 
 static enum {
 	JL_STATE_START,
diff --git arch/x86/kernel/module.c arch/x86/kernel/module.c
index 06b53ea94..8d0de0b6e 100644
--- arch/x86/kernel/module.c
+++ arch/x86/kernel/module.c
@@ -25,6 +25,9 @@
 #include <asm/setup.h>
 #include <asm/unwind.h>
 
+#include <asm/bhv/patch.h>
+#include <bhv/integrity.h>
+
 #if 0
 #define DEBUGP(fmt, ...)				\
 	printk(KERN_DEBUG fmt, ##__VA_ARGS__)
@@ -256,6 +259,12 @@ int module_finalize(const Elf_Ehdr *hdr,
 		*retpolines = NULL, *returns = NULL;
 	char *secstrings = (void *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
 
+#ifdef CONFIG_BHV_VAS
+	void *alt_start = NULL;
+	void *alt_end = NULL;
+	struct bhv_alternatives_mod_arch arch;
+#endif
+
 	for (s = sechdrs; s < sechdrs + hdr->e_shnum; s++) {
 		if (!strcmp(".text", secstrings + s->sh_name))
 			text = s;
@@ -291,9 +300,29 @@ int module_finalize(const Elf_Ehdr *hdr,
 		void *rseg = (void *)returns->sh_addr;
 		apply_returns(rseg, rseg + returns->sh_size);
 	}
+
+#ifdef CONFIG_BHV_VAS
+	if (alt) {
+		alt_start = (void *)alt->sh_addr;
+		alt_end = alt_start + alt->sh_size;
+	}
+
+	if (locks && text) {
+		arch.locks_begin = (void *)locks->sh_addr;
+		arch.locks_end = (void *)locks->sh_addr + locks->sh_size;
+		arch.text_begin = (void *)text->sh_addr;
+		arch.text_end = (void *)text->sh_addr + text->sh_size;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	if (alt) {
 		/* patch .altinstructions */
 		void *aseg = (void *)alt->sh_addr;
+#ifdef CONFIG_BHV_VAS
+		if (bhv_integrity_is_enabled()) {
+			bhv_alternatives_add_module(alt_start, alt_end, &arch);
+		}
+#endif /* CONFIG_BHV_VAS */
 		apply_alternatives(aseg, aseg + alt->sh_size);
 	}
 	if (locks && text) {
diff --git arch/x86/kernel/static_call.c arch/x86/kernel/static_call.c
index 2fc4f9670..b3335abca 100644
--- arch/x86/kernel/static_call.c
+++ arch/x86/kernel/static_call.c
@@ -4,12 +4,17 @@
 #include <linux/bug.h>
 #include <asm/text-patching.h>
 
+#include <bhv/patch.h>
+#include <bhv/integrity.h>
+
+#ifndef CONFIG_BHV_VAS
 enum insn_type {
 	CALL = 0, /* site call */
 	NOP = 1,  /* site cond-call */
 	JMP = 2,  /* tramp / site tail-call */
 	RET = 3,  /* tramp / site cond-tail-call */
 };
+#endif
 
 /*
  * ud1 %esp, %ecx - a 3 byte #UD that is unique to trampolines, chosen such
@@ -32,6 +37,13 @@ static void __ref __static_call_transform(void *insn, enum insn_type type,
 	int size = CALL_INSN_SIZE;
 	const void *code;
 
+#ifdef CONFIG_BHV_VAS
+	if (bhv_integrity_is_enabled()) {
+		bhv_static_call_transform(insn, type, func, modinit);
+		return;
+	}
+#endif /* CONFIG_BHV_VAS */
+
 	switch (type) {
 	case CALL:
 		code = text_gen_insn(CALL_INSN_OPCODE, insn, func);
diff --git arch/x86/kernel/vmlinux.lds.S arch/x86/kernel/vmlinux.lds.S
index c1efcd194..00f604fd9 100644
--- arch/x86/kernel/vmlinux.lds.S
+++ arch/x86/kernel/vmlinux.lds.S
@@ -137,6 +137,7 @@ SECTIONS
 		ALIGN_ENTRY_TEXT_END
 		SOFTIRQENTRY_TEXT
 		STATIC_CALL_TEXT
+		BHV_TEXT
 		*(.fixup)
 		*(.gnu.warning)
 
@@ -333,15 +334,29 @@ SECTIONS
 		__apicdrivers_end = .;
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#else
 	. = ALIGN(8);
+#endif
 	/*
 	 * .exit.text is discarded at runtime, not link time, to deal with
 	 *  references from .altinstructions
 	 */
 	.exit.text : AT(ADDR(.exit.text) - LOAD_OFFSET) {
+#ifdef CONFIG_BHV_VAS
+		_sexittext = .;
+#endif
 		EXIT_TEXT
+#ifdef CONFIG_BHV_VAS
+		_eexittext = .;
+#endif
 	}
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+#endif
+
 	.exit.data : AT(ADDR(.exit.data) - LOAD_OFFSET) {
 		EXIT_DATA
 	}
@@ -375,6 +390,17 @@ SECTIONS
 	}
 #endif
 
+#ifdef CONFIG_BHV_VAS
+	. = ALIGN(PAGE_SIZE);
+	.bhv.data : AT(ADDR(.bhv.data) - LOAD_OFFSET) {
+		__bhv_data_start = .;
+		. += PAGE_SIZE;
+		*(.bhv.data)
+		. = ALIGN(PAGE_SIZE);
+		__bhv_data_end = .;
+	}
+#endif
+
 	/* BSS */
 	. = ALIGN(PAGE_SIZE);
 	.bss : AT(ADDR(.bss) - LOAD_OFFSET) {
diff --git arch/x86/mm/init.c arch/x86/mm/init.c
index 0e3667e52..fe936f129 100644
--- arch/x86/mm/init.c
+++ arch/x86/mm/init.c
@@ -36,6 +36,9 @@
 
 #include "mm_internal.h"
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 /*
  * Tables translating between page_cache_type_t and pte encoding.
  *
@@ -947,6 +950,8 @@ void __ref free_initmem(void)
 
 	mem_encrypt_free_decrypted_mem();
 
+	bhv_start();
+
 	free_kernel_image_pages("unused kernel image (initmem)",
 				&__init_begin, &__init_end);
 }
diff --git drivers/block/zram/zram_drv.c drivers/block/zram/zram_drv.c
index 6383c81ac..a7c6eea7e 100644
--- drivers/block/zram/zram_drv.c
+++ drivers/block/zram/zram_drv.c
@@ -929,7 +929,7 @@ static ssize_t read_block_state(struct file *file, char __user *buf,
 	return written;
 }
 
-static const struct file_operations proc_zram_block_state_op = {
+const struct file_operations proc_zram_block_state_op = {
 	.open = simple_open,
 	.read = read_block_state,
 	.llseek = default_llseek,
diff --git drivers/char/mem.c drivers/char/mem.c
index 1c596b5cd..aca3b8045 100644
--- drivers/char/mem.c
+++ drivers/char/mem.c
@@ -640,7 +640,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define write_iter_zero	write_iter_null
 #define open_mem	open_port
 
-static const struct file_operations __maybe_unused mem_fops = {
+const struct file_operations __maybe_unused mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
@@ -652,7 +652,7 @@ static const struct file_operations __maybe_unused mem_fops = {
 #endif
 };
 
-static const struct file_operations null_fops = {
+const struct file_operations null_fops = {
 	.llseek		= null_lseek,
 	.read		= read_null,
 	.write		= write_null,
@@ -661,14 +661,14 @@ static const struct file_operations null_fops = {
 	.splice_write	= splice_write_null,
 };
 
-static const struct file_operations __maybe_unused port_fops = {
+const struct file_operations __maybe_unused port_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_port,
 	.write		= write_port,
 	.open		= open_port,
 };
 
-static const struct file_operations zero_fops = {
+const struct file_operations zero_fops = {
 	.llseek		= zero_lseek,
 	.write		= write_zero,
 	.read_iter	= read_iter_zero,
@@ -681,7 +681,7 @@ static const struct file_operations zero_fops = {
 #endif
 };
 
-static const struct file_operations full_fops = {
+const struct file_operations full_fops = {
 	.llseek		= full_lseek,
 	.read_iter	= read_iter_zero,
 	.write		= write_full,
diff --git drivers/tty/tty_io.c drivers/tty/tty_io.c
index 6616d4a0d..36df942d9 100644
--- drivers/tty/tty_io.c
+++ drivers/tty/tty_io.c
@@ -471,7 +471,7 @@ static void tty_show_fdinfo(struct seq_file *m, struct file *file)
 		tty->ops->show_fdinfo(tty, m);
 }
 
-static const struct file_operations tty_fops = {
+const struct file_operations tty_fops = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= tty_write,
@@ -486,7 +486,7 @@ static const struct file_operations tty_fops = {
 	.show_fdinfo	= tty_show_fdinfo,
 };
 
-static const struct file_operations console_fops = {
+const struct file_operations console_fops = {
 	.llseek		= no_llseek,
 	.read_iter	= tty_read,
 	.write_iter	= redirected_tty_write,
diff --git fs/exec.c fs/exec.c
index 7d424337b..de4674ff1 100644
--- fs/exec.c
+++ fs/exec.c
@@ -47,6 +47,7 @@
 #include <linux/personality.h>
 #include <linux/binfmts.h>
 #include <linux/utsname.h>
+#include <linux/mem_namespace.h>
 #include <linux/pid_namespace.h>
 #include <linux/module.h>
 #include <linux/namei.h>
@@ -75,6 +76,8 @@
 
 #include <trace/events/sched.h>
 
+#include <bhv/domain.h>
+
 static int bprm_creds_from_file(struct linux_binprm *bprm);
 
 int suid_dumpable = 0;
@@ -277,7 +280,12 @@ static int __bprm_mm_init(struct linux_binprm *bprm)
 	if (err)
 		goto err;
 
+	err = bhv_domain_register_vma(mm, vma, false);
+	if (err)
+		goto err;
+
 	mm->stack_vm = mm->total_vm = 1;
+
 	mmap_write_unlock(mm);
 	bprm->p = vma->vm_end - sizeof(void *);
 	return 0;
@@ -853,6 +861,12 @@ int setup_arg_pages(struct linux_binprm *bprm,
 #endif
 	current->mm->start_stack = bprm->p;
 	ret = expand_stack(vma, stack_base);
+	if (ret) {
+		ret = -EFAULT;
+		goto out_unlock;
+	}
+
+	ret = bhv_domain_register_vma(mm, vma, false);
 	if (ret)
 		ret = -EFAULT;
 
diff --git fs/libfs.c fs/libfs.c
index 51b4de3b3..b6d025c11 100644
--- fs/libfs.c
+++ fs/libfs.c
@@ -1322,7 +1322,7 @@ static int empty_dir_readdir(struct file *file, struct dir_context *ctx)
 	return 0;
 }
 
-static const struct file_operations empty_dir_operations = {
+const struct file_operations empty_dir_operations = {
 	.llseek		= empty_dir_llseek,
 	.read		= generic_read_dir,
 	.iterate_shared	= empty_dir_readdir,
diff --git fs/proc/base.c fs/proc/base.c
index 300d53ee7..d4ef4a291 100644
--- fs/proc/base.c
+++ fs/proc/base.c
@@ -373,7 +373,7 @@ static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_pid_cmdline_ops = {
+const struct file_operations proc_pid_cmdline_ops = {
 	.read	= proc_pid_cmdline_read,
 	.llseek	= generic_file_llseek,
 };
@@ -537,7 +537,7 @@ static ssize_t lstats_write(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_lstats_operations = {
+const struct file_operations proc_lstats_operations = {
 	.open		= lstats_open,
 	.read		= seq_read,
 	.write		= lstats_write,
@@ -786,7 +786,7 @@ static int proc_single_open(struct inode *inode, struct file *filp)
 	return single_open(filp, proc_single_show, inode);
 }
 
-static const struct file_operations proc_single_file_operations = {
+const struct file_operations proc_single_file_operations = {
 	.open		= proc_single_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -926,7 +926,7 @@ static int mem_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_mem_operations = {
+const struct file_operations proc_mem_operations = {
 	.llseek		= mem_lseek,
 	.read		= mem_read,
 	.write		= mem_write,
@@ -1002,7 +1002,7 @@ static ssize_t environ_read(struct file *file, char __user *buf,
 	return ret;
 }
 
-static const struct file_operations proc_environ_operations = {
+const struct file_operations proc_environ_operations = {
 	.open		= environ_open,
 	.read		= environ_read,
 	.llseek		= generic_file_llseek,
@@ -1029,7 +1029,7 @@ static ssize_t auxv_read(struct file *file, char __user *buf,
 				       nwords * sizeof(mm->saved_auxv[0]));
 }
 
-static const struct file_operations proc_auxv_operations = {
+const struct file_operations proc_auxv_operations = {
 	.open		= auxv_open,
 	.read		= auxv_read,
 	.llseek		= generic_file_llseek,
@@ -1189,7 +1189,7 @@ static ssize_t oom_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_adj_operations = {
+const struct file_operations proc_oom_adj_operations = {
 	.read		= oom_adj_read,
 	.write		= oom_adj_write,
 	.llseek		= generic_file_llseek,
@@ -1240,7 +1240,7 @@ static ssize_t oom_score_adj_write(struct file *file, const char __user *buf,
 	return err < 0 ? err : count;
 }
 
-static const struct file_operations proc_oom_score_adj_operations = {
+const struct file_operations proc_oom_score_adj_operations = {
 	.read		= oom_score_adj_read,
 	.write		= oom_score_adj_write,
 	.llseek		= default_llseek,
@@ -1308,7 +1308,7 @@ static ssize_t proc_loginuid_write(struct file * file, const char __user * buf,
 	return count;
 }
 
-static const struct file_operations proc_loginuid_operations = {
+const struct file_operations proc_loginuid_operations = {
 	.read		= proc_loginuid_read,
 	.write		= proc_loginuid_write,
 	.llseek		= generic_file_llseek,
@@ -1330,7 +1330,7 @@ static ssize_t proc_sessionid_read(struct file * file, char __user * buf,
 	return simple_read_from_buffer(buf, count, ppos, tmpbuf, length);
 }
 
-static const struct file_operations proc_sessionid_operations = {
+const struct file_operations proc_sessionid_operations = {
 	.read		= proc_sessionid_read,
 	.llseek		= generic_file_llseek,
 };
@@ -1385,7 +1385,7 @@ static ssize_t proc_fault_inject_write(struct file * file,
 	return count;
 }
 
-static const struct file_operations proc_fault_inject_operations = {
+const struct file_operations proc_fault_inject_operations = {
 	.read		= proc_fault_inject_read,
 	.write		= proc_fault_inject_write,
 	.llseek		= generic_file_llseek,
@@ -1426,7 +1426,7 @@ static ssize_t proc_fail_nth_read(struct file *file, char __user *buf,
 	return simple_read_from_buffer(buf, count, ppos, numbuf, len);
 }
 
-static const struct file_operations proc_fail_nth_operations = {
+const struct file_operations proc_fail_nth_operations = {
 	.read		= proc_fail_nth_read,
 	.write		= proc_fail_nth_write,
 };
@@ -1475,7 +1475,7 @@ static int sched_open(struct inode *inode, struct file *filp)
 	return single_open(filp, sched_show, inode);
 }
 
-static const struct file_operations proc_pid_sched_operations = {
+const struct file_operations proc_pid_sched_operations = {
 	.open		= sched_open,
 	.read		= seq_read,
 	.write		= sched_write,
@@ -1550,7 +1550,7 @@ static int sched_autogroup_open(struct inode *inode, struct file *filp)
 	return ret;
 }
 
-static const struct file_operations proc_pid_sched_autogroup_operations = {
+const struct file_operations proc_pid_sched_autogroup_operations = {
 	.open		= sched_autogroup_open,
 	.read		= seq_read,
 	.write		= sched_autogroup_write,
@@ -1653,7 +1653,7 @@ static int timens_offsets_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timens_offsets_show, inode);
 }
 
-static const struct file_operations proc_timens_offsets_operations = {
+const struct file_operations proc_timens_offsets_operations = {
 	.open		= timens_offsets_open,
 	.read		= seq_read,
 	.write		= timens_offsets_write,
@@ -1712,7 +1712,7 @@ static int comm_open(struct inode *inode, struct file *filp)
 	return single_open(filp, comm_show, inode);
 }
 
-static const struct file_operations proc_pid_set_comm_operations = {
+const struct file_operations proc_pid_set_comm_operations = {
 	.open		= comm_open,
 	.read		= seq_read,
 	.write		= comm_write,
@@ -2430,7 +2430,7 @@ proc_map_files_readdir(struct file *file, struct dir_context *ctx)
 	return ret;
 }
 
-static const struct file_operations proc_map_files_operations = {
+const struct file_operations proc_map_files_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_map_files_readdir,
 	.llseek		= generic_file_llseek,
@@ -2529,7 +2529,7 @@ static int proc_timers_open(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_timers_operations = {
+const struct file_operations proc_timers_operations = {
 	.open		= proc_timers_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
@@ -2621,7 +2621,7 @@ static int timerslack_ns_open(struct inode *inode, struct file *filp)
 	return single_open(filp, timerslack_ns_show, inode);
 }
 
-static const struct file_operations proc_pid_set_timerslack_ns_operations = {
+const struct file_operations proc_pid_set_timerslack_ns_operations = {
 	.open		= timerslack_ns_open,
 	.read		= seq_read,
 	.write		= timerslack_ns_write,
@@ -2794,7 +2794,7 @@ static ssize_t proc_pid_attr_write(struct file * file, const char __user * buf,
 	return rv;
 }
 
-static const struct file_operations proc_pid_attr_operations = {
+const struct file_operations proc_pid_attr_operations = {
 	.open		= proc_pid_attr_open,
 	.read		= proc_pid_attr_read,
 	.write		= proc_pid_attr_write,
@@ -2870,7 +2870,7 @@ static int proc_attr_dir_readdir(struct file *file, struct dir_context *ctx)
 				   attr_dir_stuff, ARRAY_SIZE(attr_dir_stuff));
 }
 
-static const struct file_operations proc_attr_dir_operations = {
+const struct file_operations proc_attr_dir_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_attr_dir_readdir,
 	.llseek		= generic_file_llseek,
@@ -2962,7 +2962,7 @@ static ssize_t proc_coredump_filter_write(struct file *file,
 	return count;
 }
 
-static const struct file_operations proc_coredump_filter_operations = {
+const struct file_operations proc_coredump_filter_operations = {
 	.read		= proc_coredump_filter_read,
 	.write		= proc_coredump_filter_write,
 	.llseek		= generic_file_llseek,
@@ -3085,7 +3085,7 @@ static int proc_projid_map_open(struct inode *inode, struct file *file)
 	return proc_id_map_open(inode, file, &proc_projid_seq_operations);
 }
 
-static const struct file_operations proc_uid_map_operations = {
+const struct file_operations proc_uid_map_operations = {
 	.open		= proc_uid_map_open,
 	.write		= proc_uid_map_write,
 	.read		= seq_read,
@@ -3093,7 +3093,7 @@ static const struct file_operations proc_uid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_gid_map_operations = {
+const struct file_operations proc_gid_map_operations = {
 	.open		= proc_gid_map_open,
 	.write		= proc_gid_map_write,
 	.read		= seq_read,
@@ -3101,7 +3101,7 @@ static const struct file_operations proc_gid_map_operations = {
 	.release	= proc_id_map_release,
 };
 
-static const struct file_operations proc_projid_map_operations = {
+const struct file_operations proc_projid_map_operations = {
 	.open		= proc_projid_map_open,
 	.write		= proc_projid_map_write,
 	.read		= seq_read,
@@ -3152,7 +3152,7 @@ static int proc_setgroups_release(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static const struct file_operations proc_setgroups_operations = {
+const struct file_operations proc_setgroups_operations = {
 	.open		= proc_setgroups_open,
 	.write		= proc_setgroups_write,
 	.read		= seq_read,
@@ -3199,7 +3199,7 @@ static int proc_stack_depth(struct seq_file *m, struct pid_namespace *ns,
 /*
  * Thread groups
  */
-static const struct file_operations proc_task_operations;
+const struct file_operations proc_task_operations;
 static const struct inode_operations proc_task_inode_operations;
 
 static const struct pid_entry tgid_base_stuff[] = {
@@ -3320,7 +3320,7 @@ static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
 				   tgid_base_stuff, ARRAY_SIZE(tgid_base_stuff));
 }
 
-static const struct file_operations proc_tgid_base_operations = {
+const struct file_operations proc_tgid_base_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tgid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3661,7 +3661,7 @@ static struct dentry *proc_tid_base_lookup(struct inode *dir, struct dentry *den
 				  tid_base_stuff + ARRAY_SIZE(tid_base_stuff));
 }
 
-static const struct file_operations proc_tid_base_operations = {
+const struct file_operations proc_tid_base_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_tid_base_readdir,
 	.llseek		= generic_file_llseek,
@@ -3870,7 +3870,7 @@ static const struct inode_operations proc_task_inode_operations = {
 	.permission	= proc_pid_permission,
 };
 
-static const struct file_operations proc_task_operations = {
+const struct file_operations proc_task_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_task_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/fd.c fs/proc/fd.c
index 913bef0d2..be25b0890 100644
--- fs/proc/fd.c
+++ fs/proc/fd.c
@@ -99,7 +99,7 @@ static int seq_fdinfo_open(struct inode *inode, struct file *file)
 	return single_open(file, seq_show, inode);
 }
 
-static const struct file_operations proc_fdinfo_file_operations = {
+const struct file_operations proc_fdinfo_file_operations = {
 	.open		= seq_fdinfo_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
diff --git fs/proc/generic.c fs/proc/generic.c
index d32f69aaa..30fb7d46a 100644
--- fs/proc/generic.c
+++ fs/proc/generic.c
@@ -338,7 +338,7 @@ int proc_readdir(struct file *file, struct dir_context *ctx)
  * use the in-memory "struct proc_dir_entry" tree to parse
  * the /proc directory.
  */
-static const struct file_operations proc_dir_operations = {
+const struct file_operations proc_dir_operations = {
 	.llseek			= generic_file_llseek,
 	.read			= generic_read_dir,
 	.iterate_shared		= proc_readdir,
diff --git fs/proc/inode.c fs/proc/inode.c
index 599eb724f..a6e6a7a52 100644
--- fs/proc/inode.c
+++ fs/proc/inode.c
@@ -567,7 +567,7 @@ static int proc_reg_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
-static const struct file_operations proc_reg_file_ops = {
+const struct file_operations proc_reg_file_ops = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -579,7 +579,7 @@ static const struct file_operations proc_reg_file_ops = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops = {
+const struct file_operations proc_iter_file_ops = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.write		= proc_reg_write,
@@ -593,7 +593,7 @@ static const struct file_operations proc_iter_file_ops = {
 };
 
 #ifdef CONFIG_COMPAT
-static const struct file_operations proc_reg_file_ops_compat = {
+const struct file_operations proc_reg_file_ops_compat = {
 	.llseek		= proc_reg_llseek,
 	.read		= proc_reg_read,
 	.write		= proc_reg_write,
@@ -606,7 +606,7 @@ static const struct file_operations proc_reg_file_ops_compat = {
 	.release	= proc_reg_release,
 };
 
-static const struct file_operations proc_iter_file_ops_compat = {
+const struct file_operations proc_iter_file_ops_compat = {
 	.llseek		= proc_reg_llseek,
 	.read_iter	= proc_reg_read_iter,
 	.splice_read	= generic_file_splice_read,
diff --git fs/proc/namespaces.c fs/proc/namespaces.c
index 8e159fc78..d0272143b 100644
--- fs/proc/namespaces.c
+++ fs/proc/namespaces.c
@@ -11,7 +11,6 @@
 #include <linux/user_namespace.h>
 #include "internal.h"
 
-
 static const struct proc_ns_operations *ns_entries[] = {
 #ifdef CONFIG_NET_NS
 	&netns_operations,
@@ -23,8 +22,7 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&ipcns_operations,
 #endif
 #ifdef CONFIG_PID_NS
-	&pidns_operations,
-	&pidns_for_children_operations,
+	&pidns_operations,    &pidns_for_children_operations,
 #endif
 #ifdef CONFIG_USER_NS
 	&userns_operations,
@@ -34,8 +32,10 @@ static const struct proc_ns_operations *ns_entries[] = {
 	&cgroupns_operations,
 #endif
 #ifdef CONFIG_TIME_NS
-	&timens_operations,
-	&timens_for_children_operations,
+	&timens_operations,   &timens_for_children_operations,
+#endif
+#ifdef CONFIG_MEM_NS
+	&memns_operations,
 #endif
 };
 
diff --git fs/proc/proc_sysctl.c fs/proc/proc_sysctl.c
index 013fc5931..6ea0e6382 100644
--- fs/proc/proc_sysctl.c
+++ fs/proc/proc_sysctl.c
@@ -19,9 +19,9 @@
 #include "internal.h"
 
 static const struct dentry_operations proc_sys_dentry_operations;
-static const struct file_operations proc_sys_file_operations;
+const struct file_operations proc_sys_file_operations;
 static const struct inode_operations proc_sys_inode_operations;
-static const struct file_operations proc_sys_dir_file_operations;
+const struct file_operations proc_sys_dir_file_operations;
 static const struct inode_operations proc_sys_dir_operations;
 
 /* shared constants to be used in various sysctls */
@@ -846,7 +846,7 @@ static int proc_sys_getattr(struct user_namespace *mnt_userns,
 	return 0;
 }
 
-static const struct file_operations proc_sys_file_operations = {
+const struct file_operations proc_sys_file_operations = {
 	.open		= proc_sys_open,
 	.poll		= proc_sys_poll,
 	.read_iter	= proc_sys_read,
@@ -856,7 +856,7 @@ static const struct file_operations proc_sys_file_operations = {
 	.llseek		= default_llseek,
 };
 
-static const struct file_operations proc_sys_dir_file_operations = {
+const struct file_operations proc_sys_dir_file_operations = {
 	.read		= generic_read_dir,
 	.iterate_shared	= proc_sys_readdir,
 	.llseek		= generic_file_llseek,
diff --git fs/proc/root.c fs/proc/root.c
index c7e3b1350..9f268fa36 100644
--- fs/proc/root.c
+++ fs/proc/root.c
@@ -342,7 +342,7 @@ static int proc_root_readdir(struct file *file, struct dir_context *ctx)
  * <pid> directories. Thus we don't use the generic
  * directory handling functions for that..
  */
-static const struct file_operations proc_root_operations = {
+const struct file_operations proc_root_operations = {
 	.read		 = generic_read_dir,
 	.iterate_shared	 = proc_root_readdir,
 	.llseek		= generic_file_llseek,
diff --git include/asm-generic/sections.h include/asm-generic/sections.h
index 72f1e2a8c..5201954a9 100644
--- include/asm-generic/sections.h
+++ include/asm-generic/sections.h
@@ -58,6 +58,14 @@ extern char __noinstr_text_start[], __noinstr_text_end[];
 
 extern __visible const void __nosave_begin, __nosave_end;
 
+#ifdef CONFIG_BHV_VAS
+extern char _sexittext[], _eexittext[];
+extern char __bhv_text_start[];
+extern char __bhv_text_end[];
+extern char __bhv_data_start[];
+extern char __bhv_data_end[];
+#endif
+
 /* Function descriptor handling (if any).  Override in asm/sections.h */
 #ifndef dereference_function_descriptor
 #define dereference_function_descriptor(p) ((void *)(p))
diff --git include/asm-generic/vmlinux.lds.h include/asm-generic/vmlinux.lds.h
index 9eac202fb..35d9b4585 100644
--- include/asm-generic/vmlinux.lds.h
+++ include/asm-generic/vmlinux.lds.h
@@ -641,6 +641,17 @@
 		*(.static_call.text)					\
 		__static_call_text_end = .;
 
+#ifdef CONFIG_BHV_VAS
+#define BHV_TEXT							\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_start = .;				\
+		*(.bhv.text)						\
+		. = ALIGN(PAGE_SIZE);				\
+		__bhv_text_end = .;
+#else
+#define BHV_TEXT
+#endif
+
 /* Section used for early init (in .S files) */
 #define HEAD_TEXT  KEEP(*(.head.text))
 
diff --git include/bhv/acl.h include/bhv/acl.h
new file mode 100644
index 000000000..17a08085a
--- /dev/null
+++ include/bhv/acl.h
@@ -0,0 +1,61 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_ACL_H__
+#define __BHV_ACL_H__
+
+#if defined CONFIG_BHV_VAS && !defined VASKM
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+void __init bhv_acl_mm_init(void);
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_PROC_ACL, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_DRIVER_ACL, bhv_configuration_bitmap);
+}
+
+bool bhv_block_driver(const char *target);
+bool bhv_block_process(const char *target);
+
+#else // defined CONFIG_BHV_VAS && !defined VASKM
+
+static inline bool bhv_acl_is_proc_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_acl_is_driver_acl_enabled(void)
+{
+	return false;
+}
+
+static inline bool bhv_block_driver(const char *target)
+{
+	return false;
+}
+
+static inline bool bhv_block_process(const char *target)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS && !defined VASKM
+#endif /* __BHV_ACL_H__ */
\ No newline at end of file
diff --git include/bhv/bhv.h include/bhv/bhv.h
new file mode 100644
index 000000000..03a83c958
--- /dev/null
+++ include/bhv/bhv.h
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_H__
+#define __BHV_BHV_H__
+
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/mm.h>
+#include <asm/bug.h>
+#include <asm/io.h>
+
+#define __bhv_text __section(".bhv.text") noinline
+
+#ifndef VASKM // inside kernel tree
+#define __bhv_data __section(".bhv.data") noinline
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+#define __init_km
+#else // out of tree
+#define __init_km __init
+#endif // VASKM
+
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+#define bhv_fail(fmt, ...) panic(fmt, ##__VA_ARGS__)
+#else
+#define bhv_fail(fmt, ...) pr_err(fmt, ##__VA_ARGS__)
+#endif
+
+#ifdef CONFIG_BHV_VAS
+extern bool bhv_initialized __ro_after_init;
+extern unsigned long *bhv_configuration_bitmap __ro_after_init;
+
+static inline bool is_bhv_initialized(void)
+{
+	BUG_ON(bhv_initialized && bhv_configuration_bitmap == NULL);
+	return bhv_initialized;
+}
+
+void __init bhv_mm_init(void);
+
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr);
+
+
+extern bool __bhv_init_done;
+
+/**
+ * \brief General virtual to physical translation function.
+ *
+ * NOTE: Currently, does not support huge pages.
+ *
+ * \param address The address to translate.
+ * \return phys_addr_t The physical address.
+ */
+static inline phys_addr_t bhv_virt_to_phys(void *address)
+{
+	BUG_ON(!address);
+
+	if (__bhv_init_done && is_vmalloc_or_module_addr(address)) {
+		struct page *p = bhv_vmalloc_to_page(address);
+		uint64_t offset = (uint64_t)address & (PAGE_SIZE - 1);
+
+		BUG_ON(!p);
+		BUG_ON(PageCompound(p));
+
+		return (page_to_pfn(p) << PAGE_SHIFT) | offset;
+	} else {
+		return virt_to_phys(address);
+	}
+}
+#else /* CONFIG_BHV_VAS */
+static inline bool is_bhv_initialized(void)
+{
+	return false;
+}
+static inline void bhv_mm_init(void)
+{
+}
+
+static inline phys_addr_t bhv_virt_to_phys(volatile void *address)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_H__ */
diff --git include/bhv/bhv_print.h include/bhv/bhv_print.h
new file mode 100644
index 000000000..35d267d47
--- /dev/null
+++ include/bhv/bhv_print.h
@@ -0,0 +1,26 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_BHV_PRINT_H__
+#define __BHV_BHV_PRINT_H__
+
+#ifdef CONFIG_BHV_VAS
+
+// Common print prefix
+#ifndef pr_fmt
+#define pr_fmt(fmt) "[BHV-VAS] " fmt
+#endif
+
+#ifdef CONFIG_BHV_VAS_DEBUG
+#define bhv_debug(fmt, ...)                                                    \
+	printk(KERN_DEBUG pr_fmt("[DEBUG] " fmt), ##__VA_ARGS__)
+#else
+#define bhv_debug(fmt, ...) no_printk(KERN_DEBUG pr_fmt(fmt), ##__VA_ARGS__)
+#endif /* CONFIG_BHV_VAS_DEBUG */
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_BHV_PRINT_H__ */
\ No newline at end of file
diff --git include/bhv/creds.h include/bhv/creds.h
new file mode 100644
index 000000000..a35690733
--- /dev/null
+++ include/bhv/creds.h
@@ -0,0 +1,88 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_CREDS_H__
+#define __BHV_CREDS_H__
+
+#include <linux/sched.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/init.h>
+
+#if defined CONFIG_BHV_VAS
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_CREDS, bhv_configuration_bitmap);
+}
+
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags);
+#ifdef VASKM // out of tree
+int bhv_cred_assign_init(struct task_struct *t);
+#endif // VASKM
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon);
+void bhv_cred_commit(struct cred *c);
+void bhv_cred_release(struct cred *c);
+int bhv_cred_verify(struct task_struct *t);
+
+int __init bhv_cred_init(void);
+void __init bhv_cred_mm_init(void);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline int bhv_cred_init(void)
+{
+	return 0;
+}
+
+static inline bool bhv_cred_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return 0;
+}
+
+#ifdef VASKM // out of tree
+static inline int bhv_cred_assign_init(struct task_struct *t) {
+	return 0;
+}
+#endif // VASKM
+
+static inline int bhv_cred_assign_priv(struct cred *c, struct task_struct *d)
+{
+	return 0;
+}
+
+static inline void bhv_cred_commit(struct cred *c)
+{
+}
+
+static inline void bhv_cred_release(struct cred *c)
+{
+}
+
+static inline int bhv_cred_verify(struct task_struct *t)
+{
+	return 0;
+}
+
+static inline void bhv_cred_mm_init(void)
+{
+}
+
+
+
+#endif // defined CONFIG_BHV_VAS
+
+#endif /* __BHV_CREDS_H__ */
diff --git include/bhv/domain.h include/bhv/domain.h
new file mode 100644
index 000000000..878b56cd4
--- /dev/null
+++ include/bhv/domain.h
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_DOMAIN_H__
+#define __BHV_DOMAIN_H__
+
+#include <linux/mem_namespace.h>
+#include <linux/sched.h>
+#include <linux/sched/task.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/init.h>
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+#include <asm/bhv/domain.h>
+#endif
+
+#ifdef CONFIG_MEM_NS
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return !!test_bit(BHV_CONFIG_STRONG_ISOLATION,
+			  bhv_configuration_bitmap);
+}
+int bhv_domain_mm_init(void);
+
+int bhv_domain_create(uint64_t *domid);
+void bhv_domain_destroy(uint64_t domid);
+int bhv_domain_switch(uint64_t domid, bool in_atomic);
+
+int bhv_domain_register_vma(struct mm_struct *const mm,
+			    struct vm_area_struct *const vma, bool in_atomic);
+int bhv_domain_update_vma(struct mm_struct *const mm,
+			  struct vm_area_struct *const vma,
+			  struct page *const p, size_t count, bool in_atomic);
+int bhv_domain_dup_mmap(struct mm_struct *const old_mm,
+			struct mm_struct *const new_mm);
+int bhv_domain_move_mm(struct mm_struct *const mm, struct nsproxy *const old_ns,
+		       struct nsproxy *const new_ns);
+int bhv_domain_swap_page(uint64_t pfn);
+
+void bhv_domain_release_vma(struct vm_area_struct *const vma);
+void bhv_domain_release_mm(struct mm_struct *const mm);
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	/*
+	 * Kernel threads, and threads that do not act on behalf of a user space
+	 * task, do not have a valid nsproxy. These threads shall switch to the
+	 * default domain that we use for the init_task. Alternatively, we can
+	 * define a dedicated domain, which all kernel threads will enter if
+	 * they do not execute on behalf of a user space task.
+	 */
+	if (task->nsproxy == NULL)
+		return init_task.nsproxy->mem_ns->domain;
+
+	return task->nsproxy->mem_ns->domain;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next,
+				    bool in_atomic)
+{
+	bhv_domain_switch(bhv_get_domain(next), in_atomic);
+}
+
+static inline pgd_t *bhv_domain_get_user_pgd(const pgd_t *pgd)
+{
+	pgd_t *pgd_normalized = (pgd_t *)pgd;
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	/*
+	 * We need to ensure that the kernel was not configured to disable KPTI,
+	 * despite CONFIG_PAGE_TABLE_ISOLATION being set. Only then, we can
+	 * normalize the PGD pointer. The normalized PGD pointer ensures that
+	 * BRASS becomes able to always associate both user and kernel memory
+	 * accesses with the virtual address space, and the domain it belongs
+	 * to.
+	 */
+	if (static_cpu_has(X86_FEATURE_PTI))
+		pgd_normalized = bhv_domain_arch_get_user_pgd(pgd_normalized);
+#endif
+	return pgd_normalized;
+}
+
+#else /* CONFIG_MEM_NS */
+
+static inline bool bhv_domain_is_enabled(void)
+{
+	return false;
+}
+
+static inline int bhv_domain_create(uint64_t *domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_destroy(uint64_t domid)
+{
+	return 0;
+}
+
+static inline int bhv_domain_switch(uint64_t domid, bool in_atomic)
+{
+	return 0;
+}
+
+static inline int bhv_domain_register_vma(struct mm_struct *const mm,
+					  struct vm_area_struct *const vma,
+					  bool in_atomic)
+{
+	return 0;
+}
+
+static inline int bhv_domain_update_vma(struct mm_struct *const mm,
+					struct vm_area_struct *const vma,
+					struct page *const p, size_t count,
+					bool in_atomic)
+{
+	return 0;
+}
+
+static inline void bhv_domain_release_vma(struct vm_area_struct *const vma)
+{
+}
+
+static inline void bhv_domain_release_mm(struct mm_struct *const vma)
+{
+}
+
+static inline int bhv_domain_dup_mmap(struct mm_struct *const old_mm,
+				      struct mm_struct *const new_mm)
+{
+	return 0;
+}
+
+static inline int bhv_domain_move_mm(struct mm_struct *const mm,
+				     struct nsproxy *const old_ns,
+				     struct nsproxy *const new_ns)
+{
+	return 0;
+}
+
+static inline int bhv_domain_swap_page(uint64_t pfn)
+{
+	return 0;
+}
+
+static inline void bhv_domain_enter(const struct task_struct *next,
+				    bool in_atomic)
+{
+}
+
+static inline uint64_t bhv_get_domain(const struct task_struct *task)
+{
+	return 0;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* __BHV_DOMAIN_H__ */
diff --git include/bhv/event.h include/bhv/event.h
new file mode 100644
index 000000000..48a576cd1
--- /dev/null
+++ include/bhv/event.h
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_EVENT_H__
+#define __BHV_EVENT_H__
+
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/dcache.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/event.h>
+
+static inline int get_path_from_task(struct task_struct *task, char *buf,
+				     bool get_full_path)
+{
+	int rv;
+	char tmp_buf[EVENT_MAX_PATH_SZ];
+	char *path = NULL;
+
+	if (get_full_path && task->active_mm != NULL &&
+	    task->active_mm->exe_file != NULL) {
+		path = d_path(&task->active_mm->exe_file->f_path, tmp_buf,
+			      EVENT_MAX_PATH_SZ);
+
+		if (IS_ERR(path)) {
+			// Cleanup
+			rv = PTR_ERR(path);
+			return rv;
+		} else {
+			strncpy(buf, path, EVENT_MAX_PATH_SZ);
+			buf[EVENT_MAX_PATH_SZ - 1] = '\0';
+		}
+
+	} else {
+		strncpy(buf, task->comm, EVENT_MAX_PATH_SZ);
+		buf[EVENT_MAX_PATH_SZ - 1] = '\0';
+	}
+
+	return 0;
+}
+
+static inline int populate_event_context(bhv_event_context_t *context,
+					 bool get_full_path)
+{
+	int rv;
+
+	BUG_ON(context == NULL);
+
+	context->vcpu_id = raw_smp_processor_id();
+	context->uid = current_uid().val;
+	context->euid = current_euid().val;
+	context->gid = current_gid().val;
+	context->egid = current_egid().val;
+	memcpy(&context->cap_effective, &current_cred_xxx(cap_effective).cap[0],
+	       sizeof(context->cap_effective));
+	memcpy(&context->cap_permitted, &current_cred_xxx(cap_permitted).cap[0],
+	       sizeof(context->cap_permitted));
+	context->pid = current->tgid;
+
+	rv = get_path_from_task(current, context->comm, get_full_path);
+	if (rv != 0) {
+		context->valid = false;
+		return rv;
+	}
+
+	if (current->real_parent != NULL) {
+		context->parent_pid = current->real_parent->tgid;
+
+		rv = get_path_from_task(current->real_parent,
+					context->parent_comm, get_full_path);
+		if (rv != 0) {
+			context->valid = false;
+			return rv;
+		}
+	} else {
+		context->parent_pid = 0;
+		context->parent_comm[0] = '\0';
+	}
+
+	context->valid = true;
+	return 0;
+}
+
+#endif /* __BHV_EVENT_H__ */
\ No newline at end of file
diff --git include/bhv/file_protection.h include/bhv/file_protection.h
new file mode 100644
index 000000000..967b39f66
--- /dev/null
+++ include/bhv/file_protection.h
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILE_PROTECTION_H__
+#define __BHV_FILE_PROTECTION_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/init.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/file_protection.h>
+
+extern bhv_file_protection_config_t bhv_file_protection_config __ro_after_init;
+
+void __init bhv_file_protection_init(void);
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILE_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+static inline bool bhv_read_only_file_protection_is_enabled(void)
+{
+	{
+		if (!bhv_file_protection_is_enabled())
+			return false;
+
+		if (!test_bit(
+			    BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION,
+			    (unsigned long *)&bhv_file_protection_config
+				    .feature_bitmap))
+			return false;
+
+		return true;
+	}
+}
+
+bool bhv_block_read_only_file_write(const char *target);
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_file_protection_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILE_PROTECTION_H__ */
\ No newline at end of file
diff --git include/bhv/fileops_internal.h include/bhv/fileops_internal.h
new file mode 100644
index 000000000..4b52bdffe
--- /dev/null
+++ include/bhv/fileops_internal.h
@@ -0,0 +1,155 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS__
+#define __BHV_FILEOPS__
+
+// used by security/bhv.c
+
+#ifdef CONFIG_BHV_VAS
+#include <linux/fs.h> // simple_dir_operations
+#include <linux/ramfs.h> // ramfs_file_operations
+#include <linux/printk.h> // kmsg_fops
+#include <linux/mnt_namespace.h> // proc_mount{s,stats,info}_operations
+
+#include <bhv/interface/fileops_protection.h>
+
+#ifdef CONFIG_EXT4_FS
+// fs/ext4/ext4.h
+extern const struct file_operations ext4_dir_operations;
+extern const struct file_operations ext4_file_operations;
+#endif
+// mm/shmem.c
+#ifdef CONFIG_SHMEM
+extern const struct file_operations shmem_file_operations;
+#else
+#define shmem_file_operations ramfs_file_operations
+#endif
+// drivers/char/mem.c
+extern const struct file_operations mem_fops;
+extern const struct file_operations null_fops;
+extern const struct file_operations port_fops;
+extern const struct file_operations zero_fops;
+extern const struct file_operations full_fops;
+// drivers/char/random.c
+extern const struct file_operations random_fops;
+extern const struct file_operations urandom_fops;
+// drivers/tty/tty_io.c
+extern const struct file_operations tty_fops;
+extern const struct file_operations console_fops;
+// fs/proc/inode.c
+extern const struct file_operations proc_reg_file_ops;
+extern const struct file_operations proc_iter_file_ops;
+#ifdef CONFIG_COMPAT
+extern const struct file_operations proc_reg_file_ops_compat;
+extern const struct file_operations proc_iter_file_ops_compat;
+#endif
+// fs/proc/root.c
+extern const struct file_operations proc_root_operations;
+// fs/proc/proc_sysctl.c
+extern const struct file_operations proc_sys_file_operations;
+extern const struct file_operations proc_sys_dir_file_operations;
+// fs/proc/fd.c
+extern const struct file_operations proc_fd_operations;
+// fs/proc/base.c
+extern const struct file_operations proc_oom_score_adj_operations;
+extern const struct file_operations proc_pid_cmdline_ops;
+#ifdef CONFIG_LATENCYTOP
+extern const struct file_operations proc_lstats_operations;
+#endif
+extern const struct file_operations proc_mem_operations;
+extern const struct file_operations proc_environ_operations;
+extern const struct file_operations proc_auxv_operations;
+extern const struct file_operations proc_oom_adj_operations;
+extern const struct file_operations proc_loginuid_operations;
+#ifdef CONFIG_AUDIT
+extern const struct file_operations proc_sessionid_operations;
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+extern const struct file_operations proc_fault_inject_operations;
+extern const struct file_operations proc_fail_nth_operations;
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+extern const struct file_operations proc_pid_sched_operations;
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+extern const struct file_operations proc_pid_sched_autogroup_operations;
+#endif
+#ifdef CONFIG_TIME_NS
+extern const struct file_operations proc_timens_offsets_operations;
+#endif
+extern const struct file_operations proc_pid_set_comm_operations;
+extern const struct file_operations proc_map_files_operations;
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+extern const struct file_operations proc_timers_operations;
+#endif
+extern const struct file_operations proc_pid_set_timerslack_ns_operations;
+#ifdef CONFIG_SECURITY
+extern const struct file_operations proc_pid_attr_operations;
+extern const struct file_operations proc_attr_dir_operations;
+#endif
+#ifdef CONFIG_ELF_CORE
+extern const struct file_operations proc_coredump_filter_operations;
+#endif
+#ifdef CONFIG_USER_NS
+extern const struct file_operations proc_uid_map_operations;
+extern const struct file_operations proc_gid_map_operations;
+extern const struct file_operations proc_projid_map_operations;
+extern const struct file_operations proc_setgroups_operations;
+#endif
+extern const struct file_operations proc_tgid_base_operations;
+extern const struct file_operations proc_tid_base_operations;
+extern const struct file_operations proc_task_operations;
+extern const struct file_operations proc_single_file_operations;
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+extern const struct file_operations proc_zram_block_state_op;
+#endif
+#ifdef CONFIG_PAGE_OWNER
+// mm/page_owner.c
+extern const struct file_operations proc_page_owner_operations;
+#endif
+// fs/proc/internal.h
+extern const struct file_operations proc_ns_dir_operations;
+extern const struct file_operations proc_net_operations;
+extern const struct file_operations proc_pid_maps_operations;
+#ifdef CONFIG_NUMA
+extern const struct file_operations proc_pid_numa_maps_operations;
+#endif
+extern const struct file_operations proc_pid_smaps_operations;
+extern const struct file_operations proc_pid_smaps_rollup_operations;
+extern const struct file_operations proc_clear_refs_operations;
+extern const struct file_operations proc_pagemap_operations;
+#ifdef CONFIG_PROC_CHILDREN
+extern const struct file_operations proc_tid_children_operations;
+#endif
+// fs/proc/generic.c
+extern const struct file_operations proc_dir_operations;
+// fs/proc/fd.h
+extern const struct file_operations proc_fdinfo_operations;
+// fs/proc/fd.c
+extern const struct file_operations proc_fdinfo_file_operations;
+
+#ifdef CONFIG_XFS_FS
+// fs/xfs/xfs_iops.h
+extern const struct file_operations xfs_dir_file_operations;
+extern const struct file_operations xfs_file_operations;
+#endif
+
+extern const struct file_operations empty_dir_operations;
+
+typedef const struct file_operations *fops_t[2];
+
+// basic regular file + directory file ops
+extern const fops_t fileops_map[];
+
+// additional /proc/ file operations
+extern struct file_operations const *proc_fops[] __ro_after_init;
+
+void __init bhv_fileops_init(void);
+bool is_valid_proc_fop(const struct file_operations **);
+
+#endif /* CONFIG_BHV_VAS */
+#endif /* __BHV_FILEOPS__ */
diff --git include/bhv/fileops_protection.h include/bhv/fileops_protection.h
new file mode 100644
index 000000000..12e82c80a
--- /dev/null
+++ include/bhv/fileops_protection.h
@@ -0,0 +1,36 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_FILEOPS_PROTECTION_H__
+#define __BHV_FILEOPS_PROTECTION_H__
+
+#if defined CONFIG_BHV_VAS && !defined VASKM
+#include <linux/init.h>
+#include <bhv/interface/common.h>
+
+void __init bhv_fileops_protection_mm_init(void);
+
+static inline bool bhv_fileops_protection_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_FILEOPS_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+bool bhv_block_fileops(const char *, u8, bool);
+u8 bhv_fileops_type(u32 fs_magic);
+
+#else // defined CONFIG_BHV_VAS && !defined VASKM
+
+static inline bool bhv_fileops_protection_is_enabled(void)
+{
+	return false;
+}
+
+#endif // defined CONFIG_BHV_VAS && !defined VASKM
+#endif /* __BHV_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/guestconn.h include/bhv/guestconn.h
new file mode 100644
index 000000000..7c990fa74
--- /dev/null
+++ include/bhv/guestconn.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTCONN_H__
+#define __BHV_GUESTCONN_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/guestconn.h>
+
+int __init bhv_guestconn_init(uint32_t cid, uint32_t port);
+void __init bhv_guestconn_mm_init(void);
+void bhv_guestconn_start(void);
+
+int bhv_guestconn_send(uint16_t type, void *data, size_t size);
+
+#endif /* __BHV_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/guestlog.h include/bhv/guestlog.h
new file mode 100644
index 000000000..96066d276
--- /dev/null
+++ include/bhv/guestlog.h
@@ -0,0 +1,87 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTLOG_H__
+#define __BHV_GUESTLOG_H__
+
+#include <linux/types.h>
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/guestlog.h>
+
+extern bhv_guestlog_config_t bhv_guestlog_config __ro_after_init;
+
+int __init bhv_guestlog_init(void);
+
+static inline bool bhv_guestlog_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_LOGGING, bhv_configuration_bitmap);
+}
+
+static inline bool bhv_guestlog_log_process_events(void)
+{
+	if (!bhv_guestlog_enabled())
+		return false;
+
+	if (!bhv_guestlog_config.valid ||
+	    !test_bit(BHV_GUESTLOG_CONFIG_LOG_PROCESS_EVENTS,
+		      (unsigned long *)&bhv_guestlog_config.log_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline bool bhv_guestlog_log_driver_events(void)
+{
+	if (!bhv_guestlog_enabled())
+		return false;
+
+	if (!bhv_guestlog_config.valid ||
+	    !test_bit(BHV_GUESTLOG_CONFIG_LOG_DRIVER_EVENTS,
+		      (unsigned long *)&bhv_guestlog_config.log_bitmap))
+		return false;
+
+	return true;
+}
+
+static inline uint16_t bhv_guestlog_calc_msg_sz(uint16_t type, size_t buf_sz)
+{
+	switch (type) {
+	case BHV_GUESTLOG_MSG_TYPE_STR:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_str_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_fork_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_exec_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_process_exit_t) + buf_sz;
+	case BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD:
+		return sizeof(guestlog_msg_header_t) +
+		       sizeof(guestlog_msg_driver_load_t) + buf_sz;
+	default:
+		BUG();
+		return 0;
+	}
+}
+
+int bhv_guestlog_log_str(char *fmt, ...);
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm);
+int bhv_guestlog_log_process_exec(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm);
+int bhv_guestlog_log_driver_load(const char *name);
+
+#endif /* __BHV_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/guestpolicy.h include/bhv/guestpolicy.h
new file mode 100644
index 000000000..8f4ebeb4f
--- /dev/null
+++ include/bhv/guestpolicy.h
@@ -0,0 +1,19 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+#ifndef __BHV_GUESTPOLICY_H__
+#define __BHV_GUESTPOLICY_H__
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+
+static inline bool bhv_guest_policy_is_enabled(void)
+{
+	if (!is_bhv_initialized() || bhv_configuration_bitmap == NULL)
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_GUEST_POLICY,
+			      bhv_configuration_bitmap);
+}
+#endif /* __BHV_GUESTPOLICY_H__ */
\ No newline at end of file
diff --git include/bhv/init.h include/bhv/init.h
new file mode 100644
index 000000000..67fe9bdb8
--- /dev/null
+++ include/bhv/init.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INIT_H__
+#define __BHV_INIT_H__
+
+#include <bhv/interface/integrity.h>
+#include <bhv/interface/init.h>
+
+/* This constant must take into account any regions added in bhv_init_hyp_arch(...) */
+#define BHV_INIT_MAX_REGIONS 7
+
+int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size);
+void __init bhv_init_hyp_arch(bhv_mem_region_t *init_phys_mem_regions,
+			      unsigned int *region_counter);
+int bhv_start_hyp(bhv_init_start_config_t *config);
+
+#endif /* __BHV_INIT_H__ */
diff --git include/bhv/integrity.h include/bhv/integrity.h
new file mode 100644
index 000000000..340f862b3
--- /dev/null
+++ include/bhv/integrity.h
@@ -0,0 +1,218 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTEGRITY_H__
+#define __BHV_INTEGRITY_H__
+
+#include <asm/io.h>
+#include <linux/list.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/integrity.h>
+
+#ifdef CONFIG_BHV_VAS
+struct bhv_mem_region_node {
+	bhv_mem_region_t region;
+	struct list_head list;
+};
+typedef struct bhv_mem_region_node bhv_mem_region_node_t;
+
+extern struct kmem_cache *bhv_mem_region_cache;
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_INTEGRITY, bhv_configuration_bitmap);
+}
+
+void __init bhv_integrity_mm_init(void);
+
+int bhv_start_integrity_arch(void);
+
+int bhv_integrity_freeze_events(uint64_t flags);
+int bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+					bhv_mem_region_t *region_head);
+int bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head);
+int bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head);
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner);
+
+static inline void bhv_release_arg_list(struct list_head *head)
+{
+	bhv_mem_region_node_t *entry, *tmp;
+	list_for_each_entry_safe (entry, tmp, head, list)
+		kmem_cache_free(bhv_mem_region_cache, entry);
+}
+
+static inline void bhv_mem_region_create_ctor(bhv_mem_region_t *curr_item,
+					      bhv_mem_region_t *prev_item,
+					      uint64_t addr, uint64_t size,
+					      uint32_t type, uint64_t flags,
+					      const char *label)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_mem_region_t){
+		.bhv_mem_region_create =
+			(bhv_mem_region_create_t){
+				.start_addr = addr,
+				.size = size,
+				.type = type,
+				.flags = flags,
+				.next = BHV_INVALID_PHYS_ADDR,
+			}
+	};
+	strncpy(curr_item->bhv_mem_region_create.label, label,
+		BHV_VAS_INTEGRITY_MAY_LABEL_SZ);
+	curr_item->bhv_mem_region_create
+		.label[BHV_VAS_INTEGRITY_MAY_LABEL_SZ - 1] = '\0';
+
+	if (prev_item)
+		prev_item->bhv_mem_region_create.next =
+			bhv_virt_to_phys(curr_item);
+}
+
+static inline int bhv_link_node_op_create(struct list_head *head, uint64_t addr,
+					  uint64_t size, uint32_t type,
+					  uint64_t flags, const char *label)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	bhv_mem_region_create_ctor(&n->region, NULL, addr, size, type, flags,
+				   label);
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.bhv_mem_region_create.next =
+			bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_update(struct list_head *head, uint64_t addr,
+					  uint32_t type, uint64_t flags)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.bhv_mem_region_update.start_addr = addr;
+	n->region.bhv_mem_region_update.type = type;
+	n->region.bhv_mem_region_update.flags = flags;
+	n->region.bhv_mem_region_update.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		struct bhv_mem_region_node *tail =
+			list_last_entry(head, struct bhv_mem_region_node, list);
+		tail->region.bhv_mem_region_update.next =
+			bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+static inline int bhv_link_node_op_remove(struct list_head *head, uint64_t addr)
+{
+	bhv_mem_region_node_t *n =
+		kmem_cache_alloc(bhv_mem_region_cache, GFP_KERNEL);
+	if (n == NULL) {
+		bhv_fail("BHV: failed to allocate mem region");
+		return -ENOMEM;
+	}
+
+	n->region.bhv_mem_region_remove.start_addr = addr;
+	n->region.bhv_mem_region_remove.next = BHV_INVALID_PHYS_ADDR;
+
+	/*
+	 * XXX: Consider moving the field 'next' out of the union in
+	 * bhv_mem_region_t. This will allow to move the remaining
+	 * list-maintenance operations into the calling function.
+	 */
+
+	if (!list_empty(head)) {
+		bhv_mem_region_node_t *tail =
+			list_last_entry(head, bhv_mem_region_node_t, list);
+		tail->region.bhv_mem_region_remove.next =
+			bhv_virt_to_phys(&n->region);
+	}
+
+	list_add_tail(&n->list, head);
+
+	return 0;
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_integrity_is_enabled(void)
+{
+	return false;
+}
+
+static inline void bhv_integrity_mm_init(void)
+{
+}
+
+static inline int bhv_integrity_freeze_events(uint64_t)
+{
+	return 0;
+}
+
+static inline int
+bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+				    bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int
+bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head)
+{
+	return 0;
+}
+
+static inline int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner)
+{
+	return 0;
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_INTEGRITY_H__ */
diff --git include/bhv/interface/acl.h include/bhv/interface/acl.h
new file mode 100644
index 000000000..d6d60fbfd
--- /dev/null
+++ include/bhv/interface/acl.h
@@ -0,0 +1,36 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_ACL_H__
+#define __BHV_INTERFACE_ACL_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/event.h>
+
+/* BHV VAS ACL BACKEND OPS */
+#define BHV_VAS_ACL_OP_INIT_PROC_ACL 0U
+#define BHV_VAS_ACL_OP_INIT_DRIVER_ACL 1U
+#define BHV_VAS_ACL_OP_VIOLATION_PROC_ACL 2U
+#define BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL 3U
+
+typedef struct {
+	uint8_t valid;
+	uint8_t is_allow;
+	uint16_t num_pages;
+	uint16_t list_len;
+	uint16_t padding;
+	uint64_t list[];
+} __attribute__((__packed__)) bhv_acl_config_t;
+
+typedef struct {
+	bhv_event_context_t context;
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_acl_violation_t;
+#endif /* __BHV_INTERFACE_ACL_H__ */
diff --git include/bhv/interface/common.h include/bhv/interface/common.h
new file mode 100644
index 000000000..1730caa33
--- /dev/null
+++ include/bhv/interface/common.h
@@ -0,0 +1,62 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_COMMON_H__
+#define __BHV_INTERFACE_COMMON_H__
+
+/* BHV VAS ABI version */
+
+#define __BHV_VAS_ABI_VERSION(rel_year, rel_week, rel_extra, internal_info)    \
+	({                                                                     \
+		static_assert((unsigned long)(rel_year) < 0x99);               \
+		static_assert((unsigned long)(rel_week) < 0x53);               \
+		static_assert((unsigned long)(rel_extra) < 0xff);              \
+		static_assert((unsigned long)(internal_info) > 0x00000);       \
+		static_assert((unsigned long)(internal_info) < 0xfffff);       \
+		(unsigned long)0xbedUL << (13 * 4) |                           \
+			(unsigned long)(rel_year) << (11 * 4) |                \
+			(unsigned long)(rel_week) << (9 * 4) |                 \
+			(unsigned long)(rel_extra) << (7 * 4) |                \
+			(unsigned long)(0x00UL) << (5 * 4) |                   \
+			(unsigned long)(internal_info) << (0 * 4);             \
+	})
+
+#define BHV_VAS_ABI_VERSION __BHV_VAS_ABI_VERSION(0x23, 0x17, 0, 0x1d5)
+
+/* BHV Targets */
+
+#define TARGET_BHV_VAS 1
+
+/* BHV VAS Backends */
+
+#define BHV_VAS_BACKEND_INIT 1
+#define BHV_VAS_BACKEND_INTEGRITY 2
+#define BHV_VAS_BACKEND_PATCH 3
+#define BHV_VAS_BACKEND_VAULT 4
+#define BHV_VAS_BACKEND_ACL 5
+#define BHV_VAS_BACKEND_GUESTLOG 6
+#define BHV_VAS_BACKEND_CREDS 7
+#define BHV_VAS_BACKEND_FILE_PROTECTION 8
+#define BHV_VAS_BACKEND_FILEOPS_PROTECTION 9
+#define BHV_VAS_BACKEND_REGISTER_PROTECTION 10
+#define BHV_VAS_BACKEND_DOMAIN 11
+
+/* BHV CONFIGURATION BITS */
+#define BHV_CONFIG_INTEGRITY 0
+#define BHV_CONFIG_PROC_ACL 1
+#define BHV_CONFIG_DRIVER_ACL 2
+#define BHV_CONFIG_LOGGING 3
+#define BHV_CONFIG_CREDS 4
+#define BHV_CONFIG_FILE_PROTECTION 5
+#define BHV_CONFIG_GUEST_POLICY 6
+#define BHV_CONFIG_FILEOPS_PROTECTION 7
+#define BHV_CONFIG_REGISTER_PROTECTION 8
+#define BHV_CONFIG_STRONG_ISOLATION 9
+
+/* Common Defines */
+#define BHV_INVALID_PHYS_ADDR (~0ULL)
+
+#endif /* __BHV_INTERFACE_COMMON_H__ */
diff --git include/bhv/interface/creds.h include/bhv/interface/creds.h
new file mode 100644
index 000000000..3698c2406
--- /dev/null
+++ include/bhv/interface/creds.h
@@ -0,0 +1,93 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_CREDS_H__
+#define __BHV_INTERFACE_CREDS_H__
+
+#include <linux/types.h>
+
+#include <bhv/event.h>
+
+/* BHV VAS CREDS BACKEND OPS */
+#define BHV_VAS_CREDS_OP_CONFIGURE 0U
+#define BHV_VAS_CREDS_OP_REGISTER_INIT_TASK 1U
+#define BHV_VAS_CREDS_OP_ASSIGN 2U
+#define BHV_VAS_CREDS_OP_ASSIGN_PRIV 3U
+#define BHV_VAS_CREDS_OP_COMMIT 4U
+#define BHV_VAS_CREDS_OP_RELEASE 5U
+#define BHV_VAS_CREDS_OP_VERIFICATION 6U
+#define BHV_VAS_CREDS_OP_LOG 7U
+
+enum event_type {
+	EVENT_NONE = 0,
+	CORRUPTION,
+	INVALID_ASSIGNMENT,
+	DOUBLE_ASSIGNMENT,
+	INVALID_COMMIT,
+	DOUBLE_COMMIT,
+	MAX_EVENTS
+};
+
+typedef struct {
+	uint64_t addr;
+	uint64_t cred;
+	uint64_t hmac;
+} __attribute__((__packed__)) bhv_task_cred_t;
+
+typedef struct {
+	bhv_task_cred_t init_task;
+} __attribute__((__packed__)) bhv_creds_init_task_arg_t;
+
+typedef struct {
+	bhv_task_cred_t new_task;
+	bhv_task_cred_t parent;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_arg_t;
+
+typedef struct {
+	uint64_t cred;
+	uint64_t daemon;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_assign_priv_arg_t;
+
+typedef struct {
+	bhv_task_cred_t cur;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_commit_arg_t;
+
+typedef struct {
+	uint64_t cred;
+} __attribute__((__packed__)) bhv_creds_release_arg_t;
+
+typedef struct {
+	bhv_task_cred_t task;
+	uint8_t ret;
+} __attribute__((__packed__)) bhv_creds_verification_arg_t;
+
+typedef struct {
+	bhv_event_context_t context;
+	uint8_t event_type;
+	uint8_t block;
+	uint16_t pad1;
+	uint32_t task_pid;
+	uint64_t task_addr;
+	uint64_t task_cred;
+	char task_name[TASK_COMM_LEN];
+} __attribute__((__packed__)) bhv_creds_log_arg_t;
+
+typedef struct {
+	union {
+		bhv_creds_init_task_arg_t creds_register;
+		bhv_creds_assign_arg_t creds_assign;
+		bhv_creds_assign_priv_arg_t creds_assign_priv;
+		bhv_creds_commit_arg_t creds_commit;
+		bhv_creds_release_arg_t creds_release;
+		bhv_creds_verification_arg_t creds_verify;
+		bhv_creds_log_arg_t creds_log;
+	};
+} __attribute__((__packed__)) bhv_creds_arg_t;
+
+#endif /* __BHV_INTERFACE_CREDS_H__ */
diff --git include/bhv/interface/domain.h include/bhv/interface/domain.h
new file mode 100644
index 000000000..69791e6a9
--- /dev/null
+++ include/bhv/interface/domain.h
@@ -0,0 +1,66 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_DOMAIN_H__
+#define __BHV_INTERFACE_DOMAIN_H__
+
+#include <linux/types.h>
+
+#define BHV_VAS_DOMAIN_OP_CONFIGURE 0U
+#define BHV_VAS_DOMAIN_OP_CREATE 1U
+#define BHV_VAS_DOMAIN_OP_DESTROY 2U
+#define BHV_VAS_DOMAIN_OP_SWITCH 3U
+#define BHV_VAS_DOMAIN_OP_VMA_REGISTER 4U
+#define BHV_VAS_DOMAIN_OP_VMA_UPDATE 5U
+#define BHV_VAS_DOMAIN_OP_VMA_RELEASE 6U
+#define BHV_VAS_DOMAIN_OP_VMA_RELEASE_ALL 7U
+#define BHV_VAS_DOMAIN_OP_CLONE_PGD 8U
+#define BHV_VAS_DOMAIN_OP_MOVE_MM 9U
+#define BHV_VAS_DOMAIN_OP_SWAP_PAGE 10U
+
+typedef struct {
+	uint64_t pti;
+} __attribute__((__packed__)) bhv_domain_config_arg_t;
+
+typedef struct {
+	uint64_t id;
+	uint64_t pgd;
+} __attribute__((__packed__)) bhv_domain_t;
+
+typedef struct {
+	uint64_t start;
+	uint64_t end;
+	uint32_t flags;
+	uint32_t pad1;
+	/*
+     * TODO: perms!
+     * Consider differentiating among code/lib/stack/heap/VDSO/VVAR/VSYSCALL/...?)
+     */
+} __attribute__((__packed__)) bhv_domain_vma_arg_t;
+
+typedef struct {
+	uint64_t pfn;
+	uint64_t count;
+} __attribute__((__packed__)) bhv_domain_pfn_arg_t;
+
+typedef struct {
+	bhv_domain_vma_arg_t vma;
+	bhv_domain_pfn_arg_t pfn;
+} __attribute__((__packed__)) bhv_domain_vma_update_arg_t;
+
+typedef struct {
+	bhv_domain_t domain;
+	union {
+		bhv_domain_config_arg_t config;
+		bhv_domain_vma_arg_t vma;
+		bhv_domain_vma_update_arg_t vma_update;
+		/* XXX: Do we need this? */
+		uint64_t pgd;
+		uint64_t id;
+	};
+} __attribute__((__packed__)) bhv_domain_arg_t;
+
+#endif /* __BHV_INTERFACE_DOMAIN_H__ */
diff --git include/bhv/interface/event.h include/bhv/interface/event.h
new file mode 100644
index 000000000..8cd9e67da
--- /dev/null
+++ include/bhv/interface/event.h
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_EVENT_H__
+#define __BHV_INTERFACE_EVENT_H__
+
+#include <linux/types.h>
+
+#define EVENT_MAX_PATH_SZ 256
+
+typedef struct {
+	uint8_t valid;
+	uint8_t padding1;
+	uint16_t padding2;
+	uint32_t vcpu_id;
+	uint32_t uid;
+	uint32_t euid;
+	uint32_t gid;
+	uint32_t egid;
+	uint64_t cap_effective;
+	uint64_t cap_permitted;
+	uint32_t pid;
+	uint32_t parent_pid;
+	char comm[EVENT_MAX_PATH_SZ];
+	char parent_comm[EVENT_MAX_PATH_SZ];
+} __attribute__((__packed__)) bhv_event_context_t;
+
+#endif /* __BHV_INTERFACE_EVENT_H__ */
diff --git include/bhv/interface/file_protection.h include/bhv/interface/file_protection.h
new file mode 100644
index 000000000..23b6606f5
--- /dev/null
+++ include/bhv/interface/file_protection.h
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_FILE_PROTECTION_H__
+#define __BHV_INTERFACE_FILE_PROTECTION_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/event.h>
+
+/* BHV VAS FILE PROTECTION BACKEND OPS */
+#define BHV_VAS_FILE_PROTECTION_OP_INIT 0U
+#define BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION 1U
+
+/* BHV VAS FILE PROTECTION FEATURES */
+#define BHV_VAS_FILE_PROTECTION_FEATURE_READ_ONLY_FILE_PROTECTION 0U
+
+#define BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ 256
+
+typedef struct {
+	uint64_t feature_bitmap;
+} __attribute__((__packed__)) bhv_file_protection_config_t;
+
+typedef struct {
+	bhv_event_context_t context;
+	uint64_t name;
+	uint16_t name_len;
+	uint8_t block;
+} __attribute__((__packed__)) bhv_file_protection_violation_t;
+#endif /* __BHV_INTERFACE_FILE_PROTECTION_H__ */
diff --git include/bhv/interface/fileops_protection.h include/bhv/interface/fileops_protection.h
new file mode 100644
index 000000000..f8afb923d
--- /dev/null
+++ include/bhv/interface/fileops_protection.h
@@ -0,0 +1,49 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_FILEOPS_PROTECTION_H__
+#define __BHV_INTERFACE_FILEOPS_PROTECTION_H__
+
+#include <linux/types.h>
+
+#include <bhv/interface/event.h>
+
+/* BHV VAS FILE OPERATIONS PROTECTION BACKEND OPS */
+#define BHV_VAS_FILEOPS_PROTECTION_OP_VIOLATION 0U
+
+/* supported file system and device types */
+#ifdef CONFIG_EXT4_FS
+#define BHV_VAS_FILEOPS_PROTECTION_EXT4 0U
+#endif
+#define BHV_VAS_FILEOPS_PROTECTION_TMPFS 1U
+#define BHV_VAS_FILEOPS_PROTECTION_MEM 2U
+#define BHV_VAS_FILEOPS_PROTECTION_NULL 3U
+#define BHV_VAS_FILEOPS_PROTECTION_PORT 4U
+#define BHV_VAS_FILEOPS_PROTECTION_ZERO 5U
+#define BHV_VAS_FILEOPS_PROTECTION_FULL 6U
+#define BHV_VAS_FILEOPS_PROTECTION_RANDOM 7U
+#define BHV_VAS_FILEOPS_PROTECTION_URANDOM 8U
+#define BHV_VAS_FILEOPS_PROTECTION_KMSG 9U
+#define BHV_VAS_FILEOPS_PROTECTION_TTY 10U
+#define BHV_VAS_FILEOPS_PROTECTION_CONSOLE 11U
+#define BHV_VAS_FILEOPS_PROTECTION_PROC 12U
+#ifdef CONFIG_XFS_FS
+#define BHV_VAS_FILEOPS_PROTECTION_XFS 13U
+#endif
+#define BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED 255U
+
+#define BHV_VAS_FILEOPS_PATH_MAX_SZ 1024
+
+typedef struct {
+	bhv_event_context_t context;
+	uint8_t fops_type;
+	uint8_t is_dir;
+	uint8_t block;
+	char padding[5];
+	char path_name[BHV_VAS_FILEOPS_PATH_MAX_SZ];
+} __attribute__((__packed__)) bhv_fileops_protection_violation_t;
+
+#endif /* __BHV_INTERFACE_FILEOPS_PROTECTION_H__ */
diff --git include/bhv/interface/guestconn.h include/bhv/interface/guestconn.h
new file mode 100644
index 000000000..ca02dccd2
--- /dev/null
+++ include/bhv/interface/guestconn.h
@@ -0,0 +1,28 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_GUESTCONN_H__
+#define __BHV_INTERFACE_GUESTCONN_H__
+
+#include <linux/types.h>
+
+/* BHV GUESTCONN MESSAGE TYPES */
+#define BHV_GUESTCONN_MSG_TYPE_LOG 0U
+
+#define BHV_GUESTCONN_MAX_BODY_SZ 1024
+
+typedef struct {
+	uint16_t type;
+	uint16_t sz;
+	uint8_t body[];
+} __attribute__((__packed__)) guestconn_msg_header_t;
+
+#define BHV_GUESTCONN_MAX_MSG_SZ                                               \
+	(sizeof(guestconn_msg_header_t) + BHV_GUESTCONN_MAX_BODY_SZ)
+
+#endif /* __BHV_INTERFACE_GUESTCONN_H__ */
\ No newline at end of file
diff --git include/bhv/interface/guestlog.h include/bhv/interface/guestlog.h
new file mode 100644
index 000000000..7408a63bc
--- /dev/null
+++ include/bhv/interface/guestlog.h
@@ -0,0 +1,101 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_GUESTLOG_H__
+#define __BHV_INTERFACE_GUESTLOG_H__
+
+#include <bhv/interface/guestconn.h>
+
+#include <linux/types.h>
+
+#include <bhv/interface/event.h>
+
+/* BHV VAS ACL BACKEND OPS */
+#define BHV_VAS_GUESTLOG_OP_INIT_GUESTLOG 0U
+
+/* BHV GUESTLOG MESSAGE TYPES */
+#define BHV_GUESTLOG_MSG_TYPE_STR 0U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK 1U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC 2U
+#define BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT 3U
+#define BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD 4U
+
+typedef struct {
+	uint16_t type;
+	uint16_t sz;
+} __attribute__((__packed__)) guestlog_msg_header_t;
+
+#define BHV_GUESTLOG_MAX_MSG_SZ (BHV_GUESTCONN_MAX_BODY_SZ)
+#define BHV_GUESTLOG_MAX_MSG_DATA_SZ                                           \
+	(BHV_GUESTLOG_MAX_MSG_SZ - sizeof(guestlog_msg_header_t))
+
+typedef struct {
+	bhv_event_context_t context;
+	char buf[];
+} __attribute__((__packed__)) guestlog_msg_str_t;
+static_assert(sizeof(guestlog_msg_str_t) <= BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	bhv_event_context_t context;
+	uint32_t child_pid;
+	uint32_t parent_pid;
+	uint32_t child_comm_offset;
+	uint32_t parent_comm_offset;
+	char buf[];
+} __attribute__((__packed__)) guestlog_msg_process_fork_t;
+static_assert(sizeof(guestlog_msg_process_fork_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	bhv_event_context_t context;
+	uint32_t pid;
+	uint32_t parent_pid;
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_process_exec_t;
+static_assert(sizeof(guestlog_msg_process_exec_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	bhv_event_context_t context;
+	uint32_t pid;
+	uint32_t parent_pid;
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_process_exit_t;
+static_assert(sizeof(guestlog_msg_process_exit_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	bhv_event_context_t context;
+	char name[];
+} __attribute__((__packed__)) guestlog_msg_driver_load_t;
+static_assert(sizeof(guestlog_msg_driver_load_t) <=
+	      BHV_GUESTLOG_MAX_MSG_DATA_SZ);
+
+typedef struct {
+	guestlog_msg_header_t header;
+	union {
+		guestlog_msg_str_t str;
+		guestlog_msg_process_fork_t process_fork;
+		guestlog_msg_process_exec_t process_exec;
+		guestlog_msg_process_exit_t process_exit;
+		guestlog_msg_driver_load_t driver_load;
+		uint8_t __raw_data[BHV_GUESTLOG_MAX_MSG_DATA_SZ];
+	};
+} __attribute__((__packed__)) guestlog_msg_t;
+static_assert(sizeof(guestlog_msg_t) <= BHV_GUESTLOG_MAX_MSG_SZ);
+
+#define BHV_GUESTLOG_CONFIG_LOG_PROCESS_EVENTS 0
+#define BHV_GUESTLOG_CONFIG_LOG_DRIVER_EVENTS 1
+
+typedef struct {
+	uint64_t log_bitmap;
+	uint8_t valid;
+} __attribute__((__packed__)) bhv_guestlog_config_t;
+
+#endif /* __BHV_INTERFACE_GUESTLOG_H__ */
\ No newline at end of file
diff --git include/bhv/interface/hypercall.h include/bhv/interface/hypercall.h
new file mode 100644
index 000000000..5d1881d3c
--- /dev/null
+++ include/bhv/interface/hypercall.h
@@ -0,0 +1,46 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef _ASM_INTERFACE_BHV_HYPERCALL_H
+#define _ASM_INTERFACE_BHV_HYPERCALL_H
+
+#include <linux/kernel.h>
+#include <asm/bhv/hypercall.h>
+#include <asm/io.h>
+
+#include <bhv/interface/common.h>
+
+static __always_inline unsigned long bhv_hypercall_vas(uint32_t backend,
+						       uint32_t op, void *arg)
+{
+	unsigned long rv;
+	uint64_t phys_addr = BHV_INVALID_PHYS_ADDR;
+
+	if (arg != NULL)
+		phys_addr = virt_to_phys(arg);
+
+	rv = BHV_HYPERCALL(TARGET_BHV_VAS, backend, op, BHV_VAS_ABI_VERSION,
+			   phys_addr);
+
+	if (rv) {
+#ifdef CONFIG_BHV_PANIC_ON_FAIL
+		panic("BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %p %llx)",
+		      rv, TARGET_BHV_VAS, backend, op, BHV_VAS_ABI_VERSION, arg,
+		      phys_addr);
+#else
+		pr_warn(
+		       "BHV Hypercall failure! hypercall returned %lu (%u %u %u %lx %p %llx)",
+		       rv, TARGET_BHV_VAS, backend, op, BHV_VAS_ABI_VERSION,
+		       arg, phys_addr);
+#endif /* CONFIG_BHV_PANIC_ON_FAIL */
+	}
+
+	return rv;
+}
+
+#endif /* _ASM_INTERFACE_BHV_HYPERCALL_H */
diff --git include/bhv/interface/init.h include/bhv/interface/init.h
new file mode 100644
index 000000000..aa3597a1d
--- /dev/null
+++ include/bhv/interface/init.h
@@ -0,0 +1,48 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *	    Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_INIT_H__
+#define __BHV_INTERFACE_INIT_H__
+
+/* BHV VAS INIT BACKEND OPS */
+#define BHV_VAS_INIT_OP_INIT 0
+#define BHV_VAS_INIT_OP_START 1
+
+#define BHV_SRT_VAULT 42
+#define BHV_SRT_DATA 43
+
+typedef struct {
+	uint64_t gpa_start;
+	uint64_t size;
+	uint8_t type; // BHV_SRT_*
+	uint64_t next;
+} __attribute__((__packed__)) bhv_init_init_bhv_section_run_t;
+
+typedef struct {
+	uint64_t modprobe_path_sz;
+	uint64_t modprobe_path;
+	uint64_t owner;
+	uint64_t bhv_region_head;
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_init_init_arg_t;
+
+typedef struct {
+	uint8_t valid;
+	uint8_t padding;
+	uint16_t num_pages;
+	uint32_t data_sz;
+	uint8_t data[];
+} __attribute__((__packed__)) bhv_init_start_config_t;
+
+typedef struct {
+	union {
+		bhv_init_init_arg_t bhv_init_init_arg;
+		bhv_init_start_config_t bhv_init_start_config;
+	};
+} __attribute__((__packed__)) bhv_init_arg_t;
+
+#endif /* __BHV_INTERFACE_INIT_H__ */
diff --git include/bhv/interface/integrity.h include/bhv/interface/integrity.h
new file mode 100644
index 000000000..01d6e1695
--- /dev/null
+++ include/bhv/interface/integrity.h
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_INTEGRITY_H__
+#define __BHV_INTERFACE_INTEGRITY_H__
+
+#include <linux/slab.h>
+
+#define BHV_MEM_TYPE_UNKNOWN			0U
+#define BHV_MEM_TYPE_CODE			1U
+#define BHV_MEM_TYPE_CODE_WRITABLE		2U
+#define BHV_MEM_TYPE_CODE_PATCHABLE		3U
+#define BHV_MEM_TYPE_DATA			4U
+#define BHV_MEM_TYPE_DATA_READ_ONLY		5U
+#define BHV_MEM_TYPE_VDSO 6U
+
+#define BHV_MEM_FLAGS_NONE			0UL
+#define BHV_MEM_FLAGS_TRANSIENT		(1UL << 0)
+#define BHV_MEM_FLAGS_MUTABLE		(1UL << 1)
+
+/* BHV VAS INTEGRITY BACKEND OPS */
+#define BHV_VAS_INTEGRITY_OP_CREATE_PHYS	0U
+#define BHV_VAS_INTEGRITY_OP_UPDATE_PHYS	1U
+#define BHV_VAS_INTEGRITY_OP_REMOVE_PHYS	2U
+#define BHV_VAS_INTEGRITY_OP_FREEZE 		3U
+
+/*********************************************
+ * BHV memory region definitions
+ ********************************************/
+#define BHV_VAS_INTEGRITY_MAY_LABEL_SZ 32
+
+typedef struct {
+	uint64_t start_addr;
+	uint64_t size;
+	uint32_t type;
+	uint32_t pad;
+	uint64_t flags;
+	char label[BHV_VAS_INTEGRITY_MAY_LABEL_SZ];
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_create_t;
+
+typedef struct {
+	uint64_t start_addr;
+	uint32_t type;
+	uint32_t pad;
+	uint64_t flags;
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_update_t;
+
+typedef struct {
+	uint64_t start_addr;
+	uint64_t next;
+} __attribute__((__packed__)) bhv_mem_region_remove_t;
+
+typedef struct {
+	union {
+		bhv_mem_region_create_t bhv_mem_region_create;
+		bhv_mem_region_update_t bhv_mem_region_update;
+		bhv_mem_region_remove_t bhv_mem_region_remove;
+	};
+} __attribute__((__packed__)) bhv_mem_region_t;
+
+/*********************************************
+ * BHV arg definitions
+ ********************************************/
+
+typedef struct {
+	uint64_t owner;
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_integrity_create_arg_t;
+
+typedef struct {
+	uint64_t region_head;
+} __attribute__((__packed__)) bhv_integrity_update_arg_t;
+
+typedef struct {
+	uint64_t rm_by_owner;
+	union {
+		uint64_t owner;
+		uint64_t region_head;
+	};
+} __attribute__((__packed__)) bhv_integrity_remove_arg_t;
+
+typedef struct {
+	uint64_t flags;
+} __attribute__((__packed__)) bhv_integrity_freeze_arg_t;
+
+typedef struct {
+	union {
+		bhv_integrity_create_arg_t bhv_integrity_create_arg;
+		bhv_integrity_update_arg_t bhv_integrity_update_arg;
+		bhv_integrity_remove_arg_t bhv_integrity_remove_arg;
+		bhv_integrity_freeze_arg_t bhv_integrity_freeze_arg;
+	};
+} __attribute__((__packed__)) bhv_integrity_arg_t;
+
+#define BHV_FREEZE_FLAGS_DENY_NONE 0
+#define BHV_FREEZE_FLAGS_DENY_CREATE (1UL << 0)
+#define BHV_FREEZE_FLAGS_DENY_UPDATE (1UL << 1)
+#define BHV_FREEZE_FLAGS_DENY_REMOVE (1UL << 2)
+#define BHV_FREEZE_FLAGS_DENY_PATCH (1UL << 3)
+#define BHV_FREEZE_FLAGS_MAX (1UL << 4)
+
+#endif /* __BHV_INTERFACE_INTEGRITY_H__ */
diff --git include/bhv/interface/patch.h include/bhv/interface/patch.h
new file mode 100644
index 000000000..b862189bc
--- /dev/null
+++ include/bhv/interface/patch.h
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_PATCH_H__
+#define __BHV_INTERFACE_PATCH_H__
+
+#include <linux/types.h>
+
+/* BHV VAS PATCH BACKEND OPS */
+#define BHV_VAS_PATCH_OP_PATCH 0
+#define BHV_VAS_PATCH_OP_PATCH_NO_CLOSE 1
+
+#define BHV_MAX_PATCH_SZ 32
+
+typedef struct {
+	uint64_t dest_phys_addr;
+	uint8_t src_value[BHV_MAX_PATCH_SZ];
+	uint64_t size;
+} __attribute__((__packed__)) bhv_patch_patch_arg_t;
+
+typedef struct {
+	union {
+		bhv_patch_patch_arg_t bhv_patch_patch_arg;
+	};
+} __attribute__((__packed__)) bhv_patch_arg_t;
+
+#endif /* __BHV_INTERFACE_PATCH_H__ */
diff --git include/bhv/interface/reg_protect.h include/bhv/interface/reg_protect.h
new file mode 100644
index 000000000..d4a4fb089
--- /dev/null
+++ include/bhv/interface/reg_protect.h
@@ -0,0 +1,65 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_REGPROTECT_H__
+#define __BHV_INTERFACE_REGPROTECT_H__
+
+#include <linux/types.h>
+
+#define BHV_VAS_REGPROTECT_OP_FREEEZE_PHYS 0U
+
+#ifdef CONFIG_X86_64
+#define BHV_NUM_FREEZABLE_REGISTERS 4
+#define BHV_FREEZABLE_REGISTERS Q(CR0) Q(CR3) Q(CR4)
+
+#define BHV_REG_PROTECT_REG_INVALID 0UL
+#define BHV_REG_PROTECT_REG_CR0 (1UL << 0)
+#define BHV_REG_PROTECT_REG_CR3 (1UL << 1)
+#define BHV_REG_PROTECT_REG_CR4 (1UL << 2)
+#define BHV_REG_PROTECT_REG_IDTR (1UL << 3)
+// #define BHV_REG_PROTECT_REG_MAX_VALUE (1UL << NUM_TRAPPABLE_REGISTERS) - 1
+
+#elif CONFIG_ARM64
+#define BHV_NUM_FREEZABLE_REGISTERS 5
+#define BHV_FREEZABLE_REGISTERS Q(TTBR0) Q(TTBR1) Q(TCR) Q(SCTLR) Q(VBAR)
+
+#define BHV_REG_PROTECT_REG_INVALID 0UL
+#define BHV_REG_PROTECT_REG_TTBR0 (1UL << 0)
+#define BHV_REG_PROTECT_REG_TTBR1 (1UL << 1)
+#define BHV_REG_PROTECT_REG_TCR (1UL << 2)
+#define BHV_REG_PROTECT_REG_SCTLR (1UL << 3)
+#define BHV_REG_PROTECT_REG_VBAR (1UL << 4)
+// #define BHV_REG_PROTECT_REG_MAX_VALUE (1UL << NUM_TRAPPABLE_REGISTERS) - 1
+
+#else
+#error Unsupported architecture
+#endif
+
+// inline const char *bhv_reg_protect_reg_to_str(uint64_t reg)
+// {
+// 	switch (reg) {
+// #define Q(reg)                                                             
+// 	case BHV_REG_PROTECT_REG_##reg:                                       
+// 		return #reg;
+// 		BHV_FREEZABLE_REGISTERS
+// #undef Q
+// 	default:
+// 		return "UNKNOWN";
+// 	}
+// }
+
+typedef struct {
+	uint64_t register_selector;
+	uint64_t freeze_bitfield;
+} __attribute__((__packed__)) bhv_reg_protect_freeze_t;
+
+typedef struct {
+	union {
+		bhv_reg_protect_freeze_t bhv_reg_protect_freeze;
+	};
+} __attribute__((__packed__)) bhv_reg_protect_t;
+
+#endif /* __BHV_INTERFACE_REGPROTECT_H__ */
diff --git include/bhv/interface/vault.h include/bhv/interface/vault.h
new file mode 100644
index 000000000..075752c19
--- /dev/null
+++ include/bhv/interface/vault.h
@@ -0,0 +1,14 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_INTERFACE_VAULT_H__
+#define __BHV_INTERFACE_VAULT_H__
+
+/* BHV VAS VAULT BACKEND OPS */
+#define BHV_VAS_VAULT_OP_OPEN	0
+#define BHV_VAS_VAULT_OP_CLOSE	1
+
+#endif /* __BHV_INTERFACE_VAULT_H__ */
diff --git include/bhv/kernel-kln.h include/bhv/kernel-kln.h
new file mode 100644
index 000000000..89729b2c6
--- /dev/null
+++ include/bhv/kernel-kln.h
@@ -0,0 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#define KLN_SYM(sym) ((unsigned long) sym)
+#define KLN_SYMBOL(ty, sym) ((ty)sym)
+#define KLN_SYMBOL_P(ty, sym) ((ty)&sym)
diff --git include/bhv/kversion.h include/bhv/kversion.h
new file mode 100644
index 000000000..63683b1ca
--- /dev/null
+++ include/bhv/kversion.h
@@ -0,0 +1,21 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE == KERNEL_VERSION(5, 10, 79)
+#define BHV_KVERS_5_10
+#elif LINUX_VERSION_CODE == KERNEL_VERSION(5, 15, 72)
+#define BHV_KVERS_5_15
+#else
+#error Unsupported linux version
+#endif
+
+#undef LINUX_VERSION_CODE
+#undef KERNEL_VERSION
+#undef LINUX_VERSION_MAJOR
+#undef LINUX_VERSION_PATCHLEVEL
+#undef LINUX_VERSION_SUBLEVEL
diff --git include/bhv/module.h include/bhv/module.h
new file mode 100644
index 000000000..4e5432d50
--- /dev/null
+++ include/bhv/module.h
@@ -0,0 +1,60 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_MODULE_H__
+#define __BHV_MODULE_H__
+
+#ifdef CONFIG_BHV_VAS
+void bhv_module_load_prepare(const struct module *mod);
+void bhv_module_load_complete(const struct module *mod);
+void bhv_module_unload(const struct module *mod);
+
+#ifdef VASKM // out of tree
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags);
+#endif // VASKM
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size);
+void bhv_bpf_protect_x(const void *base, uint64_t size);
+void bhv_bpf_unprotect(const void *base);
+#else /* CONFIG_BHV_VAS */
+
+static inline void bhv_module_load_prepare(const struct module *mod)
+{
+}
+
+static inline void bhv_module_load_complete(const struct module *mod)
+{
+}
+
+static inline void bhv_module_unload(const struct module *mod)
+{
+}
+
+#ifdef VASKM // out of tree
+static inline void bhv_protect_generic_memory(uint64_t owner, const void *base,
+					      uint64_t size, uint32_t type,
+					      uint64_t flags)
+{
+}
+#endif // VASKM
+
+static inline void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+}
+
+static inline void bhv_bpf_unprotect(const void *base)
+{
+}
+
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_MODULE_H__ */
diff --git include/bhv/patch.h include/bhv/patch.h
new file mode 100644
index 000000000..91bef66c7
--- /dev/null
+++ include/bhv/patch.h
@@ -0,0 +1,97 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifndef __BHV_PATCH_H__
+#define __BHV_PATCH_H__
+
+#include <linux/slab.h>
+#include <linux/jump_label.h>
+#include <linux/module.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kversion.h>
+#endif // VASKM
+
+#include <asm/bhv/patch.h>
+
+#ifdef CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len);
+int bhv_jump_label_add_module(struct module *mod);
+void bhv_jump_label_del_module(struct module *mod);
+#endif /* CONFIG_JUMP_LABEL */
+
+enum bhv_alternatives_mod_delete_policy {
+	BHV_ALTERNATIVES_DELETE_AFTER_PATCH = 0,
+	BHV_ALTERNATIVES_DELETE_AFTER_INIT,
+};
+
+struct bhv_alternatives_mod {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+	enum bhv_alternatives_mod_delete_policy delete_policy;
+	bool allocated;
+	struct bhv_alternatives_mod_arch arch;
+	struct list_head next;
+};
+
+typedef bool (*bhv_alternatives_filter_t)(void *search_params,
+					  struct bhv_alternatives_mod *cur);
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch);
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter);
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch);
+void bhv_alternatives_delete_after_init(void);
+
+#ifndef VASKM // inside kernel tree
+#if defined BHV_KVERS_5_15
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+void __init_or_module bhv_apply_retpolines(s32 *s);
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s);
+#endif /* CONFIG_RETHUNK */
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+#endif // BHV_KVERS_5_15
+#endif // VASKM
+
+#ifdef CONFIG_PARAVIRT
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p);
+#endif /* CONFIG_PARAVIRT */
+
+#else // CONFIG_BHV_VAS
+
+#ifdef CONFIG_JUMP_LABEL
+static inline int bhv_patch_jump_label(struct jump_entry *entry,
+				       const void *opcode, size_t len)
+{
+	return 0;
+}
+
+static inline int bhv_jump_label_add_module(struct module *mod)
+{
+	return 0;
+}
+
+static inline void bhv_jump_label_del_module(struct module *mod)
+{
+}
+#endif /* CONFIG_JUMP_LABEL */
+
+static inline void
+bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+			    struct bhv_alternatives_mod_arch *arch)
+{
+}
+
+#endif // CONFIG_BHV_VAS
+
+#endif /* __BHV_PATCH_H__ */
diff --git include/bhv/start.h include/bhv/start.h
new file mode 100644
index 000000000..a41a7873d
--- /dev/null
+++ include/bhv/start.h
@@ -0,0 +1,29 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_START_H__
+#define __BHV_START_H__
+
+#ifdef CONFIG_BHV_VAS
+bool __init bhv_init_platform(void);
+bool bhv_start(void);
+int bhv_start_arch(void);
+#else /* CONFIG_BHV_VAS */
+static inline bool bhv_init_platform(void)
+{
+	return true;
+}
+static inline bool bhv_start(void)
+{
+	return true;
+}
+static inline int bhv_start_arch(void)
+{
+	return 0;
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_START_H__ */
\ No newline at end of file
diff --git include/bhv/sysfs.h include/bhv/sysfs.h
new file mode 100644
index 000000000..e44778cad
--- /dev/null
+++ include/bhv/sysfs.h
@@ -0,0 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+void bhv_setup_sysfs(void);
\ No newline at end of file
diff --git include/bhv/sysfs_integrity_freeze.h include/bhv/sysfs_integrity_freeze.h
new file mode 100644
index 000000000..159489d15
--- /dev/null
+++ include/bhv/sysfs_integrity_freeze.h
@@ -0,0 +1,12 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+void bhv_setup_sysfs_integrity_freeze(struct kobject *kobj);
+
+extern bool bhv_allow_kmod_loads;
+extern bool bhv_allow_patch;
\ No newline at end of file
diff --git include/bhv/sysfs_reg_protect.h include/bhv/sysfs_reg_protect.h
new file mode 100644
index 000000000..0fa375ce9
--- /dev/null
+++ include/bhv/sysfs_reg_protect.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#pragma once
+
+#include <bhv/interface/common.h>
+
+#ifdef CONFIG_BHV_VAS
+void bhv_setup_sysfs_reg_protect(struct kobject *kobj);
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	if (!is_bhv_initialized())
+		return false;
+
+	return (bool)test_bit(BHV_CONFIG_REGISTER_PROTECTION,
+			      bhv_configuration_bitmap);
+}
+
+#else /* CONFIG_BHV_VAS */
+
+static inline bool bhv_reg_protect_is_enabled(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_BHV_VAS */
diff --git include/bhv/vault.h include/bhv/vault.h
new file mode 100644
index 000000000..902647976
--- /dev/null
+++ include/bhv/vault.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#ifndef __BHV_VAULT_H__
+#define __BHV_VAULT_H__
+
+#ifdef CONFIG_BHV_VAS
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/vault.h>
+#include <bhv/bhv.h>
+
+static __always_inline int bhv_vault_open_hyp(void)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_VAULT,
+					    BHV_VAS_VAULT_OP_OPEN, NULL);
+	if (r)
+		return -EINVAL;
+
+	return 0;
+}
+
+static __always_inline void bhv_vault_close_hyp(void)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_VAULT,
+					    BHV_VAS_VAULT_OP_CLOSE, NULL);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+}
+#else /* CONFIG_BHV_VAS */
+static inline int bhv_vault_open_hyp(void)
+{
+}
+
+static inline void bhv_vault_close_hyp(void)
+{
+}
+#endif /* CONFIG_BHV_VAS */
+
+#endif /* __BHV_VAULT_H__ */
diff --git include/linux/filter.h include/linux/filter.h
index a9956b681..e5c9dc057 100644
--- include/linux/filter.h
+++ include/linux/filter.h
@@ -28,6 +28,8 @@
 #include <uapi/linux/filter.h>
 #include <uapi/linux/bpf.h>
 
+#include <bhv/module.h>
+
 struct sk_buff;
 struct sock;
 struct seccomp_data;
@@ -878,6 +880,7 @@ static inline void bpf_prog_lock_ro(struct bpf_prog *fp)
 	if (!fp->jited) {
 		set_vm_flush_reset_perms(fp);
 		set_memory_ro((unsigned long)fp, fp->pages);
+		bhv_bpf_protect_ro(fp, fp->pages << PAGE_SHIFT);
 	}
 #endif
 }
@@ -887,6 +890,7 @@ static inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)
 	set_vm_flush_reset_perms(hdr);
 	set_memory_ro((unsigned long)hdr, hdr->pages);
 	set_memory_x((unsigned long)hdr, hdr->pages);
+	bhv_bpf_protect_x(hdr, hdr->pages << PAGE_SHIFT);
 }
 
 static inline struct bpf_binary_header *
@@ -923,6 +927,9 @@ void __bpf_prog_free(struct bpf_prog *fp);
 
 static inline void bpf_prog_unlock_free(struct bpf_prog *fp)
 {
+	if (!fp->jited)
+		bhv_bpf_unprotect(fp);
+
 	__bpf_prog_free(fp);
 }
 
diff --git include/linux/kmod.h include/linux/kmod.h
index 68f69362d..288036c58 100644
--- include/linux/kmod.h
+++ include/linux/kmod.h
@@ -17,7 +17,12 @@
 #define KMOD_PATH_LEN 256
 
 #ifdef CONFIG_MODULES
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+extern const char
+	modprobe_path[KMOD_PATH_LEN] __section(".rodata"); /* for sysctl */
+#else
 extern char modprobe_path[]; /* for sysctl */
+#endif
 /* modprobe exit status on success, -ve on error.  Return value
  * usually useless though. */
 extern __printf(2, 3)
diff --git include/linux/mem_namespace.h include/linux/mem_namespace.h
new file mode 100644
index 000000000..4b4771498
--- /dev/null
+++ include/linux/mem_namespace.h
@@ -0,0 +1,69 @@
+#ifndef _LINUX_MEM_NS_H
+#define _LINUX_MEM_NS_H
+
+#include <linux/kref.h>
+#include <linux/nsproxy.h>
+#include <linux/ns_common.h>
+
+#ifdef CONFIG_MEM_NS
+#define bhv_pr_info(msg, ...)   pr_info("[-BHV-] %s: " msg "\n", __FUNCTION__, ##__VA_ARGS__)
+#else
+#define bhv_pr_info(msg, ...)
+#endif
+
+#define BHV_INIT_DOMAIN		0UL
+#define BHV_INVALID_DOMAIN	(-1UL)
+
+struct mem_namespace {
+	struct kref kref;
+	struct user_namespace *user_ns;
+	struct ucounts *ucounts;
+	struct ns_common ns;
+	uint64_t domain;
+} __randomize_layout;
+
+extern struct mem_namespace init_mem_ns;
+
+#ifdef CONFIG_MEM_NS
+static inline struct mem_namespace *get_mem_ns(struct mem_namespace *ns)
+{
+	kref_get(&ns->kref);
+	return ns;
+}
+
+extern void free_mem_ns(struct kref *kref);
+
+static inline void put_mem_ns(struct mem_namespace *ns)
+{
+	/*
+	 * XXX: Determine, whether or not we need to establish nested mem
+	 * namespaces. If so, we need to traverse all parents and release put
+	 * the refcount on the namespace.
+	 */
+
+	if (ns)
+		kref_put(&ns->kref, free_mem_ns);
+}
+
+extern struct mem_namespace *copy_mem_ns(unsigned long flags,
+					 struct user_namespace *user_ns,
+					 struct mem_namespace *old_ns);
+
+#else /* CONFIG_MEM_NS */
+
+static inline void get_mem_ns(struct mem_namespace *ns) {}
+static inline void put_mem_ns(struct mem_namespace *ns) {}
+
+static inline struct mem_namespace *copy_mem_ns(unsigned long flags,
+						struct user_namespace *user_ns,
+						struct mem_namespace *old_ns)
+{
+	if (flags & CLONE_NEWMEM)
+		return ERR_PTR(-EINVAL);
+
+	return old_ns;
+}
+
+#endif /* CONFIG_MEM_NS */
+
+#endif /* _LINUX_MEM_NS_H */
diff --git include/linux/nsproxy.h include/linux/nsproxy.h
index cdb171efc..499668ef1 100644
--- include/linux/nsproxy.h
+++ include/linux/nsproxy.h
@@ -11,6 +11,7 @@ struct ipc_namespace;
 struct pid_namespace;
 struct cgroup_namespace;
 struct fs_struct;
+struct mem_namespace;
 
 /*
  * A structure to contain pointers to all per-process
@@ -38,6 +39,7 @@ struct nsproxy {
 	struct time_namespace *time_ns;
 	struct time_namespace *time_ns_for_children;
 	struct cgroup_namespace *cgroup_ns;
+	struct mem_namespace *mem_ns;
 };
 extern struct nsproxy init_nsproxy;
 
diff --git include/linux/proc_ns.h include/linux/proc_ns.h
index 75807ecef..ff1046249 100644
--- include/linux/proc_ns.h
+++ include/linux/proc_ns.h
@@ -34,6 +34,7 @@ extern const struct proc_ns_operations mntns_operations;
 extern const struct proc_ns_operations cgroupns_operations;
 extern const struct proc_ns_operations timens_operations;
 extern const struct proc_ns_operations timens_for_children_operations;
+extern const struct proc_ns_operations memns_operations;
 
 /*
  * We always define these enumerators
@@ -46,6 +47,7 @@ enum {
 	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
 	PROC_CGROUP_INIT_INO	= 0xEFFFFFFBU,
 	PROC_TIME_INIT_INO	= 0xEFFFFFFAU,
+	PROC_MEM_INIT_INO	= 0xEFFFFFF9U,
 };
 
 #ifdef CONFIG_PROC_FS
diff --git include/linux/user_namespace.h include/linux/user_namespace.h
index 33a4240e6..dc9b341fa 100644
--- include/linux/user_namespace.h
+++ include/linux/user_namespace.h
@@ -58,6 +58,7 @@ enum ucount_type {
 	UCOUNT_RLIMIT_MSGQUEUE,
 	UCOUNT_RLIMIT_SIGPENDING,
 	UCOUNT_RLIMIT_MEMLOCK,
+	UCOUNT_MEM_NAMESPACES,
 	UCOUNT_COUNTS,
 };
 
diff --git include/uapi/linux/perf_event.h include/uapi/linux/perf_event.h
index f92880a15..9bd6b33bb 100644
--- include/uapi/linux/perf_event.h
+++ include/uapi/linux/perf_event.h
@@ -776,6 +776,7 @@ enum {
 	USER_NS_INDEX		= 4,
 	MNT_NS_INDEX		= 5,
 	CGROUP_NS_INDEX		= 6,
+	MEM_NS_INDEX		= 7,
 
 	NR_NAMESPACES,		/* number of available namespaces */
 };
diff --git include/uapi/linux/sched.h include/uapi/linux/sched.h
index 3bac0a8ce..94224cf51 100644
--- include/uapi/linux/sched.h
+++ include/uapi/linux/sched.h
@@ -41,6 +41,7 @@
  * cloning flags intersect with CSIGNAL so can be used with unshare and clone3
  * syscalls only:
  */
+#define CLONE_NEWMEM	0x00000040	/* New memory namespace */
 #define CLONE_NEWTIME	0x00000080	/* New time namespace */
 
 #ifndef __ASSEMBLY__
diff --git init/main.c init/main.c
index 649d9e420..c398cc259 100644
--- init/main.c
+++ init/main.c
@@ -114,6 +114,9 @@
 
 #include <kunit/test.h>
 
+#include <bhv/bhv.h>
+#include <bhv/start.h>
+
 static int kernel_init(void *);
 
 extern void init_IRQ(void);
@@ -859,6 +862,8 @@ static void __init mm_init(void)
 	init_espfix_bsp();
 	/* Should be run after espfix64 is set up. */
 	pti_init();
+
+	bhv_mm_init();
 }
 
 #ifdef CONFIG_HAVE_ARCH_RANDOMIZE_KSTACK_OFFSET
diff --git kernel/Makefile kernel/Makefile
index 0e119c52a..8090ed664 100644
--- kernel/Makefile
+++ kernel/Makefile
@@ -53,6 +53,7 @@ obj-y += rcu/
 obj-y += livepatch/
 obj-y += dma/
 obj-y += entry/
+obj-y += bhv/
 
 obj-$(CONFIG_KCMP) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
@@ -82,6 +83,7 @@ obj-$(CONFIG_CGROUPS) += cgroup/
 obj-$(CONFIG_UTS_NS) += utsname.o
 obj-$(CONFIG_USER_NS) += user_namespace.o
 obj-$(CONFIG_PID_NS) += pid_namespace.o
+obj-$(CONFIG_MEM_NS) += mem_namespace.o
 obj-$(CONFIG_IKCONFIG) += configs.o
 obj-$(CONFIG_IKHEADERS) += kheaders.o
 obj-$(CONFIG_SMP) += stop_machine.o
diff --git kernel/bhv/Kconfig kernel/bhv/Kconfig
new file mode 100644
index 000000000..ec27d2dd6
--- /dev/null
+++ kernel/bhv/Kconfig
@@ -0,0 +1,30 @@
+config BHV_PANIC_ON_FAIL
+	def_bool y
+	bool "BHV guest panics on Hypercall failure"
+	depends on BHV_VAS
+	help
+	  Say Y if you want the kernel to panic in the case a
+	  BRASS hypercall fails.  This will prevent the guest
+	  continuing execution if a security critical hypercall
+	  fails.
+
+config BHV_VAS_DEBUG
+	def_bool n
+	bool "Build BHV guest support with DEBUG information"
+	depends on BHV_VAS
+	help
+	  Say Y if you want to include DEBUG output when using BHV VAS.
+
+config BHV_ALLOW_SELINUX_GUEST_ADMIN
+	def_bool n
+	bool "Allow the guest to perform SELinux administration if the host disabled guestpolicy support"
+	depends on BHV_VAS
+	help
+	  Say Y if you want to allow the guest to perform SELinux administration if the host disabled guestpolicy support.
+
+config BHV_CONST_MODPROBE_PATH
+	def_bool y
+	bool "Make modprobe_path global constant such that it cannot be updated"
+	depends on BHV_VAS && MODULES
+	help
+	  Say Y if you want to make the modprobe_path global constant such that it cannot be updated.
diff --git kernel/bhv/Makefile kernel/bhv/Makefile
new file mode 100644
index 000000000..2067bdc4b
--- /dev/null
+++ kernel/bhv/Makefile
@@ -0,0 +1,27 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+# Copyright (C) 2022 - BedRock Systems Inc
+# Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+#          Sergej Proskurin <sergej@bedrocksystems.com>
+#          Sebastian Vogl <sebastian@bedrocksystems.com>
+
+obj-$(CONFIG_BHV_VAS)		:= bhv.o
+obj-$(CONFIG_BHV_VAS)		+= start.o
+obj-$(CONFIG_BHV_VAS)		+= init.o
+obj-$(CONFIG_BHV_VAS)		+= integrity.o
+ifeq ($(CONFIG_JUMP_LABEL),y)
+obj-$(CONFIG_BHV_VAS)		+= patch_jump_label.o
+endif
+obj-$(CONFIG_BHV_VAS)		+= patch_alternative.o
+obj-$(CONFIG_BHV_VAS)		+= module.o
+obj-$(CONFIG_BHV_VAS)		+= acl.o
+obj-$(CONFIG_BHV_VAS)		+= guestconn.o
+obj-$(CONFIG_BHV_VAS)		+= guestlog.o
+obj-$(CONFIG_BHV_VAS)		+= creds.o
+obj-$(CONFIG_BHV_VAS)		+= file_protection.o
+obj-$(CONFIG_BHV_VAS)		+= fileops_protection.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_integrity_freeze.o
+obj-$(CONFIG_BHV_VAS)		+= sysfs_reg_protect.o
+obj-$(CONFIG_BHV_VAS)		+= vmalloc_to_page.o
+obj-$(CONFIG_BHV_VAS)		+= domain.o
diff --git kernel/bhv/acl.c kernel/bhv/acl.c
new file mode 100644
index 000000000..8706769d4
--- /dev/null
+++ kernel/bhv/acl.c
@@ -0,0 +1,305 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/acl.h>
+
+#include <bhv/interface/integrity.h>
+#include <bhv/integrity.h>
+
+#include <bhv/acl.h>
+
+#define PATH_DELIMITER '/'
+
+bhv_acl_config_t *process_acl_config __ro_after_init = NULL;
+bhv_acl_config_t *driver_acl_config __ro_after_init = NULL;
+
+struct kmem_cache *bhv_acl_violation_cache;
+
+static void __init bhv_acl_init_acl(uint32_t op, bhv_acl_config_t **global_acl_config)
+{
+	unsigned long r;
+	bhv_acl_config_t *acl_config;
+	static bhv_mem_region_t config_region;
+
+	bhv_acl_violation_cache = kmem_cache_create(
+		"bhv_acl_violation_cache", sizeof(bhv_acl_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	acl_config = (bhv_acl_config_t *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (acl_config == NULL) {
+		bhv_fail("Unable to allocate process acl config");
+		return;
+	}
+
+	acl_config->num_pages = 1;
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+	if (r) {
+		pr_err("proc acl init fail");
+		return;
+	}
+
+	if (!acl_config->valid) {
+		free_pages((unsigned long)acl_config, 0);
+
+		acl_config =
+		(bhv_acl_config_t *)__get_free_pages(GFP_KERNEL, order_base_2(acl_config->num_pages));
+
+		if (acl_config == NULL) {
+			bhv_fail("Unable to allocate process acl config");
+			return;
+		}
+
+		r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_config);
+		if (r) {
+			pr_err("proc acl init fail");
+			return;
+		}
+
+		if (!acl_config->valid) {
+			bhv_fail("host returned invalid configuration");
+			return;
+		}
+	}
+
+	// Protect memory
+	if (bhv_integrity_is_enabled()) {
+		config_region.bhv_mem_region_create.start_addr =
+			virt_to_phys(acl_config);
+		config_region.bhv_mem_region_create.size =
+			acl_config->num_pages * PAGE_SIZE;
+		config_region.bhv_mem_region_create.type =
+			BHV_MEM_TYPE_DATA_READ_ONLY;
+		config_region.bhv_mem_region_create.flags = BHV_MEM_FLAGS_NONE;
+		config_region.bhv_mem_region_create.next =
+			BHV_INVALID_PHYS_ADDR;
+
+		r = bhv_create_kern_phys_mem_region_hyp(0, &config_region);
+		if (r) {
+			pr_err("Unable to protect acl config");
+			return;
+		}
+	}
+
+	*global_acl_config = acl_config;
+}
+
+void __init bhv_acl_mm_init(void)
+{
+	if (bhv_acl_is_proc_acl_enabled())
+		bhv_acl_init_acl(BHV_VAS_ACL_OP_INIT_PROC_ACL,
+				 &process_acl_config);
+	if (bhv_acl_is_driver_acl_enabled())
+		bhv_acl_init_acl(BHV_VAS_ACL_OP_INIT_DRIVER_ACL,
+				 &driver_acl_config);
+}
+
+static size_t _get_ext_len(const char *str)
+{
+	char *str_ext = strrchr(str, (int)'.');
+
+	if (str_ext == NULL)
+		return 0;
+
+	return strnlen(str_ext, PATH_MAX);
+}
+
+static bool _match_names(const char *cur, const char *target,
+			 size_t target_ext_len, bool strip_ext)
+{
+	// Get filename of path
+	const char *cur_tmp = strrchr(cur, (int)PATH_DELIMITER);
+	const char *target_tmp = strrchr(target, (int)PATH_DELIMITER);
+	size_t cur_tmp_len = 0;
+	size_t target_tmp_len = 0;
+
+	if (cur_tmp == NULL)
+		cur_tmp = cur;
+	else
+		cur_tmp++;
+
+	if (target_tmp == NULL)
+		target_tmp = target;
+	else
+		target_tmp++;
+
+	// Get length of filename
+	cur_tmp_len = strnlen(cur_tmp, PATH_MAX);
+	target_tmp_len = strnlen(target_tmp, PATH_MAX);
+
+	// Remove extension
+	if (strip_ext) {
+		cur_tmp_len -= _get_ext_len(cur_tmp);
+		target_tmp_len -= target_ext_len;
+	}
+
+	if (cur_tmp_len == 0 || cur_tmp_len >= PATH_MAX ||
+	    target_tmp_len == 0 || target_tmp_len >= PATH_MAX)
+		return false;
+
+	// Check if length matches
+	if (cur_tmp_len != target_tmp_len)
+		return false;
+
+	return strncmp(cur_tmp, target_tmp, cur_tmp_len) == 0;
+}
+
+static bool _matches(const char *target, bool strip_ext,
+		     bhv_acl_config_t *acl_config)
+{
+	size_t target_len = 0;
+	size_t target_ext_len = 0;
+	uint16_t i;
+
+	BUG_ON(target[0] != PATH_DELIMITER);
+
+	// Get target len
+	target_len = strnlen(target, PATH_MAX);
+	if (strip_ext) {
+		target_ext_len = _get_ext_len(target);
+		target_len -= target_ext_len;
+	}
+
+	if (target_len == 0 || target_len >= PATH_MAX)
+		return false;
+
+	for (i = 0; i < acl_config->list_len; i++) {
+		const char *cur = ((char *)acl_config) + acl_config->list[i];
+		size_t cur_len = 0;
+
+		if (cur[0] != PATH_DELIMITER) {
+			if (_match_names(cur, target, target_ext_len,
+					 strip_ext))
+				return true;
+			else
+				continue;
+		}
+
+		cur_len = strnlen(cur, PATH_MAX);
+		if (strip_ext) {
+			cur_len -= _get_ext_len(cur);
+		}
+
+		if (cur_len == 0 || cur_len >= PATH_MAX)
+			continue;
+
+		if (cur[cur_len - 1] == '*') {
+			cur_len--;
+
+			if (target_len < cur_len)
+				continue;
+		} else if (target_len != cur_len)
+			continue;
+
+		if (strncmp(cur, target, cur_len) == 0)
+			return true;
+	}
+
+	return false;
+}
+
+static bool _block_entity(const char *target, bool strip_ext,
+			  bhv_acl_config_t *acl_config, uint32_t op)
+{
+	bool rv;
+	unsigned long r;
+	// Tell the compiler this points to volatile data as the hypercall
+	// will update it.
+	bhv_acl_violation_t *volatile acl_violation = NULL;
+	size_t target_len = strlen(target);
+	bool m;
+
+	if (acl_config == NULL || !acl_config->valid) {
+		bhv_fail("unable to resolve entity due to init error");
+		return false;
+	}
+
+	m = _matches(target, strip_ext, acl_config);
+
+	// Is this entity part of the allow list?
+	if (m && acl_config->is_allow)
+		return false;
+	// Is this entity _NOT_ in the deny list?
+	if (!m && !acl_config->is_allow)
+		return false;
+
+	BUG_ON(target_len >= PAGE_SIZE);
+
+	// Prepare hypercall
+	acl_violation = kmem_cache_alloc(bhv_acl_violation_cache, GFP_KERNEL);
+	if (acl_violation == NULL) {
+		bhv_fail("Unable to allocate acl violation");
+		return true;
+	}
+
+	// Get Context
+	r = populate_event_context(&acl_violation->context, true);
+	if (r) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	// Setup arg
+	acl_violation->name_len = target_len;
+	acl_violation->name = virt_to_phys((volatile void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_ACL, op, acl_violation);
+	if (r) {
+		pr_err("entity hypercall failed");
+		kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+		return true;
+	}
+
+	// Read block and free
+	rv = (bool)acl_violation->block;
+	kmem_cache_free(bhv_acl_violation_cache, acl_violation);
+	return rv;
+}
+
+bool bhv_block_driver(const char *target)
+{
+	if (!bhv_acl_is_driver_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename. For example, init_module call. => BLOCK
+		return true;
+	}
+
+	return _block_entity(target, true, driver_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_DRIVER_ACL);
+}
+
+bool bhv_block_process(const char *target)
+{
+	if (!bhv_acl_is_proc_acl_enabled())
+		return false;
+
+	if (target == NULL) {
+		// Unknown filename => BLOCK
+		return true;
+	}
+
+	if (target[0] != PATH_DELIMITER) {
+		return false;
+	}
+
+	return _block_entity(target, false, process_acl_config,
+			     BHV_VAS_ACL_OP_VIOLATION_PROC_ACL);
+}
\ No newline at end of file
diff --git kernel/bhv/bhv.c kernel/bhv/bhv.c
new file mode 100644
index 000000000..d7d25bcfb
--- /dev/null
+++ kernel/bhv/bhv.c
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/creds.h>
+#include <bhv/integrity.h>
+#include <bhv/guestconn.h>
+#include <bhv/file_protection.h>
+
+#include <bhv/bhv.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/fileops_protection.h>
+#include <bhv/acl.h>
+#endif // VASKM
+
+bool bhv_initialized __ro_after_init = false;
+unsigned long *bhv_configuration_bitmap __ro_after_init = NULL;
+
+void __init bhv_mm_init(void)
+{
+	bhv_integrity_mm_init();
+#ifndef VASKM // inside kernel tree
+	bhv_acl_mm_init();
+#endif // VASKM
+	bhv_guestconn_mm_init();
+	bhv_cred_mm_init();
+	bhv_file_protection_init();
+#ifndef VASKM // inside kernel tree
+	bhv_fileops_protection_mm_init();
+#endif // VASKM
+}
diff --git kernel/bhv/creds.c kernel/bhv/creds.c
new file mode 100644
index 000000000..21220858e
--- /dev/null
+++ kernel/bhv/creds.c
@@ -0,0 +1,480 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com> 
+ *           Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/init_task.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/siphash.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/creds.h>
+#include <bhv/interface/hypercall.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+#define BHV_CREDS_HYP(op, arg) bhv_hypercall_vas(BHV_VAS_BACKEND_CREDS, op, arg)
+
+#define BHV_CREDS_CONFIGURE_HYP(arg)                                           \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_CONFIGURE, arg)
+#define BHV_CREDS_REGISTER_INIT_TASK_HYP(arg)                                  \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_REGISTER_INIT_TASK, arg)
+#define BHV_CREDS_ASSIGN_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN, arg)
+#define BHV_CREDS_ASSIGN_PRIV_HYP(arg)                                         \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_ASSIGN_PRIV, arg)
+#define BHV_CREDS_COMMIT_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_COMMIT, arg)
+#define BHV_CREDS_RELEASE_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_RELEASE, arg)
+#define BHV_CREDS_VERIFY_HYP(arg)                                              \
+	BHV_CREDS_HYP(BHV_VAS_CREDS_OP_VERIFICATION, arg)
+#define BHV_CREDS_LOG_HYP(arg) BHV_CREDS_HYP(BHV_VAS_CREDS_OP_LOG, arg)
+
+struct kmem_cache *bhv_creds_arg_cache = NULL;
+
+static siphash_key_t bhv_siphash_key __ro_after_init = { 0 };
+
+static size_t collect_cred_invariants(char *buf, const struct cred *c,
+				      const struct task_struct *context,
+				      size_t max_size)
+{
+	char *_buf = NULL;
+	uint64_t bound_context = 0;
+	struct cred cred_copy;
+
+	static const size_t buf_size = sizeof(uint64_t) + sizeof(struct cred);
+
+	BUG_ON(!buf && max_size < buf_size);
+
+	_buf = buf;
+
+	memcpy(&cred_copy, c, sizeof(struct cred));
+
+	/* Exclude mutable fields from the credentials to be hashed. */
+
+	atomic_set(&cred_copy.usage, 0);
+#ifdef CONFIG_DEBUG_CREDENTIALS
+	atomic_set(&cred_copy.subscribers, 0);
+	cred_copy.put_addr = NULL;
+	cred_copy.magic = 0;
+#endif
+#ifdef CONFIG_SECURITY
+	/*
+	 * Consider tracking the integrity of the security pointer. This would
+	 * require a credential tag update on every update of the security
+	 * pointer.
+	 */
+	cred_copy.security = NULL;
+#endif
+	memset(&cred_copy.rcu, 0, sizeof(struct rcu_head));
+
+	/*
+	 * Bind the credentials to the given context; incorporate this
+	 * information into the hash.
+	 */
+
+	bound_context = (uint64_t)c ^ (uint64_t)context;
+
+	_buf = memcpy(_buf, &bound_context, sizeof(uint64_t));
+	_buf += sizeof(uint64_t);
+
+	_buf = memcpy(_buf, &cred_copy, sizeof(struct cred));
+
+	return buf_size;
+}
+
+static uint64_t siphash_cred_context(const struct cred *const c,
+				     const struct task_struct *const context)
+{
+#define MAX_BUF_SIZE sizeof(struct cred) + sizeof(uint64_t)
+	char buf[MAX_BUF_SIZE];
+	size_t size = collect_cred_invariants(buf, c, context, MAX_BUF_SIZE);
+	return siphash(&buf, size, &bhv_siphash_key);
+}
+
+static int __bhv_cred_assign(struct task_struct *t,
+			     struct task_struct *_current, uint64_t clone_flags)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	struct task_struct *parent = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	/*
+	 * Note that we verify the integrity of the currently active process,
+	 * instead of the "real_parent" of the to be assigned credentials.
+	 * Consider verifying the real_parent of the task as well.
+	 */
+	rc = bhv_cred_verify(_current);
+	if (rc)
+		return -EPERM;
+
+	if (clone_flags & (CLONE_THREAD | CLONE_PARENT))
+		parent = _current->real_parent;
+	else
+		parent = _current;
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_assign.new_task.addr = (uint64_t)t;
+	arg->creds_assign.new_task.cred = (uint64_t)t->cred;
+	arg->creds_assign.new_task.hmac = hmac;
+	arg->creds_assign.parent.addr = (uint64_t)parent;
+	arg->creds_assign.parent.cred = (uint64_t)parent->cred;
+
+	rc = BHV_CREDS_ASSIGN_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot assign credentials @ 0x%llx to task @ 0x%llx (pid=%d)",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)parent,
+		       parent->pid);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		/* Note that we currently log only the parent's information. */
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)parent;
+		arg->creds_log.task_cred = (uint64_t)parent->cred;
+		arg->creds_log.task_pid = parent->pid;
+		strscpy(arg->creds_log.task_name, parent->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_cred_assign(struct task_struct *t, uint64_t clone_flags)
+{
+	return __bhv_cred_assign(t, current, clone_flags);
+}
+
+#ifdef VASKM // out of tree
+int __init bhv_cred_assign_init(struct task_struct *t)
+{
+	return __bhv_cred_assign(t, t->real_parent, 0);
+}
+#endif // VASKM
+
+int bhv_cred_assign_priv(struct cred *c, struct task_struct *daemon)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	/* XXX: Do we need to compute an (incomplete) hmac? */
+
+	arg->creds_assign_priv.cred = (uint64_t)c;
+	arg->creds_assign_priv.daemon = (uint64_t)daemon;
+
+	rc = BHV_CREDS_ASSIGN_PRIV_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot prepare priv credentials @ 0x%llx (daemon @ 0x%llx)",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)daemon);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_assign_priv.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_commit(struct cred *c)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	hmac = siphash_cred_context(c, current);
+
+	arg->creds_commit.cur.cred = (uint64_t)c;
+	arg->creds_commit.cur.addr = (uint64_t)current;
+	arg->creds_commit.cur.hmac = hmac;
+
+	rc = BHV_CREDS_COMMIT_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot commit credentials @ 0x%llx to current @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c, (uint64_t)current);
+	}
+
+	type = arg->creds_commit.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)current;
+		arg->creds_log.task_cred = (uint64_t)c;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, current->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/*
+		 * Note that we cannot block this function, yet, the corrupted
+		 * credentials will be identified on the next verification
+		 * point.
+		 */
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+int bhv_cred_verify(struct task_struct *t)
+{
+	int rc = 0;
+	uint64_t hmac = 0;
+	bhv_creds_arg_t *arg = NULL;
+	enum event_type type = EVENT_NONE;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return -ENOMEM;
+	}
+
+	hmac = siphash_cred_context(t->cred, t);
+
+	arg->creds_verify.task.cred = (uint64_t)t->cred;
+	arg->creds_verify.task.addr = (uint64_t)t;
+	arg->creds_verify.task.hmac = hmac;
+
+	rc = BHV_CREDS_VERIFY_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot verify credentials @ 0x%llx of task @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t->cred, (uint64_t)t);
+		rc = -EINVAL;
+	}
+
+	type = arg->creds_verify.ret;
+
+	if (!rc && type != EVENT_NONE) {
+		rc = populate_event_context(&arg->creds_log.context, true);
+		if (rc) {
+			pr_err("%s: BHV cannot retrieve event context",
+			       __FUNCTION__);
+		}
+
+		arg->creds_log.event_type = type;
+		arg->creds_log.task_addr = (uint64_t)t;
+		arg->creds_log.task_cred = (uint64_t)t->cred;
+		arg->creds_log.task_pid = current->pid;
+		strscpy(arg->creds_log.task_name, t->comm, TASK_COMM_LEN);
+
+		rc = BHV_CREDS_LOG_HYP(arg);
+		if (rc) {
+			pr_err("%s: BHV Cannot log event with type=%d",
+			       __FUNCTION__, type);
+		}
+
+		/* Check if the policy is configured to be blocking. */
+		if (arg->creds_log.block) {
+			rc = -EPERM;
+		}
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_cred_release(struct cred *c)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	/*
+	 * XXX: Find a way to better integrate BHV into the RCU mechanism in
+	 * order to batch multpile credentials to be released and hence to avoid
+	 * unnecessary hypercalls.
+	 */
+
+	arg->creds_release.cred = (uint64_t)c;
+
+	rc = BHV_CREDS_RELEASE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot release credentials @ 0x%llx",
+		       __FUNCTION__, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+static void __init bhv_cred_register_init_task(struct cred *const c,
+					       struct task_struct *const t)
+{
+	int rc = 0;
+	bhv_creds_arg_t *arg = NULL;
+
+	if (!bhv_cred_is_enabled())
+		return;
+
+	arg = kmem_cache_alloc(bhv_creds_arg_cache, GFP_KERNEL);
+	if (arg == NULL) {
+		return;
+	}
+
+	arg->creds_register.init_task.addr = (uint64_t)t;
+	arg->creds_register.init_task.cred = (uint64_t)c;
+	arg->creds_register.init_task.hmac = siphash_cred_context(c, t);
+
+	rc = BHV_CREDS_REGISTER_INIT_TASK_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register init_task @ 0x%llx with cred @ 0x%llx",
+		       __FUNCTION__, (uint64_t)t, (uint64_t)c);
+	}
+
+	kmem_cache_free(bhv_creds_arg_cache, arg);
+}
+
+void __init bhv_cred_mm_init(void)
+{
+	if (!bhv_cred_is_enabled())
+		return;
+
+	bhv_creds_arg_cache = kmem_cache_create(
+		"bhv_creds_arg_cache", sizeof(bhv_creds_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	bhv_cred_register_init_task(KLN_SYMBOL_P(struct cred *const, init_cred),
+				    &init_task);
+}
+
+int __init bhv_cred_init(void)
+{
+	int rc = 0;
+
+	if (!bhv_cred_is_enabled())
+		return 0;
+
+#ifndef VASKM // inside kernel tree
+	/*
+	 * Inform BRASS about the location of the siphash key. Note that this
+	 * step has to be done first and very early in the bootstrapping phase
+	 * so that we do not miss the instantiation of new credentials.
+	 */
+	rc = BHV_CREDS_CONFIGURE_HYP(&bhv_siphash_key);
+	if (rc) {
+		return -EINVAL;
+	}
+
+#else // out of tree
+	siphash_key_t *bhv_siphash_key_ptr =
+		(siphash_key_t *)kmalloc(sizeof(siphash_key_t), GFP_KERNEL);
+	
+	if (!bhv_siphash_key_ptr) {
+		return -EINVAL;
+	}
+
+	rc = BHV_CREDS_CONFIGURE_HYP(bhv_siphash_key_ptr);
+	if (rc) {
+		kfree(bhv_siphash_key_ptr);
+		return -EINVAL;
+	}
+
+	bhv_siphash_key = *bhv_siphash_key_ptr;
+	kfree(bhv_siphash_key_ptr);
+#endif // VASKM	
+	
+	return 0;
+}
+
diff --git kernel/bhv/domain.c kernel/bhv/domain.c
new file mode 100644
index 000000000..d3cbd7568
--- /dev/null
+++ kernel/bhv/domain.c
@@ -0,0 +1,771 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors:  Sergej Proskurin <sergej@bedrocksystems.com>
+ *           Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+
+#ifdef CONFIG_MEM_NS
+
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/nsproxy.h>
+#include <linux/mem_namespace.h>
+#include <linux/mm.h>
+#include <linux/mmu_notifier.h>
+
+#include <bhv/bhv.h>
+#include <bhv/domain.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/domain.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/integrity.h>
+
+// #define BHV_IN_ATOMIC_DEBUG 1
+
+/*
+ * XXX: Some general remarks for the future:
+ *
+ * - Consider using MMU notifiers to register at least for some callbacks to
+ *   reduce in-tree changes.
+ * - Consider using the paravirt interface that notifies the requester
+ *   about (very likely) all events that we are interested in (see PVOPS).
+ * - Consider adding our changes into the (architecturally-dependent)
+ *   mmu_context (in mmu_context.hpp) to limit our in-tree changes.
+ * - Consider adding a dedicated domain for pure kernel threads.
+ * - Consider that dup_mm will not necessarily duplicate all VMAs into the
+ *   child's address space. Some VMAs are flagged such that they are not
+ *   considered. This must be adjusted.
+ * - Consider PTI.
+ */
+
+#define BHV_DOMAIN_HYP(op, arg)                                                \
+	bhv_hypercall_vas(BHV_VAS_BACKEND_DOMAIN, op, arg)
+
+#define BHV_DOMAIN_CONFIGURE_HYP(arg)                                          \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_CONFIGURE, arg)
+#define BHV_DOMAIN_CREATE_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_CREATE, arg)
+#define BHV_DOMAIN_DESTROY_HYP(arg)                                            \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_DESTROY, arg)
+#define BHV_DOMAIN_SWITCH_HYP(arg) BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_SWITCH, arg)
+#define BHV_DOMAIN_VMA_REGISTER_HYP(arg)                                       \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_VMA_REGISTER, arg)
+#define BHV_DOMAIN_VMA_UPDATE_HYP(arg)                                         \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_VMA_UPDATE, arg)
+#define BHV_DOMAIN_VMA_RELEASE_HYP(arg)                                        \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_VMA_RELEASE, arg)
+#define BHV_DOMAIN_VMA_RELEASE_ALL_HYP(arg)                                    \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_VMA_RELEASE_ALL, arg)
+#define BHV_DOMAIN_HYP_CLONE_PGD(arg)                                          \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_CLONE_PGD, arg)
+#define BHV_DOMAIN_HYP_MOVE_MM(arg)                                            \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_MOVE_MM, arg)
+#define BHV_DOMAIN_HYP_SWAP(arg)                                               \
+	BHV_DOMAIN_HYP(BHV_VAS_DOMAIN_OP_SWAP_PAGE, arg)
+
+struct kmem_cache *bhv_domain_arg_cache = NULL;
+bool initialized __ro_after_init = false;
+
+#if 0
+
+/*
+ * LEAVE THIS FOR NOW (MMU NOTIFIERS)!
+ * ===================================
+ *
+ * The notifiers can be very helpful, yet, might not be the right (or rather
+ * complete) choice for our purposes. Yet, we should further investigate their
+ * adoption for our purposes. The reason is that they provide a simplified way
+ * of tracking permission/mapping changes in (some of) the VMAs. For instance,
+ * COWed pages that get duplicated whose permissions get updated will cause the
+ * notifiers to trigger and let the system know about the changes. The MMU
+ * notifieers, however, seem to be inconsistent with regard to the information
+ * they are delivering. With the exception of .mmu_notifier_release, the exact
+ * corner cases should be further analyzed before we can safely make use of
+ * them.
+ *
+ * Note: Consider using the MMU notifiers for tracking swapped-out page frames.
+ */
+
+static struct kmem_cache *bhv_domain_mm_cache = NULL;
+static LIST_HEAD(bhv_mm_head);
+
+/* Consider managing bhv_domains that hold RB trees holding mm's. */
+typedef struct bhv_domain_mm {
+	uint64_t domid;
+	struct mmu_notifier mmu_notifier;
+	struct list_head list;
+} bhv_domain_mm_t;
+
+static bhv_domain_mm_t *mmu_notifier_to_bhv_domain_mm(struct mmu_notifier *mn)
+{
+	return container_of(mn, bhv_domain_mm_t, mmu_notifier);
+}
+
+
+static void bhv_mmu_notifier_change_pte(struct mmu_notifier *mn,
+					struct mm_struct *mm,
+					unsigned long address, pte_t pte)
+{
+	bhv_domain_mm_t *bhv_mm = NULL;
+
+	bhv_mm = mmu_notifier_to_bhv_domain_mm(mn);
+	if (bhv_mm == NULL)
+		return;
+
+	pr_info("[BHV] %s: Domain[%llu] pte 0x%lx for PGD @ 0x%llx",
+		__FUNCTION__, bhv_mm->domid, pte.pte,
+		virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)));
+}
+
+static void bhv_mmu_notifier_invalidate_range(struct mmu_notifier *mn,
+					      struct mm_struct *mm,
+					      unsigned long start,
+					      unsigned long end)
+{
+	bhv_domain_mm_t *bhv_mm = NULL;
+
+	bhv_mm = mmu_notifier_to_bhv_domain_mm(mn);
+	if (bhv_mm == NULL)
+		return;
+
+	pr_info("[BHV] %s: Domain[%llu] range [0x%lx:0x%lx] for PGD @ 0x%llx",
+		__FUNCTION__, bhv_mm->domid, start, end, virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)));
+}
+
+static void
+bhv_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
+				      const struct mmu_notifier_range *range)
+{
+	bhv_domain_mm_t *bhv_mm = NULL;
+	struct mm_struct *mm = range->mm;
+	struct vm_area_struct *vma = range->vma;
+
+	bhv_mm = mmu_notifier_to_bhv_domain_mm(mn);
+	if (bhv_mm == NULL)
+		return;
+
+	/*
+	 * XXX: Check copy_page_range (memory.c):
+	 * -> Secondary MMU gets notified about a duplicated page range only if
+	 *  the change would affect/downgrade the permissions in the parent mm.
+	 *  (This is the case if the mapping is COW.)
+	 */
+
+	pr_info("[BHV] %s: Domain[%llu] range [0x%lx:0x%lx] (flags = 0x%x | %c%c%c) vs. [0x%lx:0x%lx] (flags = 0x%x | %c%c%c) for PGD @ 0x%llx (event 0x%x)",
+		__FUNCTION__, bhv_mm->domid, range->start, range->end,
+		range->flags,
+		(range->flags & VM_READ) ? 'R' : '-',
+		(range->flags & VM_WRITE) ? 'W' : '-',
+		(range->flags & VM_EXEC) ? 'X' : '-',
+		vma->vm_start, vma->vm_end,
+		vma->vm_flags,
+		(vma->vm_flags & VM_READ) ? 'R' : '-',
+		(vma->vm_flags & VM_WRITE) ? 'W' : '-',
+		(vma->vm_flags & VM_EXEC) ? 'X' : '-', virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)),
+		range->event);
+}
+
+
+static void bhv_mmu_notifier_release(struct mmu_notifier *mn,
+				     struct mm_struct *mm)
+{
+	bhv_domain_mm_t *bhv_mm = NULL;
+
+	//pr_info("[BHV] %s", __FUNCTION__);
+
+	bhv_mm = mmu_notifier_to_bhv_domain_mm(mn);
+	if (bhv_mm == NULL)
+		return;
+
+	pr_info("[BHV] %s: Domain[%llu] releasing mm for PGD @ 0x%llx",
+		__FUNCTION__, bhv_mm->domid, virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)));
+
+	bhv_domain_release_mm_SIMPLIFIED(bhv_mm->domid, mm);
+	//bhv_domain_release_mm(mm);
+}
+
+static const struct mmu_notifier_ops bhv_mmu_notifier_ops = {
+	//.invalidate_range_start = bhv_domain_mmu_notifier_invalidate_range_start,
+	//.invalidate_range = bhv_mmu_notifier_invalidate_range,
+	//.invalidate_range_end = bhv_mmu_notifier_invalidate_range_end,
+	//.change_pte = bhv_mmu_notifier_change_pte,
+	.release = bhv_mmu_notifier_release,
+};
+
+int register_mmu_notifier(struct mm_struct *const mm)
+{
+	bhv_domain_mm_t *bhv_mm =
+		kmem_cache_alloc(bhv_domain_mm_cache, GFP_KERNEL);
+	if (bhv_mm == NULL)
+		return -ENOMEM;
+
+	bhv_mm->domid = mm->owner->nsproxy->mem_ns->domain;
+	bhv_mm->mmu_notifier.ops = &bhv_mmu_notifier_ops;
+
+	list_add_tail(&bhv_mm->list, &bhv_mm_head);
+	if (list_empty(&bhv_mm_head)) {
+		kmem_cache_free(bhv_domain_mm_cache, bhv_mm);
+		return -EINVAL;
+	}
+
+#if 0
+	pr_info("[BHV] %s: registering MMU notifier for mm with PGD @ 0x%llx (%s)",
+		__FUNCTION__, virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)), mm->owner->comm);
+#endif
+
+	mmap_write_unlock(mm);
+	mmu_notifier_register(&bhv_mm->mmu_notifier, mm);
+	mmap_write_lock(mm);
+
+	return 0;
+	//return mmu_notifier_get_locked(&bhv_mm->mmu_notifier, mm);
+}
+#endif
+/* TEST END */
+
+static int bhv_vm_flags_to_mem_type(unsigned long vm_flags)
+{
+	int flags = 0;
+
+	/*
+	 * XXX: Do we want to consider VM_STACK_(*)_FLAGS?
+	 *
+	 * XXX: We need to consider VM_MAY(READ|WRITE|EXEC). This memory region
+	 * does not configure a memory access permissions (---), yet, alllows
+	 * processes to use mprotect to map the memory according to its VM_MAY*
+	 * flags. Often, this memory is marked as MAY_RWX, which requires us to
+	 * consider the fact that his memory is yet about to be mapped with any
+	 * permissions. Consider using a MUTABLE flag.
+	 *
+	 * XXX: So far, we do not consider mprotect fixups of already mapped
+	 * memory regions. This is ok, as long as we do not want to mimic the
+	 * same (more restrictive) permissions. E.g., if a memory region was
+	 * initially marked as (RW-) and then part of this region is marked as
+	 * (R--), we will not notice this, without extending mprotect.
+	 */
+
+	switch (vm_flags & VM_ACCESS_FLAGS) {
+	case VM_READ:
+		flags = BHV_MEM_TYPE_DATA_READ_ONLY;
+		break;
+	case VM_READ | VM_WRITE:
+		flags = BHV_MEM_TYPE_DATA;
+		break;
+	case VM_READ | VM_EXEC:
+		flags = BHV_MEM_TYPE_CODE;
+		break;
+	default:
+		flags = BHV_MEM_TYPE_UNKNOWN;
+	}
+
+	return flags;
+}
+
+int bhv_domain_create(uint64_t *domid)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (domid == NULL)
+		return -EINVAL;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	arg->domain.id = BHV_INVALID_DOMAIN;
+
+	rc = BHV_DOMAIN_CREATE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot create new domain", __FUNCTION__);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	*domid = arg->domain.id;
+
+	return rc;
+}
+
+void bhv_domain_destroy(uint64_t domid)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return;
+
+	arg->domain.id = domid;
+
+	rc = BHV_DOMAIN_DESTROY_HYP(arg);
+	if (rc)
+		pr_err("%s: BHV cannot destroy domain[%llu]", __FUNCTION__,
+		       domid);
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+}
+
+int bhv_domain_switch(uint64_t domid, bool in_atomic)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+	gfp_t flags;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (domid == BHV_INVALID_DOMAIN)
+		return -EINVAL;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic() && !in_atomic) {
+		pr_err("%s: in atomic (%d)\n", __FUNCTION__, in_atomic);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	if (in_atomic) {
+		flags = GFP_ATOMIC;
+	} else {
+		flags = GFP_KERNEL;
+	}
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, flags);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	arg->domain.id = domid;
+
+	rc = BHV_DOMAIN_SWITCH_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot switch to domain[%llu]", __FUNCTION__,
+		       domid);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+void bhv_domain_release_mm(struct mm_struct *const mm)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return;
+
+	if (mm == NULL)
+		return;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return;
+
+	/*
+	 * XXX: Consider PTI
+	 * - pgdp_maps_userspace, pti.c
+	 * - kernel_to_user_pgdp, pgtable.h
+	 *
+	 * ALSO:
+	 * - pgd_prepopulate_user_pmd, pgtable.c
+	 * -
+	 */
+
+	if (mm->owner == NULL)
+		arg->domain.id = bhv_get_domain(&init_task);
+	else
+		arg->domain.id = bhv_get_domain(mm->owner);
+
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+
+	rc = BHV_DOMAIN_VMA_RELEASE_ALL_HYP(arg);
+	if (rc)
+		pr_err("%s: BHV cannot release MM with PGD @ 0x%016llx",
+		       __FUNCTION__, arg->domain.pgd);
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+}
+
+void bhv_domain_release_vma(struct vm_area_struct *const vma)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+	struct mm_struct *mm = vma->vm_mm;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return;
+
+	if (mm == NULL)
+		return;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return;
+
+	arg->domain.id = mm->owner->nsproxy->mem_ns->domain;
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg->vma.start = vma->vm_start;
+	arg->vma.end = vma->vm_end;
+
+#if 0
+	pr_info("%s: Domain[0x%llx] BHV release VMA [0x%lx:0x%lx] (0x%lx | (%c%c%c))",
+		__FUNCTION__, arg->domain.pgd, vma->vm_start,
+		vma->vm_end, vma->vm_flags,
+		(vma->vm_flags & VM_READ) ? 'R' : '-',
+		(vma->vm_flags & VM_WRITE) ? 'W' : '-',
+		(vma->vm_flags & VM_EXEC) ? 'X' : '-');
+#endif
+
+	rc = BHV_DOMAIN_VMA_RELEASE_HYP(arg);
+	if (rc)
+		pr_err("%s: BHV cannot release VMA [0x%16lx:0x%16lx]",
+		       __FUNCTION__, vma->vm_start, vma->vm_end);
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+}
+
+int bhv_domain_update_vma(struct mm_struct *const mm,
+			  struct vm_area_struct *const vma,
+			  struct page *const p, size_t count, bool in_atomic)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+	gfp_t flags;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (mm == NULL || vma == NULL || p == NULL)
+		return -EINVAL;
+
+#if 0
+	pr_info("[BHV] %s: Domain[%llx] updating PFN @ 0x%lx in VMA [0x%016lx:0x%016lx] for PGD @ 0x%llx (%s)",
+		__FUNCTION__,
+		bhv_get_domain(mm->owner), page_to_pfn(p), vma->vm_start,
+		vma->vm_end, virt_to_phys(bhv_domain_get_user_pgd(mm->pgd)),
+		mm->owner->comm);
+#endif
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic() && !in_atomic) {
+		pr_err("%s: in atomic (%d)\n", __FUNCTION__, in_atomic);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	if (in_atomic) {
+		flags = GFP_ATOMIC;
+	} else {
+		flags = GFP_KERNEL;
+	}
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, flags);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	/* XXX: Do we need to pass the VMA flags? */
+	arg->domain.id = bhv_get_domain(mm->owner);
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg->vma_update.vma.start = vma->vm_start;
+	arg->vma_update.vma.end = vma->vm_end;
+	arg->vma_update.vma.flags = bhv_vm_flags_to_mem_type(vma->vm_flags);
+	arg->vma_update.pfn.pfn = page_to_pfn(p);
+	arg->vma_update.pfn.count = count;
+
+	rc = BHV_DOMAIN_VMA_UPDATE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot update VMA [0x%16lx:0x%16lx]",
+		       __FUNCTION__, vma->vm_start, vma->vm_end);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_domain_register_vma(struct mm_struct *const mm,
+			    struct vm_area_struct *const vma, bool in_atomic)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+	gfp_t flags;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (mm == NULL || vma == NULL)
+		return -EINVAL;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic() && !in_atomic) {
+		pr_err("%s: in atomic (%d)\n", __FUNCTION__, in_atomic);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	if (in_atomic) {
+		flags = GFP_ATOMIC;
+	} else {
+		flags = GFP_KERNEL;
+	}
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, flags);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	arg->domain.id = mm->owner->nsproxy->mem_ns->domain;
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg->vma.start = vma->vm_start;
+	arg->vma.end = vma->vm_end;
+	arg->vma.flags = bhv_vm_flags_to_mem_type(vma->vm_flags);
+
+	rc = BHV_DOMAIN_VMA_REGISTER_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot register VMA [0x%16lx:0x%16lx]",
+		       __FUNCTION__, vma->vm_start, vma->vm_end);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_domain_move_mm(struct mm_struct *const mm, struct nsproxy *const old_ns,
+		       struct nsproxy *const new_ns)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (mm == NULL)
+		return -EINVAL;
+
+	if (old_ns == NULL || new_ns == NULL)
+		return -EINVAL;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	arg->domain.id = old_ns->mem_ns->domain;
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(mm->pgd));
+	arg->id = new_ns->mem_ns->domain;
+
+#if 0
+	pr_info("[BHV] %s: Moving mm -- PGD @ 0x%llx (%s) (domid 0x%llx -> 0x%llx)",
+		__FUNCTION__, arg->domain.pgd, mm->owner->comm,
+		old_ns->mem_ns->domain, new_ns->mem_ns->domain);
+#endif
+
+	rc = BHV_DOMAIN_HYP_MOVE_MM(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot move PGD 0x%llx to domain[%llu]",
+		       __FUNCTION__, arg->domain.pgd, new_ns->mem_ns->domain);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_domain_dup_mmap(struct mm_struct *const old_mm,
+			struct mm_struct *const new_mm)
+{
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	/*
+	 * XXX: Investigate whether init_mm was actually registered. The
+	 * function poking_init() attempts to duplicate the init_mm struct early
+	 * on, before StrongIsolation has been initialized.
+	 */
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+	if (old_mm == NULL || new_mm == NULL || old_mm->owner == NULL)
+		return -EINVAL;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	/*
+	 * XXX: Note that not all VMAs have to be copied. Consider chaining
+	 * a list VMA update events when calling the hypercall to avoid
+	 * duplicating VMAs that are marked with the following flags
+	 * (investigate the VMA flags):
+	 *
+	 * vma->vm_flags & VM_DONTCOPY
+	 * vma->vm_flags & VM_WIPEONFORK
+	 */
+
+	arg->domain.id = bhv_get_domain(new_mm->owner);
+	arg->domain.pgd = virt_to_phys(bhv_domain_get_user_pgd(old_mm->pgd));
+	arg->pgd = virt_to_phys(bhv_domain_get_user_pgd(new_mm->pgd));
+
+	rc = BHV_DOMAIN_HYP_CLONE_PGD(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot clone PGD 0x%llx", __FUNCTION__,
+		       arg->domain.pgd);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+int bhv_domain_swap_page(uint64_t pfn) {
+	int rc = 0;
+	bhv_domain_arg_t *arg = NULL;
+
+	if (!bhv_domain_is_enabled() || !initialized)
+		return 0;
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return -ENOMEM;
+
+	arg->vma_update.pfn.pfn = pfn;
+	arg->vma_update.pfn.count = 1;
+
+	rc = BHV_DOMAIN_HYP_SWAP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot swap page 0x%llx", __FUNCTION__,
+		       pfn);
+		rc = -EINVAL;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+
+	return rc;
+}
+
+static void __init bhv_domain_init(void)
+{
+	int rc;
+	bool pti_active = false;
+	bhv_domain_arg_t *arg = NULL;
+
+	pr_info("[BHV] %s: ", __FUNCTION__);
+
+	if (initialized)
+		return;
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		pti_active = 1;
+#endif
+
+#ifdef BHV_IN_ATOMIC_DEBUG
+	if (in_atomic()) {
+		pr_err("%s: in atomic\n", __FUNCTION__);
+		dump_stack();
+	}
+#endif /* BHV_IN_ATOMIC_DEBUG */
+
+	arg = kmem_cache_alloc(bhv_domain_arg_cache, GFP_KERNEL);
+	if (arg == NULL)
+		return;
+
+	arg->config.pti = (uint64_t)pti_active;
+
+	rc = BHV_DOMAIN_CONFIGURE_HYP(arg);
+	if (rc) {
+		pr_err("%s: BHV cannot configure StrongIsolation ",
+		       __FUNCTION__);
+	} else {
+		initialized = true;
+	}
+
+	kmem_cache_free(bhv_domain_arg_cache, arg);
+}
+
+int __init bhv_domain_mm_init(void)
+{
+	if (!bhv_domain_is_enabled())
+		return 0;
+
+	bhv_domain_arg_cache = kmem_cache_create(
+		"bhv_domain_arg_cache", sizeof(bhv_domain_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	bhv_domain_init();
+
+	return 0;
+}
+
+#if CONFIG_MEM_NS
+__initcall(bhv_domain_mm_init);
+#endif
+
+#endif /* CONFIG_MEM_NS */
diff --git kernel/bhv/file_protection.c kernel/bhv/file_protection.c
new file mode 100644
index 000000000..987e52664
--- /dev/null
+++ kernel/bhv/file_protection.c
@@ -0,0 +1,122 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <bhv/bhv_print.h>
+
+#include <linux/cache.h>
+#include <linux/gfp.h>
+#include <linux/limits.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/file_protection.h>
+
+#include <bhv/file_protection.h>
+
+
+bhv_file_protection_config_t bhv_file_protection_config __ro_after_init = { 0 };
+
+struct kmem_cache *bhv_file_protection_violation_cache;
+
+void __init bhv_file_protection_init(void)
+{
+	unsigned long r;
+
+	if (!bhv_file_protection_is_enabled())
+		return;
+
+	// Create slab cache
+	bhv_file_protection_violation_cache = kmem_cache_create(
+		"bhv_file_protection_violation_cache",
+		sizeof(bhv_file_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_file_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+
+#ifndef VASKM // inside kernel tree
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION,
+			      BHV_VAS_FILE_PROTECTION_OP_INIT,
+			      &bhv_file_protection_config);
+	if (r) {
+		pr_err("File protection init failed");
+		return;
+	}
+
+#else // out of tree
+	bhv_file_protection_config_t *bhv_file_protection_config_tmp_ptr =
+		(bhv_file_protection_config_t *)kmalloc(
+			sizeof(bhv_file_protection_config_t), GFP_KERNEL);
+	if (!bhv_file_protection_config_tmp_ptr) {
+		pr_err("BHV: cannot allocate bhv_file_protection_config\n");
+		return;
+	}
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_FILE_PROTECTION,
+			      BHV_VAS_FILE_PROTECTION_OP_INIT,
+			      bhv_file_protection_config_tmp_ptr);
+	if (r) {
+		pr_err("File protection init failed");
+		kfree(bhv_file_protection_config_tmp_ptr);
+		return;
+	}
+
+	bhv_file_protection_config = *bhv_file_protection_config_tmp_ptr;
+
+	kfree(bhv_file_protection_config_tmp_ptr);
+#endif // VASKM
+}
+
+bool bhv_block_read_only_file_write(const char *target)
+{
+	unsigned long r;
+	bool rv;
+	bhv_file_protection_violation_t *volatile violation = NULL;
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return false;
+
+	// Prepare hypercall
+	violation = kmem_cache_alloc(bhv_file_protection_violation_cache,
+				     GFP_KERNEL);
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate file protection violation");
+		return true;
+	}
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	// Setup arg
+	violation->name_len = strlen(target);
+
+	violation->name = bhv_virt_to_phys((void *)target);
+
+	// Hypercall
+	r = bhv_hypercall_vas(
+		BHV_VAS_BACKEND_FILE_PROTECTION,
+		BHV_VAS_FILE_PROTECTION_OP_VIOLATION_READ_ONLY_FILE_PROTECTION,
+		violation);
+	if (r) {
+		pr_err("file protection violation hypercall failed");
+		kmem_cache_free(bhv_file_protection_violation_cache, violation);
+		return true;
+	}
+
+	// Read block and free
+	rv = (bool)violation->block;
+	kmem_cache_free(bhv_file_protection_violation_cache, violation);
+	return rv;
+}
\ No newline at end of file
diff --git kernel/bhv/fileops_protection.c kernel/bhv/fileops_protection.c
new file mode 100644
index 000000000..c686b27a0
--- /dev/null
+++ kernel/bhv/fileops_protection.c
@@ -0,0 +1,260 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Author: Robert Gawlik <robert@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <linux/slab.h>
+#include <uapi/linux/magic.h>
+#include <linux/sort.h>
+#include <linux/bsearch.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/fileops_protection.h>
+
+#include <bhv/fileops_protection.h>
+#include <bhv/fileops_internal.h>
+
+struct kmem_cache *bhv_fileops_protection_violation_cache;
+
+void __init bhv_fileops_protection_mm_init(void)
+{
+	if (!bhv_fileops_protection_is_enabled())
+		return;
+
+	// Create slab cache for violation data being set by host
+	bhv_fileops_protection_violation_cache = kmem_cache_create(
+		"bhv_fileops_protection_violation_cache",
+		sizeof(bhv_fileops_protection_violation_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_fileops_protection_violation_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache.");
+		return;
+	}
+}
+
+u8 bhv_fileops_type(u32 fs_magic)
+{
+	u8 op;
+	switch (fs_magic) {
+#ifdef CONFIG_EXT4_FS
+	case EXT4_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_EXT4;
+		break;
+#endif
+#ifdef CONFIG_XFS_FS
+	case XFS_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_XFS;
+		break;
+#endif
+	case TMPFS_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_TMPFS;
+		break;
+	case PROC_SUPER_MAGIC:
+		op = BHV_VAS_FILEOPS_PROTECTION_PROC;
+		break;
+	default:
+		op = BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED;
+	}
+
+	return op;
+}
+
+bool bhv_block_fileops(const char *target, u8 fops_type, bool is_dir)
+{
+	unsigned long err;
+	bool retval;
+	bhv_fileops_protection_violation_t *violation;
+	size_t path_sz;
+	int rv;
+
+	if (!bhv_fileops_protection_is_enabled())
+		return false;
+
+	violation = kmem_cache_alloc(bhv_fileops_protection_violation_cache,
+				     GFP_KERNEL);
+	if (violation == NULL) {
+		bhv_fail("Unable to allocate fileops protection violation");
+		return true;
+	}
+
+	rv = populate_event_context(&violation->context, true);
+	if (rv) {
+		bhv_fail("%s: BHV cannot retrieve event context", __FUNCTION__);
+		return true;
+	}
+
+	path_sz = strlen(target);
+	if (path_sz >= BHV_VAS_FILEOPS_PATH_MAX_SZ) {
+		path_sz = BHV_VAS_FILEOPS_PATH_MAX_SZ - 1;
+	}
+
+	memcpy(violation->path_name, target, path_sz);
+	violation->fops_type = fops_type;
+	violation->path_name[path_sz] = '\0';
+	violation->is_dir = (uint8_t)is_dir;
+
+	/* ask the host whether to log or block that violation
+	 * send file name and file system type */
+	err = bhv_hypercall_vas(BHV_VAS_BACKEND_FILEOPS_PROTECTION,
+				BHV_VAS_FILEOPS_PROTECTION_OP_VIOLATION,
+				violation);
+	if (err) {
+		pr_err("File operations protection hypercall failed");
+		kmem_cache_free(bhv_fileops_protection_violation_cache,
+				violation);
+		return true;
+	}
+
+	retval = (bool)violation->block;
+	kmem_cache_free(bhv_fileops_protection_violation_cache, violation);
+
+	return retval;
+}
+
+const fops_t fileops_map[] = {
+#ifdef CONFIG_EXT4_FS
+	// ext4 files and dirs
+	[BHV_VAS_FILEOPS_PROTECTION_EXT4] = { &ext4_file_operations,
+					      &ext4_dir_operations },
+#endif
+	// tmpfs files and dirs
+	[BHV_VAS_FILEOPS_PROTECTION_TMPFS] = { &shmem_file_operations,
+					       &simple_dir_operations },
+	// special chardevs
+	[BHV_VAS_FILEOPS_PROTECTION_MEM] = { &mem_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_NULL] = { &null_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_PORT] = { &port_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_ZERO] = { &zero_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_FULL] = { &full_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_RANDOM] = { &random_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_URANDOM] = { &urandom_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_KMSG] = { &kmsg_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_TTY] = { &tty_fops, NULL },
+	[BHV_VAS_FILEOPS_PROTECTION_CONSOLE] = { &console_fops, NULL },
+	// proc basic
+	[BHV_VAS_FILEOPS_PROTECTION_PROC] = { &proc_reg_file_ops,
+					      &proc_root_operations },
+#ifdef CONFIG_XFS_FS
+	// xfs files and dirs
+	[BHV_VAS_FILEOPS_PROTECTION_XFS] = { &xfs_file_operations,
+					     &xfs_dir_file_operations }
+#endif
+
+};
+
+// additional /proc/ file operations
+struct file_operations const *proc_fops[] __ro_after_init = {
+	&empty_dir_operations,
+	&proc_pid_cmdline_ops,
+	&proc_pid_maps_operations,
+	&proc_pid_set_comm_operations,
+#ifdef CONFIG_SECURITY
+	&proc_pid_attr_operations,
+	&proc_attr_dir_operations,
+#endif
+	&proc_mounts_operations,
+	&proc_mountstats_operations,
+	&proc_sys_file_operations,
+	&proc_sys_dir_file_operations,
+	&proc_single_file_operations,
+	&proc_iter_file_ops,
+	&proc_fd_operations,
+	&proc_environ_operations,
+	&proc_loginuid_operations,
+	&proc_mountinfo_operations,
+	&proc_oom_score_adj_operations,
+#ifdef CONFIG_LATENCYTOP
+	&proc_lstats_operations,
+#endif
+	&proc_mem_operations,
+	&proc_auxv_operations,
+	&proc_oom_adj_operations,
+#ifdef CONFIG_AUDIT
+	&proc_sessionid_operations,
+#endif
+#ifdef CONFIG_FAULT_INJECTION
+	&proc_fault_inject_operations,
+	&proc_fail_nth_operations,
+#endif
+#ifdef CONFIG_SCHED_DEBUG
+	&proc_pid_sched_operations,
+#endif
+#ifdef CONFIG_SCHED_AUTOGROUP
+	&proc_pid_sched_autogroup_operations,
+#endif
+#ifdef CONFIG_TIME_NS
+	&proc_timens_offsets_operations,
+#endif
+	&proc_map_files_operations,
+#if defined(CONFIG_CHECKPOINT_RESTORE) && defined(CONFIG_POSIX_TIMERS)
+	&proc_timers_operations,
+#endif
+	&proc_pid_set_timerslack_ns_operations,
+#ifdef CONFIG_ELF_CORE
+	&proc_coredump_filter_operations,
+#endif
+#ifdef CONFIG_USER_NS
+	&proc_uid_map_operations,
+	&proc_gid_map_operations,
+	&proc_projid_map_operations,
+	&proc_setgroups_operations,
+#endif
+	&proc_tgid_base_operations,
+	&proc_tid_base_operations,
+	&proc_task_operations,
+#ifdef CONFIG_COMPAT
+	&proc_reg_file_ops_compat,
+	&proc_iter_file_ops_compat,
+#endif
+#if defined(CONFIG_ZRAM) && defined(CONFIG_ZRAM_MEMORY_TRACKING)
+	&proc_zram_block_state_op,
+#endif
+#ifdef CONFIG_PAGE_OWNER
+	&proc_page_owner_operations,
+#endif
+	&proc_ns_dir_operations,
+	&proc_net_operations,
+#ifdef CONFIG_NUMA
+	&proc_pid_numa_maps_operations,
+#endif
+	&proc_pid_smaps_operations,
+	&proc_pid_smaps_rollup_operations,
+	&proc_clear_refs_operations,
+	&proc_pagemap_operations,
+#ifdef CONFIG_PROC_CHILDREN
+	&proc_tid_children_operations,
+#endif
+	&proc_dir_operations,
+	&proc_fdinfo_operations,
+	&proc_fdinfo_file_operations
+};
+
+int cmp_fileops(const void *a_ptr, const void *b_ptr)
+{
+	u64 a = *(u64 *)a_ptr;
+	u64 b = *(u64 *)b_ptr;
+	return ((a == b) ? 0 : (a > b ? 1 : -1));
+}
+
+void __init bhv_fileops_init(void)
+{
+	if (bhv_fileops_protection_is_enabled()) {
+		sort(proc_fops, sizeof(proc_fops) / sizeof(proc_fops[0]),
+		     sizeof(proc_fops[0]), cmp_fileops, NULL);
+	}
+}
+
+bool is_valid_proc_fop(const struct file_operations **fop_ptr)
+{
+	if (bsearch(fop_ptr, proc_fops,
+		    sizeof(proc_fops) / sizeof(proc_fops[0]),
+		    sizeof(proc_fops[0]), cmp_fileops))
+		return true;
+
+	return false;
+}
diff --git kernel/bhv/guestconn.c kernel/bhv/guestconn.c
new file mode 100644
index 000000000..42e9585cc
--- /dev/null
+++ kernel/bhv/guestconn.c
@@ -0,0 +1,236 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ */
+#include <linux/atomic.h>
+#include <linux/net.h>
+#include <linux/socket.h>
+#include <linux/types.h>
+#include <linux/reboot.h>
+#include <linux/workqueue.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <net/net_namespace.h>
+
+#include <net/vsock_addr.h>
+
+#include <bhv/bhv.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/guestconn.h>
+
+typedef struct {
+	struct list_head list;
+	size_t to_send;
+	guestconn_msg_header_t header;
+	uint8_t body[BHV_GUESTCONN_MAX_MSG_SZ];
+} bhv_guestconn_send_item_t;
+
+uint32_t bhv_guestconn_cid __ro_after_init = 0;
+uint32_t bhv_guestconn_port __ro_after_init = 0;
+
+static struct socket *vsock = NULL;
+
+static atomic_t workqueue_ready = ATOMIC_INIT(0);
+static atomic_t reboot_in_progress = ATOMIC_INIT(0);
+static LIST_HEAD(bhv_guestconn_msg_list);
+static DEFINE_SPINLOCK(bhv_guestconn_msg_lock);
+
+static struct workqueue_struct *bhv_guestconn_workqueue = NULL;
+static struct work_struct bhv_guestconn_work_struct;
+static struct delayed_work bhv_guestconn_delayed_work_struct;
+static struct kmem_cache *bhv_guestconn_send_item_cache;
+
+static inline size_t bhv_send(void *data, size_t size, size_t to_send)
+{
+	int r;
+	struct kvec vec;
+	struct msghdr msghdr = { .msg_flags = MSG_DONTWAIT };
+	while (to_send > 0) {
+		vec.iov_base = data + (size - to_send);
+		vec.iov_len = to_send;
+		r = kernel_sendmsg(vsock, &msghdr, &vec, 1, vec.iov_len);
+		if (r == -EAGAIN) {
+			return to_send;
+		} else if (r < 0) {
+			pr_err("BHV GuestLog: Send Failed (%d)", r);
+			return 0;
+		}
+		to_send -= r;
+	}
+	return 0;
+}
+
+static void bhv_guestconn_sendmsg(struct work_struct *ws)
+{
+	bhv_guestconn_send_item_t *item;
+
+	while (true) {
+		spin_lock(&bhv_guestconn_msg_lock);
+		if (list_empty(&bhv_guestconn_msg_list)) {
+			spin_unlock(&bhv_guestconn_msg_lock);
+			return;
+		}
+		item = list_first_entry(&bhv_guestconn_msg_list,
+					bhv_guestconn_send_item_t, list);
+		spin_unlock(&bhv_guestconn_msg_lock);
+
+		item->to_send =
+			bhv_send(&item->header, item->header.sz, item->to_send);
+
+		if (item->to_send == 0 ||
+		    unlikely(atomic_read(&reboot_in_progress))) {
+			spin_lock(&bhv_guestconn_msg_lock);
+			list_del(&(item->list));
+			spin_unlock(&bhv_guestconn_msg_lock);
+			kmem_cache_free(bhv_guestconn_send_item_cache, item);
+		} else {
+			queue_delayed_work(bhv_guestconn_workqueue,
+					   &bhv_guestconn_delayed_work_struct,
+					   msecs_to_jiffies(1000));
+			return;
+		}
+	}
+}
+
+int bhv_guestconn_send(uint16_t type, void *data, size_t size)
+{
+	bhv_guestconn_send_item_t *cur;
+
+	BUG_ON(size > BHV_GUESTCONN_MAX_BODY_SZ);
+
+	pr_debug("BHV GuestConn: Queuing msg of type %u with size %lu", type,
+		 size);
+
+	BUG_ON(!bhv_guestconn_send_item_cache);
+
+	cur = kmem_cache_alloc(bhv_guestconn_send_item_cache, GFP_KERNEL);
+	if (cur == NULL) {
+		bhv_fail("BHV: Unable to allocate send item");
+		return -ENOMEM;
+	}
+
+	cur->header.type = type;
+	cur->header.sz = sizeof(guestconn_msg_header_t) + size;
+
+	memcpy(cur->body, data, size);
+
+	cur->to_send = cur->header.sz;
+
+	spin_lock(&bhv_guestconn_msg_lock);
+
+	if (unlikely(atomic_read(&reboot_in_progress))) {
+		spin_unlock(&bhv_guestconn_msg_lock);
+		kmem_cache_free(bhv_guestconn_send_item_cache, cur);
+		return 0;
+	}
+
+	list_add_tail(&(cur->list), &bhv_guestconn_msg_list);
+	spin_unlock(&bhv_guestconn_msg_lock);
+
+	if (atomic_read(&workqueue_ready))
+		queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+
+	return 0;
+}
+
+static int bhv_guestconn_reboot(struct notifier_block *notifier,
+				unsigned long val, void *v)
+{
+	if (atomic_read(&workqueue_ready)) {
+		atomic_inc(&reboot_in_progress);
+
+		// Cancel pending work.
+		cancel_delayed_work_sync(&bhv_guestconn_delayed_work_struct);
+
+		// Drain the workqueue
+		drain_workqueue(bhv_guestconn_workqueue);
+
+		// We assume all messages are gone now and we shut down the socket
+		sock_release(vsock);
+		vsock = NULL;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bhv_guestconn_reboot_notifier = {
+	.notifier_call = bhv_guestconn_reboot,
+	.priority = 0,
+};
+
+void bhv_guestconn_start(void)
+{
+	int err;
+	struct sockaddr_vm addr;
+
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	vsock_addr_init(&addr, bhv_guestconn_cid, bhv_guestconn_port);
+	pr_info("bhv_guestconn_started with cid %u, port %u", bhv_guestconn_cid,
+		bhv_guestconn_port);
+
+	err = sock_create_kern(&init_net, AF_VSOCK, SOCK_STREAM, 0, &vsock);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not create kernel socket (%d)", err);
+		return;
+	}
+
+	err = kernel_connect(vsock, (struct sockaddr *)&addr,
+			     sizeof(struct sockaddr_vm), 0);
+	if (err < 0) {
+		bhv_fail("GuestConn: Could not connect to host (%d)", err);
+		return;
+	}
+
+	// Initialize work queue
+	INIT_WORK(&bhv_guestconn_work_struct, bhv_guestconn_sendmsg);
+	INIT_DELAYED_WORK(&bhv_guestconn_delayed_work_struct,
+			  bhv_guestconn_sendmsg);
+	bhv_guestconn_workqueue =
+		alloc_workqueue("bhv_guestconn_workqueue", WQ_UNBOUND, 1);
+	// queue = create_singlethread_workqueue("bhv_guestlog_work_queue");
+	if (bhv_guestconn_workqueue == NULL) {
+		bhv_fail("BHV: Could not allocate work queue!");
+		kmem_cache_destroy(bhv_guestconn_send_item_cache);
+		return;
+	}
+	atomic_inc(&workqueue_ready);
+
+	register_reboot_notifier(&bhv_guestconn_reboot_notifier);
+
+	queue_work(bhv_guestconn_workqueue, &bhv_guestconn_work_struct);
+}
+
+void __init bhv_guestconn_mm_init(void)
+{
+	if (!is_bhv_initialized())
+		return;
+
+	BUG_ON(bhv_guestconn_cid == 0 && bhv_guestconn_port == 0);
+
+	// Create cache
+	bhv_guestconn_send_item_cache = kmem_cache_create(
+		"bhv_guestconn_send_item_cache",
+		sizeof(bhv_guestconn_send_item_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+	if (bhv_guestconn_send_item_cache == NULL) {
+		bhv_fail("BHV: Could not create kmem_cache for work items!");
+		return;
+	}
+}
+
+int __init bhv_guestconn_init(uint32_t cid, uint32_t port)
+{
+	if (!is_bhv_initialized())
+		return 0;
+	bhv_guestconn_cid = cid;
+	bhv_guestconn_port = port;
+	return 0;
+}
\ No newline at end of file
diff --git kernel/bhv/guestlog.c kernel/bhv/guestlog.c
new file mode 100644
index 000000000..0aeffb6c9
--- /dev/null
+++ kernel/bhv/guestlog.c
@@ -0,0 +1,207 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include <bhv/bhv.h>
+#include <bhv/event.h>
+
+#include <bhv/guestconn.h>
+#include <bhv/guestlog.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/guestlog.h>
+
+bhv_guestlog_config_t bhv_guestlog_config __ro_after_init = { 0, false };
+
+int bhv_guestlog_log_str(char *fmt, ...)
+{
+	int len;
+	va_list args;
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// format string and set vector
+	va_start(args, fmt);
+	len = 1 /* null terminator */ +
+	      vscnprintf(msg.str.buf,
+			 BHV_GUESTLOG_MAX_MSG_DATA_SZ -
+				 sizeof(bhv_event_context_t),
+			 fmt, args);
+	va_end(args);
+
+	sz = bhv_guestlog_calc_msg_sz(BHV_GUESTLOG_MSG_TYPE_STR, len);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_STR;
+	msg.header.sz = sz;
+
+	if (populate_event_context(&msg.str.context, true)) {
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_fork(uint32_t child_pid, const char *child_comm,
+				  uint32_t parent_pid, const char *parent_comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_FORK;
+
+	if (populate_event_context(&msg.process_fork.context, true)) {
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	msg.process_fork.child_pid = child_pid;
+	msg.process_fork.parent_pid = parent_pid;
+	msg.process_fork.child_comm_offset = 0;
+	strscpy(msg.process_fork.buf, child_comm, TASK_COMM_LEN);
+	msg.process_fork.parent_comm_offset =
+		strnlen(msg.process_fork.buf, TASK_COMM_LEN) + 1;
+	strscpy(&msg.process_fork.buf[msg.process_fork.parent_comm_offset],
+		parent_comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		msg.process_fork.parent_comm_offset +
+			strnlen(&msg.process_fork.buf
+					 [msg.process_fork.parent_comm_offset],
+				TASK_COMM_LEN) +
+			1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_exec(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_EXEC;
+
+	if (populate_event_context(&msg.process_exec.context, true)) {
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	msg.process_exec.pid = pid;
+	msg.process_exec.parent_pid = parent_pid;
+	strscpy(msg.process_exec.name, comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		strnlen(msg.process_exec.name, TASK_COMM_LEN) + 1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_process_exit(uint32_t pid, uint32_t parent_pid,
+				  const char *comm)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_PROCESS_EXIT;
+
+	if (populate_event_context(&msg.process_exit.context, false)) {
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	msg.process_exit.pid = pid;
+	msg.process_exit.parent_pid = parent_pid;
+	strscpy(msg.process_exit.name, comm, TASK_COMM_LEN);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		strnlen(msg.process_exit.name, TASK_COMM_LEN) + 1);
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int bhv_guestlog_log_driver_load(const char *name)
+{
+	guestlog_msg_t msg;
+	size_t sz;
+
+	// Setup msg
+	msg.header.type = BHV_GUESTLOG_MSG_TYPE_DRIVER_LOAD;
+
+	if (populate_event_context(&msg.driver_load.context, true)) {
+		pr_err("%s: BHV cannot retrieve event context", __FUNCTION__);
+	}
+
+	strscpy(msg.driver_load.name, name,
+		BHV_GUESTLOG_MAX_MSG_DATA_SZ - sizeof(bhv_event_context_t) - 1);
+	sz = bhv_guestlog_calc_msg_sz(
+		msg.header.type,
+		strnlen(msg.driver_load.name,
+			BHV_GUESTLOG_MAX_MSG_DATA_SZ -
+				sizeof(bhv_event_context_t) - 1));
+	BUG_ON(sz > BHV_GUESTLOG_MAX_MSG_SZ);
+	msg.header.sz = sz;
+
+	// Send
+	return bhv_guestconn_send(BHV_GUESTCONN_MSG_TYPE_LOG, &msg, sz);
+}
+
+int __init bhv_guestlog_init()
+{
+	unsigned long r;
+
+	if (!bhv_guestlog_enabled())
+		return 0;
+
+#ifndef VASKM // inside kernel tree
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_GUESTLOG,
+			      BHV_VAS_GUESTLOG_OP_INIT_GUESTLOG,
+			      &bhv_guestlog_config);
+	if (r) {
+		bhv_fail("BHV: guestlog init failed");
+		return -EFAULT;
+	}
+
+#else // out of tree
+	bhv_guestlog_config_t *bhv_guestlog_config_tmp_ptr =
+		(bhv_guestlog_config_t *)kmalloc(sizeof(bhv_guestlog_config_t),
+						 GFP_KERNEL);
+	if (!bhv_guestlog_config_tmp_ptr) {
+		pr_err("BHV: cannot allocate bhv_guestlog_config\n");
+		return -EFAULT;
+	}
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_GUESTLOG,
+			      BHV_VAS_GUESTLOG_OP_INIT_GUESTLOG,
+			      bhv_guestlog_config_tmp_ptr);
+	if (r) {
+		bhv_fail("BHV: guestlog init failed");
+		kfree(bhv_guestlog_config_tmp_ptr);
+		return -EFAULT;
+	}
+
+	bhv_guestlog_config = *bhv_guestlog_config_tmp_ptr;
+
+	kfree(bhv_guestlog_config_tmp_ptr);
+#endif // VASKM
+
+	return 0;
+}
diff --git kernel/bhv/init.c kernel/bhv/init.c
new file mode 100644
index 000000000..0f4b18a3f
--- /dev/null
+++ kernel/bhv/init.c
@@ -0,0 +1,178 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <asm/io.h>
+#include <asm/sections.h>
+#include <linux/kmod.h>
+#include <linux/mm.h>
+
+#include <bhv/interface/common.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/init.h>
+#include <bhv/interface/integrity.h>
+#include <bhv/integrity.h>
+#include <bhv/init.h>
+#include <bhv/creds.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+
+#else // out of tree
+#include <kln.h>
+#include <common.h>
+
+extern uint8_t __bhv_text_start[];
+extern uint8_t __bhv_text_end[];
+#endif // VASKM
+
+static inline void
+bhv_section_run_ctor(bhv_init_init_bhv_section_run_t *curr_item,
+		     bhv_init_init_bhv_section_run_t *prev_item,
+		     uint64_t gpa_start, uint64_t size, uint8_t type)
+{
+	BUG_ON(!curr_item);
+	*curr_item = (bhv_init_init_bhv_section_run_t){
+		.gpa_start = gpa_start,
+		.size = size,
+		.type = type,
+		.next = BHV_INVALID_PHYS_ADDR,
+	};
+
+	if (prev_item)
+		prev_item->next = bhv_virt_to_phys(curr_item);
+}
+
+
+int __init bhv_init_hyp(void *bhv_data, size_t bhv_data_size)
+{
+	unsigned long r;
+	unsigned int region_counter = 0;
+
+	struct {
+		bhv_init_arg_t init_arg;
+		bhv_mem_region_t mem_regions[BHV_INIT_MAX_REGIONS];
+		bhv_init_init_bhv_section_run_t bhv_section_runs[];
+	} *arg = bhv_data;
+	BUG_ON((void *)&arg->bhv_section_runs[2] - bhv_data >
+	       +bhv_data_size);
+
+
+#ifndef VASKM // inside kernel tree
+#define BI_ALIGN_START(start) (unsigned long)(start)
+#else // out of tree
+#define BI_ALIGN_START(start) round_down((unsigned long)(start), PAGE_SIZE)
+#endif // VASKM
+
+#define BI_ALIGN_START_SIZE(start, end)                                        \
+	(bhv_virt_to_phys((void *)BI_ALIGN_START(start))),                     \
+		(round_up((unsigned long)(end), PAGE_SIZE) -                   \
+		 BI_ALIGN_START(start))
+#define BI_ALIGN_START_SIZE_KLN(start, end)                                    \
+	BI_ALIGN_START_SIZE(KLN_SYM(start), KLN_SYM(end))
+#define BI_LL_FIRST(ll) &(ll)[region_counter], NULL
+#define BI_LL_NEXT(ll) &(ll)[region_counter], &(ll)[region_counter - 1]
+
+	bhv_mem_region_create_ctor(BI_LL_FIRST(arg->mem_regions),
+				   BI_ALIGN_START_SIZE_KLN(_stext, _etext),
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_NONE, "KERNEL TEXT SECTION");
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sinittext, _einittext),
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_TRANSIENT,
+				   "KERNEL INIT TEXT SECTION");
+	region_counter++;
+
+	bhv_mem_region_create_ctor(BI_LL_NEXT(arg->mem_regions),
+				   BI_ALIGN_START_SIZE(_sexittext, _eexittext),
+				   BHV_MEM_TYPE_CODE_PATCHABLE,
+				   BHV_MEM_FLAGS_TRANSIENT,
+				   "KERNEL EXIT TEXT SECTION");
+	region_counter++;
+#endif // VASKM
+
+	bhv_init_hyp_arch(arg->mem_regions, &region_counter);
+
+	region_counter = 0;
+	BUG_ON((unsigned long)bhv_data & ~PAGE_MASK);
+	bhv_section_run_ctor(BI_LL_FIRST(arg->bhv_section_runs),
+			     bhv_virt_to_phys(bhv_data), bhv_data_size,
+			     BHV_SRT_DATA);
+	region_counter++;
+
+#ifndef VASKM // inside kernel tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(BI_LL_NEXT(arg->bhv_section_runs),
+			     BI_ALIGN_START_SIZE(__bhv_text_start,
+						 __bhv_text_end),
+			     BHV_SRT_VAULT);
+	arg->init_arg.bhv_init_init_arg.modprobe_path_sz = KMOD_PATH_LEN;
+	arg->init_arg.bhv_init_init_arg.modprobe_path =
+		bhv_virt_to_phys((void *)&modprobe_path);
+	region_counter++;
+
+#else // out of tree
+	BUG_ON((unsigned long)__bhv_text_start & ~PAGE_MASK);
+	bhv_section_run_ctor(BI_LL_NEXT(arg->bhv_section_runs),
+			     vmalloc_to_phys(__bhv_text_start), PAGE_SIZE,
+			     BHV_SRT_VAULT);
+
+	for (uint8_t *p = __bhv_text_start + PAGE_SIZE; p < __bhv_text_end;
+	     p += PAGE_SIZE) {
+		phys_addr_t phy = vmalloc_to_phys(p);
+		if (phy == arg->bhv_section_runs[region_counter].gpa_start +
+				   arg->bhv_section_runs[region_counter].size) {
+			arg->bhv_section_runs[region_counter].size += PAGE_SIZE;
+		} else {
+			region_counter++;
+			BUG_ON((uint64_t)&arg->bhv_section_runs[region_counter +
+								1] -
+				       (uint64_t)arg >
+			       bhv_data_size);
+
+			bhv_section_run_ctor(BI_LL_NEXT(arg->bhv_section_runs),
+					     phy, PAGE_SIZE, BHV_SRT_VAULT);
+		}
+	}
+	region_counter++;
+
+	for (int i = 0; i < region_counter; i++)
+		printk(KERN_INFO "%s: SRT %d: %llx %llx (%d)", __FUNCTION__, i,
+		       arg->bhv_section_runs[i].gpa_start,
+		       arg->bhv_section_runs[i].size,
+		       arg->bhv_section_runs[i].type);
+
+	arg->init_arg.bhv_init_init_arg.modprobe_path_sz = 0;
+	arg->init_arg.bhv_init_init_arg.modprobe_path = BHV_INVALID_PHYS_ADDR;
+#endif // VASKM
+
+	arg->init_arg.bhv_init_init_arg.owner = 0;
+	arg->init_arg.bhv_init_init_arg.region_head =
+		bhv_virt_to_phys(&arg->mem_regions);
+	arg->init_arg.bhv_init_init_arg.bhv_region_head =
+		bhv_virt_to_phys(&arg->bhv_section_runs);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INIT, BHV_VAS_INIT_OP_INIT,
+			      arg);
+	if (r)
+		return -EINVAL;
+	return 0;
+}
+
+int __init_km bhv_start_hyp(bhv_init_start_config_t *config)
+{
+	unsigned long r = bhv_hypercall_vas(BHV_VAS_BACKEND_INIT,
+					    BHV_VAS_INIT_OP_START, config);
+	if (r)
+		return -EINVAL;
+	return 0;
+}
diff --git kernel/bhv/integrity.c kernel/bhv/integrity.c
new file mode 100644
index 000000000..7a0a6469d
--- /dev/null
+++ kernel/bhv/integrity.c
@@ -0,0 +1,145 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ */
+
+#include <linux/types.h>
+#include <asm/io.h>
+
+#include <bhv/bhv.h>
+#include <bhv/interface/hypercall.h>
+
+#include <bhv/integrity.h>
+
+struct kmem_cache *bhv_mem_region_cache;
+struct kmem_cache *bhv_integrity_arg_cache;
+
+void __init bhv_integrity_mm_init(void)
+{
+	bhv_mem_region_cache = kmem_cache_create(
+		"bhv_mem_region_cache", sizeof(bhv_mem_region_node_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+
+	bhv_integrity_arg_cache = kmem_cache_create(
+		"bhv_integrity_arg_cache", sizeof(bhv_integrity_arg_t), 0,
+		SLAB_HWCACHE_ALIGN | SLAB_PANIC | SLAB_ACCOUNT, NULL);
+}
+
+int bhv_integrity_freeze_events(uint64_t flags)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_freeze_arg.flags = flags;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_FREEZE, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_create_kern_phys_mem_region_hyp(uint64_t owner,
+					bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_create_arg.owner = owner;
+	bhv_arg->bhv_integrity_create_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_CREATE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_update_kern_phys_mem_region_hyp(bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_update_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_UPDATE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_remove_kern_phys_mem_region_by_region_hyp(bhv_mem_region_t *region_head)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_remove_arg.rm_by_owner = 0;
+	bhv_arg->bhv_integrity_remove_arg.region_head =
+		virt_to_phys(region_head);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_REMOVE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
+
+int bhv_remove_kern_phys_mem_region_by_owner_hyp(uint64_t owner)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_integrity_arg_t *bhv_arg =
+		kmem_cache_alloc(bhv_integrity_arg_cache, GFP_KERNEL);
+	if (!bhv_arg) {
+		bhv_fail("BHV: failed to allocate arg.");
+		return -ENOMEM;
+	}
+
+	bhv_arg->bhv_integrity_remove_arg.rm_by_owner = 1;
+	bhv_arg->bhv_integrity_remove_arg.owner = owner;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_INTEGRITY,
+			      BHV_VAS_INTEGRITY_OP_REMOVE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kmem_cache_free(bhv_integrity_arg_cache, bhv_arg);
+	return rv;
+}
diff --git kernel/bhv/module.c kernel/bhv/module.c
new file mode 100644
index 000000000..42100fe19
--- /dev/null
+++ kernel/bhv/module.c
@@ -0,0 +1,494 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+
+#include <asm/io.h>
+
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/bhv_print.h>
+
+typedef int (*bhv_link_node_cb_t)(struct list_head *, uint64_t, uint64_t,
+				  uint32_t, uint64_t, const char *);
+
+static int _bhv_link_node_op_create(struct list_head *head, uint64_t pfn,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label)
+{
+	return bhv_link_node_op_create(head, pfn << PAGE_SHIFT, size, type,
+				       flags, label);
+}
+
+#ifdef CONFIG_MODULES
+static int _bhv_link_node_op_update(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t type,
+				    uint64_t flags, const char *unused2)
+{
+	return bhv_link_node_op_update(head, pfn << PAGE_SHIFT, type, flags);
+}
+
+static int _bhv_link_node_op_remove(struct list_head *head, uint64_t pfn,
+				    uint64_t unused1, uint32_t unused2,
+				    uint64_t unused3, const char *unused4)
+{
+	return bhv_link_node_op_remove(head, pfn << PAGE_SHIFT);
+}
+#endif /* CONFIG_MODULES */
+
+static void bhv_prepare_mod_section(struct list_head *head, const void *base,
+				    uint64_t size, uint32_t type,
+				    uint64_t flags, const char *label,
+				    bhv_link_node_cb_t link_node_cb)
+{
+	int rv;
+	uint64_t i = 0;
+	uint64_t nr_pages = 0;
+	uint64_t pfn = 0;
+	uint64_t pfn_count_consecutive = 0;
+
+	BUG_ON(!PAGE_ALIGNED(base));
+	BUG_ON(!PAGE_ALIGNED(size));
+
+	if (base == NULL || size == 0)
+		return;
+
+	/* This is ok, because size is always a number of pages. */
+	nr_pages = (((uint64_t)base + size) - (uint64_t)base) >> PAGE_SHIFT;
+
+	for (i = 0; i < nr_pages; ++i) {
+		struct page *p = NULL;
+		uint64_t size_consecutive = 0;
+
+		p = vmalloc_to_page(base + (i << PAGE_SHIFT));
+		if (p == NULL) {
+			pr_err("%s: Cannot translate addr @ 0x%llx",
+			       __FUNCTION__,
+			       (uint64_t)(base + (i << PAGE_SHIFT)));
+			return;
+		}
+
+		if (pfn_count_consecutive == 0) {
+			pfn = page_to_pfn(p);
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		if ((page_to_pfn(p) - pfn) == pfn_count_consecutive) {
+			pfn_count_consecutive++;
+			continue;
+		}
+
+		/* We have found a physically non-contiguous section. */
+
+		if ((pfn_count_consecutive << PAGE_SHIFT) > size)
+			size_consecutive = size;
+		else
+			size_consecutive = pfn_count_consecutive << PAGE_SHIFT;
+
+		rv = link_node_cb(head, pfn, size_consecutive, type, flags,
+				  label);
+		if (rv) {
+			pr_err("%s: failed to allocate mem region",
+			       __FUNCTION__);
+			return;
+		}
+
+		pfn = page_to_pfn(p);
+		pfn_count_consecutive = 1;
+		size -= size_consecutive;
+	}
+
+	rv = link_node_cb(head, pfn, size, type, flags, label);
+	if (rv) {
+		pr_err("%s: failed to allocate mem region", __FUNCTION__);
+		return;
+	}
+}
+
+static void bhv_create_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags,
+			       const char *label)
+{
+	if (type == BHV_MEM_TYPE_UNKNOWN)
+		return;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, label,
+				_bhv_link_node_op_create);
+}
+
+static void bhv_release_memory_by_owner(uint64_t owner)
+{
+	int rc = bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+	if (rc) {
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+}
+
+#ifdef CONFIG_MODULES
+static void bhv_update_section(struct list_head *head, const void *base,
+			       uint64_t size, uint32_t type, uint64_t flags)
+{
+	if (type == BHV_MEM_TYPE_UNKNOWN)
+		return;
+
+	type &= ~BHV_MEM_FLAGS_MUTABLE;
+
+	bhv_prepare_mod_section(head, base, size, type, flags, "INVALID",
+				_bhv_link_node_op_update);
+}
+
+static void bhv_remove_section(struct list_head *head, const void *base,
+			       uint64_t size)
+{
+	bhv_prepare_mod_section(head, base, size, BHV_MEM_TYPE_UNKNOWN,
+				BHV_MEM_FLAGS_NONE, "INVALID",
+				_bhv_link_node_op_remove);
+}
+
+static void bhv_prepare_mod_layout(struct list_head *head,
+				   const struct module_layout *layout,
+				   unsigned long base_flags)
+{
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_create_section(head, layout->base, layout->text_size,
+			   BHV_MEM_TYPE_CODE_PATCHABLE, base_flags,
+			   "MODULE TEXT SECTION");
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_create_section(head, (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size),
+				   BHV_MEM_TYPE_DATA_READ_ONLY, base_flags,
+				   "MODULE READ-ONLY SECTION");
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_create_section(
+			head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size),
+			BHV_MEM_TYPE_DATA, base_flags | BHV_MEM_FLAGS_MUTABLE,
+			"MODULE READ-ONLY AFTER INIT SECTION");
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_create_section(head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size),
+				   BHV_MEM_TYPE_DATA, base_flags,
+				   "MODULE DATA SECTION");
+	}
+}
+
+static void bhv_prepare_mod(struct list_head *head, const struct module *mod)
+{
+	bhv_prepare_mod_layout(head, &mod->init_layout,
+			       BHV_MEM_FLAGS_TRANSIENT);
+
+#ifndef VASKM // inside kernel tree
+	bhv_prepare_mod_layout(head, &mod->core_layout,
+			       BHV_MEM_FLAGS_TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_prepare_mod_layout(head, &mod->core_layout,
+				       BHV_MEM_FLAGS_NONE);
+	} else {
+		bhv_prepare_mod_layout(head, &mod->core_layout,
+				       BHV_MEM_FLAGS_TRANSIENT);
+	}
+#endif // VASKM
+}
+
+void bhv_module_load_prepare(const struct module *mod)
+{
+	int rc = 0;
+	uint64_t owner = (uint64_t)mod;
+	struct bhv_mem_region_node *n = NULL;
+
+	/*
+	 * Note: list operations do not require locking, because the scope of
+	 * the list is limited to the function call; parallel calls to this
+	 * function will create their own lists.
+	 */
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	/*
+	 * XXX: Check whether the addresses are part of the region
+	 * [module_alloc_base;module_alloc_end]
+	 */
+
+	bhv_prepare_mod(&bhv_region_list_head, mod);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	/*
+	 * XXX: Consider using either the owner or an additional identifier for
+	 * page frames that belong to a given memory layout region. This would
+	 * allow us to efficiently release the respective memory regions.
+	 */
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+static void bhv_complete_free_init(const struct module_layout *layout)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (layout->size == 0)
+		return;
+
+	/* Prepare the module region's .text section. */
+	bhv_remove_section(&bhv_region_list_head, layout->base,
+			   layout->text_size);
+
+	/* Prepare the module region's .rodata section. */
+	if (layout->ro_size - layout->text_size) {
+		bhv_remove_section(&bhv_region_list_head,
+				   (layout->base + layout->text_size),
+				   (layout->ro_size - layout->text_size));
+	}
+
+	/* Prepare the module region's .ro_after_init section. */
+	if (layout->ro_after_init_size - layout->ro_size) {
+		bhv_remove_section(
+			&bhv_region_list_head, (layout->base + layout->ro_size),
+			(layout->ro_after_init_size - layout->ro_size));
+	}
+
+	/* Prepare the module region's .data section. */
+	if (layout->size - layout->ro_after_init_size) {
+		bhv_remove_section(&bhv_region_list_head,
+				   (layout->base + layout->ro_after_init_size),
+				   (layout->size - layout->ro_after_init_size));
+	}
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_remove_kern_phys_mem_region_by_region_hyp(&n->region);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot remove the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+static void bhv_update_ro_after_init(const struct module *mod,
+				     unsigned long base_flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+
+	void *base = mod->core_layout.base + mod->core_layout.ro_size;
+	unsigned int size =
+		mod->core_layout.ro_after_init_size - mod->core_layout.ro_size;
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (size == 0) {
+		return;
+	}
+
+	bhv_update_section(&bhv_region_list_head, base, size,
+			   BHV_MEM_TYPE_DATA_READ_ONLY,
+			   base_flags);
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		return;
+
+	rc = bhv_update_kern_phys_mem_region_hyp(&n->region);
+	if (rc) {
+		/* XXX: Determine a strategy for failed update attempts. */
+		pr_err("%s: Cannot update the module's memory regions",
+		       __FUNCTION__);
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+}
+
+void bhv_module_load_complete(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+#ifndef VASKM // inside kernel tree
+	bhv_update_ro_after_init(mod, BHV_MEM_FLAGS_TRANSIENT);
+#else // out of tree
+	if (mod == THIS_MODULE) {
+		bhv_update_ro_after_init(mod, BHV_MEM_FLAGS_NONE);
+	} else {
+		bhv_update_ro_after_init(mod, BHV_MEM_FLAGS_TRANSIENT);
+	}
+#endif // VASKM
+	bhv_complete_free_init(&mod->init_layout);
+}
+
+void bhv_module_unload(const struct module *mod)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)mod);
+}
+#endif /* CONFIG_MODULES */
+
+#ifndef VASKM // inside kernel tree
+static void bhv_bpf_protect(const void *base, uint64_t size, uint32_t type,
+			    uint64_t flags)
+{
+	int rc = 0;
+
+	/*
+	 * XXX: Note that we currently do not group subprograms of a BPF
+	 * program. Instead we protect them individually. Consider changing this
+	 * in the future.
+	 */
+	uint64_t owner = (uint64_t)base;
+	struct bhv_mem_region_node *n = NULL;
+
+	LIST_HEAD(bhv_section_list_head);
+
+	/* Prepare the section belonging to the bpf (sub)program. */
+	bhv_create_section(&bhv_section_list_head, base, size, type, flags,
+			   "BPF SECTION");
+
+	if (list_empty(&bhv_section_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_section_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_section_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_section_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+void bhv_bpf_protect_ro(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, BHV_MEM_TYPE_DATA_READ_ONLY,
+			BHV_MEM_FLAGS_TRANSIENT);
+}
+
+void bhv_bpf_protect_x(const void *base, uint64_t size)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_bpf_protect(base, size, BHV_MEM_TYPE_CODE, BHV_MEM_FLAGS_TRANSIENT);
+}
+
+void bhv_bpf_unprotect(const void *base)
+{
+	if (!bhv_integrity_is_enabled())
+		return;
+	bhv_release_memory_by_owner((uint64_t)base);
+}
+
+#else // out of tree
+
+
+void bhv_protect_generic_memory(uint64_t owner, const void *base, uint64_t size,
+				uint32_t type, uint64_t flags)
+{
+	int rc = 0;
+	struct bhv_mem_region_node *n = NULL;
+	BUG_ON(owner == 0);
+
+	LIST_HEAD(bhv_region_list_head);
+
+	if (!bhv_integrity_is_enabled())
+		return;
+
+	bhv_debug("%s: protecting %px-%px (%d, %lld)\n", __FUNCTION__, base,
+		  base + size - 1, type, flags);
+
+	bhv_create_section(&bhv_region_list_head, base, size, type, flags,
+			   "GENERIC");
+
+	if (list_empty(&bhv_region_list_head))
+		return;
+
+	n = list_first_entry_or_null(&bhv_region_list_head,
+				     struct bhv_mem_region_node, list);
+	if (n == NULL)
+		goto err;
+
+	rc = bhv_create_kern_phys_mem_region_hyp(owner, &n->region);
+	if (rc) {
+		pr_err("%s: Cannot protect the module's memory regions",
+		       __FUNCTION__);
+		goto err;
+	}
+
+	bhv_release_arg_list(&bhv_region_list_head);
+
+	return;
+
+err:
+	bhv_release_arg_list(&bhv_region_list_head);
+	bhv_remove_kern_phys_mem_region_by_owner_hyp(owner);
+}
+
+#endif // VASKM
diff --git kernel/bhv/patch_alternative.c kernel/bhv/patch_alternative.c
new file mode 100644
index 000000000..cca1b0aa3
--- /dev/null
+++ kernel/bhv/patch_alternative.c
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ */
+
+#include <bhv/bhv.h>
+#include <bhv/vault.h>
+#include <bhv/patch.h>
+#include <bhv/interface/patch.h>
+#include <bhv/kversion.h>
+
+#include <asm/bhv/patch.h>
+
+static DEFINE_MUTEX(bhv_alternatives_mutex);
+static LIST_HEAD(bhv_alternatives_head);
+
+static void __always_inline bhv_alternatives_lock(void)
+{
+	mutex_lock(&bhv_alternatives_mutex);
+}
+
+static void __always_inline bhv_alternatives_unlock(void)
+{
+	mutex_unlock(&bhv_alternatives_mutex);
+}
+
+void bhv_alternatives_add_module(struct alt_instr *begin, struct alt_instr *end,
+				 struct bhv_alternatives_mod_arch *arch)
+{
+	struct bhv_alternatives_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_alternatives_mod), GFP_KERNEL);
+	if (!n) {
+		bhv_fail("No memory left!");
+		return;
+	}
+
+	n->begin = begin;
+	n->end = end;
+	n->delete_policy = BHV_ALTERNATIVES_DELETE_AFTER_PATCH;
+	n->allocated = true;
+	memcpy(&n->arch, arch, sizeof(n->arch));
+
+	bhv_alternatives_lock();
+	list_add(&(n->next), &bhv_alternatives_head);
+	bhv_alternatives_unlock();
+}
+
+void bhv_alternatives_delete_after_init(void)
+{
+	struct bhv_alternatives_mod *i, *tmp;
+
+	bhv_alternatives_lock();
+	list_for_each_entry_safe (i, tmp, &bhv_alternatives_head, next) {
+		if (i->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_INIT) {
+			list_del(&(i->next));
+			if (i->allocated) {
+				kfree(i);
+			}
+		}
+	}
+	bhv_alternatives_unlock();
+}
+
+// LOCK MUST BE HELD!
+static void __bhv_text
+bhv_alternatives_add_module_no_alloc(struct bhv_alternatives_mod *n)
+{
+	n->allocated = false;
+	list_add(&(n->next), &bhv_alternatives_head);
+}
+
+static void __bhv_text bhv_alternatives_init(void)
+{
+	uint32_t static_mods, i;
+	struct bhv_alternatives_mod *n =
+		bhv_alternatives_get_static_mods_vault(&static_mods);
+
+	for (i = 0; i < static_mods; i++)
+		bhv_alternatives_add_module_no_alloc(&n[i]);
+}
+
+static int __bhv_text bhv_alternatives_apply_vault(
+	void *search_param, void *arch, bhv_alternatives_filter_t filter,
+	bhv_patch_arg_t *arg)
+{
+	static bool initialized = false;
+
+	struct bhv_alternatives_mod *i, *tmp, *found;
+	int rv;
+
+	rv = bhv_vault_open_hyp();
+	if (rv) {
+		return rv;
+	}
+
+	if (!initialized) {
+		bhv_alternatives_init();
+		initialized = true;
+	}
+
+	found = NULL;
+	list_for_each_entry_safe (i, tmp, &bhv_alternatives_head, next) {
+		if (filter(search_param, i)) {
+			found = i;
+			break;
+		}
+	}
+
+	// Unknown module.
+	if (found == NULL) {
+		bhv_vault_close_hyp();
+		return -EACCES;
+	}
+
+	rv = bhv_alternatives_apply_vault_arch(found, arch, arg);
+
+	// Delete module. Only one patch allowed.
+	if (found->delete_policy == BHV_ALTERNATIVES_DELETE_AFTER_PATCH) {
+		list_del(&(found->next));
+		if (found->allocated) {
+			kfree(found);
+		}
+	}
+
+	// Close vault.
+	bhv_vault_close_hyp();
+
+	return rv;
+}
+
+struct alt_inst_search {
+	struct alt_instr *begin;
+	struct alt_instr *end;
+};
+static bool __bhv_text bhv_alternatives_find_by_alt(
+	void *search_param, struct bhv_alternatives_mod *cur)
+{
+	struct alt_inst_search *param = search_param;
+
+	if (cur->begin == param->begin && cur->end == param->end) {
+		return true;
+	}
+
+	return false;
+}
+
+int bhv_alternatives_apply(struct alt_instr *begin, struct alt_instr *end,
+			   void *arch)
+{
+	int rv = 0;
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+	struct alt_inst_search search = { .begin = begin, .end = end };
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(
+		&search, arch, bhv_alternatives_find_by_alt, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+int bhv_alternatives_apply_custom_filter(void *search_param, void *arch,
+					 bhv_alternatives_filter_t filter)
+{
+	int rv = 0;
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	rv = bhv_alternatives_apply_vault(search_param, arch, filter, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+
+	return rv;
+}
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) &&           \
+	defined(CONFIG_PARAVIRT)
+void __init_or_module bhv_apply_paravirt(struct paravirt_patch_site *p)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_paravirt_vault(p, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+/* && defined(CONFIG_PARAVIRT) */
+
+#if defined BHV_KVERS_5_15
+
+#if defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION)
+
+void __init_or_module bhv_apply_retpolines(s32 *s)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_retpolines_vault(s, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+
+#ifdef CONFIG_RETHUNK
+void __init_or_module bhv_apply_returns(s32 *s)
+{
+	unsigned long flags;
+	static bhv_patch_arg_t bhv_arg;
+
+	bhv_alternatives_lock();
+	local_irq_save(flags);
+	bhv_apply_returns_vault(s, &bhv_arg);
+	local_irq_restore(flags);
+	bhv_alternatives_unlock();
+}
+#endif /* CONFIG_RETHUNK */
+
+#endif /* defined(CONFIG_RETPOLINE) && defined(CONFIG_STACK_VALIDATION) */
+
+#endif // defined BHV_KVERS_5_15
diff --git kernel/bhv/patch_jump_label.c kernel/bhv/patch_jump_label.c
new file mode 100644
index 000000000..9782a5619
--- /dev/null
+++ kernel/bhv/patch_jump_label.c
@@ -0,0 +1,194 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/jump_label.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/irqflags.h>
+#include <asm/bhv/patch.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/interface/patch.h>
+#include <bhv/patch.h>
+#include <bhv/vault.h>
+
+#ifndef VASKM // inside kernel tree
+#include <bhv/kernel-kln.h>
+#else // out of tree
+#include <kln.h>
+#endif // VASKM
+
+static DEFINE_MUTEX(bhv_jump_label_mutex);
+static LIST_HEAD(bhv_static_key_mod_head);
+
+struct bhv_static_key_mod {
+	struct jump_entry *entries_start;
+	struct jump_entry *entries_stop;
+#ifdef CONFIG_MODULES
+	struct module *mod;
+#endif /* CONFIG_MODULES */
+	struct list_head list;
+};
+
+static void __always_inline bhv_jump_label_lock(void)
+{
+	mutex_lock(&bhv_jump_label_mutex);
+}
+
+static void __always_inline bhv_jump_label_unlock(void)
+{
+	mutex_unlock(&bhv_jump_label_mutex);
+}
+
+#ifdef CONFIG_MODULES
+int bhv_jump_label_add_module(struct module *mod)
+{
+	struct bhv_static_key_mod *n;
+
+	n = kzalloc(sizeof(struct bhv_static_key_mod), GFP_KERNEL);
+	if (!n)
+		return -ENOMEM;
+
+	n->entries_start = mod->jump_entries;
+	n->entries_stop = mod->jump_entries + mod->num_jump_entries;
+	n->mod = mod;
+
+	bhv_jump_label_lock();
+	list_add(&(n->list), &bhv_static_key_mod_head);
+	bhv_jump_label_unlock();
+
+	return 0;
+}
+
+void bhv_jump_label_del_module(struct module *mod)
+{
+	struct bhv_static_key_mod *i, *tmp;
+
+	bhv_jump_label_lock();
+	list_for_each_entry_safe (i, tmp, &bhv_static_key_mod_head, list) {
+		if (i->mod == mod)
+			list_del(&(i->list));
+	}
+	bhv_jump_label_unlock();
+}
+#endif /* CONFIG_MODULES */
+
+enum jump_label_type __bhv_text bhv_jump_label_type(struct jump_entry *entry)
+{
+	struct static_key *key = jump_entry_key(entry);
+	bool enabled = static_key_enabled(key);
+	bool branch = jump_entry_is_branch(entry);
+
+	/* See the comment in linux/jump_label.h */
+	return enabled ^ branch;
+}
+
+bool __bhv_text validate_jmp_labels(struct jump_entry *entry,
+				    const void *opcode, size_t len,
+				    uint64_t *dest_phys_addr)
+{
+	struct bhv_static_key_mod *i;
+	*dest_phys_addr = bhv_virt_to_phys((void *)jump_entry_code(entry));
+
+	if (entry >= KLN_SYMBOL(struct jump_entry *, __start___jump_table) &&
+	    entry < KLN_SYMBOL(struct jump_entry *, __stop___jump_table)) {
+		return bhv_jump_label_validate_opcode(
+			entry, bhv_jump_label_type(entry), opcode, len);
+	}
+
+	list_for_each_entry (i, &bhv_static_key_mod_head, list) {
+		if (entry >= i->entries_start && entry < i->entries_stop) {
+			return bhv_jump_label_validate_opcode(
+				entry, bhv_jump_label_type(entry), opcode, len);
+		}
+	}
+
+	return false;
+}
+
+int __bhv_text bhv_vault_patch_jump_label(struct jump_entry *entry,
+					  const void *opcode, size_t len,
+					  bhv_patch_arg_t *bhv_arg)
+{
+	int rv = 0;
+	unsigned long r;
+	bool validation_ok;
+	uint64_t dest_phys_addr;
+
+	rv = bhv_vault_open_hyp();
+	if (rv) {
+		return rv;
+	}
+
+	if (len > BHV_MAX_PATCH_SZ) {
+		bhv_vault_close_hyp();
+		return -E2BIG;
+	}
+
+	memcpy(bhv_arg->bhv_patch_patch_arg.src_value, opcode, len);
+	bhv_arg->bhv_patch_patch_arg.size = (uint64_t)len;
+
+	validation_ok =
+		validate_jmp_labels(entry,
+				    bhv_arg->bhv_patch_patch_arg.src_value, len,
+				    &dest_phys_addr);
+	if (!validation_ok) {
+		bhv_vault_close_hyp();
+		return -EACCES;
+	}
+
+	bhv_arg->bhv_patch_patch_arg.dest_phys_addr = dest_phys_addr;
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_PATCH, BHV_VAS_PATCH_OP_PATCH,
+			      bhv_arg);
+	if (r)
+		panic("BHV vault close failure! hypercall returned %lu", r);
+	return 0;
+}
+
+int bhv_patch_jump_label(struct jump_entry *entry, const void *opcode,
+			 size_t len)
+{
+	int rv = 0;
+	unsigned long flags;
+	bhv_patch_arg_t *bhv_arg_ptr;
+
+#ifndef VASKM // inside kernel tree
+	/*
+	 * This allocation assumes synchronization around
+	 * bhv_vault_patch_jump_label(...).  This is the current solution
+	 * here as jump label patching happens before kmem caches are
+	 * allocated.
+	 */
+	static bhv_patch_arg_t bhv_arg;
+	bhv_arg_ptr = &bhv_arg;
+#endif // VASKM
+
+	if (!bhv_allow_patch)
+		panic("Patch requested but bhv_allow_patch is false\n");
+
+#ifdef VASKM // out of tree
+	bhv_arg_ptr =
+		(bhv_patch_arg_t *)kmalloc(sizeof(bhv_patch_arg_t), GFP_KERNEL);
+
+	if (!bhv_arg_ptr) {
+		pr_err("BHV: cannot allocate bhv_arg\n");
+		return -ENOMEM;
+	}
+#endif // VASKM
+
+	bhv_jump_label_lock();
+	local_irq_save(flags);
+	rv = bhv_vault_patch_jump_label(entry, opcode, len, bhv_arg_ptr);
+	local_irq_restore(flags);
+	bhv_jump_label_unlock();
+
+#ifdef VASKM // out of tree
+	kfree(bhv_arg_ptr);
+#endif // VASKM
+
+	return rv;
+}
diff --git kernel/bhv/start.c kernel/bhv/start.c
new file mode 100644
index 000000000..8d6a0e33d
--- /dev/null
+++ kernel/bhv/start.c
@@ -0,0 +1,247 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Sergej Proskurin <sergej@bedrocksystems.com>
+ *          Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <bhv/bhv_print.h>
+
+#include <asm/sections.h>
+#include <asm/page.h>
+#include <asm/io.h>
+#include <asm/syscall.h>
+
+#include <linux/init_task.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+
+#include <bhv/bhv.h>
+#include <bhv/creds.h>
+#include <bhv/guestconn.h>
+#include <bhv/fileops_internal.h>
+#include <bhv/guestlog.h>
+#include <bhv/guestpolicy.h>
+#include <bhv/init.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/common.h>
+#include <bhv/patch.h>
+#include <bhv/start.h>
+#include <bhv/sysfs.h>
+
+#ifndef VASKM // inside kernel tree
+#ifdef CONFIG_SECURITY_SELINUX
+extern int selinux_enabled_boot __initdata;
+int sel_direct_load(void *data, size_t count);
+#endif /* CONFIG_SECURITY_SELINUX */
+#endif // VASKM
+
+bool __bhv_init_done __ro_after_init = false;
+
+bool __init bhv_init_platform(void)
+{
+	uint32_t cid, port;
+
+#ifndef VASKM // inside kernel tree
+	int rv = bhv_init_hyp(__bhv_data_start,
+			      __bhv_data_end - __bhv_data_start);
+	void *bhv_data_ptr = __bhv_data_start;
+
+	bhv_debug("Kernel text: start=0x%px end=0x%px", _stext, _etext);
+	bhv_debug("System call table: start=0x%px", sys_call_table);
+
+#else // out of tree
+	void *bhv_data_ptr = NULL;
+	const uint64_t bhv_data_size = PAGE_SIZE;
+
+	bhv_data_ptr = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	// no kfree on purpose
+	if (!bhv_data_ptr) {
+		pr_err("BHV: cannot allocate bhv_data\n");
+		return false;
+	}
+
+	int rv = bhv_init_hyp(bhv_data_ptr, bhv_data_size);
+#endif // VASKM
+
+	if (rv) {
+		pr_err("BHV: init hypercall failed: hypercall returned %u", rv);
+		return false;
+	}
+
+	bhv_initialized = true;
+	bhv_configuration_bitmap = (unsigned long *)bhv_data_ptr;
+	cid = *((uint32_t *)(bhv_data_ptr + sizeof(unsigned long)));
+	port = *((uint32_t *)(bhv_data_ptr + sizeof(unsigned long) +
+			      sizeof(uint32_t)));
+
+	rv = bhv_guestconn_init(cid, port);
+	if (rv) {
+		bhv_fail(
+			"BHV: Cannot configure the BHV guest connection subsystem");
+		return false;
+	}
+
+	rv = bhv_guestlog_init();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV logging subsystem");
+		return false;
+	}
+
+	rv = bhv_cred_init();
+	if (rv) {
+		bhv_fail("BHV: Cannot configure the BHV creds subsystem");
+		return false;
+	}
+
+#ifndef VASKM // inside kernel tree
+	bhv_fileops_init();
+#if defined(CONFIG_SECURITY_SELINUX) &&                                        \
+	!defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+	selinux_enabled_boot = bhv_guest_policy_is_enabled() ? 1 : 0;
+#endif
+#endif // VASKM
+
+	__bhv_init_done = true;
+
+	return true;
+}
+
+static void __init_km do_start(void)
+{
+	int rc;
+	uint16_t num_pages = 1;
+	bhv_init_start_config_t *config =
+		(bhv_init_start_config_t *)__get_free_pages(GFP_KERNEL, 0);
+
+	if (config == NULL) {
+		bhv_fail("Unable to allocate start config");
+		return;
+	}
+
+	config->num_pages = num_pages;
+
+	rc = bhv_start_hyp(config);
+	if (rc) {
+		pr_err("BHV: start hypercall failed: %d", rc);
+		free_pages((unsigned long)config, 0);
+		return;
+	}
+
+	if (!config->valid) {
+		num_pages = config->num_pages;
+		free_pages((unsigned long)config, 0);
+
+		config = (bhv_init_start_config_t *)__get_free_pages(
+			GFP_KERNEL, order_base_2(num_pages));
+
+		if (config == NULL) {
+			bhv_fail("Unable to allocate start config");
+			return;
+		}
+
+		config->num_pages = num_pages;
+
+		rc = bhv_start_hyp(config);
+		if (rc) {
+			pr_err("BHV: start hypercall failed: %d", rc);
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		if (!config->valid) {
+			bhv_fail("host returned invalid configuration");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+	}
+
+	if (bhv_guest_policy_is_enabled()) {
+#if !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		if ((sizeof(bhv_init_start_config_t) + config->data_sz) >
+		    (num_pages * PAGE_SIZE)) {
+			bhv_fail("invalid guest policy size");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+
+		rc = sel_direct_load(config->data, config->data_sz);
+		if (rc) {
+			bhv_fail("guest policy load fail");
+			free_pages((unsigned long)config,
+				   order_base_2(num_pages));
+			return;
+		}
+#else // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+		bhv_fail("guest policy available without target LSM");
+#endif // !defined VASKM && defined CONFIG_SECURITY_SELINUX
+	}
+
+	free_pages((unsigned long)config, order_base_2(num_pages));
+}
+
+bool __init_km bhv_start(void)
+{
+	int rc;
+#ifndef VASKM // inside kernel tree
+	bhv_mem_region_node_t *n[2];
+#endif // VASKM
+
+	if (!is_bhv_initialized())
+		return false;
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		rc = kmem_cache_alloc_bulk(bhv_mem_region_cache, GFP_KERNEL, 2,
+					   (void **)&n);
+		if (!rc) {
+			bhv_fail("BHV: failed to allocate mem region");
+			return false;
+		}
+
+		/* Remove init text from host mappings */
+		n[0]->region.bhv_mem_region_remove.start_addr =
+			virt_to_phys(_sinittext);
+		n[0]->region.bhv_mem_region_remove.next =
+			virt_to_phys(&(n[1]->region));
+
+		/* Remove exit text from host mappings */
+		n[1]->region.bhv_mem_region_remove.start_addr =
+			virt_to_phys(_sexittext);
+		n[1]->region.bhv_mem_region_remove.next = BHV_INVALID_PHYS_ADDR;
+
+		rc = bhv_remove_kern_phys_mem_region_by_region_hyp(
+			&(n[0]->region));
+		if (rc)
+			pr_err("BHV: remove region hypercall failed: %d", rc);
+
+		kmem_cache_free_bulk(bhv_mem_region_cache, 2, (void **)&n);
+	}
+#endif // VASKM
+
+	rc = bhv_start_arch();
+	if (rc)
+		pr_err("BHV: bhv_start_arch failed");
+
+#ifndef VASKM // inside kernel tree
+	if (bhv_integrity_is_enabled()) {
+		// Free alternatives used during init
+		bhv_alternatives_delete_after_init();
+	}
+#endif // VASKM
+
+	bhv_guestconn_start();
+
+	do_start();
+
+	if (bhv_integrity_is_enabled()) {
+		bhv_setup_sysfs();
+	}
+
+	return true;
+}
diff --git kernel/bhv/sysfs.c kernel/bhv/sysfs.c
new file mode 100644
index 000000000..f21bd1755
--- /dev/null
+++ kernel/bhv/sysfs.c
@@ -0,0 +1,45 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#ifndef CONFIG_SYSFS
+#error CONFIG_SYSFS required!
+#endif
+
+#include <linux/kobject.h>
+
+#include <bhv/bhv.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/sysfs_reg_protect.h>
+
+void __init_km bhv_setup_sysfs(void)
+{
+	struct kobject *bhv_kobj;
+	struct kobject *integrity_kobj;
+	struct kobject *integrity_freeze_kobj;
+	struct kobject *register_kobj;
+	struct kobject *register_freeze_kobj;
+
+#define CREATE_KOBJ(kobj, name, parent)                                        \
+	kobj = kobject_create_and_add(name, parent);                           \
+	BUG_ON(!kobj);
+
+#ifndef CONFIG_SYS_HYPERVISOR
+	struct kobject *hypervisor_kobj;
+	CREATE_KOBJ(hypervisor_kobj, "hypervisor", NULL);
+#endif // CONFIG_SYS_HYPERVISOR
+
+	CREATE_KOBJ(bhv_kobj, "bhv", hypervisor_kobj);
+	CREATE_KOBJ(integrity_kobj, "integrity", bhv_kobj);
+	CREATE_KOBJ(integrity_freeze_kobj, "freeze", integrity_kobj);
+	bhv_setup_sysfs_integrity_freeze(integrity_freeze_kobj);
+
+	if (bhv_reg_protect_is_enabled()) {
+		CREATE_KOBJ(register_kobj, "register", bhv_kobj);
+		CREATE_KOBJ(register_freeze_kobj, "freeze", register_kobj);
+
+		bhv_setup_sysfs_reg_protect(register_freeze_kobj);
+	}
+}
diff --git kernel/bhv/sysfs_integrity_freeze.c kernel/bhv/sysfs_integrity_freeze.c
new file mode 100644
index 000000000..87472e2c1
--- /dev/null
+++ kernel/bhv/sysfs_integrity_freeze.c
@@ -0,0 +1,162 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/kobject.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+
+#include <bhv/bhv.h>
+#include <bhv/integrity.h>
+#include <bhv/interface/integrity.h>
+
+#ifndef VASKM // inside kernel tree
+#ifndef CONFIG_KALLSYMS
+#error CONFIG_KALLSYMS required
+#endif
+#include <linux/kallsyms.h>
+#define KLN_SYMBOL(ty, sym) ((ty) kallsyms_lookup_name(#sym))
+
+#else // out of tree
+#include <kln.h>
+#endif //VASKM
+
+
+static void lock_all_modules(void)
+{
+        static bool all_modules_locked = false;
+	struct module *mptr;
+
+        if (all_modules_locked)
+                return;
+
+	mutex_lock(KLN_SYMBOL(struct mutex *, module_mutex));
+
+	list_for_each_entry (mptr, KLN_SYMBOL(struct list_head *, modules),
+			     list) {
+		if (mptr != THIS_MODULE) {
+			if (!try_module_get(mptr)) {
+				printk(KERN_WARNING
+                                       "%s: Cannot lock module %s\n",
+                                       __FUNCTION__, mptr->name);
+			}
+		}
+	}
+
+	mutex_unlock(KLN_SYMBOL(struct mutex *, module_mutex));
+        all_modules_locked = true;
+	return;
+}
+
+bool bhv_allow_kmod_loads = true;
+bool bhv_allow_patch = true;
+
+int bhv_enable_freeze_flag(uint64_t flag, bool skip_locks)
+{
+        if (!skip_locks) {
+        	if (flag & BHV_FREEZE_FLAGS_DENY_CREATE ||
+        	    flag & BHV_FREEZE_FLAGS_DENY_UPDATE ||
+                    flag & BHV_FREEZE_FLAGS_DENY_REMOVE ||
+                    flag & BHV_FREEZE_FLAGS_DENY_PATCH) {
+        		bhv_allow_kmod_loads = false;
+        		lock_all_modules();
+        	}
+        	if (flag & BHV_FREEZE_FLAGS_DENY_PATCH) {
+        		bhv_allow_patch = false;
+        	}
+        } else {
+		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+		       __FUNCTION__);
+	}
+
+	return bhv_integrity_freeze_events(flag);
+}
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count);
+
+struct bhv_intfr_sysfs_data {
+	const struct kobj_attribute attr;
+	char flag;
+	const uint64_t param;
+} static _bhv_intfr_sysfs_data[] = {
+	{ .attr = __ATTR(create, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .flag = '0',
+	  .param = BHV_FREEZE_FLAGS_DENY_CREATE },
+	{ .attr = __ATTR(update, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .flag = '0',
+	  .param = BHV_FREEZE_FLAGS_DENY_UPDATE },
+	{ .attr = __ATTR(remove, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .flag = '0',
+	  .param = BHV_FREEZE_FLAGS_DENY_REMOVE },
+	{ .attr = __ATTR(patch, 0640, _bhv_intfr_sysfs_show,
+			 _bhv_intfr_sysfs_store),
+	  .flag = '0',
+	  .param = BHV_FREEZE_FLAGS_DENY_PATCH },
+};
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_intfr_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_intfr_sysfs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+	buf[0] = data->flag;
+	buf[1] = '\n';
+        buf[2] = '\0';
+	return 2;
+}
+
+static ssize_t _bhv_intfr_sysfs_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buf, size_t count)
+{
+	if (((count == 1 && buf[1] == '\0') ||
+	     (count == 2 && buf[1] == '\n' && buf[2] == '\0')) &&
+	    (buf[0] == '0' || buf[0] == '1' || buf[0] == '2')) {
+		struct bhv_intfr_sysfs_data *data = attr_to_bsd(attr);
+		if (data->flag == buf[0]) {
+			printk(KERN_INFO "%s: No-op write\n", __FUNCTION__);
+			return count;
+
+		} else if ((data->flag == '0') &&
+			   (buf[0] == '1' || buf[0] == '2')) {
+			int ret = bhv_enable_freeze_flag(data->param,
+							  buf[0] == '2');
+			if (ret) {
+				return ret;
+			} else {
+				data->flag = buf[0];
+				return count;
+			}
+		} else {
+			return -EPERM;
+		}
+	}
+
+	return -EINVAL;
+}
+
+void __init_km bhv_setup_sysfs_integrity_freeze(struct kobject *kobj)
+{
+	int i;
+	const struct attribute
+		*attr_array[ARRAY_SIZE(_bhv_intfr_sysfs_data) + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_intfr_sysfs_data); ++i)
+		attr_array[i] = &_bhv_intfr_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
diff --git kernel/bhv/sysfs_reg_protect.c kernel/bhv/sysfs_reg_protect.c
new file mode 100644
index 000000000..d9eddacbb
--- /dev/null
+++ kernel/bhv/sysfs_reg_protect.c
@@ -0,0 +1,168 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2023 - BedRock Systems Inc
+ * Authors: Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+// #include <linux/list.h>
+// #include <linux/module.h>
+// #include <linux/printk.h>
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+
+#include <bhv/bhv.h>
+// #include <bhv/integrity.h>
+#include <bhv/interface/hypercall.h>
+#include <bhv/interface/reg_protect.h>
+
+// #ifndef VASKM // inside kernel tree
+// #ifndef CONFIG_KALLSYMS
+// #error CONFIG_KALLSYMS required
+// #endif
+// #include <linux/kallsyms.h>
+// #define KLN_SYMBOL(ty, sym) ((ty) kallsyms_lookup_name(#sym))
+
+// #else // out of tree
+// #include <kln.h>
+// #endif //VASKM
+
+// static void lock_all_modules(void)
+// {
+//         static bool all_modules_locked = false;
+// 	struct module *mptr;
+
+//         if (all_modules_locked)
+//                 return;
+
+// 	mutex_lock(KLN_SYMBOL(struct mutex *, module_mutex));
+
+// 	list_for_each_entry (mptr, KLN_SYMBOL(struct list_head *, modules),
+// 			     list) {
+// 		if (mptr != THIS_MODULE) {
+// 			if (!try_module_get(mptr)) {
+// 				printk(KERN_WARNING
+//                                        "%s: Cannot lock module %s\n",
+//                                        __FUNCTION__, mptr->name);
+// 			}
+// 		}
+// 	}
+
+// 	mutex_unlock(KLN_SYMBOL(struct mutex *, module_mutex));
+//         all_modules_locked = true;
+// 	return;
+// }
+
+// bool bhv_allow_kmod_loads = true;
+// bool bhv_allow_patch = true;
+
+// int bhv_enable_freeze_flag(uint64_t flag, bool skip_locks)
+// {
+//         if (!skip_locks) {
+//         	if (flag & BHV_FREEZE_FLAGS_DENY_CREATE ||
+//         	    flag & BHV_FREEZE_FLAGS_DENY_UPDATE ||
+//                     flag & BHV_FREEZE_FLAGS_DENY_REMOVE ||
+//                     flag & BHV_FREEZE_FLAGS_DENY_PATCH) {
+//         		bhv_allow_kmod_loads = false;
+//         		lock_all_modules();
+//         	}
+//         	if (flag & BHV_FREEZE_FLAGS_DENY_PATCH) {
+//         		bhv_allow_patch = false;
+//         	}
+//         } else {
+// 		printk(KERN_WARNING "%s: Not restricting frozen operations\n",
+// 		       __FUNCTION__);
+// 	}
+
+// 	return bhv_integrity_freeze_events(flag);
+// }
+
+static int bhv_reg_protect_freeze(uint64_t reg_selector,
+				  uint64_t freeze_bitfield)
+{
+	int rv = 0;
+	unsigned long r;
+	bhv_reg_protect_t *bhv_arg =
+		kmalloc(sizeof(bhv_reg_protect_t), GFP_KERNEL);
+	BUG_ON(!bhv_arg);
+
+	bhv_arg->bhv_reg_protect_freeze.register_selector = reg_selector;
+	bhv_arg->bhv_reg_protect_freeze.freeze_bitfield = freeze_bitfield;
+
+	printk(KERN_INFO "%s: Sending %16phN\n", __FUNCTION__, bhv_arg);
+
+	r = bhv_hypercall_vas(BHV_VAS_BACKEND_REGISTER_PROTECTION,
+			      BHV_VAS_REGPROTECT_OP_FREEEZE_PHYS, bhv_arg);
+	if (r)
+		rv = -EINVAL;
+
+	kfree(bhv_arg);
+	return rv;
+}
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf);
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count);
+
+struct bhv_rp_sysfs_data {
+	const struct kobj_attribute attr;
+	const uint64_t reg;
+	uint64_t curr_status;
+} static _bhv_rp_sysfs_data[] = {
+#define Q(regn)                                                                \
+	{ .attr = __ATTR(regn, 0640, _bhv_rp_sysfs_show, _bhv_rp_sysfs_store), \
+	  .reg = BHV_REG_PROTECT_REG_##regn },
+	BHV_FREEZABLE_REGISTERS
+#undef Q
+};
+#define attr_to_bsd(ptr) container_of((ptr), struct bhv_rp_sysfs_data, attr)
+
+// https://www.kernel.org/doc/html/latest/filesystems/sysfs.html#reading-writing-attribute-data
+
+static ssize_t _bhv_rp_sysfs_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	int b = snprintf(buf, PAGE_SIZE, "%016llx", data->curr_status);
+	return b;
+}
+
+static ssize_t _bhv_rp_sysfs_store(struct kobject *kobj,
+				   struct kobj_attribute *attr, const char *buf,
+				   size_t count)
+{
+	struct bhv_rp_sysfs_data *data = attr_to_bsd(attr);
+	uint64_t nval;
+	int rv = 0;
+	int i = sscanf(buf, "%llx", &nval);
+	if (i != 1) {
+		return -EINVAL;
+	}
+
+	nval |= data->curr_status;
+
+	rv = bhv_reg_protect_freeze(data->reg, nval);
+	if (!rv)
+		data -> curr_status = nval;
+
+	if (rv < 0) {
+		return rv;
+	} else {
+		return count;
+	}
+}
+
+void __init_km bhv_setup_sysfs_reg_protect(struct kobject *kobj)
+{
+	int i;
+	const struct attribute *attr_array[BHV_NUM_FREEZABLE_REGISTERS + 1];
+
+	for (i = 0; i < ARRAY_SIZE(_bhv_rp_sysfs_data); ++i)
+		attr_array[i] = &_bhv_rp_sysfs_data[i].attr.attr;
+	attr_array[i] = NULL;
+
+	BUG_ON(sysfs_create_files(kobj, attr_array));
+}
diff --git kernel/bhv/vmalloc_to_page.c kernel/bhv/vmalloc_to_page.c
new file mode 100644
index 000000000..a83c30219
--- /dev/null
+++ kernel/bhv/vmalloc_to_page.c
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copied from kernel v6.1, mm/vmalloc.c
+/*
+ *  Copyright (C) 1993  Linus Torvalds
+ *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
+ *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000
+ *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002
+ *  Numa awareness, Christoph Lameter, SGI, June 2005
+ *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/mm.h>
+
+#ifdef VASKM // out of tree
+#include <kln.h>
+#undef pgd_offset_k
+#define pgd_offset_k(address)                                                  \
+	pgd_offset(KLN_SYMBOL(struct mm_struct *, init_mm), (address))
+#endif // VASKM
+
+
+
+/*
+ * Walk a vmap address to the struct page it maps. Huge vmap mappings will
+ * return the tail page that corresponds to the base page address, which
+ * matches small vmap mappings.
+ */
+struct page *bhv_vmalloc_to_page(const void *vmalloc_addr)
+{
+	unsigned long addr = (unsigned long) vmalloc_addr;
+	struct page *page = NULL;
+	pgd_t *pgd = pgd_offset_k(addr);
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *ptep, pte;
+
+	/*
+	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for
+	 * architectures that do not vmalloc module space
+	 */
+	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
+
+	if (pgd_none(*pgd))
+		return NULL;
+	if (WARN_ON_ONCE(pgd_leaf(*pgd)))
+		return NULL; /* XXX: no allowance for huge pgd */
+	if (WARN_ON_ONCE(pgd_bad(*pgd)))
+		return NULL;
+
+	p4d = p4d_offset(pgd, addr);
+	if (p4d_none(*p4d))
+		return NULL;
+	if (p4d_leaf(*p4d))
+		return p4d_page(*p4d) + ((addr & ~P4D_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(p4d_bad(*p4d)))
+		return NULL;
+
+	pud = pud_offset(p4d, addr);
+	if (pud_none(*pud))
+		return NULL;
+	if (pud_leaf(*pud))
+		return pud_page(*pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pud_bad(*pud)))
+		return NULL;
+
+	pmd = pmd_offset(pud, addr);
+	if (pmd_none(*pmd))
+		return NULL;
+	if (pmd_leaf(*pmd))
+		return pmd_page(*pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
+	if (WARN_ON_ONCE(pmd_bad(*pmd)))
+		return NULL;
+
+	ptep = pte_offset_map(pmd, addr);
+	pte = *ptep;
+	if (pte_present(pte))
+		page = pte_page(pte);
+	pte_unmap(ptep);
+
+	return page;
+}
\ No newline at end of file
diff --git kernel/bpf/bpf_struct_ops.c kernel/bpf/bpf_struct_ops.c
index 9abcc33f0..07196b9c0 100644
--- kernel/bpf/bpf_struct_ops.c
+++ kernel/bpf/bpf_struct_ops.c
@@ -453,6 +453,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 
 	set_memory_ro((long)st_map->image, 1);
 	set_memory_x((long)st_map->image, 1);
+	bhv_bpf_protect_x(st_map->image, PAGE_SIZE);
 	err = st_ops->reg(kdata);
 	if (likely(!err)) {
 		/* Pair with smp_load_acquire() during lookup_elem().
@@ -471,6 +472,7 @@ static int bpf_struct_ops_map_update_elem(struct bpf_map *map, void *key,
 	 */
 	set_memory_nx((long)st_map->image, 1);
 	set_memory_rw((long)st_map->image, 1);
+	bhv_bpf_unprotect(st_map->image);
 	bpf_map_put(map);
 
 reset_unlock:
diff --git kernel/bpf/core.c kernel/bpf/core.c
index 4ce500eac..de8d0cf3f 100644
--- kernel/bpf/core.c
+++ kernel/bpf/core.c
@@ -906,6 +906,12 @@ void bpf_jit_binary_free(struct bpf_binary_header *hdr)
 {
 	u32 pages = hdr->pages;
 
+	/*
+	 * XXX: bpf_jit_free_exec is a weak symbol. As long as we do not
+	 * directly free memory sections from inside module_memfree, we will not
+	 * be able to place bhv_bpf_unprotect into bpf_jit_free_exec.
+	 */
+	bhv_bpf_unprotect(hdr);
 	bpf_jit_free_exec(hdr);
 	bpf_jit_uncharge_modmem(pages);
 }
diff --git kernel/bpf/trampoline.c kernel/bpf/trampoline.c
index 4fa75791b..fbaddcc83 100644
--- kernel/bpf/trampoline.c
+++ kernel/bpf/trampoline.c
@@ -39,6 +39,11 @@ void *bpf_jit_alloc_exec_page(void)
 	 * everytime new program is attached or detached.
 	 */
 	set_memory_x((long)image, 1);
+	/*
+	 * XXX: Make sure that we foresee all cases that would allow new
+	 * programs to attach/detach and handle the permissions appropriately.
+	 */
+	bhv_bpf_protect_x(image, PAGE_SIZE);
 	return image;
 }
 
@@ -203,6 +208,7 @@ static void __bpf_tramp_image_put_deferred(struct work_struct *work)
 
 	im = container_of(work, struct bpf_tramp_image, work);
 	bpf_image_ksym_del(&im->ksym);
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 	bpf_jit_uncharge_modmem(1);
 	percpu_ref_exit(&im->pcref);
@@ -321,6 +327,7 @@ static struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key, u32 idx)
 	return im;
 
 out_free_image:
+	bhv_bpf_unprotect(im->image);
 	bpf_jit_free_exec(im->image);
 out_uncharge:
 	bpf_jit_uncharge_modmem(1);
diff --git kernel/cred.c kernel/cred.c
index 933155c96..d3747cfb4 100644
--- kernel/cred.c
+++ kernel/cred.c
@@ -17,6 +17,8 @@
 #include <linux/cn_proc.h>
 #include <linux/uidgid.h>
 
+#include <bhv/creds.h>
+
 #if 0
 #define kdebug(FMT, ...)						\
 	printk("[%-5.5s%5u] " FMT "\n",					\
@@ -113,6 +115,7 @@ static void put_cred_rcu(struct rcu_head *rcu)
 #endif
 
 	security_cred_free(cred);
+	bhv_cred_release(cred);
 	key_put(cred->session_keyring);
 	key_put(cred->process_keyring);
 	key_put(cred->thread_keyring);
@@ -461,6 +464,8 @@ int commit_creds(struct cred *new)
 #endif
 	BUG_ON(atomic_read(&new->usage) < 1);
 
+	bhv_cred_commit(new);
+
 	get_cred(new); /* we will require a ref for the subj creds too */
 
 	/* dumpability changes */
@@ -720,6 +725,11 @@ struct cred *prepare_kernel_cred(struct task_struct *daemon)
 
 	kdebug("prepare_kernel_cred() alloc %p", new);
 
+	if (bhv_cred_assign_priv(new, daemon)){
+		kmem_cache_free(cred_jar, new);
+		return NULL;
+	}
+
 	if (daemon)
 		old = get_task_cred(daemon);
 	else
diff --git kernel/events/core.c kernel/events/core.c
index c6c7a4d80..42091f450 100644
--- kernel/events/core.c
+++ kernel/events/core.c
@@ -8196,6 +8196,10 @@ void perf_event_namespaces(struct task_struct *task)
 	perf_fill_ns_link_info(&ns_link_info[CGROUP_NS_INDEX],
 			       task, &cgroupns_operations);
 #endif
+#ifdef CONFIG_MEM_NS
+	perf_fill_ns_link_info(&ns_link_info[MEM_NS_INDEX],
+			       task, &memns_operations);
+#endif
 
 	perf_iterate_sb(perf_event_namespaces_output,
 			&namespaces_event,
diff --git kernel/exit.c kernel/exit.c
index aefe74455..4f76c3f2a 100644
--- kernel/exit.c
+++ kernel/exit.c
@@ -69,6 +69,8 @@
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;
diff --git kernel/fork.c kernel/fork.c
index 908ba3c93..bc26cf41d 100644
--- kernel/fork.c
+++ kernel/fork.c
@@ -97,6 +97,10 @@
 #include <linux/scs.h>
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
+#include <linux/mem_namespace.h>
+
+#include <bhv/creds.h>
+#include <bhv/domain.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -617,6 +621,13 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	}
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
+	if (retval)
+		goto out;
+
+	retval = bhv_domain_dup_mmap(oldmm, mm);
+	if (retval)
+		bhv_domain_release_mm(mm);
+
 out:
 	mmap_write_unlock(mm);
 	flush_tlb_mm(oldmm);
@@ -1979,6 +1990,8 @@ static __latent_entropy struct task_struct *copy_process(
 	/*
 	 * If the new process will be in a different pid or user namespace
 	 * do not allow it to share a thread group with the forking task.
+	 *
+	 * XXX: Consider adding additional constraints for memory namespaces.
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
@@ -2177,9 +2190,12 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
-	retval = copy_semundo(clone_flags, p);
+	retval = bhv_cred_assign(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_security;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_bhv_assign;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -2192,15 +2208,15 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_signal(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_sighand;
-	retval = copy_mm(clone_flags, p);
+	retval = copy_namespaces(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_signal;
-	retval = copy_namespaces(clone_flags, p);
+	retval = copy_mm(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_mm;
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_io(clone_flags, p);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_mm;
 	retval = copy_thread(clone_flags, args->stack, args->stack_size, p, args->tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
@@ -2446,13 +2462,13 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);
-bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm) {
 		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
 	}
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2464,6 +2480,7 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_bhv_assign:
 bad_fork_cleanup_security:
 	security_task_free(p);
 bad_fork_cleanup_audit:
@@ -2964,7 +2981,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
 				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
-				CLONE_NEWTIME))
+				CLONE_NEWTIME|CLONE_NEWMEM))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing
@@ -3067,6 +3084,11 @@ int ksys_unshare(unsigned long unshare_flags)
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
 
+	/*
+	 * XXX: Consider CLONE_NEWMEM! Do we need to unshare the thread group
+	 * via CLONE_THREAD?
+	 */
+
 	err = check_unshare_flags(unshare_flags);
 	if (err)
 		goto bad_unshare_out;
diff --git kernel/jump_label.c kernel/jump_label.c
index b156e152d..84c2d165c 100644
--- kernel/jump_label.c
+++ kernel/jump_label.c
@@ -19,6 +19,8 @@
 #include <linux/cpu.h>
 #include <asm/sections.h>
 
+#include <bhv/patch.h>
+
 /* mutex to protect coming/going of the jump_label table */
 static DEFINE_MUTEX(jump_label_mutex);
 
@@ -680,7 +682,7 @@ static int jump_label_add_module(struct module *mod)
 			__jump_label_update(key, iter, iter_stop, true);
 	}
 
-	return 0;
+	return bhv_jump_label_add_module(mod);
 }
 
 static void jump_label_del_module(struct module *mod)
@@ -691,6 +693,8 @@ static void jump_label_del_module(struct module *mod)
 	struct static_key *key = NULL;
 	struct static_key_mod *jlm, **prev;
 
+	bhv_jump_label_del_module(mod);
+
 	for (iter = iter_start; iter < iter_stop; iter++) {
 		if (jump_entry_key(iter) == key)
 			continue;
diff --git kernel/kmod.c kernel/kmod.c
index b717134eb..d83c9d351 100644
--- kernel/kmod.c
+++ kernel/kmod.c
@@ -58,7 +58,11 @@ static DECLARE_WAIT_QUEUE_HEAD(kmod_wq);
 /*
 	modprobe_path is set via /proc/sys.
 */
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+const char modprobe_path[KMOD_PATH_LEN] __section(".rodata") = CONFIG_MODPROBE_PATH;
+#else
 char modprobe_path[KMOD_PATH_LEN] = CONFIG_MODPROBE_PATH;
+#endif
 
 static void free_modprobe_argv(struct subprocess_info *info)
 {
@@ -84,7 +88,7 @@ static int call_modprobe(char *module_name, int wait)
 	if (!module_name)
 		goto free_argv;
 
-	argv[0] = modprobe_path;
+	argv[0] = (char *)modprobe_path;
 	argv[1] = "-q";
 	argv[2] = "--";
 	argv[3] = module_name;	/* check free_modprobe_argv() */
diff --git kernel/mem_namespace.c kernel/mem_namespace.c
new file mode 100644
index 000000000..4bdb8aa61
--- /dev/null
+++ kernel/mem_namespace.c
@@ -0,0 +1,215 @@
+#include <linux/user_namespace.h>
+#include <linux/mem_namespace.h>
+#include <linux/proc_ns.h>
+#include <linux/cred.h>
+#include <linux/sched/task.h>
+#include <linux/slab.h>
+
+#include <bhv/domain.h>
+
+uint64_t get_free_domain(void)
+{
+	uint64_t domain = BHV_INVALID_DOMAIN;
+	bhv_domain_create(&domain);
+	return domain;
+}
+
+void put_domain(uint64_t domain)
+{
+	/*
+	 * XXX: Consider checking whether the domain has been allocated before.
+	 * We can make use of IDR (deprecated) or XArray to hold the domain IDs.
+	 */
+
+	if (domain == BHV_INVALID_DOMAIN)
+		return;
+
+	/*
+	 * We assume that the caller takes the necessary steps to switch to
+	 * another, valid domain before putting/releasing the given domain.
+	 */
+
+	BUG_ON(bhv_get_domain(current) == domain);
+
+	bhv_domain_destroy(domain);
+}
+
+static struct kmem_cache *mem_ns_cache;
+
+struct mem_namespace init_mem_ns = {
+	.kref = KREF_INIT(2),
+	.user_ns = &init_user_ns,
+	.domain = BHV_INIT_DOMAIN,
+	.ns.inum = PROC_MEM_INIT_INO,
+#ifdef CONFIG_MEM_NS
+	.ns.ops = &memns_operations,
+#endif
+};
+EXPORT_SYMBOL_GPL(init_mem_ns);
+
+static struct ucounts *inc_mem_namespaces(struct user_namespace *ns)
+{
+	return inc_ucount(ns, current_euid(), UCOUNT_MEM_NAMESPACES);
+}
+
+static void dec_mem_namespaces(struct ucounts *ucounts)
+{
+	dec_ucount(ucounts, UCOUNT_MEM_NAMESPACES);
+}
+
+static struct mem_namespace *create_mem_namespace(struct user_namespace *user_ns,
+						  struct mem_namespace *old_ns)
+{
+	struct mem_namespace *ns = NULL;
+	struct ucounts *ucounts;
+	uint64_t domain = 0;
+	int err = -EINVAL;
+
+	if (!in_userns(old_ns->user_ns, user_ns))
+		goto out;
+
+	domain = get_free_domain();
+	if (domain == BHV_INVALID_DOMAIN)
+		goto out;
+
+	ucounts = inc_mem_namespaces(user_ns);
+	if (!ucounts)
+		goto out_domain;
+
+	err = -ENOMEM;
+	ns = kmem_cache_zalloc(mem_ns_cache, GFP_KERNEL);
+	if (ns == NULL)
+		goto out_dec;
+
+	err = ns_alloc_inum(&ns->ns);
+	if (err)
+		goto out_free;
+
+	kref_init(&ns->kref);
+	ns->ns.ops = &memns_operations;
+	ns->domain = domain;
+	ns->user_ns = get_user_ns(user_ns);
+	ns->ucounts = ucounts;
+
+	return ns;
+
+out_free:
+	kmem_cache_free(mem_ns_cache, ns);
+out_dec:
+	dec_mem_namespaces(ucounts);
+out_domain:
+	put_domain(domain);
+out:
+	bhv_pr_info("Failed to create new memory namespace");
+
+	return ERR_PTR(err);
+}
+
+struct mem_namespace *copy_mem_ns(unsigned long flags,
+				  struct user_namespace *user_ns,
+				  struct mem_namespace *old_ns)
+{
+	BUG_ON(!old_ns);
+
+	if (!(flags & CLONE_NEWMEM)) {
+		bhv_pr_info("Copy memory namespace with domain %llu (kref: %d)",
+			    old_ns->domain, kref_read(&old_ns->kref));
+
+		return get_mem_ns(old_ns);
+	}
+
+	/*
+	 * XXX: Consider performing additional checks (see pid_namespaces.c); we
+	 * shall proceed only if the old_ns corresponds to the namespace, in
+	 * which the current task resides.
+	 */
+
+	return create_mem_namespace(user_ns, old_ns);
+}
+
+static void destroy_mem_namespace(struct mem_namespace *ns)
+{
+	put_domain(ns->domain);
+	ns_free_inum(&ns->ns);
+
+	/*
+	 * XXX: Make the namespace leverage RCU (see pid_namespace.c)!
+	 */
+
+	dec_mem_namespaces(ns->ucounts);
+	put_user_ns(ns->user_ns);
+
+	kmem_cache_free(mem_ns_cache, ns);
+}
+
+void free_mem_ns(struct kref *kref)
+{
+	struct mem_namespace *ns;
+	ns = container_of(kref, struct mem_namespace, kref);
+	destroy_mem_namespace(ns);
+}
+
+static inline struct mem_namespace *to_mem_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct mem_namespace, ns);
+}
+
+static struct ns_common *memns_get(struct task_struct *task)
+{
+	struct mem_namespace *ns = NULL;
+	struct nsproxy *nsproxy;
+
+	task_lock(task);
+	nsproxy = task->nsproxy;
+	if (nsproxy) {
+		ns = nsproxy->mem_ns;
+		get_mem_ns(ns);
+	}
+	task_unlock(task);
+
+	return ns ? &ns->ns : NULL;
+}
+
+static void memns_put(struct ns_common *ns)
+{
+	put_mem_ns(to_mem_ns(ns));
+}
+
+static int memns_install(struct nsset *nsset, struct ns_common *ns)
+{
+	struct nsproxy *nsproxy = nsset->nsproxy;
+	struct mem_namespace *new = to_mem_ns(ns);
+
+	bhv_pr_info("Install memory namespace");
+
+	if (!ns_capable(new->user_ns, CAP_SYS_ADMIN) ||
+	    !ns_capable(nsset->cred->user_ns, CAP_SYS_ADMIN))
+		return -EPERM;
+
+	put_mem_ns(nsproxy->mem_ns);
+	nsproxy->mem_ns = get_mem_ns(new);
+
+	return 0;
+}
+
+static struct user_namespace *memns_owner(struct ns_common *ns)
+{
+	return to_mem_ns(ns)->user_ns;
+}
+
+const struct proc_ns_operations memns_operations = {
+	.name		= "mem",
+	.type		= CLONE_NEWMEM,
+	.get		= memns_get,
+	.put		= memns_put,
+	.install	= memns_install,
+	.owner		= memns_owner,
+};
+
+static int __init mem_ns_init(void)
+{
+	mem_ns_cache = KMEM_CACHE(mem_namespace, SLAB_PANIC);
+	return 0;
+}
+
+__initcall(mem_ns_init);
diff --git kernel/module.c kernel/module.c
index ef79f4dbd..3755954ff 100644
--- kernel/module.c
+++ kernel/module.c
@@ -60,6 +60,8 @@
 #include <uapi/linux/module.h>
 #include "module-internal.h"
 
+#include <bhv/module.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/module.h>
 
@@ -268,6 +270,14 @@ static void module_assert_mutex_or_preempt(void)
 }
 
 #ifdef CONFIG_MODULE_SIG
+#if defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS)
+static const bool sig_enforce = true;
+
+void set_module_sig_enforced(void)
+{
+}
+
+#else /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
 module_param(sig_enforce, bool_enable_only, 0644);
 
@@ -275,6 +285,7 @@ void set_module_sig_enforced(void)
 {
 	sig_enforce = true;
 }
+#endif /* defined(CONFIG_MODULE_SIG_FORCE) && defined(CONFIG_BHV_VAS) */
 #else
 #define sig_enforce false
 #endif
@@ -2198,6 +2209,8 @@ static void free_module(struct module *mod)
 	/* Clean up CFI for the module. */
 	cfi_cleanup(mod);
 
+	bhv_module_unload(mod);
+
 	/* This may be empty, but that's OK */
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
@@ -3626,6 +3639,7 @@ static void module_deallocate(struct module *mod, struct load_info *info)
 	module_arch_freeing_init(mod);
 	module_memfree(mod->init_layout.base);
 	module_memfree(mod->core_layout.base);
+	bhv_module_unload(mod);
 }
 
 int __weak module_finalize(const Elf_Ehdr *hdr,
@@ -3769,6 +3783,7 @@ static noinline int do_init_module(struct module *mod)
 	module_enable_ro(mod, true);
 	mod_tree_remove_init(mod);
 	module_arch_freeing_init(mod);
+	bhv_module_load_complete(mod);
 	mod->init_layout.base = NULL;
 	mod->init_layout.size = 0;
 	mod->init_layout.ro_size = 0;
@@ -4082,6 +4097,8 @@ static int load_module(struct load_info *info, const char __user *uargs,
 	if (err)
 		goto ddebug_cleanup;
 
+	bhv_module_load_prepare(mod);
+
 	err = prepare_coming_module(mod);
 	if (err)
 		goto bug_cleanup;
diff --git kernel/nsproxy.c kernel/nsproxy.c
index eec72ca96..38c51adb7 100644
--- kernel/nsproxy.c
+++ kernel/nsproxy.c
@@ -16,6 +16,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/utsname.h>
 #include <linux/pid_namespace.h>
+#include <linux/mem_namespace.h>
 #include <net/net_namespace.h>
 #include <linux/ipc_namespace.h>
 #include <linux/time_namespace.h>
@@ -27,6 +28,8 @@
 #include <linux/cgroup.h>
 #include <linux/perf_event.h>
 
+#include <bhv/domain.h>
+
 static struct kmem_cache *nsproxy_cachep;
 
 struct nsproxy init_nsproxy = {
@@ -47,6 +50,9 @@ struct nsproxy init_nsproxy = {
 	.time_ns		= &init_time_ns,
 	.time_ns_for_children	= &init_time_ns,
 #endif
+#ifdef CONFIG_MEM_NS
+	.mem_ns			= &init_mem_ns,
+#endif
 };
 
 static inline struct nsproxy *create_nsproxy(void)
@@ -121,8 +127,19 @@ static struct nsproxy *create_new_namespaces(unsigned long flags,
 	}
 	new_nsp->time_ns = get_time_ns(tsk->nsproxy->time_ns);
 
+	new_nsp->mem_ns = copy_mem_ns(flags, user_ns, tsk->nsproxy->mem_ns);
+	if (IS_ERR(new_nsp->mem_ns)) {
+		err = PTR_ERR(new_nsp->mem_ns);
+		goto out_mem;
+	}
+
 	return new_nsp;
 
+out_mem:
+	if (new_nsp->time_ns)
+		put_time_ns(new_nsp->time_ns);
+	if (new_nsp->time_ns_for_children)
+		put_time_ns(new_nsp->time_ns_for_children);
 out_time:
 	put_net(new_nsp->net_ns);
 out_net:
@@ -156,7 +173,7 @@ int copy_namespaces(unsigned long flags, struct task_struct *tsk)
 
 	if (likely(!(flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			      CLONE_NEWPID | CLONE_NEWNET |
-			      CLONE_NEWCGROUP | CLONE_NEWTIME)))) {
+			      CLONE_NEWCGROUP | CLONE_NEWTIME | CLONE_NEWMEM)))) {
 		if (likely(old_ns->time_ns_for_children == old_ns->time_ns)) {
 			get_nsproxy(old_ns);
 			return 0;
@@ -199,6 +216,8 @@ void free_nsproxy(struct nsproxy *ns)
 		put_time_ns(ns->time_ns);
 	if (ns->time_ns_for_children)
 		put_time_ns(ns->time_ns_for_children);
+	if (ns->mem_ns)
+		put_mem_ns(ns->mem_ns);
 	put_cgroup_ns(ns->cgroup_ns);
 	put_net(ns->net_ns);
 	kmem_cache_free(nsproxy_cachep, ns);
@@ -216,7 +235,7 @@ int unshare_nsproxy_namespaces(unsigned long unshare_flags,
 
 	if (!(unshare_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 			       CLONE_NEWNET | CLONE_NEWPID | CLONE_NEWCGROUP |
-			       CLONE_NEWTIME)))
+			       CLONE_NEWTIME | CLONE_NEWMEM)))
 		return 0;
 
 	user_ns = new_cred ? new_cred->user_ns : current_user_ns();
@@ -247,6 +266,24 @@ void switch_task_namespaces(struct task_struct *p, struct nsproxy *new)
 
 	if (ns)
 		put_nsproxy(ns);
+	/*
+	 * Move the task's address space to the given domain only if we do not
+	 * destroy the nsproxy that the task is about to switch to is valid.
+	 * Switching to an invalid nsproxy (nsproxy == NULL) means that the task
+	 * is about to be destroyed.
+	 */
+	if (new != NULL)
+		bhv_domain_move_mm(p->mm, ns, new);
+
+	/*
+	 * Note that  bhv_domain_enter will automatically determine which domain
+	 * to switch to (i.e., to the task's domain maintained by its nsproxy or
+	 * to the default domain of init_task).
+	 *
+	 * XXX: If the task switches to an invalid nsproxy, we should consider
+	 * switching to the parent's domain.
+	 */
+	bhv_domain_enter(p, false);
 }
 
 void exit_task_namespaces(struct task_struct *p)
@@ -258,7 +295,7 @@ static int check_setns_flags(unsigned long flags)
 {
 	if (!flags || (flags & ~(CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
 				 CLONE_NEWNET | CLONE_NEWTIME | CLONE_NEWUSER |
-				 CLONE_NEWPID | CLONE_NEWCGROUP)))
+				 CLONE_NEWPID | CLONE_NEWCGROUP | CLONE_NEWMEM)))
 		return -EINVAL;
 
 #ifndef CONFIG_USER_NS
@@ -289,6 +326,10 @@ static int check_setns_flags(unsigned long flags)
 	if (flags & CLONE_NEWTIME)
 		return -EINVAL;
 #endif
+#ifndef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM)
+		return -EINVAL;
+#endif
 
 	return 0;
 }
@@ -471,6 +512,14 @@ static int validate_nsset(struct nsset *nsset, struct pid *pid)
 	}
 #endif
 
+#ifdef CONFIG_MEM_NS
+	if (flags & CLONE_NEWMEM) {
+		ret = validate_ns(nsset, &nsp->mem_ns->ns);
+		if (ret)
+			goto out;
+	}
+#endif
+
 out:
 	if (pid_ns)
 		put_pid_ns(pid_ns);
diff --git kernel/sched/core.c kernel/sched/core.c
index 85be68468..b751078f2 100644
--- kernel/sched/core.c
+++ kernel/sched/core.c
@@ -17,6 +17,9 @@
 #include <linux/kcov.h>
 #include <linux/scs.h>
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+
 #include <asm/switch_to.h>
 #include <asm/tlb.h>
 
@@ -4988,6 +4991,9 @@ context_switch(struct rq *rq, struct task_struct *prev,
 
 	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
 
+	if (bhv_get_domain(prev) != bhv_get_domain(next))
+		bhv_domain_enter(next, true);
+
 	prepare_lock_switch(rq, next, rf);
 
 	/* Here we just switch the register state and the stack. */
diff --git kernel/sysctl.c kernel/sysctl.c
index 23c08bf3d..ade7a7860 100644
--- kernel/sysctl.c
+++ kernel/sysctl.c
@@ -2108,9 +2108,15 @@ static struct ctl_table kern_table[] = {
 #ifdef CONFIG_MODULES
 	{
 		.procname	= "modprobe",
+#ifdef CONFIG_BHV_CONST_MODPROBE_PATH
+		.data		= (char *)&modprobe_path,
+		.maxlen		= KMOD_PATH_LEN,
+		.mode		= 0444,
+#else
 		.data		= &modprobe_path,
 		.maxlen		= KMOD_PATH_LEN,
 		.mode		= 0644,
+#endif
 		.proc_handler	= proc_dostring,
 	},
 	{
diff --git kernel/ucount.c kernel/ucount.c
index a1d672615..f7699fa11 100644
--- kernel/ucount.c
+++ kernel/ucount.c
@@ -91,6 +91,7 @@ static struct ctl_table user_table[] = {
 	{ },
 	{ },
 	{ },
+	UCOUNT_ENTRY("max_mem_namespaces"),
 	{ }
 };
 #endif /* CONFIG_SYSCTL */
diff --git mm/filemap.c mm/filemap.c
index dbc461703..31e805e6f 100644
--- mm/filemap.c
+++ mm/filemap.c
@@ -46,6 +46,9 @@
 #include <asm/tlbflush.h>
 #include "internal.h"
 
+#include <linux/mem_namespace.h>
+#include <bhv/domain.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/filemap.h>
 
@@ -3300,6 +3303,11 @@ vm_fault_t filemap_map_pages(struct vm_fault *vmf,
 	unsigned int mmap_miss = READ_ONCE(file->f_ra.mmap_miss);
 	vm_fault_t ret = 0;
 
+#ifdef CONFIG_MEM_NS
+	uint64_t pfn_count_consecutive = 0;
+	uint64_t pfn_start = 0;
+#endif /* CONFIG_MEM_NS */
+
 	rcu_read_lock();
 	head = first_map_page(mapping, &xas, end_pgoff);
 	if (!head)
@@ -3334,6 +3342,27 @@ vm_fault_t filemap_map_pages(struct vm_fault *vmf,
 		do_set_pte(vmf, page, addr);
 		/* no need to invalidate: a not-present page won't be cached */
 		update_mmu_cache(vma, addr, vmf->pte);
+
+#ifdef CONFIG_MEM_NS
+		if (pfn_count_consecutive == 0) {
+			pfn_start = page_to_pfn(page);
+			pfn_count_consecutive++;
+		} else {
+			if (page_to_pfn(page) - pfn_start == pfn_count_consecutive) {
+				pfn_count_consecutive++;
+			} else {
+				/* XXX: Handle huge pages! */
+				bhv_domain_update_vma(vmf->vma->vm_mm, vmf->vma,
+						      pfn_to_page(pfn_start),
+						      pfn_count_consecutive,
+						      true);
+
+				pfn_start = page_to_pfn(page);
+				pfn_count_consecutive = 1;
+			}
+		}
+#endif /* CONFIG_MEM_NS */
+
 		unlock_page(head);
 		continue;
 unlock:
@@ -3342,7 +3371,16 @@ vm_fault_t filemap_map_pages(struct vm_fault *vmf,
 	} while ((head = next_map_page(mapping, &xas, end_pgoff)) != NULL);
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 out:
+
+#ifdef CONFIG_MEM_NS
+	if (pfn_count_consecutive > 0)
+		bhv_domain_update_vma(vmf->vma->vm_mm, vmf->vma,
+				      pfn_to_page(pfn_start),
+				      pfn_count_consecutive, true);
+#endif /* CONFIG_MEM_NS */
+
 	rcu_read_unlock();
+
 	WRITE_ONCE(file->f_ra.mmap_miss, mmap_miss);
 	return ret;
 }
diff --git mm/mmap.c mm/mmap.c
index 5c2c7651c..3af017f75 100644
--- mm/mmap.c
+++ mm/mmap.c
@@ -53,6 +53,8 @@
 #include <asm/tlb.h>
 #include <asm/mmu_context.h>
 
+#include <bhv/domain.h>
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/mmap.h>
 
@@ -1848,6 +1850,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 		mapping_unmap_writable(file->f_mapping);
 	file = vma->vm_file;
 out:
+	bhv_domain_register_vma(mm, vma, true);
 	perf_event_mmap(vma);
 
 	vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);
@@ -2444,6 +2447,7 @@ int expand_upwards(struct vm_area_struct *vma, unsigned long address)
 					vma_gap_update(vma->vm_next);
 				else
 					mm->highest_vm_end = vm_end_gap(vma);
+				bhv_domain_register_vma(mm, vma, true);
 				spin_unlock(&mm->page_table_lock);
 
 				perf_event_mmap(vma);
@@ -2522,12 +2526,14 @@ int expand_downwards(struct vm_area_struct *vma,
 				vma->vm_pgoff -= grow;
 				anon_vma_interval_tree_post_update_vma(vma);
 				vma_gap_update(vma);
+				bhv_domain_register_vma(mm, vma, true);
 				spin_unlock(&mm->page_table_lock);
 
 				perf_event_mmap(vma);
 			}
 		}
 	}
+
 	anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
 	validate_mm(mm);
@@ -2621,6 +2627,11 @@ static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 		if (vma->vm_flags & VM_ACCOUNT)
 			nr_accounted += nrpages;
 		vm_stat_account(mm, vma->vm_flags, -nrpages);
+		/*
+		 * XXX: Create a hypercall that registers all VMAs that are to
+		 * be released.
+		 */
+		bhv_domain_release_vma(vma);
 		vma = remove_vma(vma);
 	} while (vma);
 	vm_unacct_memory(nr_accounted);
@@ -3090,6 +3101,7 @@ static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long fla
 	vma->vm_page_prot = vm_get_page_prot(flags);
 	vma_link(mm, vma, prev, rb_link, rb_parent);
 out:
+	bhv_domain_register_vma(mm, vma, true);
 	perf_event_mmap(vma);
 	mm->total_vm += len >> PAGE_SHIFT;
 	mm->data_vm += len >> PAGE_SHIFT;
@@ -3141,6 +3153,7 @@ void exit_mmap(struct mm_struct *mm)
 
 	/* mm's last user has gone, and its about to be pulled down */
 	mmu_notifier_release(mm);
+	bhv_domain_release_mm(mm);
 
 	if (unlikely(mm_is_oom_victim(mm))) {
 		/*
@@ -3463,6 +3476,10 @@ static struct vm_area_struct *__install_special_mapping(
 	if (ret)
 		goto out;
 
+	ret = bhv_domain_register_vma(mm, vma, true);
+	if (ret)
+		goto out;
+
 	vm_stat_account(mm, vma->vm_flags, len >> PAGE_SHIFT);
 
 	perf_event_mmap(vma);
diff --git mm/page_owner.c mm/page_owner.c
index 62402d225..8e4bec8f0 100644
--- mm/page_owner.c
+++ mm/page_owner.c
@@ -618,7 +618,7 @@ static void init_early_allocated_pages(void)
 		init_zones_in_node(pgdat);
 }
 
-static const struct file_operations proc_page_owner_operations = {
+const struct file_operations proc_page_owner_operations = {
 	.read		= read_page_owner,
 };
 
diff --git mm/shmem.c mm/shmem.c
index 342d1bc72..abc14d509 100644
--- mm/shmem.c
+++ mm/shmem.c
@@ -243,7 +243,7 @@ static inline void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 
 static const struct super_operations shmem_ops;
 const struct address_space_operations shmem_aops;
-static const struct file_operations shmem_file_operations;
+const struct file_operations shmem_file_operations;
 static const struct inode_operations shmem_inode_operations;
 static const struct inode_operations shmem_dir_inode_operations;
 static const struct inode_operations shmem_special_inode_operations;
@@ -3793,7 +3793,7 @@ const struct address_space_operations shmem_aops = {
 };
 EXPORT_SYMBOL(shmem_aops);
 
-static const struct file_operations shmem_file_operations = {
+const struct file_operations shmem_file_operations = {
 	.mmap		= shmem_mmap,
 	.get_unmapped_area = shmem_get_unmapped_area,
 #ifdef CONFIG_TMPFS
diff --git mm/vmscan.c mm/vmscan.c
index 74296c2d1..228b384dd 100644
--- mm/vmscan.c
+++ mm/vmscan.c
@@ -57,6 +57,8 @@
 #include <linux/swapops.h>
 #include <linux/balloon_compaction.h>
 
+#include <bhv/domain.h>
+
 #include "internal.h"
 
 #define CREATE_TRACE_POINTS
@@ -1761,8 +1763,10 @@ static unsigned int shrink_page_list(struct list_head *page_list,
 		 */
 		if (unlikely(PageTransHuge(page)))
 			destroy_compound_page(page);
-		else
+		else {
+			bhv_domain_swap_page(page_to_pfn(page));
 			list_add(&page->lru, &free_pages);
+		}
 		continue;
 
 activate_locked_split:
diff --git security/Kconfig security/Kconfig
index 5d412b3dd..6d6df34c0 100644
--- security/Kconfig
+++ security/Kconfig
@@ -222,11 +222,20 @@ config STATIC_USERMODEHELPER_PATH
 	  If you wish for all usermode helper programs to be disabled,
 	  specify an empty string here (i.e. "").
 
+config MEM_NS
+	bool "Enable memory namespaces"
+	depends on MEMCG
+	depends on BHV_VAS
+	default y
+	help
+	  Enable memory namespaces.
+
 source "security/selinux/Kconfig"
 source "security/smack/Kconfig"
 source "security/tomoyo/Kconfig"
 source "security/apparmor/Kconfig"
 source "security/loadpin/Kconfig"
+source "security/bhv/Kconfig"
 source "security/yama/Kconfig"
 source "security/safesetid/Kconfig"
 source "security/lockdown/Kconfig"
@@ -270,11 +279,11 @@ endchoice
 
 config LSM
 	string "Ordered list of enabled LSMs"
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf" if DEFAULT_SECURITY_SMACK
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
-	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,smack,selinux,tomoyo,apparmor,bpf,bhv" if DEFAULT_SECURITY_SMACK
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf,bhv" if DEFAULT_SECURITY_APPARMOR
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf,bhv" if DEFAULT_SECURITY_TOMOYO
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf,bhv" if DEFAULT_SECURITY_DAC
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf,bhv"
 	help
 	  A comma-separated list of LSMs, in initialization order.
 	  Any LSMs left off this list will be ignored. This can be
diff --git security/Makefile security/Makefile
index 18121f8f8..99425138e 100644
--- security/Makefile
+++ security/Makefile
@@ -19,6 +19,7 @@ obj-$(CONFIG_SECURITY_TOMOYO)		+= tomoyo/
 obj-$(CONFIG_SECURITY_APPARMOR)		+= apparmor/
 obj-$(CONFIG_SECURITY_YAMA)		+= yama/
 obj-$(CONFIG_SECURITY_LOADPIN)		+= loadpin/
+obj-$(CONFIG_SECURITY_BHV)		+= bhv/
 obj-$(CONFIG_SECURITY_SAFESETID)       += safesetid/
 obj-$(CONFIG_SECURITY_LOCKDOWN_LSM)	+= lockdown/
 obj-$(CONFIG_CGROUPS)			+= device_cgroup.o
diff --git security/bhv/Kconfig security/bhv/Kconfig
new file mode 100644
index 000000000..309c33608
--- /dev/null
+++ security/bhv/Kconfig
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config SECURITY_BHV
+	bool "Enable the BHV LSM."
+	depends on SECURITY
+	depends on BHV_VAS
+	default y
+	help
+	  Enables the BHV LSM. The BHV LSM provides hooking capabilities for
+	  additional protections such as the process and the driver ACL.
diff --git security/bhv/Makefile security/bhv/Makefile
new file mode 100644
index 000000000..f921bdf38
--- /dev/null
+++ security/bhv/Makefile
@@ -0,0 +1,2 @@
+# SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_SECURITY_BHV) += bhv.o
diff --git security/bhv/bhv.c security/bhv/bhv.c
new file mode 100644
index 000000000..48d928a4e
--- /dev/null
+++ security/bhv/bhv.c
@@ -0,0 +1,498 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2022 - BedRock Systems Inc
+ * Authors: Sebastian Vogl <sebastian@bedrocksystems.com>
+ *          Jonas Pfoh <jonas@bedrocksystems.com>
+ *          Robert Gawlik <robert@bedrocksystems.com>
+ *          Tommaso Frassetto <tommaso.frassetto@bedrocksystems.com>
+ */
+
+#include <linux/binfmts.h>
+#include <linux/fdtable.h>
+#include <linux/kernel.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/slab.h>
+#include <uapi/linux/magic.h>
+
+#ifndef VASKM // inside kernel tree
+#include <linux/lsm_hooks.h>
+#include <bhv/fileops_internal.h>
+
+#define static_vk static
+
+#else // out of tree
+#define static_vk
+#endif // VASKM
+
+#include <bhv/acl.h>
+#include <bhv/bhv.h>
+#include <bhv/file_protection.h>
+#include <bhv/fileops_protection.h>
+#include <bhv/sysfs_integrity_freeze.h>
+#include <bhv/guestlog.h>
+
+
+#ifndef VASKM // inside kernel tree
+static int bhv_read_file(struct file *file, enum kernel_read_file_id id,
+			 bool whole_file)
+{
+	if (id == READING_MODULE) {
+		char *filename = NULL;
+		char *filename_buf = NULL;
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (file != NULL && whole_file) {
+			filename_buf = (char *)__get_free_page(GFP_KERNEL);
+			if (filename_buf == NULL) {
+				bhv_fail(
+					"BHV: Unable to allocate acl violation filename buf");
+				return -ENOMEM;
+			}
+			filename =
+				d_path(&file->f_path, filename_buf, PAGE_SIZE);
+			if (IS_ERR(filename))
+				filename = NULL;
+		}
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(filename)) {
+				if (filename_buf)
+					free_page((unsigned long)filename_buf);
+				return -EPERM;
+			}
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			if (filename != NULL)
+				bhv_guestlog_log_driver_load(filename);
+			else
+				bhv_guestlog_log_driver_load(
+					"[ UNKNOWN DRIVER ]");
+		}
+
+		if (filename_buf)
+			free_page((unsigned long)filename_buf);
+	}
+	return 0;
+}
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+static int bhv_load_data(enum kernel_load_data_id id, bool contents)
+{
+	const char *origin = kernel_read_file_id_str(id);
+	pr_debug("[bhv] LOAD DATA HOOK: %s", origin);
+
+	if (id == LOADING_MODULE) {
+
+		if (!bhv_allow_kmod_loads)
+			return -EPERM;
+
+		if (bhv_acl_is_driver_acl_enabled()) {
+			if (bhv_block_driver(NULL))
+				return -EPERM;
+		}
+
+		if (bhv_guestlog_log_driver_events()) {
+			bhv_guestlog_log_driver_load("[ UNKNOWN DRIVER ]");
+		}
+	}
+
+	return 0;
+}
+#endif // VASKM
+
+static_vk int bhv_task_alloc(struct task_struct *target,
+			     long unsigned int clone_flags)
+{
+	pr_debug("[bhv] TASK CREATE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	// Filters kernel treads
+	if (bhv_acl_is_proc_acl_enabled() && target->mm != NULL) {
+		char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&target->mm->exe_file->f_path, filename_buf,
+				  PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_fork(target->pid, target->comm,
+					      target->parent->pid,
+					      target->parent->comm);
+	}
+
+	return 0;
+}
+
+static_vk void bhv_task_free(struct task_struct *target)
+{
+	pr_debug("[bhv] TASK FREE HOOK:");
+	pr_debug("\t-> PID: %d", target->pid);
+	pr_debug("\t-> PPID: %d", target->parent->pid);
+	pr_debug("\t-> NAME: %s", target->comm);
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exit(target->pid, target->parent->pid,
+					      target->comm);
+	}
+}
+
+static_vk int bhv_bprm_check_security(struct linux_binprm *bprm)
+{
+	int rv = 0;
+	pr_debug("[bhv] BPRM CHECK SECURITY HOOK:");
+	pr_debug("\t-> FILENAME: %s", bprm->filename);
+
+	if (bhv_acl_is_proc_acl_enabled()) {
+		const char *filename = NULL;
+		char *filename_buf = (char *)__get_free_page(GFP_KERNEL);
+		if (filename_buf == NULL) {
+			bhv_fail(
+				"BHV: Unable to allocate acl violation filename buf");
+			return -ENOMEM;
+		}
+		filename = d_path(&bprm->file->f_path, filename_buf, PAGE_SIZE);
+		if (IS_ERR(filename))
+			filename = NULL;
+
+		// check executed filename (name on cli)
+		if (bhv_block_process(bprm->filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		// check underlying file (e.g., busybox or interpreter)
+		if (bhv_block_process(filename)) {
+			free_page((unsigned long)filename_buf);
+			return -EPERM;
+		}
+
+		free_page((unsigned long)filename_buf);
+	}
+
+	if (bhv_guestlog_log_process_events()) {
+		bhv_guestlog_log_process_exec(
+			current->pid, current->parent->pid, bprm->filename);
+	}
+
+	return rv;
+}
+
+#ifndef VASKM // inside kernel tree
+static const inline char *get_pathname(struct file *file, char *buf,
+				       size_t buf_sz)
+{
+	const char *name;
+	if (buf == NULL) {
+		name = "UNKNOWN";
+	} else {
+		name = d_path(&file->f_path, buf, buf_sz);
+		if (IS_ERR(name))
+			name = "UNKNOWN";
+	}
+
+	return name;
+}
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+static bool bhv_perform_check_fileops(struct file *file, u8 bhv_fops,
+				      bool is_dir)
+{
+	const char *pathname = NULL;
+	char *pathname_buf = NULL;
+	bool block = false;
+
+	if (bhv_fops == BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED) {
+		// file system or device not supported
+		return false;
+	}
+
+	if (file->f_op != fileops_map[bhv_fops][is_dir == true ? 1 : 0]) {
+		switch (bhv_fops) {
+		// additional check for dummy fops
+		case BHV_VAS_FILEOPS_PROTECTION_NULL:
+		case BHV_VAS_FILEOPS_PROTECTION_URANDOM:
+		case BHV_VAS_FILEOPS_PROTECTION_RANDOM:
+		case BHV_VAS_FILEOPS_PROTECTION_TTY:
+		case BHV_VAS_FILEOPS_PROTECTION_CONSOLE:
+		case BHV_VAS_FILEOPS_PROTECTION_KMSG:
+		case BHV_VAS_FILEOPS_PROTECTION_MEM:
+			if (file->f_op == &def_chr_fops) {
+				// we're all set
+				return false;
+			}
+			break;
+		case BHV_VAS_FILEOPS_PROTECTION_PROC:
+			// additional check for proc fops:
+			if (is_valid_proc_fop(&(file->f_op)))
+				return false;
+			break;
+		default:
+			break;
+		}
+
+		pathname_buf = kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ, GFP_KERNEL);
+		pathname = get_pathname(file, pathname_buf,
+					BHV_VAS_FILEOPS_PATH_MAX_SZ);
+		if (bhv_block_fileops(pathname, bhv_fops, is_dir)) {
+			// block file operation
+			block = true;
+		}
+
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+
+	return block;
+}
+#endif // VASKM
+
+#ifndef VASKM // inside kernel tree
+static bool bhv_check_fileops(struct file *file)
+{
+	// set up fops check data
+	u8 bhv_fops = BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED;
+	bool is_dir = false;
+
+	unsigned dev_major = imajor(file->f_inode);
+	unsigned dev_minor = iminor(file->f_inode);
+
+	if (d_can_lookup(file->f_path.dentry)) {
+		/* directory S_ISDIR(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+		is_dir = true;
+	} else if (d_is_reg(file->f_path.dentry)) {
+		/* regular file  S_ISREG(file->f_inode->i_mode) == true */
+		bhv_fops = bhv_fileops_type(file->f_inode->i_sb->s_magic);
+	} else if (d_is_special(file->f_path.dentry)) {
+		// DCACHE_SPECIAL_TYPE
+		if (S_ISCHR(file->f_inode->i_mode)) {
+			// character device
+			if (dev_major == 1) {
+				// mem
+				switch (dev_minor) {
+				case 1: // DEVMEM_MINOR
+					// /dev/mem
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_MEM;
+					break;
+				case 3:
+					// /dev/null
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_NULL;
+					break;
+				case 4:
+					// /dev/port
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_PORT;
+					break;
+				case 5:
+					// /dev/zero
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_ZERO;
+					break;
+				case 7:
+					// /dev/full
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_FULL;
+					break;
+				case 8:
+					// /dev/random
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_RANDOM;
+					break;
+				case 9:
+					// /dev/urandom
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_URANDOM;
+					break;
+				case 11:
+					// /dev/kmsg
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_KMSG;
+					break;
+				default:
+					break;
+				}
+			} else if (dev_major == 5) {
+				switch (dev_minor) {
+				case 0:
+					// /dev/tty
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_TTY;
+					break;
+				case 1:
+					// /dev/console
+					bhv_fops =
+						BHV_VAS_FILEOPS_PROTECTION_CONSOLE;
+					break;
+				default:
+					break;
+				}
+			}
+		}
+	}
+
+#ifdef DEBUG
+	if (bhv_fops == BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED) {
+		char *pathname_buf =
+			kzalloc(BHV_VAS_FILEOPS_PATH_MAX_SZ, GFP_KERNEL);
+		const char *pathname = get_pathname(
+			file, pathname_buf, BHV_VAS_FILEOPS_PATH_MAX_SZ);
+		pr_debug(
+			"name: %s magic: 0x%x dentry_type: 0x%x stat: 0o%o path: %s fops: 0x%px ",
+			file->f_inode->i_sb->s_type->name,
+			file->f_inode->i_sb->s_magic,
+			file->f_path.dentry->d_flags & DCACHE_ENTRY_TYPE,
+			(file->f_inode->i_mode & S_IFMT), pathname, file->f_op);
+		pathname = NULL;
+		kfree(pathname_buf);
+	}
+#endif
+
+	// perform fops check
+	if (bhv_fops != BHV_VAS_FILEOPS_PROTECTION_UNSUPPORTED)
+		return bhv_perform_check_fileops(file, bhv_fops, is_dir);
+
+	return false;
+}
+#else // out of tree
+static inline bool bhv_check_fileops(struct file *file) {
+	return false;
+}
+#endif // VASKM
+
+static_vk int bhv_check_files_dirty_pipe(const void *address_space,
+					 struct file *file, unsigned int number)
+{
+	char *filename_buf = NULL;
+	const char *filename = NULL;
+	int rv;
+
+	/*
+	 * We check whether the mapping is the same between the pipe and a file that
+	 * we have opened in the current process. In addition, we check whether the
+	 * file is readonly.
+	 */
+	if (file->f_mapping == address_space &&
+	    !(file->f_mode & FMODE_CAN_WRITE)) {
+		// Allocate memory
+		filename_buf = kzalloc(BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ,
+				       GFP_KERNEL);
+
+		if (filename_buf == NULL) {
+			pr_err("Could not allocate file buffer");
+			filename = "UNKNOWN";
+		} else {
+			// Get Path of the file we are trying to write
+			filename = d_path(&file->f_path, filename_buf,
+					  BHV_VAS_FILE_PROTECTION_MAX_PATH_SZ);
+			if (IS_ERR(filename)) {
+				pr_err("Could not retrieve file name (%ld)",
+				       PTR_ERR(filename));
+				filename = "UNKNOWN";
+			}
+		}
+
+		// Ask the HOST whether we should block this attempt.
+		rv = bhv_block_read_only_file_write(filename);
+
+		if (filename_buf != NULL) {
+			kfree(filename_buf);
+		}
+
+		return rv;
+	}
+
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+static int bhv_file_open(struct file *file)
+{
+	if (bhv_fileops_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	return 0;
+}
+#endif // VASKM
+
+static_vk int bhv_file_permission(struct file *file, int mask)
+{
+	struct pipe_inode_info *info;
+	struct pipe_buffer *buf;
+
+	if (bhv_fileops_protection_is_enabled() && bhv_check_fileops(file))
+		return -EFAULT;
+
+	if (!bhv_read_only_file_protection_is_enabled())
+		return 0;
+
+	/*
+	 * Dirty pipe detection. Whenever we write to a file and this file is
+	 * a pipe, we are going to check whether this pipe points to a read-only
+	 * file. This check happens in `bhv_check_files_dirty_pipe` above.
+	 */
+	if ((mask & MAY_WRITE) == MAY_WRITE && file->f_inode &&
+	    file->f_inode->i_sb->s_magic == PIPEFS_MAGIC) {
+		// This seems to be a pipe
+		info = file->private_data;
+		if (!info)
+			return 0;
+
+		// Check current buffer in the pipe for dirty pipe
+		buf = &info->bufs[(info->head - 1) & (info->ring_size - 1)];
+		// Iterate over all open files and see whether the pipe points to the same file.
+		if (buf && buf->page && current->files) {
+			if (iterate_fd(current->files, 0,
+				       bhv_check_files_dirty_pipe,
+				       buf->page->mapping)) {
+				return -EACCES;
+			}
+		}
+	}
+
+	return 0;
+}
+
+#ifndef VASKM // inside kernel tree
+static struct security_hook_list bhv_hooks[] __lsm_ro_after_init = {
+	LSM_HOOK_INIT(kernel_read_file, bhv_read_file), // finit_module
+	LSM_HOOK_INIT(kernel_load_data, bhv_load_data), // init_module
+	LSM_HOOK_INIT(task_alloc, bhv_task_alloc), // fork
+	LSM_HOOK_INIT(task_free, bhv_task_free), // exit
+	LSM_HOOK_INIT(bprm_check_security, bhv_bprm_check_security), // execve
+	LSM_HOOK_INIT(file_permission, bhv_file_permission), // file read/write
+	LSM_HOOK_INIT(file_open, bhv_file_open), // file open
+};
+
+static int __init bhv_init(void)
+{
+	pr_info("[bhv] LSM active");
+	security_add_hooks(bhv_hooks, ARRAY_SIZE(bhv_hooks), "bhv");
+	return 0;
+}
+
+DEFINE_LSM(bhv) = {
+	.name = "bhv",
+	.init = bhv_init,
+};
+#endif // VASKM
diff --git security/selinux/hooks.c security/selinux/hooks.c
index 9ce029b2f..1fe9c9a27 100644
--- security/selinux/hooks.c
+++ security/selinux/hooks.c
@@ -125,7 +125,7 @@ __setup("enforcing=", enforcing_setup);
 #endif
 
 int selinux_enabled_boot __initdata = 1;
-#ifdef CONFIG_SECURITY_SELINUX_BOOTPARAM
+#if defined(CONFIG_SECURITY_SELINUX_BOOTPARAM) && !defined(CONFIG_BHV_VAS)
 static int __init selinux_enabled_setup(char *str)
 {
 	unsigned long enabled;
diff --git security/selinux/selinuxfs.c security/selinux/selinuxfs.c
index f2f6203e0..3bef6c2d9 100644
--- security/selinux/selinuxfs.c
+++ security/selinux/selinuxfs.c
@@ -32,6 +32,10 @@
 #include <linux/kobject.h>
 #include <linux/ctype.h>
 
+#ifdef CONFIG_BHV_VAS
+#include <bhv/guestpolicy.h>
+#endif /* CONFIG_BHV_VAS */
+
 /* selinuxfs pseudo filesystem for exporting the security policy API.
    Based on the proc code and the fs/nfsd/nfsctl.c code. */
 
@@ -610,9 +614,9 @@ static int sel_make_policy_nodes(struct selinux_fs_info *fsi,
 	return ret;
 }
 
-static ssize_t sel_write_load(struct file *file, const char __user *buf,
-			      size_t count, loff_t *ppos)
-
+#if !defined(CONFIG_BHV_VAS) || defined(CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN)
+static inline ssize_t _sel_write_load(struct file *file, const char __user *buf,
+				      size_t count, loff_t *ppos)
 {
 	struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
 	struct selinux_load_state load_state;
@@ -667,6 +671,32 @@ static ssize_t sel_write_load(struct file *file, const char __user *buf,
 	vfree(data);
 	return length;
 }
+#endif
+
+static ssize_t sel_write_load(struct file *file, const char __user *buf,
+			      size_t count, loff_t *ppos)
+
+{
+#ifdef CONFIG_BHV_VAS
+#ifdef CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN
+	if (bhv_guest_policy_is_enabled()) {
+		if (current->pid == 1) {
+			return count;
+		}
+		return -EPERM;
+	} else {
+		return _sel_write_load(file, buf, count, ppos);
+	}
+#else /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+	if (current->pid == 1) {
+		return count;
+	}
+	return -EPERM;
+#endif /* CONFIG_BHV_ALLOW_SELINUX_GUEST_ADMIN */
+#else /* CONFIG_BHV_VAS */
+	return _sel_write_load(file, buf, count, ppos);
+#endif /* CONFIG_BHV_VAS */
+}
 
 static const struct file_operations sel_load_ops = {
 	.write		= sel_write_load,
@@ -2249,6 +2279,40 @@ static int __init init_sel_fs(void)
 
 __initcall(init_sel_fs);
 
+#ifdef CONFIG_BHV_VAS
+int sel_direct_load(void *data, size_t count)
+{
+	struct selinux_fs_info *fsi = selinuxfs_mount->mnt_sb->s_fs_info;
+	//struct selinux_fs_info *fsi = file_inode(file)->i_sb->s_fs_info;
+	struct selinux_load_state load_state;
+	int rv = 0;
+
+	mutex_lock(&fsi->state->policy_mutex);
+
+	rv = security_load_policy(fsi->state, data, count, &load_state);
+	if (rv) {
+		pr_warn_ratelimited("SELinux: failed to load policy\n");
+		goto out;
+	}
+
+	rv = sel_make_policy_nodes(fsi, load_state.policy);
+	if (rv) {
+		selinux_policy_cancel(fsi->state, &load_state);
+		goto out;
+	}
+
+	selinux_policy_commit(fsi->state, &load_state);
+
+	audit_log(audit_context(), GFP_KERNEL, AUDIT_MAC_POLICY_LOAD,
+		  "auid=%u ses=%u lsm=selinux res=1",
+		  from_kuid(&init_user_ns, audit_get_loginuid(current)),
+		  audit_get_sessionid(current));
+out:
+	mutex_unlock(&fsi->state->policy_mutex);
+	return rv;
+}
+#endif
+
 #ifdef CONFIG_SECURITY_SELINUX_DISABLE
 void exit_sel_fs(void)
 {
